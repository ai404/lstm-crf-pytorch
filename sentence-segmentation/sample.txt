Galileo Galilei made a huge leap away from the Aristotelian approach of perceiving the natural world, taking the crucial step in working towards explaining how things work, rather than speculating as to why they do so.
As mentioned previously, experiment is the most distinctive component of modern science and this was the backbone of Galileo's work.

A successful mathematician, he gained the highly esteemed post of Chair of Mathematics at the University of Padua in 1592 2, and it was his employment of his mathematical skill that allowed him to describe interactions that he observed in the world.
With the chief goal of finding the solution to a specific problem, experiment is a system of observations and controlled actions which aim to give reasonable values relevant to credible conclusions.

Galileo repeatedly tested his hypotheses in this manner and used his results to decide whether these should be revised, disregarded or revered.
It is important here to refer back to Galileo's proposed title of 'first scientist' and consider that although he may have been the first to utilise this technique expertly, he may not have been the very first to formulate it.

In fact Galileo described the earlier William Gilbert (1544-1603) as the founder of the experimental method of science who alleged that 'stronger reasons are obtained from sure experiments and demonstrated arguments than from probable conjectures and the opinions of philosophical speculators'.
2 Although Gilbert used experimental method he did not make the further advancement of Galileo, in using a quantitative approach which allowed the use of mathematical formulas to analyze the data.

In this approach, Galileo was a radical.
A well known example of Galileo's new method of obtaining knowledge being subject to critique is over his claim that 'different weights fall at the same speed'.

A professor, remaining defiant over the entrenched beliefs of his Aristotelian school, challenged the claim using the experiments carried out by engineer Simon Stein, who dropped lead weights from a tower and published these results 2.
On this issue Galileo comments: 'Aristotle says that a hundred-pound ball falling from a height of one hundred cubits hits the ground before a one-pound ball has fallen one cubit.

I say they arrive at the same time.
You find on making the test, that the larger ball beats the smaller one by two inches.

Now, behind those two inches you want to hide Aristotle's ninety-nine cubits and, speaking only of my tiny error, remain silent about his enormous mistake.' On accepting the position in Padua, Galileo had access to many influential characters.
The rapport he succeeded to build must have supported him during a time when it was not unheard of for the people who adhered to beliefs contradicting religious teaching, to be severely prosecuted, e.g. Giordano Bruno in 1600.

It proved particularly advantageous when Galileo became acquainted with the Cardinal Roberto Bellarmine who was a leading scholar of the church and was partly accountable for Bruno's execution for heresy.
Both figures were dedicated Copernicans, but only Galileo escaped the fate of death, though confined to house arrest in 1633, his case was influenced enough by Bellarmine for his punishment to be lenient.

Like many concepts rejected by the church, the public teaching of the Copernican theory was forbidden and this obviously made the extension of the work arduous (arduous maybe, but not impossible).
During Galileo's time as Professor of mathematics in the University of Pisa, he gave additional teaching to those who could afford to pay for the benefits.

This assisted his objective in spreading his own work, as during these private lectures he was able to teach scholars his own ideas, rather than the commonplace knowledge of the time.
He effectively initiated his inspirations upon an influential circle which gave height to his stature 2, and as a result 'the beginnings of the scientific movement were confined to a minority among the intellectual elite' 4.

The Copernican system was finally sanctioned by the church in 1835 partly on account of support from observations of the solar system, enabled by Galileo's development of the telescope.
Existing at a pivotal point in history, Galileo is a famous representative of changes which would most likely have been undertaken by countless other individuals in the time following, if it were not for his presence.

The arrival of the Renaissance hurried these developments, and an example of this is suggested with the application of a 'renaissance style of art that privileged realism contributed profoundly to Galileo's ability to imagine valleys and mountains on the moon when in fact all he could see in his telescope were shadows'1.
It is essentially impossible to determine the 'first scientist' in the strictest sense, since there is no hard evidence to prove who initially used the all important method we would now classify as 'experiment'.

It is reasonable to confer this title instead, on the person who first utilised this skill to the advantage of progress, and considering his tremendous achievements, as a pioneer of the new science, Galileo does seem worthy of this honour.
Galileo's development of the telescope was able to shed light on the structure of the universe and gave support to the famous Copernican theory, and there is no argument to the immense advancement that this bestowed on science.

In addition, he is credited for a wide range of important discoveries, inventions and hypotheses, including his work on pendulums and theories of motion; mechanics; invention of the first thermometer, and his numerous contributions to amassing knowledge in astronomy.
On the matter of Gilbert's influence over the experimental method of Galileo, it is debatable as to whether he should gain more recognition for the birth of Science as we know it.

However this is, in essence the expected progression in light of the topic and the claim that science is built on what was established previously.
Therefore it may be unwarranted to use this in counter to suggestion of resting the dignified award of 'first scientist' on Galileo.

The term 'seclusion', meaning to 'remove from sight, or contact with others' should be approached with care when arguing this topic of Athenian women.
David Cohen makes this point in his article suggesting a "tendency to confuse separation and seclusion." He also suggests that even though men and women have their different and separate 'spheres' in the household, this does not necessarily mean that women are 'secluded' from men or vice versa.

With this in mind is it too dogmatic to purely state that women and men were merely separated because they had to under social dictation?
Could it be argued that their difference in roles in the household formed a 'team' rather than a separation or even seclusion?

Another factor to consider when looking at this topic is which class of women are being looked at; "much depends on precisely which women we are talking about; daughters, sisters, or wives; rich or poor, free or unfree women" considering these factors it is very hard to generalise to the whole of the female race in fifth century Athens, in terms of being secluded.
However, by looking at evidence of this time we can learn what the majority of women may have experienced.

Women were expected to be inside the house and men outside.
It is argued by scholars that "men are associated with commerce and politics, the market place, café, fields...the women with the home." (Cohen 1989) Men would be in charge of harvesting crops in the fields, involved in most public affairs and particularly matters of the state.

(Polis) Women were expected to stay inside the house and organise the food brought in from the harvest.
They would also be in charge of supervising the slaves inside the house and responsible for sending out slaves to the fields.

In addition to this, women would be in charge of the accounts of the household and the general daily activities of the family.
Primary evidence shows Xenophon's oeconomicus declaring that the gods arranged the "household to the woman and the outdoors a mans" Even with this 'separation' of the sexes in the household, it could just be that men's roles were simply 'outside' for such activities as the harvest, because they were physically stronger and arguably more capable of agriculture work.

This in turn would leave the women to remain 'inside', as someone needs to organise the household.
This then could be seen as separation rather than an intention to seclude.

Further to this, women were not expected to leave the house unless for a valid reason.
Bourdieu states "husbands find them [their wives] out of the house and want to know what they have been doing.

The husbands know that they go out, but they should not be found to have been out." This coincides with the Ancient Athenian feeling that women should not attract attention to themselves, especially from men.
One argument to this could be that men might not have liked their wives being seen and gossiped about.

Women were not supposed to be talked about in society, whether good or bad.
In Xenophon's Oeconomicus, Ischomachus challenges his wife for wearing makeup claiming it's deceptive.

When considering this evidence it must be remembered that this is a fictional piece of writing.
Additionally, Xenophon believed girls should be trained on how to be wives before 15years old.

Although it shows a biased opinion, it does not mean that it only belonged to one man and so this evidence can be useful to historians today.
Ischomachus, as well as finding his wife's makeup deceiving, could also find her act suspicion for an affair.

This is seen with Euphiletus in Lysias, O n The Murder of Eratosthenes.
"...how I had had the idea that my wife's face was rouged.

All these things rushed into my mind, and I was filled with suspicion" It could be argued that although makeup was probably allowed, it did, like the above passage, create suspicion and fear of adultery when married.
This fear of adultery, which can be seen in many different extracts, was because it posed so many problems.

If a married woman were found guilty of adultery, the legitimacy of her children would be questioned; whether they belonged to the husband and also whether they were true citizens of the 'polis'.
She would in turn be divorced and humiliated by society.

For these reasons we can expect that fifth century Athenian men would want to make sure their wives were faithful, and so they got suspicious at makeup and if she would talk to another man.
Women, especially those married, would not be well spoken of if found conversing with another man other than their husband.

J. Williams details "insulting a woman by saying she seizes passers-by out of the street, or that she answers the door herself, or that she talks with men, is equivalent to 'this house is a brothel'" So with this in mind it would be understandable in fifth century Athens to be suspicious of a 'well off' woman talking to a man on the street.
As well as in public, women would not be allowed to talk to a visiting male in her home.

"It was a common thing for a man to invite a friend to dinner, at which his wife would not be present".
However, Euripides' Medea, shows Medea talking in public to the Athenian king, Aegeus and neither of them seem phased.

It could be argued that the secrecy of married upper class women and withdrawing them from the male population was considered appropriate.
However due to errands and coincidences this did not always happen.

Due to this, it is questionable that some women may have had to talk to men in public at some times as they could not escape it, but it would have been frowned upon.
This point can also be seen in Medea, as Aegeus does not enter the house, nor does Medea offer, as this would be seen socially unacceptable.

From the majority of this evidence it is very clear that married upper class women were physically 'separated' from other men.
And in addition to this separation it can be argued that married 'well off' women were secluded from the male population, to maintain their honour, children's legitimacy, and for what can be seen as the main reason, preventing adultery.

When looking at the other social classes of women in Athens, there is a different story.
The richer women, would send out slaves to carry out the work outside of the house so she would not have to.

This was not the case for the poorer women, as they obviously could not afford to have slaves.
"Athenian women participated in a wide range of activities which regularly took them out of their houses.

This included working in the fields." It can be see that not all women were confined to their house and 'separated' from society or men.
It could be argued that it was only richer women that could afford to send slaves to go out that were secluded, avoiding the possibility of adultery.

One role in society where women were very important was religion.
They were priestesses in over 40 different cults and had a large role allotted in ritual.

The most well known of these public festivals was the festival of the harvest; the Thesmophoria, in which only women would organise and celebrate.
The festival celebrated the new harvests and fertility, which was considered a 'womanly' aspect in society.

Other festivals included the Kallynteria and the Plynteria, in which women would either wash or make a new cloak for the statue of Athena.
This part of Athenian society arguably does not seclude women; they were considered 'purer' than men and were seen as the middle person between the gods and man.

Religious festivals often coincided with theatrical performances.
There is dispute among scholars whether women were permitted into the theatre; as some views in fifth century Athens were that women would be too frightened or that it was too much of a public place.

However, The world of Athens says that male or female foreigners were allowed to attend the courts, theatres and festivals once they had paid a tax.
Even with this evidence, it is still not accurate to say that all Athenian women went to the theatre as no known evidence shows freeborn citizen women going to the theatre.

By looking at the previous evidence of citizen women and their prohibited relations with men, it can be suggested that it may have been unacceptable for most citizen women to attend the theatre.
They would be in a very public place and around many men.

Without knowing exact evidence it is impossible to make a rigid argument on fifth century Athenian women attending the theatre.
However, it is likely to assume that they would not be permitted to the theatre.

The mass of male influence and the constant anxiety of female dishonour and corruption were on many husbands' minds causing them to 'seclude' to maintain their reputations.
As stated earlier, political aspects of Athens were in the male 'sphere' of society.

This is seen as women could not defend themselves in court, and had to have a male do it for her.
This can be seen in a speech extract where a woman's son is defending her right as a citizen in a court because she is not permitted to be present As well as a citizen women, poorer non citizen women were not allowed to be there either.

The courtroom speech accuses a courtesan, Neaera, of not being a true freeborn citizen because of her sexual activities.
Further to this point, in the speech accusing Neaera, her name is mentioned on numerous occasions.

This is not the case in the other trial mentioned above, (Isaeus 8) as the citizen woman being defended is not named, arguably to maintain her honour and respectability as a freeborn citizen.
Women were also not allowed to vote and were not allowed to sit on juries.

Again, this physical separation calls in to question whether it leads automatically to seclusion.
In this case, it is possible that this physical division was created purely as an attempt to keep women at home, and nothing to do with secluding them from politics.

There is enough evidence however, to suggest that there could be a factor of seclusion from the male populous rather than politics.
The male orientated area of government could be a dangerous place to have a 'vulnerable' woman to be 'corrupted', as the importance of citizenship was so relevant to the ancient Athenians.

To conclude, when arguing this question, there are many factors to consider.
To label all women as either being secluded in fifth century Athens is too much of a dogmatic statement.

Firstly there were different social classes, each describing different accounts of women's lives in that day.
Poorer women were working in the fields and out of the home and the richer could afford to stay home.

Secondly, men have written all evidence that historians have and so understanding an Athenian woman's life from a woman is near impossible.
It can be argued that freeborn Athenian women were secluded from male interaction.

Furthermore, the poorer women would work outside their home and would be most likely in the presence of others, including men, and so arguably were not secluded as much as the upper class.
This is obvious as most of the male orientated spheres of society such as, politics, juries, and the theatre were deprived of female presence.

One argument is that it was due to the protection over the women's respectability.
Another is that the women were being blocked out of society; but I believe this to be untrue as they were heavily involved in religious festivals that would have been public and seen by the majority of the city.

Instead, I believe that women were secluded to an extent of being kept away from the male populous; as there was major anxiety of 'corruption' of a mans wife.
Additionally, I do not believe that women were purposely secluded from society, because of their involvement in religious rituals.

Myths come from the Greek word mythos, which means "word", "speech", "tale", or "story".
A myth is a story that would have usually been told orally and then written down in the later years.

Myths can also be seen on pottery, paintings, in plays and in music.
However, there are some scholars who are not content with this definition of a myth, and want to recognize myths that are 'true' and those that could be confused with a 'folktale' or a 'legend'.

Myths were used in ancient society as stories, but also as moral situations and lessons as example for society to follow.
They were well known to the public and would have been recognised if for example they were acted out in a play.

There were many 'lessons' and themes that occurred in myths such as, glorification, rape, male and female relationships, marriage and also virginity.
To understand the reason why virginity was such an important them in myths, different areas of mythology will have to be considered.

It will be important to look at the definition of virginity, any myths in which 'virgins' preside, virginity as a part of people's lives and also its position in the area of religion.
Defining virginity in its ancient terms can often be confused with the modern definition.

There are different views on what defines a virgin; some believe that it is when a females hymen is still intact, when someone has not had sexual intercourse, and some even say it is when you are unmarried.
However, a modern definition details that a virgin is "someone who has never had sexual intercourse." Furthermore, modern definitions tend to disregard the rule of an intact hymen as some sports can cause this to break.

This oversight cannot however be applied to the ancient times, as they would not partake in such sports or activities.
With this in mind, how would it be possible in ancient Greece, to test if a girl is a virgin without breaking her hymen?

There were different theories to the characteristics of a virgin in ancient times.
For example, one theory suggested that a maiden would have a high-pitched voice, and if she had sexual intercourse her voice would deepen in comparison to how it was before.

Some other theories proposed that a virgin would dress in a certain way; where she would be modest and would tend to 'hide' or cover herself up.
This shows the unproblematic way a girl could 'fake' her virtue and virginity by simply dressing herself in a certain way.

However, although the most obvious way of testing a virgin in these times would be the breaking of the hymen at intercourse, some historians believe that the Greeks were unaware of the hymen.
"Greco-Roman science never isolated the hymen that the modern anatomy identifies as occupying the vaginal introitus" Therefore, it can be seen that through this lack of knowledge and not being able to be certain of a girls virginity, moralistic 'stories' may have been introduced to give lessons to girls on how they should behave.

Thus, the myths would have persuaded girls to behave 'properly' for fear of how they may be punished by the gods or society.
The three main Olympian Goddesses that remain virgins are, Athena, Artemis, and Hestia.

Athena's birth is detailed in a hymn, "Zeus himself gave birth to her from his holy head...she quickly sprang forth." Hesiod details the reason for Athena being born from Zeus; he states that he "swallowed her [Metis] down into her belly." By swallowing Metis, Athena's mother, Zeus takes all childbearing responsibilities from her.
Could it be argued that this lack of intimacy with her mother caused Athena to have no interest in having a husband or lover?

Even when Hephaestus pursues Athena, she does not want him and rejects his advances.
Hestia is the goddess of the hearth and stayed a virgin throughout her life.

She rejected the advances of Apollo and Poseidon and she vowed to live as a virgin.
She often gained precedence at banquets for being the eldest child of Cronus and Rhea.

One hymn details, "For without you, there are no banquets for mortals where one does not offer honey-sweet wine as a libation to Hestia." Hestia's choice of remaining a virgin was reflected on her priestesses who had to follow her 'role'.
As Hestia's duty was the patron of the hearth and home, it could be argued that she did not want to 'pollute' or 'corrupt' the idea of a respectable Greek home (oikos) by voiding her virginity.

Lastly, Artemis was the goddess of hunting and archery.
"In Classical Greek literature she was characterised by a deliberately chosen and forcibly maintained virginity." She was known to punish those who violated this state and her followers were to be virgins too, just like Hestia's.

This punishment can be seen when Actaeon sees her bathing, and as she was worried he may boast at seeing her naked, she turns him into a stag.
Artemis is mentioned many times in Euripides's Hippolytus, where Hippolytus himself sees her as the 'purer' of the Gods.

He chooses to follow her by remaining a virgin himself, which later causes him his destruction.
One argument for this punishment of Hippolytus is because he remained a virgin and did not complete his 'transition process'.

It has been seen that mortals had to go through 'stages' in their life, such as becoming the age of marriage, marriage itself, and also having children.
At these stages, gifts were often given at temples to Artemis, who would overlook the transition to the next stage.

As mentioned above one of these transition stages would be the union of marriage with a partner and the consummation.
It can be seen in the play Hippolytus, that Hippolytus does not complete his transition, as he does not wish to marry, remains a virgin and thus is punished.

Another example of individuals failing to complete their transition is the myth of the fifty daughters of Danaus.
They were given in marriage to 50 other men and each given a dagger to kill them on their wedding night.

Forty-nine of the daughters did kill their husbands, and as a result of not completing their 'marriage' and 'consummation' stages they were "punished in the underworld by eternally having to fill water jars, through which the water leaked away." Both these myths show the consequences for not following the transition process in which they are supposed to follow.
Like mentioned above these myths would have acted as 'lessons' for everyday society.

Additionally, the myth of Hippolytus demonstrates the reasons why Gods and Goddesses should not be taken as 'role models'.
Although all three goddesses represent and preside over some from of chastity, it must be seen that to follow their lives would end up in punishment.

Hippolytus stays a virgin and follows the lifestyle of Artemis by hunting with her and so is punished.
One view could see this punishment caused because Hippolytus was a man.

There is no other evidence that shows women being punished for remaining virgins.
Could it be argued that men were not meant to be virgins all their life?

Another reason for this punishment could be that it was improper for any mortal to remain a virgin all their lives, but as goddesses, they were permitted and they would physically show their function to its full scale.
One aspect of society however does not fall into the above category of punishment when they are virgins.

As stated before, Hestia's priestesses had to be virgins; this is also true of Apollo's and the Vestal Virgins in Rome.
Apollo's priestesses were called a Pythia.

They were virgin girls who when people came to seek advice or prophetic answers, would become processed by the God.
It was believed that virgins had to be used for this process because they were seen as 'clean' since they were untouched by a man.

It can also be observed that these virgin patrons of religion were mostly women.
Apollo's Pythia would have been a girl, and additionally in Virgil's Aeneid, it is a female Sybil that takes him and instructs him to the underworld, under the influence of Apollo.

"'The God is here'...her face was transfigured, her colour changed...she spoke as no mortal had ever spoken when the God came to her." Through this it is visible that it was in these circumstances that virgins were permitted to remain virgins.
However it is not likely that a girl would become a priestess to purely remain a virgin, but is more likely that she would become a priestess and have to stay a virgin as part of her duty.

In conclusion it can be argued that myths served as stories and lessons for everyday society, and included many themes such as virginity.
These myths would have been 'lessons' for young girls growing up to try and ensure they behaved properly in society.

This would have been further influenced by the lack of 'tests' or knowledge the ancients had on ensuring a girls virginity.
These myths arguably looked at different aspects of virginity, such as maintaining it till marriage, but also in Hippolytus's case, of when to let it go.

As mentioned above the main influence of these myths may have been directed at girls, but some other myths could have arguably been projected at males.
Evidently with Hippolytus, it is clear that he was punished for not completing his 'transition' or 'stage' in his life.

Additionally, this myth also would have shown the downside of using a god as a role model even if it meant following them in their 'chaste life'.
Lastly, some myths also show the importance of those living as virgins who were permitted to serve the Gods.

It is clear to see their importance, as they are the ones who become possessed and trusted by the god.
With all these themes giving contemporary Greeks moralistic ways of living, it can be understandable that there were many different variations of myths.

Virginity was such an important theme in myths as it was an important issue in society.
Furthermore, it would have been these myths that would have given examples of how individuals should live their lives according to the gods and society.

There are many myths that circulated throughout Greece.
These would have served as entertainment or even as metaphorical 'moral lessons' to give knowledge in everyday life.

Myths always entailed many different themes, such as war, glorification, relationships with men and women; and almost always involved the role of a God.
Many of the myths known today, have some sense of a moral or achievement that was to be gained.

They usually involved a conquest of some sort, such as a physical fight against another man, or more occasionally, an animal-creature.
To comprehend the role of these 'creatures', it is clear that an analysis of many myths would need to be considered.

Such creatures to be looked at could be the transformation of Scylla, sirens, harpies, centaurs, and also Medusa.
One main aspect of the roles of these half human half animal creatures in myth is that they are always portrayed as evil or ghastly.

"The centaur is the only one of the fancied monsters of antiquity to which any good traits are assigned." This in turn leads to the argument that the role of these creatures provided literature heroes with a conquest and 'capture' to add to their tale.
This can be seen in the majority of all heroic stories, such as Odysseus, Aeneas, Perseus and Jason and the Argonauts.

Odysseus has to defeat many different creatures while struggling to get home to Ithaca.
While on his journey he had to sail past Scylla and Charybdis and as a result of some help from Circe, Odysseus and most of his men survive.

In Book 12, Circe gives a description of the creature; "she has 12 feet, 6 long scrawny necks, each ending in a grisly head with triple rows of fangs, set thick and close...up to her waist she is sunk in the depths of the cave." Additionally on his voyage home, Odysseus had to encounter the Sirens, who were known to lure ships to their rocky shores.
They were "bird like" creatures with a female heads that sang so beautifully that sailors were urged to jump overboard to get to them.

"They would lure passing sailors onto rocks; all around them were the whitened bones of their victims." Odysseus was one of the few that survived the passing of these bird-creatures.
The cause for this was the help of Circe; she advises Odysseus that "the sirens, who bewitch everybody who approaches them.

There is no homecoming for the man who draws near them unawares and hears the siren's voices." She advised him to have his men put wax in their ears to prevent them hearing the songs and being 'lured' to their shores.
These encounters arguably make the tale of the hero more interesting and exciting.

One other creature that was fought and slain was Medusa, the Gorgon.
"The Gorgons, whose home was somewhere on the edge of the world, usually situated in North Africa", were three daughters of Pharcys and Creto.

These three sisters were said to look so horrific that when looked upon the onlooker would turn to stone.
Medusa was the only mortal Gorgon and so makes her important as she was the only Gorgon to be killed.

She was beheaded by Perseus who skilfully only looked at the Gorgons reflection in his shield while she was asleep.
These creatures provided a part of the story to emphasise the braveness of the hero, either by being slain or overcome where no one had before.

Each of the above 'conquerors' or leaders, had to face some sort of creature, or challenge for their claim to the title of a 'true hero' or leader.
Furthermore, this role of being a conquest in a 'story' or myth could be argued as a literary technique or even symbolic.

It could be argued further that the role of these creatures in ancient myth was to illustrate the symbolic 'evils' or troubles facing their world.
It can be seen from Bulfinch's (1981) quote mentioned above that all animal creatures had no positive characteristics, with minor exceptions.

This is also evident in the above descriptions of the creatures the heroes had to encounter.
Additionally to these myths there is another creature that is shown to be evil.

Harpies, meaning "snatchers" were bird like creatures similar to the sirens.
They were held responsible for the disappearance of "whatever could not be found." They were depicted in art as birdlike creatures with faces of women, often "terrifying and a pestilence." The main known myth of the Harpies is when they torment King Phineus in Salmydessus.

Whenever he had a meal placed in front of him they would steal his food and foul the rest.
Another example of the wicked behaviour of these creatures is shown in the wedding feast of Pirithous and Hippodamia in Thessaly.

Among the human guests were the centaur creatures.
They were represented as men from head to waist and the remainder was horse.

At this feast, Eurytion, one of the centaurs, got drunk on the wine he was not accustomed to and tried to carry off the bride.
Many of the other centaurs followed his example and a large fight broke out between them and the humans Lapiths.

However, one of the centaurs is not tarnished with the same reputation, he is called Chiron.
Chiron was known to be gentle and was skilled in hunting, medicine, music and the art of prophecy.10 It was said that he had been taught by Apollo and Diana and tutored many Greek heroes.

It could be argued that the myths that portray animal creatures as bad could be suggested as an analogy for the 'evils' of the world.
It could also be said that these animals represent the non-Greek population, suggesting that Greeks should be wary of those who may be cruel and potentially dangerous.

However, the one creature that does not fit the criteria of a dangerous animal; Chiron, could be an example that although there should be uneasiness of non-Greeks, we should also accept those who are not seen as brutal or cruel.
Lastly, one other role that these animal creatures could serve in myth could be the message of punishment.

Many of these creatures were once humans, who as a result of some 'wrong' action had been turned into monsters.
This can be seen with Scylla, who was a maiden before she was turned into the creature mentioned above.

There are two different versions to how Scylla was changed, one states that Poseidon made advances on her and his wife, Amphitrite was jealous and threw magic herbs into her bathing place, causing her to change.
Another better-known version is that Glaucus fell in love with Scylla and she rejected him, and so he turned to Circe for help.

Circe however fell in love with him, and so in her jealousy poisoned the waters of Scylla's bathing place.
Although this may not seem Scylla's fault, she is still punished by Circe as she has the love of Glaucus that Circe desires.

Another example can be seen in some versions of the creation of the sirens.
Some scholars suggest that the sirens were once the companions of Persephone, when she would pick flowers before she was Hades's wife.

As a result of them letting her be abducted by him, they were punished by being turning into bird like creatures.
In conclusion, it can be argued that there were many different reasons for the roles of half animal and half human creatures in ancient myths.

Firstly, the addition of these animal creatures added excitement and interest into the mythical stories being told.
Secondly, these mythological creatures provided 'action' for the literary heroes.

These 'monsters' acted as conquests to emphasise the growing reputation and the 'heroicness' of the hero.
Creatures could also be identified in myths as a resemblance of the 'evils' or danger against the Greeks.

The killing of these animals could also show the power of the Greek and mortal people against those who are 'non-Greek'.
Lastly, the role of half human creatures could be seen as a warning for those that do something wrong, or those who do not please the Gods.

These myths of mortals being punished into terrifying creatures, could be a warning to those who do not behave the way the Gods wish, and would also show the consequences of those actions.
Physical Structure and Composition of ice cream

Ice cream is a complicated physiochemical system-colloidal system.
Air cells are dispersed in a continuous liquid phase with embedded ice crystals.

The liquid phase also contains solidified fat globules, milk proteins, insoluble salts, lactose crystals in some cases, stabilizers of colloidal dimension, sugars and soluble salts in solution.
The finished product consists of liquid, air and solid, giving a three phase system.

A typical ice cream contains about 30% ice, 50% air, 5% fat and 15% matrix (sugar solution) by volume.
The ingredients of ice cream may be classified in three groups:

Major ingredients, such as milk protein, sugar, fat and water.Minor ingredients (<1%), such as emulsifiers, stabilizers, colours and flavours.
Modifying components, such as chocolate, biscuits, wafers, fruit pieces and nuts that are combined with ice cream to make products (Clark, 2004).

Milk Proteins
Cow's milk contains about 87% water.

The remainder consists of fat (4%), proteins (3.5%), lactose (4.8%) and small quantities of inorganic salts.
Basically, milk contains two main types of protein: casein (80%) and whey proteins (20%).

Casein and whey proteins are distinguished by their differing solubility at pH 4.6 (at 20°C), caseins are insoluble, whereas whey proteins are soluble (Clark, 2004).
Most of the casein proteins are present as colloidal particles, known as casein micelles (Swaisgood, 1992).

Caseins are phosphoproteins.
The phosphate groups are responsible for many of the important characteristics of casein, especially its ability to bind relatively large amounts of calcium, making it a very nutritionally valuable protein.

All caseins have a high content (35-45%) of apolar amino acids (Val, Leu, Ile, Phe, Tyr, Pro) and would be expected to be poorly soluble in aqueous system.
However, a high content of phosphate groups, low level of sulphur containing amino acids, and high carbohydrate content in the case of κ-casein, offset the influence of apolar amino acids.

Caseins are quite stable to heat denaturation, but can be denatured by excessive heat, leading to aggregation and precipitation.
As caseins are very surface active, and make good emulsifiers.

Also, casein's high water binding capacity creates high viscosity in casein solutions.
Consequently, casein has desirable functional properties for incorporating into instant desserts (Fox & McSweeney, 1998).

Whey Protein Concentrate
Whey protein contains two well-defined groups, which could be fractionated by saturated MgSO 4 or half-saturated NH 4SO 4; the precipitate (roughly 20% of the total nitrogen) was called a lactoglobulin and the soluble protein was lactalbumin (Fox & McSweeney, 1998).

The major advantage of whey protein's application in food industry is its nutritional quality.
The main whey product developed-to-date is probably whey protein concentrate (WPC).

Commercially available WPCs contain from 35% to 95% protein, 6-10% lactose, 4-6% fat, 3-5% ash and 3-4% moisture.
The functions of WPCs in food products can be related to the functionality of their proteins.

Interactions between proteins and other molecules within a food system are necessary for a protein to manifest its functionality (Zadow, 1992).
The five tables below represent a more detailed review of the composition and functionalities of whey protein in ice cream and other food systems, and also possible modifications on whey proteins during food processing.

Stabilizers in ice cream
Stabilizers are used in such small amounts as to have a negligible influence on food value, but will modify the physical and sensory properties of ice cream.

For instance by:
Prevent the formation of objectionable large ice crystals.

High water-binding capacity effective in smoothening the texture, giving body to the finished product and giving desired resistance to melting; thus impact indirectly on flavour.
Increase viscosity and have no effect on the freezing point, hardly limit whipping ability.Prevent coarsening of texture under temperature fluctuations (Arbuckle, 1972).

Stability of Ice Cream
Ageing

Ageing takes place before ice cream mixture goes on freezing process.
Commercial ageing time is 3 to 4 hours, within which longer ageing time is beneficial.

The beneficial changes occurring in ageing are listed below:
Solidifying fat.Protein of the mix may be changed slightly and the viscosity increases.Improve the smoothness of the body and texture, resistance to melting and ease of whipping (Arbuckle, 1972).

Basic factors affecting ice cream meltdown
The meltdown properties of ice cream are related with its eating properties.

Basically, meltdown properties are determined by two parameters, one is the melting rate; the other is the shape retention of ice cream during melting.
There are several factors impacting on the meltdown properties of ice cream, as presented below: An increased fat agglomeration index (FAI) provides structural rigidity to ice cream so that the melting rate becomes lower and better shape retention results.

Certain emulsifiers (e.g. unsaturated monoglycerides) destabilize the fat globules and increase the fat agglomeration, giving a more rigid structure.
Additionally, agitation and the concentration effect of freezing increases FAI.

The lower the freezing-point of the mix, the more ice crystals will melt at a given temperature.
Proteins may affect the structure and melting properties.

Structure formed by fat and proteins in ice cream has a combined influence on the shape-retention during melting.
Samples submitted to heat shock conditions melts more slowly than when fresh, possibly due to the larger ice crystals formed (Buchheim, 1998).

Sensory Evaluation of Ice Cream
Introduction to methods:

Ice cream offers a unique combination of highly desirable sensory properties.
Those properties can be classified under the categories appearance (colour, smoothness...), aroma (smell perceived by sniffing), flavour (taste, smell perceived retronasally, trigeminal sensations) and body/texture/mouthfeel (hardness, viscosity, creaminess, grittiness and gumminess).

To assess the sensory properties of ice cream, analytical sensory tests, such as difference tests and descriptive analysis, should be used with trained panels to provide qualitative and quantitative information.
Instrumental measurements of sensory properties are used as a necessary complement to sensory analytical tests, particularly for colour and texture/mouthfeel assessments.

Affective tests, such as hedonic scaling or preference tests, can be used with consumer representative of the target population to assess product acceptance (Buchheim, 1998).
The table below displays various common sensory analytical methods and their corresponding statistical data-analysis approaches:

Preparation and evaluation sequence of ice cream:
In order to ascertain the desired body and texture characteristics of ice cream for assessment, samples should be stored at the temperature range between -18°C to -15°C (Bodyfelt, et al., 1988).

An efficient orderly sequence of evaluations is presented below to conquer the rapid change of the physical condition of ice cream while in ordinary temperature.
Colour observation - (intensity, uniformity, natural or unnatural)Aroma detection - (by sniffing)Flavour evaluation - (basic tastes, aroma by volatile/flavour-contributing substances, trigeminal sensations)Body-Texture evaluation (by rolling the sample between incisors and pressing it against the roof of mouth to detect the smoothness)Sense delayed taste-reaction (Bodyfelt, et al., 1988).

Intrinsic impact on ice cream sensory properties
Conclusion of this review

Whey protein concentrate plays a significant role in ice cream, as it introduces various impacts on both physiochemical and sensory properties of ice cream by its intrinsic characteristics and also through interactions with other ingredients in the colloidal system.
Q1: What are free radicals and what's their relationship with antioxidants?

Free radicals can be any species capable of independent existence (hence the term 'free') that contains one or more unpaired electrons.
A normal atom exists with paired electrons orbiting around its central nucleus, while each pair moves in its own region of space.

Molecules can be free radicals if one or more of the atoms present have unpaired electrons, therefore, many different free radicals exist.
Free radicals are paramagnetic and highly reactive, because they intend to "steal" one or more electrons from other species to equate their electron pairs.

Consequently, the species losing electrons are virtually oxidized.
Antioxidants can prevent oxidation by firstly minimizing free radical's formation; secondly scavenging free radicals mainly through donating their own electron(s) and proton(s) to free radicals to equate the unpaired electrons, while forming a stable molecule or compound.

Thus, free radicals become less reactive or non-reactive at all and no longer attack other species.
Q2: I've heard that free radicals cause damage in the body-what sort of damage do they cause?

Free radicals can attack DNA molecules in the body and cause the changes in their building blocks, such as purine (adenine, guanine) and pyrimidine (cytosine, thymine) bases.
Furthermore, the attack on the pentose sugar of DNA can cause DNA backbone breakage in one strand or both strands of the DNA double helix, which can lead to severe cell damages.

Free radicals can also modify protein molecules in the body from attacking them, leading to the protein backbone's breakage, oxidation of thiol (SH-) groups and other amino acid residues, cross-linking of different protein molecules by joining-together of amino acid radicals on those molecules.
These can bring about protein mutations, complete block of replication and functional changes, e.g., reaction rate of enzymes changes.

The third type of damage is oxidative degradation of lipids (lipid peroxidation) in the body.
During this event, free radicals "steal" electrons from the lipids in cell membranes, lipids are oxidized and become unstable radicals.

These radicals react readily with molecular oxygen, creating peroxyl radicals.
The peroxyl radicals will then oxidize other free lipid molecules, producing different lipid radicals.

Polyunsaturated fatty acids are particularly susceptible to peroxidation, leading to further damages to membrane-bound proteins.
This free radical chain reaction will result in cell damages and lead to various diseases, such as Atherosclerosis.

Last but not least, free radicals generated at sites of inflammation during infection can attack the host cell, leading to apoptosis and necrosis.
Q3: What benefits can I get from taking antioxidant supplements and are there any foods that contain antioxidants with the same effects?

As the endogenous (inside body) antioxidant capacity is dependent upon the concentrations of individual antioxidants and the activities of protective enzymes, the endogenous defences are not always effective and efficient, especially under oxidative stress (i.e., free radicals' generation rate is greater than their elimination rate by antioxidants).
For instance, if you were frequently exposed to air pollutants in an urban area or you smoked heavily, your body would be under more oxidative stress than your endogenous antioxidants could cope with.

Therefore, antioxidant supplements become important in diminishing the cumulative effects of oxidative damages in the body.
Also, they help reduce the susceptibility of tissues to oxidative stress and prevent the deleterious effects of free radical-mediated processes in cell membranes.

Thus, many diseases and age-related conditions can be prevented, such as cardiovascular disease and high blood pressure (by vitamin C supplementation), cancer (by carotenoids supplementation), atherosclerosis (by Vitamin E supplementation), cataract and age-related macular degeneration.
There are plenty of dietary sources offering antioxidants to us.

Vitamin-E is found in vegetable oils (e.g., salad oils and margarines), nuts and whole grains.
Vitamin C is available from green peppers, broccoli, cauliflowers, tomatoes, potatoes, oranges, and other citrus fruits.

Carrots, broccoli, tomatoes, red peppers, pumpkins, apricot and grapefruit are rich in carotenoids.
Foods containing flavonoids also have antioxidant potential, e.g., dark chocolate, tea, red wine and citrus fruits.

Soyfoods are also rich in natural antioxidants.
Recently, functional foods fortified with antioxidants have become available, too.

Q4: What are phytochemicals and what do they do?
Phytochemicals are the chemical compounds formed in the plant's normal metabolic processes and referred to as "secondary metabolites".

Phytochemicals are important in contributing to the flavour and colour of many fruits and vegetables, and products derived from them such as wine, tea and chocolate.
They may also act as building blocks for other compounds and plant defence mechanisms.

Although phytochemicals in plants we consume as food are not nutrients, they exhibit diversified physiologic and pharmacologic effects.
Their best known function, especially in flavonoids and phenolic compounds, is acting as antioxidants in human body by numbers of potential pathways.

The most important mechanisms are free radical scavenging in which polyphenols break the free radical chain reaction and chelating of metal ions (Iron and Copper) by flavonoids to stop them involving in free radical generation process.
Additionally, they can possibly interact with other physiological antioxidants like vitamin C and E to create another antioxidant pathway.

Thereby, phytochemicals can inactivate cancer-causing substances, stimulate the immune system, protect the heart from disease, and help prevent cataracts.
Phytochemicals function best when obtained from the whole plant, i.e., fruits, vegetables, seeds, legumes, and grains, rather than being extracted and taken individually in supplement form.

Q5: Do antioxidants prevent cancer?
Oxidative damage to DNA, lipids and proteins by reactive oxygen species (ROS) and free radicals in the body is an important factor leading to cancer.

Besides, ROS appear to be involved at all stages of cancer development.
Antioxidants can actually minimize free radicals and ROS' formation from precursors of various sources (e.g., tobacco smoke, industrial pollution and food contaminations), stop them reaching the targeted tissues by scavenging them, repairing the damage and suppressing the expression of tumour formation in attacked cells as well as destroying damaged target molecules and replacing them with new ones.

Therefore, they have a great potential to prevent cancer.
As the endogenous antioxidants are not powerful enough to prevent cancer, it becomes worthwhile taking dietary or supplemented antioxidants to maximize the cancer-prevention capacity of the body.

High intakes of green vegetables and fruits provide strong evidence in a cancer-protective effect.
INTRODUCTION

As consumers, all of us engage in highly informal descriptive procedures everyday.
We may denominate a food as being delicious, nondescript or unacceptable, which alone is a matter of simple description, but to arrive at that ultimate decision we go through a complex process, subconsciously for the most part, assessing several individual characteristics to make our simply expressed decision.

In order to keep or improve the product's appeals to consumers in respects of both quality and sensory properties, many food industrial firms have been trying to mimic the descriptive procedures of consumers on the products during their decision-making process, particularly by setting up various descriptive sensory analysis on their products.
In fact, the real value of descriptive procedures resides in those of a systematic nature.

From them, we derive many practical benefits as well as fundamental knowledge of the relations between composition and sensory quality.
The descriptive procedures to follow will be considered from four points of view: (1) procedures available; (2) practices allowed; (3) pitfalls to be avoided and (4) the potential for the development of uses or practices even more beneficial than those which exist at present.

Nowadays, the most common approach by the food industry is quantitative sensory profiling (QSP) of products.
The main principle of sensory profiling is to determine the nature and intensity of all the sensory characteristics of a food (i.e. appearance, taste, aroma, mouthfeel and aftertaste) or a particular product, through a series of events, including intensive descriptive analysis on each single sensory characteristic of a product, determination of the key attributes of each sensory characteristic through a group discussion, quantitative evaluation of each key attribute of the characteristics as well as the final statistical analysis of the results from the panel.

There are many variants of QSP procedures.
One of the major variants is quantitative descriptive analysis (QDA).

In essence, QDA boils down to screening would-be assessors for possible membership on a panel, developing a list of descriptive terms, training judges, using sufficient replication so that the performance of the assessors, the effectiveness of descriptive terms, product differences and possible interaction effects may be isolated and evaluated by statistical analysis, and expressing the results graphically as well as numerically.
OBEJECTIVE

In this practical session, all the students are treated as would-be panelists and will be exposed to the method of QDA and 4 similar samples of a food product.
The students are then expected to carry out the sensory profiling of the 4 samples by using the method of QDA as a group.

In the end, they will be asked to analyze the results by implementing the relevant statistical analysis tool (i.e. ANOVA) and determine the ways in which the 4 samples differ by using graphical methods of data presentation, such as Spider diagrams or other statistical methods, such as principle component analysis (PCA).
This practical session, in fact, helps to familiarize the students with the principles of sensory profiling and the approaching technique (QDA), also it enables the students to be capable of carrying out independent statistical analysis of the collected data and reaching to reasonable judgments by utilizing graphic and statistical tools.

METHODS AND MATERIALS
Materials

Stock solutions (w/w in 1:1 ethnol/1,2-propane-diol)
4 similar digestive biscuits

4 chocolate samples : ASDA, Cadbury's Dairy Milk, Galaxy Milk and Waitrose
Belgian Milk

Methods
Assessor Training

Profiling typically requires 10-20 assessors who have been selected for aptitude and then highly trained.
In this practical the class will be the assessors but in the time available it will not be possible to apply the normal selection and training procedures.

Methods of selection and training were discussed in the previous practical class.
The selection and training procedures the class went through during the first practical session included sensitivity and aptitude tests, taste thresholds test as well as adaptation test.

In this class, a particular training will be undertaken relating to the method of GDA, which is the vocabulary development training on odour description.
Basically, 10 smelling bottles of different odorant solutions will be presented to the class, which are coded as shown above.

Then, all the class will go around and detect the nature of the odour in each bottle, while recording the descriptions.
Following that, an informal discussion within the class will be held by the practical leader to gather all the descriptions from the class on the odorant nature of each bottle.

Throughout the discussion, vocabularies on the odour descriptions are therefore developed by the class.
In the end of this training exercise, the class will get a clear view on the vocabulary development procedure of the QDA method.

The other training will be provided to familiarize the class with line-scaling technique.
This is achieved by conventional profiling of the four similar digestive biscuits of different brands.

Basically, four samples will be presented to each student of the class.
Then each student will evaluate the samples in individual booth and carry out the ranking and line-scaling tests on three sensory modalities of each sample under 3 corresponding attributes respectively, while operating on the provided computer.

The sensory characteristics include: Colour (browness); Taste (sweet); Texture (Hardness).
Later on, a brief discussion will be held on the results gathered from the whole class.

In the end of this training, all the class should understand the principles of conventional profiling, its advantages and disadvantages as well as the line-scaling technique.
Development of Vocabulary

The sensory attributes of the 4 similar chocolate samples need to be determined.
These will include terms describing appearance, aroma, taste, mouthfeel and after-taste.

Firstly, each assessor will be asked to evaluate the samples in the individual sensory booth and describe the sensory attributes in their own terms.
Then the whole panel will discuss the appropriateness and meaning of all the terms used by the individual assessors with the sensory leader in order to obtain a consensus of attribute terms which all assessors agree can be detected in the products.

(Typically 15-20 terms may be agreed).
Apart from the role of the sensory leader being responsible for the preparation and presentation of test samples known to differ, he or she also facilitates the process of arriving at a consensus of terms which are appropriate and understood by the panelists, but rarely would he or she override decisions made by the assessors.

Whether a term describes a major or a minor character note, and whether it is a key to discrimination of the products or not, determine how appropriate and valuable it is.
In normal food industrial companies, standard flavours or relevant food samples may be provided by the panel leader to help assessors reach agreement on the meaning of the different terms, however, due to the time limitation in this practical, this step will be eliminated.

Quantitative Assessment of Attributes
As long as the attribute terms are determined by the class, each of 4 chocolate samples will be evaluated by each student and each attribute scored for intensity using a line scale.

Several replicate panel sessions are necessary, and at each session all the 4 different types of chocolate samples are evaluated.
In this practical, 2 replicates will be done within the class.

Analysis of Results
The results will be used to determine the way in which the 4 chocolate samples differ.

Analysis of Variance will be used to show statistically which attributes differ between the samples.
Separate ANOVAs will be carried out for each attribute with treatment (sample) and assessor as variables.

Results will be presented in Tables of Treatment Means and Tables of Significance.
In this way those attributes which differ between treatments can be shown and, within attributes, each mean can be tested to show which means are significantly different.

The variability between assessors can also be determined.
Graphical methods of data presentation, such as Spider diagrams, will also be used.

Other statistical methods, such as principle component analysis (PCA), may also be used to help to interpret the data and determine the ways in which the Treatments differ.
(In this practical class there time will not permit the use of PCA).

Data are acquired on PCs using a sensory evaluation package (TASTE) which complies the data into a spreadsheet.
The ANOVAs on this data are carried out in MS Excel using a macro designed for sensory analysis.

RESULTS AND DISCUSSION
RESULTS

Attributes and anchors developed in the vocabulary development procedure by the class Based on the large pool of data collected from the sample profiling by the whole class, a two-way ANOVA with replication (Interaction) has been done for each single attribute with treatment (sample) and assessor as variables.
All the results are shown on the appended table 1, named as "Two-way ANOVA with Replication (Interaction)".

Combining the ANOVA results shown on table "1" with the sample means by attribute, another table is generated as referred to the appended table 2 with the calculated least significant difference (LSD) on as well.
The table is named as "Table of Sample Means by Attribute".

Based on the sample means by attribute and the LSDs calculated in table "2", a statistical analysis has been carried out for each attribute on the significance level of difference between every two samples of four.
The analysis results are represented in the appended table 3, named after "Significance Level of Difference Between Every Two Samples"

DISCUSSION
Referring to appended table 2, it is clearly shown that the significant differences among the four chocolate samples are estimated to lie in 7 attributes, which are highlighted in red on the table.

They are browness (appearance), shininess (appearance), Chemical (aroma), first bite (mouthfeel), melting (mouthfeel), mouth-coating (mouthfeel) as well as bitter (aftertaste).
This conclusion is drawn by the fact that for all the 7 attributes the significance level of variance due to the samples is bigger than or equal (i.e. only for "bitter") to the significance level of variance due to the assessors.

Consequently, it can be assumed that the differences among the means of the four samples on the 7 attributes are actually due to the nature of the samples (i.e. objective factor) rather than the assessors (subjective factor).
In contrast, it can be seen that the significance level of variance due to the samples is all smaller than the significance level of variance due to the assessors for the rest of the attributes.

Therefore, these attributes are not considered to be the key factors to differ the four samples.
In order to discover the extents to which the four samples differ from each other, (i.e. to test which sample means by attribute are significantly different and which ones are not significantly different), a LSD value is calculated for every single attribute.

Thereby, if the difference between any two means under one attribute is bigger than the corresponding LSD value, it is implied that there is a significant difference between the two means, and vice versa.
However, it is important to notice that the LSD becomes meaningless and non-applicable when the attribute is not the key factor to differ the four samples, (i.e. when significance level of variance by assessors is higher than that by samples).

Consequently, LSD values are only considered for the 7 key attributes.
Referring to table 2, for instance of browness, the LSD value is 10.65.

As the difference in means between Galaxy Milk and Cadbury's Dairy Milk is 10, which is less than 10.65, they are then not considered to be significantly different by browness.
On the other hand, the difference in means between Waitrose Belgian Milk is 23.8, which is much bigger than 10.65, so these two samples can be treated to be significantly different by browness.

The comprehensive analysis on the significance level of difference among the samples by each of the 7 key attributes is displayed on table 3, (i.e. the attributes in red).
As shown on table 3, it is obvious that for the attribute of Browness, Waitrose sample is significantly different from all the other 3 samples, while the significance level lying greater in ASDA and Galaxy than in Cadbury's.

Whereas, there is no significant difference between each two of the other 3 samples.
For the attribute of Shininess, the only significant differences exist between ASDA and Cadbury's, and Galaxy and ASDA, while ASDA seems to be much more different from Galaxy than from Cadbury's.

In the case of Chemical (aroma), only Cadbury's and Waitrose are found to be significantly different from each other, while the rest are not significantly different.
Also the significance level between the two is quite high.

For the attribute of First Bite (mouthfeel), ASDA is found to be highly different from both Cadbury's and Galaxy with the same extent.
Additionally, Galaxy appears to be significantly different from Waitrose as well.

The same case happens to the attribute of Melting (Mouthfeel), whereas all the significance levels of difference are lowered.
Besides, ASDA seems to be the most significantly different from Galaxy.

Referring to Mouthcoating (Mouthfeel), the exact same pattern of differences between ASDA and Cadbury's, Galaxy is observed.
Furthermore, Cadbury's and Galaxy are both significantly different from Waitrose as well.

By comparisons, both Cadbury's and Galaxy are more significantly different from Waitrose than from ASDA.
Last but not least, in the case of Bitter (Aftertaste), again both Cadbury's and Galaxy are found to be significantly different from ASDA and Waitrose respectively.

Besides, Cadbury's is slightly more different from ASDA than from Waitrose.
As discussed so far, we have already known about the way and the extent to which the four chocolate samples differ from each other.

However, in order to quantify the significant difference lying between samples, a Spider diagram has been plotted to offer you a clear view of the overall sensory characteristics of each sample by the 7 key attributes, as referred to the appendix-4.
As shown on this diagram, Waitrose Belgian Milk turns out to be the brownest among the four samples, also it has the least mouth-coating effect in terms of mouthfeel; thereby, it is believed to have the thinnest texture among the four.

Furthermore, it seems to possess the strongest chemical aroma among the four and a very strong bitterness in after effects, too.
In terms of ASDA sample, it has the greatest after effects of bitterness and mouthfeel of first bite, so it seems to be the hardest in texture.

Also it possesses the least browness and shininess in appearance and the least melting rate of mouthfeel.
Again, it seems to be very thin in texture due to its low mouth-coating effects in mouthfeel.

In contrast, in terms of texture, both Cadbury's and Galaxy Milk are the highest in the mouth-coating effects and in melting rate of mouthfeel.
Consequently, they appear to be the thickest in texture among the four.

Also, they turn out to be the softest in texture among the four due to the lowest values of first bite.
In the sense of appearance, Galaxy Milk is found to be the shiniest and the least brownest among the four, while Cadbury's is fairly shiny and brown as well.

Apparently Cadbury's has the least content of artificial content in the formulation due to its lowest chemical-note in aroma.
Both of them are almost free from bitterness in after effects, which again possibly represent the natural characteristics of their formulations.

Last but not least, the appended PCA plot represents us with a very general idea about what attributes virtually drive the significant differences among the samples.
This plot is well in-line with the discussions so far.

As seen from this plot, the attribute distinguish Waitrose from the other is browness in appearance, and ASDA stands out among the four due to its strong hardness and lowest melting behavior.
On the other hand, Galaxy and Cadbury's are quite similar to each other due to their high degree of shininess in appearance and strong melting coating effects of mouthfeel.

SUMMARIZATION
A critical appraisal of the QDA method can be summarized as below:

Advantage
This method is very precise, as it is broken down to quite a few procedures.

Particularly, the use of a graphic scale, which reduces that part of the bias in scaling resulting from the use of numbers; the statistical treatment of the data; the separation of panelists during evaluation; and the graphic approach to presentation of data are all the beneficial points of this methods.
Disadvantage

The panel, because of lack of formal instruction, may develop erroneous terms.
For example, in this practical, just a couple of students developed the terms of bitterness in the after effects, however, there might be confusions among other students on this term.

As the nature of bitterness can be various, some of them may imply artificial note, while some of them can imply other notes, such as medical note.
In this case, different people might not have the same understanding on a particular term.

Lack of definition also may allow a senior panelist or stronger personality to dominate the proceedings in all or part of the panel population in the development of vocabulary.
The "free" approach to scaling can lead to inconsistency of results, partly because of particular panelists evaluating a product on a given day and not on another, and partly because of the context effects of one product seen after the other, with no external scale references.

The lack of immediate feedback to panelists on a regular basis reduces the opportunity for learning and expansion of terminology for greater capacity to discriminate and describe differences.
On a minor point, the practice of connecting "spokes" of the "Spider diagram" can be misleading to some users, who may expect the area under a curve to have some meaning.

In reality, the sensory dimensions shown in the "web" may be either unrelated to each other, or related in ways which cannot e represented in this manner.
Q(a) Identify the three sample plans which are specified for your combination of Code Letter and AQL (i.e. normal inspection, tightened inspection and reduced inspection).

As being allocated, my Code Letter and AQL are M and 0.65 respectively.
Referring to the tables shown on page 42 of the module booklet, three single sample plans can be identified which are specified for my combination of Code Letter and AQL.

As displayed in a table below:
Q(b) Plot 3 OC curves on a single graph.

Calculation for OC curve of normal inspection:
Referring to the acceptance number, the rejection number and the sample size of the normal inspection plan above, the percentage of defectives in the sample lying between the acceptance number and rejection number is calculated to be around 2%, i.e. (5.5/315)*100%≈2%.Consequently, a series of percentages of defectives is set up as: 0%, 0.5%, 1.0%, 1.5%, 2.0%, 2.5%, 3.0%, 3.5%, 4.0%, 4.5% and 5.0%.

As the sample size is 315, the values of a (i.e. the number of defectives) are calculated to be: 0, 1.6, 3.2, 4.7, 6.3, 7.9, 9.5, 11.0, 12.6, 14.2, 15.8.
As the acceptance number is 5, the probability of the number of defectives being less than or equal to 5 are then calculated by excel as shown below (i.e. the sum of P(0), P(1), P(2), P(3), P(4) and P(5) for each of the "a" values):

Calculation of OC curve of tightened inspection:
As the sample size of tightened inspection is the same as that of normal inspection, based on the same series of percentages of defectives applied in normal inspection, the values of a (i.e. number of defectives) will remain the same for tightened inspection as well.

As the acceptance number is 3, the probability of the number of defectives being less than or equal to 3 are then calculated by excel as shown below: (i.e. the sum of P(0), P(1), P(2), P(3) for each of the "a" values):
Calculation of OC curve of reduced inspection:

Referring to the acceptance number, the rejection number and the sample size of the reduced inspection plan above, the percentage of defectives in the sample lying between the acceptance number and rejection number is calculated to be around 2.8%, i.e. (3.5/125)*100%≈2.8%.
Consequently, a series of percentages of defectives can be set up as the same as that of normal inspection, which is 0%, 0.5%, 1.0%, 1.5%, 2.0%, 2.5%, 3.0%, 3.5%, 4.0%, 4.5% and 5.0%.

As the sample size is 125, the values of a (i.e. the number of defectives) are then calculated to be: 0, 0.625, 1.25, 1.875, 2.5, 3.125, 3.75, 4.375, 5, 5.625, 6.25.
As the acceptance number is 3, the probability of the number of defectives being less than or equal to 3 are then calculated by excel as shown below: (i.e. the sum of P(0), P(1), P(2), P(3) for each of the "a" values):

According to the 3 sets of data calculated above, 3 OC curves for normal inspection, tightened inspection and reduced inspection are plotted on the same graph, as displayed below:
Q(c) Common upon:

The shapes of the OC curves (e.g., how far from ideal are they)
As seen from the graph of OC-curves above, the OC-curve of tightened inspection is the steepest among the three, while the steepness of the reduced inspection OC-curve is the least among the three, (i.e., the most stretched).

Additionally, the OC-curve of normal inspection is neither too stretched nor too steep, lying in the middle range between the tightened and reduced inspections.
By comparing the three OC curves with the ideal curve on the graph, which is a straight vertical line, the tightened OC-curve is found to be the closest to the ideal OC-curve among the three, as it is the steepest out of the 3 curves.

Consequently, the probability of acceptance during sampling is the lowest among the three inspections.
In the same way, the OC-curve of normal inspection is the second closest to the ideal one, and the probability of acceptance during sampling is a little bit higher than that of tightened inspection.

By contrast, the OC-curve of reduced inspection is the furthest from the ideal OC-curve, therefore, the probability of the acceptance during sampling is the highest among the three sampling inspections.
The differences between them (e.g. this could be both qualitative and quantitative using measures linked to consumer and producer risks)

As discussed in the previous question, the qualitative differences among the three OC-curves can be summarized as: the steepness of the curve and the degree of closeness of the curve shape to that of ideal curve.
The steepness can be ordered as: Tightened inspection OC-curve>Normal inspection OC-curve>Reduced inspection OC-curve; the degree of closeness can be ordered as: Tightened inspection OC-curve>Normal inspection OC-curve>Reduced inspection OC-curve.

The quantitative differences among the OC-curves can be explained by calculating the slopes of the 3 OC-curves respectively and comparing them.
The slope of the curve joins the producer's risk point and the consumer's risk point and can be estimated using the equation: Slope (k)=(Producer's risk-Consumer's risk)/(Limiting quality level-Acceptance quality level), where the Producer's risk point corresponds to the probability level of 0.95 and the Consumer's risk point corresponds to the probability level of 0.10)

For normal inspection:
As shown from the OC-curve, the Acceptable quality level is found to be approximately 0.75 and the Limiting quality level is 3.

So FORMULA
For tightened inspection:

As shown from the OC-curve, the Acceptable quality level is found to be approximately 0.35 and the Limiting quality level is 2.15.
So FORMULA

3: For reduced inspection:
As shown from the OC-curve, the Acceptable quality level is found to be approximately 0.75 and the limiting quality level is approximately 4.875.

So: FORMULA As referred to the calculated results above, it is clear that the slope of the tightened inspection OC-curve is the largest, and the one of normal inspection OC-curve is the middle, while the one of reduced inspection OC-curve is the smallest.
Consequently, it is clear that the quantitative measurement of differences matches the qualitative measurement of differences.

I.E., the tightened inspection OC-curve is the closest to the ideal curve, and the normal inspection OC-curve is the moderately closest to the ideal curve, while the reduced OC-curve is the furthest from the ideal curve.
The food situations in which your sampling scheme might be used (e.g. what sort of food problems might be covered by the scheme you have been allocated)

The sampling scheme I have been allocated includes combination of the three single sampling plans identified above with rules for changing from one plan to another.
In order to locate my sampling scheme to particular food situation (s), certain aspects of the 3 single sampling plans within the scheme need to be considered in order to achieve an appropriate decision.

As referred to the extract from Codex 'General Guidelines on Sampling' cited on page 39 of the handbook 'Statistical Aspects of Q.A', the Acceptable Quality Level (AQL) for a given sampling plan is the rate of non-conforming items at which a lot will be rejected with a low probability, usually 5%.
It is also referred to as particular producers' risk.

Additionally, AQL can be used as an indexing criterion applied to a continuous series of lots which corresponds to a maximum rate of acceptable defective items in lots (or the maximum number of defective items per hundred items).
The very important aspect of AQL is that it is a quality goal fixed by the profession.

What that means is higher the rate of defective items exceeds the AQL, the greater is the probability of rejection of a lot.
Therefore, for any given sample size, if the AQL is lowered, then the protection for the consumer against accepting lots with high defective rates becomes greater, and the requirement for the producer to conform with sufficiently high quality requirements becomes greater as well.

However, any value for AQL should be realistic in practice and be economically viable.
If necessary, the value of AQL should take into account safety aspects.

Corresponding to the ISO standards cited on page 40 of the handbook of 'Statistical Aspects of Q.A', there are mainly two categories of product characteristics which are associated in matching the AQL levels.
The first category is the characteristics which may be linked to critical defects (e.g., to sanitary risks), and this shall be associated with a low AQL (i.e. 0.1% to 0.65%); whereas the second category is the compositional characteristics such as fat or water content, etc, and this may be associated with a higher AQL (e.g. 2.5% or 6.5% are the values often used for milk products).

As a result of those, for a given product, in order to monitor both of its safety and quality aspects, a single AQL should be allocated to each of the two classes of nonconformities.
I.E., a low AQL (e.g. 0.65%) being allocated to class A nonconformities (e.g. pesticide content in follow-up milk), and a higher AQL (e.g. 6.5%) being allocated to class B nonconformities (e.g. protein content in follow-up milk).

Consequently, there is a separate sampling plan for each of the two AQLs, and a lot is accepted only if it is accepted by each of the plans.
Based on the principles discussed above, referring to my sampling scheme in this exercise, as estimated in Q(c) the corresponding AQLs of the producers' risk for the 3 single sampling plans of normal inspection, tightened inspection and reduced inspection are 0.75%, 0.35% and 0.75% respectively.

As all the AQLs are close enough to 0.65%, which are considered to be low AQL values, the sampling scheme here containing the three single sampling plans can be allocated to the particular food situations (problems), which are class A nonconformities that are characteristics of food linking to critical defects, (e.g. sanitary risks like pesticide content in follow-up milk and other safety respects of the products).
Q(a) Obtaining an estimate of the standard deviation of the process based on the first set of 20 samples.

As shown on the attached datasheet of example 5, the mean range of first set of 20 samples FORMULA is 16.1.
According to the equation: FORMULA, while n=4, so referring to the table on P15 of the handbook "Statistical Aspects of Q.A", d4=2.059; so the estimated standard deviation (σest)=16.1/2.059=7.82

Q(b) Construct a Shewhart control chart for means (with action and warning limits) based on the standard deviation estimated in a).
Plot all 40 samples on the chart.

As calculated above, the σest is 7.82.
According to the equation: FORMULA Based on a Shewehart control chart for means, the action limits= FORMULA and the warning limits= FORMULA.

According to the attached datasheet, FORMULA of the first set of 20 samples=297.2875, and 3SE=11.73; 2SE=7.82.
So, upper action limit=297.2875+11.73=309.02; while lower action limit=297.2875-11.73=285.56.

Upper warning limit=297.2875+7.82=305.11; while lower warning limit=297.2875-7.82=289.47 Based on the mean value of the first set of 20 samples, action limits and warning limits, a Shewhart control chart for means is plotted as below, including all 40 samples.
Q(c) Construct a Shewhart control chart (with action limits) for either range and/or standard deviation.

Plot all 40 samples on the chart.
As mentioned in the first question, the mean range of first set of 20 samples FORMULA is 16.1, referring to the formula on P 15 of the handbook "Statistical Aspects of Q.A.", which is Upper action limit= FORMULA ; while Lower action limit= FORMULA.

As n=4, so D 4=2.282, D 3=0.000.
Consequently, FORMULA Referring to the range values of 40 samples on the attached datasheet and the action limits calculated above, a Shewhart control chart is plotted as below: All the standard deviation values for each individual sample have been worked out on the attached spreadsheet.

Referring to that, the standard deviation average value of the first set of 20 samples is found to be 7.26.
According to the equation on P 15 of the handbook "Statistical Aspects of Q.A.", which is FORMULA ; while FORMULA.

When n=4, B 4=2.266; B 3=0.00.
Consequently, UAL=2.266×7.26=16.45; LAL=0.00 Referring to the individual standard deviation values of all 40 samples shown on the datasheet and the action limits calculated above, a Shewhart control chart is plotted as below:

Q(d) Describe what would happen in a factory using your charts based on the second 20 samples.
Assume that point 21 is the start of the day and continue until you think that action would be taken to control the process.

(Remember that, for your data, no action takes place-the process continues until sample 40)
In order to find out and describe what might happen in the factory, a couple of tests set up by ISO8258 will be applied to the Means chart above.

Therefore, this means-chart is divided into 6 zones, which are zone A (between UAL and UWL), zone B (between UWL and line 1), zone C (between line 1 and the mean); zone C (between the mean and line2), zone B (between line 2 and LWL), zone A (between LWL and LAL).
Based on the assumption that the point 21 is the start of the day, two of the most common tests (Test1 and Test 5) can be taken on the process starting from point 21.

Referring to the ISO8258, Test 1 is: One point beyond the zone A (i.e. beyond the action line).
Test 5 is: Two out of three points in a row in Zone A or beyond (i.e. beyond a warning line).

As shown on this means control chart, starting from point 21, point 22, 23 24 are in a row with both point 22 and 24 are in Zone A (i.e. beyond a warning line).
This matches test-5 standard.

Additionally, point 26 jumps beyond the Zone A (i.e. beyond the action line).
This again matches test-1 standard.

Based on the two test results, which are very rare to happen in the normal process, thereby, it could be assumed that something has significantly changed in the production process，including the packing line in the factory.
Probably the machinery of the food packaging line has been adjusted in the way that more products were put into the package, consequently the weight of packed products increased at a great extent.

Q(e) Can you give an estimate when the process changed and what the new mean and/or standard deviation might be?
As seen on the Shewhart control means chart above and discussed in Q(d), the point 24 is actually the first point to meet the standardized test (i.e. test 5).

Consequently, it can be estimated that point 24 is the start point to signal the change of the process.
Referring to the datasheet attached, the new mean of the changed process is calculated starting from the sample 24 to sample 40.

So, the new mean is estimated to be 313.9559 As found on the datasheet, the mean range of the samples from 24 to 40 is calculated to be around 10.12, so referring to the equation FORMULA when n is 4.
Then, FORMULA.

So the new standard deviation of the changed process is estimated to be 4.92.
Q(f) If the actual specification limits for the production line are as follows, what is the Cpk of the production (based on the first 20 samples)?

Comment on the value.
As the mean value of the first 20 samples is 297.2875, the set of data for the Process mean of 300g, LSL of 265 and USL of 335 is chosen for calculation of C pk.

Referring to the equation that is Cpk= The lesser of (USL-X)/3σ Or (X-LSL)/ 3 σand as calculated in question (a), the process standard deviation based on the first 20 samples is estimated to be 7.82, so 3σ=23.46.
As X=297.2875, then C pk (towards the USL)=(335-297.2875)/23.46=1.61; while C pk (towards the LSL)=(297.2875-265)/23.46=1.38.

As both C pk values are greater than 1, the production process (based on the first 20 samples) is considered to be quite capable of producing the products within both the Lower and Upper Specification Limits.
However, comparing the C pk (towards the USL) and the C pk (towards the LSL), the C pk value (towards the USL) is slightly greater, so it seems that the process has a little bit more risk to produce products over the LSL than to produce products over the USL.

This situation is not ideally beneficial for customer.
Yet, the process as a whole is quite capable.

Q(g) Estimate the value of Cpk for the new process conditions given in (e).
Comment on the value.

As calculated in Q(e), the new mean and new standard deviation of the changed process are 313.9559 and 4.92 respectively.
According to the equation applied in Q(f) and using the same values for LSL and USL, C pk (towards the USL)=(335-313.9559)/(3×4.92)=1.43, C pk (towards the LSL)=(313.9559-265)/(3×4.92)=3.32.

As both C pk values are greater than 1, the new production process is considered to be capable of producing the products within both the Lower and Upper Specification Limits.
However, comparing the C pk (towards the USL) and the C pk (towards the LSL), the C pk (towards the LSL) is nearly 2 and half times more than the C pk (towards the USL).

Consequently, it seems that the process has much more risk to produce the products over the USL than to produce the products over the LSL.
This kind of situation is not beneficial to the business, regarding to the extra products put into the packages.

Metamorphoses was the only epic written by Ovid, and there are many notions of change within it; a point made immediately by the title itself, which means 'changing of forms'.
Indeed, the first words of the epic lead one in to the ostensible subject matter; "Of bodies changed to other forms I tell; You Gods, who have yourselves wrought every change..."1 But in my opinion, Ovid does not merely tell of change, but look at it from different angles, cast humorous or political allusions through it, and indeed change most concepts of what an epic poem had been up until that point.

It is these things I wish to discuss in the following essay.
There are approximately 250 stories told throughout the fifteen books of the Met., and all refer to a change in some way, in most cases as the main point of the story, but sometimes included into a familiar one as an excuse to write about it.

The changes are treated in different ways by Ovid himself, by the various storyteller mouthpieces he uses, and in their own accounts.
The Met.

starts off with the tale of creation; things being made, changed from water and earth and nothingness into living, breathing creatures, a 'step up' the ladder of classification.
In this way change is treated as something miraculous, something that transcends normal human understanding; it is brought about by the gods, but not any one of them in particular.

The first actual change of one living, breathing person into another living thing is found with the story of Lycaon 2.
Lycaon is one of the first race of humans, and so vile and corrupt that he serves up human flesh to Jupiter as a test of his divinity.

A human daring to try and outwit a god; the impious Lycaon is changed by an angry Jupiter into a monster wolf.
Thus here, change is used as a punishment, an interesting introduction by Ovid to the theme of metamorphosis.

Most of the changes of humans or nymphs into other things by the gods can be divided simply into a few categories; change as a punishment (as seen by Lycaon), change as a reward (cf. Baucis and Philemon 3, who were transformed into trees by Jupiter for their pietas), change through pity (cf. Perdix 4, who was saved from death by falling by Pallas, who changed him into a lapwing) and change through consequence, in which the gods seemed not to be involved (cf. Cygnus 5, who bemoaned the fate of his cousin Phaeton and became a swan).
The portrayal of these types of changes can be various, especially when it comes to things like punishment.

We have already seen the crime and punishment of Lycaon, and he is shown to be rightly punished, his crime being something most people would find horrific.
However, one must then take into account the other 'punishment' transformations.

In this case a horrible crime is punished by a just god; but in Callisto's 6 case, for example, Juno in a jealous rage sees fit to punish her purely because she had caught the eye of her husband Jupiter.
Surely this is not fair?

Ovid portrays both scenes neatly in the way we would expect, loathing overlaid through the text describing Lycaon's change, and pathos through the fear written through Callisto's change.
Certainly, she has become a unsightly monster, a mountain bear, with 'grisly fur' and 'hideous jaws'; but still, "Moan after moan proclaimed her misery.

She raised her hands (her paws!) towards the stars......
and many a time, forgetting what she was, hid from the creatures of the wild; a bear, she shuddered to see bears on the high hills..." She has kept her human thoughts and mind, which seems all the more cruel of Juno.

Scylla 7 was changed into a bird for her treachery, but seemed not to have known enough to regret it; Callisto's only crime was to be pretty, and it is through no fault of her own that Jupiter came and raped her.
Indeed she even fought against him.

How could this punishment be justified?
Sometimes the crime for which transformation is the punishment is caught between these two extremes, however; and the way they are portrayed seems to be mixed.

Arachne 8 was a mortal woman who dared to think she could outweave Pallas Minerva, and does in fact do so; she is transformed into a spider by the angry goddess a punishment for her hubris, which is also a recurring theme.
Is hubris that terrible a crime that it warrants transformation as a punishment?

This brings us also to discuss how, bizarrely, an action that can be used as a punishment can also be used as a reward.
Most of these transformations are simultaneously pity, in that they tend to happen as a result of a plea to the gods to save them from something terrible happening.

Examples of this would be, as I said previously, Perdix who was saved from falling to his death, or Daphne 9, who was changed into a tree by her father the river god to prevent her rape at Apollo's hands.
There were a few true rewards, as in Baucis and Philemon's case, where they lived their full lives together as they had requested from Jupiter, until the time came for them to die and they both turned into trees, to end together and live on in that way.

What is interesting about this opposition of how change can be used is recognised by Ovid himself in Book II, with the story of the crow; she herself tells how she was pitied by Pallas and turned into a bird to escape the blandishments of the Sea god, and then banished from the goddess' sight for being a tell-tale.
But this is not the worst of it, she says; "But what good was it, if Nyctimene, She who was made a bird for her foul sin, Supplants me in my place of privilege?" 10 Here Ovid is giving the reader just that curious viewpoint, and early on in the poem as well; this is setting you up to go on and read the rest of the Met.

and try and see it that way, and whether those all-powerful gods are actually something to be feared or just as human and petty as the rest of us.
And what of those changes which the gods are not even involved in?

In the proem, it clearly states that it is the gods who 'wrought every change', and yet Ovid is quite happy to mention not only changes that are wrought by almost-humans (Medea and Circe are both granddaughters of the Sun, but otherwise not gods themselves) but changes that occur seemingly with no interference at all.
Cygnus was changed to a swan and Niobe 11 to a spring (one might argue that this is part of her punishment for hubris, but I rather consider the punishment to have been the murders of her children that made her weep in the first place), and they both seem to have happened merely from the output of their emotion, rather than specific interference.

The gods have already been shown to be petty; if transformations can take place without their knowledge at all, how can they be omnipotent, something to be feared, at all?
Looking at the transformations actually told by the Metamorphoses, one might consider this all to be one point Ovid is trying to make with change, one of poking fun at the gods and the roles of humans above animals or inanimate object.

Or perhaps one might see it that it is not the physical change, but the change in emotions each character in each story experiences, and how they are 'transformed by it'; this is another whole viewpoint that I will not go into here.
But this is not merely all that can be read into this epic.

For this is epic; a format of writing Ovid as a young man had previously ridiculed as the realm of the serious, lofty poet, not something he, as the writer of love poetry, could ever bother attaining.
12 But it is not epic as is previously known, or held up by Callimachus in the Alexandrian ideal; rather, it is another change that Ovid has wrought.

Traditionally, epic has a single hero, a journey or warlike theme, and clearly divided even separations of books.
The Metamorphoses turns these ideals on their head; with its many storytellers, its rambling and continuous storyline and its often comic themes of myth phasing into history, it seems to constantly draw on older epic but change everything about them.

So, before we even examine the changes within the poem itself, we have the change from Ovid's previous works and the change in the stereotypical idea of an epic.
Of course, he does include many themes one might expect of epic, but twisted to his own unique flavour; the catalogue of ships made famous by the Iliad is reduced to a catalogue of hunting dogs 13, the older heroes of Homer assemble as young men for a heroic boar hunt, but flee like timid girls rather than the great men they have been previously portrayed as 14.

As Ovid writes through history he passes through those epic themes himself, but from other angles; the shipmates of Odysseus are given roles as storytellers, and Aeneas, the hero of the previous great Roman epic written in Augustus' time, is merely another wanderer, and themes that Virgil took up whole books upon are reduced to a few lines (Dido is mentioned but once, and in a couple of sentences only).
15 Another change that might be considered is how Ovid works from the past to the present day, and starts with far-off mythical times, eventually ending with the modern (then) day and the deification of Augustus.

Realising this, and the fact that the Metamorphoses was almost certainly finished before Ovid was sent into exile by Augustus in 8 AD 16, one cannot ignore the possible political allusions made by the poem.
As early as the first book, Apollo tells Daphne that she will always be a glorious tree, spanning even the gates of Augustus; by the end of the epic he beseeches the gods that (in a somewhat tongue in cheek manner) Augustus might be deified as was his father Caesar; "...

But how can they (these achievements) compare With being father of so fine an heir Under whose sovereignty mankind is given Such plenteous blessings by the power of heaven?"17 This is obviously flattering Augustus (and can be read in a highly unflattering way if one wishes), but the idea of political epic is not a new one; after all the Aeneid was written (some say) in the beginning as something to prove Augustus and the Roman people's stock as greater than that of the Greeks.
But the way in which this is done is a lot more straightforward, and conversely, more hidden; with Augustus came change, from civil war to a much more peaceful, prosperous time, this cannot be denied.

What of Rome itself, then?
It surely was going through a time of change; previous epic poets (e.g. Virgil) had hoped that their words would survive up until the climax of civilization, where they were currently, but Ovid is more forward-thinking than that.

In Pythagoras' speech 18 (yet another 'mouthpiece' for Ovid himself to be talking) he declaims that all cities rise and then fall, leaving nothing but names behind; after listing many famous cities that now lie in ruins, he goes on to talk of the power of Rome, but cleverly does not state that it will last forever.
Instead, the power of poetry is portrayed as lasting much longer, putting Ovid himself as a poet above those people who have achieved other fame.

With the Metamorphoses, Ovid is able to utilise so many different aspects that the word 'change' that he sings of might associate with.
The physical change of a human into some other form of being, change as a punishment or a reward, change of storyteller throughout the epic, change in the very nature of epic itself; all of these are covered admirably, and there are surely yet more beneath the surface to find.

1. Book I. 1-2 2.
Book I. 195-261 3.

Book VIII.
616-730 4.

Book VIII.
225-260 5.

Book II.
360-380 6.

Book II.
400-510 7.

Book VIII.
1-175 8.

Book VI.
1-125 9.

Book I. 450-575 10.
Book II.

550-600 11.
Book VI.

120-315 12.
Amores Poem 1 ; 'I tried to write lofty epic, but I ended up writing love poetry instead'.

13.
Book III.

198-235 14.
Book VIII.

382-418 ; The Calydonian Boar Hunt 15.
Book XIV.

64-92 16.
Commentary on Ovid's Metamorphoses trans.

Melville 17.
Book XV.

753-784 18.
Book XV.

406-441
The goddess Isis was one of the last pagan goddesses to survive in the Mediterranean world after the spread of Christianity, well into the 6 th century AD 1, and during the late Greco-Roman period her cult was astounding in strength.

A few other gods managed to make it out of their home country as cult figures (e.g. Cybele, Mithras and the amalgam that was Serapis), but no other achieved such universality.
The Greek, Macedonian and finally Roman influence on the country of Egypt is finally what allowed her name to be heard abroad, but what was it about Isis that made her particularly appealing?

In this essay I would like to discuss several concepts surrounding this question.
Isis' origins as an Egyptian goddess do not necessarily reflect that which she became.

Egypt itself was never as unified in its concepts of religion as the Greek city-states were; throughout the country gods and goddesses were identified with different aspects of each other and with different animals.
There were three main schools of Egyptian religion, centring in Memphis, from Hermopolis and from Heliopolis.

Isis came from the latter, a collection of eight (later nine) deities known as the Heliopolitan Ennead.
I will now take a brief look at the earlier myths about Isis to provide a background for her later changes (I will be using the Hellenised versions of the names of the gods throughout this essay).

The Heliopolitans believed that the world came into being with a primeval swamp, and from the swamp was born Shu and Tefnut, the god of wind and the goddess of moisture.
They begat Geb and Nut, another brother and sister who married, this time aligned to earth and sky.

Nut gave birth to four children; Osiris, Isis, Set and Nephthys.
Osiris and Isis were a loving husband and wife, as Nephthys was ostensibly the sister-wife of Set; but Set was the red god, of the desert and chaos and evil, and he constantly warred against his brother the king Osiris.

Horus was the son of Osiris and Isis, and later he was said to have been born at the same time as the other gods (as his following became larger and his importance was emphasised) and born of the alliance Isis and Osiris had before they left the womb.
This typical story told of Osiris, Isis and Set can be found recorded in several sources, including Herodotus and most notably Plutarch.

The myth goes that Osiris and Isis were reigning as a benevolent king and queen over Egypt, but their jealous brother Set tricked and killed Osiris and dismembered his corpse, scattering them to the winds.
Isis, as his sister and wife, was heartbroken, and mourning his death set out to try and restore all of the pieces.

When this was finally done she restored him to life with magic, but he was sent to become king of the underworld as one who had already seen death once.
This is why in the iconography of Osiris he is normally portrayed as a mummy with green or black skin.

While Isis was reassembling Osiris, in the original myth it was here she conceived their son the king Horus, who went on to rule Egypt in his father's stead, against the wishes of Set.
2 This myth established Isis as a mother goddess in the early Heliopolitan context, and a mistress of magic.

She, as mother of Horus, was also seen as the mother of the pharaoh, and thus a powerful deity to invoke; she was also revered as a mourner.
This is seen in many later inscriptions, even magic papyri 3 (though this is not surprising given her role as a magic user).

Unusually for an Egyptian deity she did not have a specific town as a starting place for her cult (as Horus was said to have been raised at Buto, and Bastet had her own town of Bubastis 4), and perhaps this made her more easily accepted by the rest of the country.
Isis was also associated early on with the Dog Star (or Sothis), and the star's rising signalled the seasons where the Nile flooded.

Even this early in her history as it were, Isis had several different aspects.
This was not unusual for an Egyptian deity; many of them were worshipped as amalgamations, for example Hathor, ostensibly the cow goddess of music, inebriation and sex and the wife of Horus, was identified with Sakhmet the lioness-headed goddess of war in the one particular instance where she slaughtered thousands and drank their blood, but was fooled into drinking red beer and fell into a drunken stupor, quenching her killing rage.

5 In this way the goddess of wine was identified with the goddess of inebriation.
This merging of one deity with another (or syncretism) is therefore not unknown, especially in Egypt.

Indeed, Hathor herself became merged with Isis in approximately the 23 rd dynasty; Isis gained the distinctive cow's horns on her head, but remained the mother rather than the wife of Horus.
It was in this aspect the Greek traders first probably came across her.

The Greeks, due to their seafaring ways, certainly came across many new religions and cultures.
But rather than installing their own gods, they tended to have a curious habit of looking at the local deities and observing that they were just one of their own, but under a different name.

At Naukratis she was worshipped as Isis-Aphrodite, in probably reference to her syncretism with Hathor; this also gave rise to her being the consort of Jupiter who was turned into a cow, Io 6.
Whether this is also because of the myth that Io wandered for years and ended up in but probably her most important identification came to be with Demeter.

Herodotus, writing in the 5 th century, claimed to have little interest in religion, and where he does refer to the gods in his discourse on Egypt, he uses the Greek identifications that later became almost set.
In fact he even expostulated that the Greek names for the gods came from Egypt in the first place 7, making the Egyptians older and wiser than other races.

In his books, Set became Typhon, Thoth became Hermes; in his role as he who made the soil black and the river flood Osiris became Dionysus, as a fertility god.
This then fit again with Isis as his wife becoming Demeter, the lady of harvest.

Some of these conclusions may seem a little odd.
Demeter as one who grieved (for her daughter Persephone) could be also seen as the mourning Isis, though she grieved for her husband; contrarily, Demeter never had a husband, so could not be the loving wife that Isis was.

However, the advantage of being identified with Demeter was that that goddess was one of the elder deities in Greek religion, a daughter of Cronus 8.
Writing circa.

90 AD, Plutarch used this in his 'On Isis and Osiris' essay, taking a hymn to Demeter 9 that one of the oldest epic poets, Hesiod, had written in his Theogeny, and modifying it to fit with the stories of Isis already known 10.
Whether this is Plutarch himself making this connection or something already assimilated into the corpus known about Isis in uncertain however.

So by the time the Greeks were making their trading presence known in Egypt, Isis was already known as a powerful mother goddess, guarding the king, and perhaps, despite her stolen cow's horns, less intimidating to the foreigners (to whom the zoomorphism of Egyptian religion was somewhat mystifying or disgusting 11).
That the opposite had happened with Egyptian traders, and that the cult of Isis was known in Greece itself before 330 BC, can be dated from an inscription found in the Piraeus, stating a request to 'found a shrine to Aphrodite on the same terms that the Egyptians have founded one for Isis'.

Another point that must be looked at is how the Egyptians, never seafarers before being introduced to it by the Greeks, looked to Isis as a protector of sailors and a goddess of navigation, and would have told their foreign traders the same.
It is hardly surprising, therefore, that the Greeks would worship her whilst they were in Egypt.

But why bring her cult back with them to Greece?
Certainly even by this point she was a lot more all-encompassing a goddess than any of the Greek deities, and as more and more deities and powers became syncretised with her the list of her powers could only grow.

By the time Alexander invaded, and the reign of the Ptolemies began, Isis was already identified with many other goddesses, and as such taking on their attributes and expanding her sphere of dominion, losing some of her older, original aspects in the process.
As protector of the king, the invaders that became pharaohs either truly believed or thought it politic to pay honour to she who had become a powerful mother goddess.

In the newly built city of Alexandria Isis was jointly honoured with another syncretistic god, Serapis 12; the Serapeum and Museon were huge, elaborate buildings that became iconic with the importance that Alexandria grew to have.
However the most important temple that Isis was to have was constructed at the island of Philae, and it first came into importance with the buildings added by Ptolemy IV, though many later rulers of the ancient Mediterranean world made their mark there, including Augustus and Claudius, both Roman emperors.

It is at Philae the famous Aretalogies can be found.
Whether these are Egyptian or Greek is unclear, especially because by this time it was obvious that Egyptian priests were eager to identify their religion with what foreigners might understand (c.f. Herodotus, Plutarch and Manetho, an Egyptian priest who was equally fluent in Greek) 13.

The Aretalogy is basically a list of self-proclaiming statements in which Isis declares her dominion over most aspects of the world.
Earlier, shorter statements attributed to Isis did exist; "I am Isis, one more spiritlikeand august than the gods.

" 14 The later Aretalogies show Isis saying she created writing, civilization, the paths of the stars, and that she had power over the sea and childbirth and fertility, and that she conquers fate itself 15.
All these aspects meant that the Isis that the later Romans first came across could be adopted by many areas of society.

As protector of the sea, the sailors who would spread her cult in the first place would worship her; as a loving mother and protector women would pray to her; as an all-powerful being who had dominion over and above the other gods she could be anything that one wished.
Indeed, the Greco-Roman Isis, along with many of the foreign cults that spread around the Mediterranean as the Empire made the world smaller and travel more possible, was growing much closer to a monotheistic aspect than the older religions of both Greece and Egypt (perhaps accounting for the enthusiasm with which Christianity was adopted later on).

She was known as , 'many-named', because people were admitting that perhaps all these female deities they worshipped were all just Isis in different aspects, one goddess portrayed as many 16.
The Greco-Roman world was prepared to move away from their old deities; the new and curious, especially from the strange and mysterious East, was welcomed with open arms.

Despite early suppression in places, Isis was eventually honoured with her own sanctuaries throughout the Mediterranean; she became an accepted part of the culture, appearing as just another well-known goddess in Roman epigram 17, and given a prominent role in one of the only surviving Roman novels, Apulieus' Metamorphoses (or the Golden Ass), written in the 2 nd century AD.
If we examine how Isis appears to Lucius, the main character, one might see the accumulation and syncretism of what Isis became in entirety; the cow horns and the sistrum 18, symbols of her Egyptian roots (ignoring the fact that these were originally symbols of Hathor!), the ears of corn from her identification with Demeter, and her many-coloured robe, something that was almost certainly taken from Plutarch's discourse.

So, this is what Isis became, a much changed version of her conception as a mourning wife in Egyptian religion.
I think the main reasons for theses changes can be attributed to her early syncretism within her own country, her association with the sea (so Greek traders were more likely to learn her name, with their own associations with the sea) and finally her Oriental mysticism and eventual perceived overall dominion, with a control over fate in troubled times.

Many levels of Greek and later Roman society could identify with her, and so she became strong where other deities failed.
1. The temple to Isis at Philae was finally closed on the order of Justinianus in 550 AD.

2. Plutarch De Iside et Osiride, sections 13-17 3.
PGM IV.

94-153 4.
Though later through the effects of syncretism the city was attributed to Isis herself (Aretalogy, P.Oxy.1381) 5.

Myth and Symbol in Ancient Egypt (see bibl.) 6.
Ovid, Metamorphoses I. 731-60 7.

Herodotus Histories Bk II 50.
- contrast with Plutarch De Iside et Osiride 2., where he states that the name Isis is actually Greek in origin.

8. Aretalogy, P.Oxy.1381, though this states her as the 'eldest' one, which was originally Hestia.
9. Hesiod Theogeny 'Hymn to Demeter' 10.

Plutarch De Iside et Osiride, sections 15-16 11.
Isis among the Greeks and Romans (see bibl.) 12.

'Serapis' was a syncretism of Osiris and Apis (the god worshipped as a bull).
The Greeks identified him with Hades or Dionysus.

13.
Isis among the Greeks and Romans (see bibl.) 14.

The Coffin Texts, Spell 148 15.
Aretalogy, P.Oxy.1381 16.

Hymns of Isidorus (see bibl.) 17.
Martial, Epigrams 2.14 18.

Apuleius Metamorphoses Bk XI
The Yorkshire Wolds form my home surroundings and it has always been a common sight to see them grazed by livestock, particularly in the more extreme areas of the landscape.

I would like to know if the land was always used in such a way, or when this form of land management first began so that I can decide whether this is a landscape created by livestock.
I also wish to study how the various forms of leisure pursuits carried out on the Wolds work alongside the livestock and whether the future of livestock on the Wolds is secure.

History
The characteristic appearance of the Yorkshire Wolds as they are now known was first developed in 1730 with the arrival of the Enclosure Act.

Prior to this the Wolds were a "hedgeless and treeless countryside," (Stephen Harrison, 2000) populated mainly by sheep.
Two thirds of the landscape was covered by grass and because the sheep ran wild, whatever was planted with arable crops was often destroyed, resulting in the decline of agriculture in the area (Gwendoline Hurst, date unknown).

Sheep were able to survive on the Wolds, whereas cattle couldn't, due to the shortage of available water.
The land on which the Wolds are based is very well draining and there are very few ponds and only one surface stream running through them.

Sheep are able to gain the water needed for survival through the water content of grass, but this was more difficult for cattle to achieve.
Therefore the grazing of cattle on the Wolds is a more recent development than that of cattle grazing on many other areas of the UK.

The result of the Enclosure Act was that 83, 360 hectares of the Yorkshire Wolds became enclosed (Stephen Harrison, 2000).
The main method of field division was through hedging, although some fields are now fenced with wire netting.

The division of the land caused the numbers of sheep to decline and grazing was mainly confined to the valley sides and bottoms, as new agricultural technology allowed some of the valley slopes to be ploughed.
As technology evolved through time, methods were developed of supplying water to the Wolds and this allowed cattle to begin grazing the Wolds (Stephen Harrison, 2000).

Due to the quality of the arable land, very few dairy farms have arisen on the Wolds and most of the cattle grazing the land are either fattening animals, or suckler cows and calves.
Suckler cows and calves are the most common users of the grassed valleys as the land is highly productive and can support relatively high stocking densities.

Many of the valleys are emptied of cattle over winter, due to the cold temperatures and also the timing of calving on many farms.
However, sometimes sheep are then turned onto the valleys, but normally they are left to recover over the winter.

Leisure alongside Livestock
The main leisure pursuits that are enjoyed over the Yorkshire Wolds are that of Foxhunting and Partridge shooting.

Following on from the Enclosure Act, many plantations of trees were grown, some on valley slopes, but more often at the side of valleys.
Although these are rarely used to house livestock, they became good locations for the establishment of the game birds used in shooting.

In addition to these plantations, fields of Kale were also sown to act as cover for the birds.
This has provided a second form of vegetation for sheep to be housed on, for after the shooting season ends, sheep can be turned onto the Kale, where they can then graze it down.

This has allowed sheep and cattle to be kept on the Wolds without competition for the same feed source.
Foxhunting has also aided the grazing of sheep on the Wolds, for by managing the numbers of foxes in the area, it has reduced the chance of attacks being made on lambs.

This is a big issue with sheep farming on the Yorkshire Wolds, as farmers often try to keep their sheep outside all year round to minimize the costs of bringing them inside.
Sustainability

It is currently unclear as to whether the future of livestock on the Yorkshire Wolds is stable; however I believe that it will be.
It is likely that farm machinery will continue to develop and that cultivation equipment will be designed that can cope with the steep valley sides typical of the Wolds.

However, I think that the soil erosion that would arise from the tillage of this land would be too great for it to be used as arable land.
Another factor is the financial value of livestock.

The livestock that are raised on the Yorkshire Wolds; suckler calves and lambs, have been raised in such a manner that they would qualify to be sold as value added products and this means that they can command a greater price in the market place, which is currently enjoying large demand for these products.
Some farmers are currently talking about diversifying with their land and creating adventure centres for off-road vehicles and other associated activities, however there is not enough customer demand for them all to go ahead with this move and so I believe that the farming of livestock on the Wolds is the most efficient use of the land available.

Conclusion
From the information that I have collected I now feel it is clear that the Yorkshire Wolds used to be a landscape created by grazing animals, but that is certainly no longer the case.

As technology has developed, farmers have been more able to control the behavior of livestock and this has enabled them to decide what they want to do with the land available to them.
The leisure pursuits that are enjoyed on the Wolds are considerate towards the needs of the land and I feel that they will contribute towards the sustainability of livestock on the Wolds.

I think that we will always see livestock on the Yorkshire Wolds, so long as trends in consumer demand continue in the direction that they are currently headed.
Introduction

Situation
I am the owner and manager of a 180ha farm in the county of Berkshire.

The basis of its farming activities is milk production, and at one time over 300 cows were milked.
Latterly the number of cows has been reduced, but it remains a dairy farm.

In addition to the dairy cows, there are also some sheep and some cereal production.
Staff include a dairy herdsman, tractor driver and my 19 year old son, currently at university, who helps out during holidays.

The job of a farm manager is multi-faceted and demanding, as it requires hands on work outside alongside a vast amount of paperwork.
However it contains many aspects that are common to all forms of management.

The organisational goals for any manager are to maximise output, while minimising input.
Objective

As a manager, I want to make my farm business as efficient as I possibly can in order to make a profit and create a sustainable future for myself and my staff.
In order to do this I need to stay ahead of my competitors.

Many larger farms employ farm managers alongside the farm owner to run the farm together, however I have the challenging job of undertaking both roles.
Yet, at the same time I feel that this puts me in an ideal location as I am seeing the ownership and management of the farm from both perspectives.

Although my farm is relatively small I still think that it is important to manage the business as thoroughly as a large public company would be managed, for this way every aspect of the farm will be examined and any inefficient areas of production will be highlighted.
There are many different aspects to management, for a start I must look at the role that I myself play in the business, but I also must look at what affects my business, which direction I want my business to travel in, how I work towards achieving business targets and how I manage the financial assets of the business.

These aspects are all of equal importance to me and used together they allow me to manage my business effectively.
My objective in this report is to explain how I have adapted the main functions of management to suit my business and make the most of the resources available to me.

Managerial Roles
As a farm manager I have to take on many different roles, I must plan, organise, lead, control and be responsible for the future of my farm.

That in turn leads me to be responsible for the future of my workforce, to a certain extent.
That puts a lot of pressure on me to make the right decisions.

There are three main types of skills involved in management and these go as follows:-
ConceptualUnderstanding the ways in which farm productivity can be maximised by researching the market at present.Thinking of new ways of moving the farm business forward.

To do this I must study the industry and take note of important changes that could shape my future.Constructing plans of how a new idea can fit into the existing system.Taking responsibility in selecting new ideas.Having the knowledge of staff to apply jobs to those best suited.HumanAble to give a coherent explanation to the workforce which will enable them to have a better understanding of the task.Possessing motivational skills that will cause the workforce to reach maximum output.Meet with key players outside of the farm e.g. Bank manager, to engage support.Gain key information from salesmen e.g. grain traders, machinery representatives.TechnicalUnderstand the machinery used so maximum output and lifespan can be gained from it.An understanding can also enable empathy with the staff carrying out operations.Technical knowledge and ability is important in order to gain respect from the staff.
In other industries there are many different classes of management, but in my position of farm manager I am a general manager.

I have more than one focus as I have to manage, implement and achieve goals.
According to Mintzberg, 1973, a manager fulfils informational, interpersonal and decisional roles.

These roles are all part of the tasks that a manager should complete, but often one role will help to complete more than one task.
Informational Roles

Monitor - Activity within the farm, farming activity in the wider community and economic activity.Disseminator - Receive information from my staff.Spokesperson - Give information to the bank and other outside companies that I feel will benefit the business.
Interpersonal Roles

Figurehead - Invited to attend meetings, may be asked to speak or occasionally interviewed on my experiences.
I am the public face of the farm.Leader - Direct and motivate staff towards their work.Liaison - Discuss information from similar information sources e.g. fellow farm managers, or outside information sources.

Decisional Roles
Entrepreneurial - Decide on new ways to solve existing problems and implement them.Disturbance handler - Deal with machinery/staff problems, find solutions and occasionally this may lead to me having to deal with the public.Resource Allocator - Decide on the most financially rewarding use of staff, machinery, livestock and capital.Negotiator - Formal bargaining with the bank, dealers, traders and reps.

Not all of these roles will carry equal importance for me as a farm manager, but I can guarantee that I will need to employ each of them at some point during my career as a manager.
The roles that I feel carry the most importance to me are that of a monitor, disseminator, leader, entrepreneur, resource allocator and negotiator.

The Manager's Environment
There are three main levels of environment I, as the farm manager must study in order to achieve a successful business and these are the General environment, the Task environment and the Internal environment.

By devoting a large amount of attention to the Internal environment I will achieve immediate success with the business, however, to provide the farm with a long term future I must also closely study the General and Task environments to see how the farm can compete and improve its position in the market.
General Environment

The General environment is also known as the PEST analysis as it studies the Political, Economic, Socio-cultural and Technological factors that affect a business and it goes as follows: - Political - Measures introduced by the Government and the EU such as the Common Agricultural Policy and Single Farm Payment.
These must be obeyed to gain subsidies, although sometimes they can affect productivity.

There is also much legislation concerning employee welfare and environmental protection that has to be complied with and this is both costly and time consuming.
Economic - Not only must I consider the location of my farm within my country but also within the world.

A farm located within a poorer region of either the country or the world will find it harder to compete at a national/global market meaning that they will then be low on finances to purchase more resources.
Thus they are then left further behind in the competition.

Socio-cultural - Public tastes in both the UK and the world are often changing.
I must move with these changes and understand them to keep my customers and stay in business.

Technological - Technology is constantly being developed in the agricultural sector, pushing farming forward and improving efficiency.
As a farm manager I cannot always afford new technology and am sometimes left behind.

Within the General environment also lies the International environment.
As a farm manager, my business not only competes with the national market, but also within the global market.

There may be occurrences abroad that can affect the UK market.
A perfect example of this occurred in the summer of 2006, as most of Europe and Australia was hit by drought bringing their harvest yields down.

This resulted in a greater demand for UK produced grain, so therefore the price offered to British farmers rose.
Task Environment

The main key players to any business lie within the Task environment as is explained below: - Customers - As the farm manager I need to monitor trends of customer preferences to see where the finished product of my business can meet their needs.
This also allows me to distinguish where customers are not being served by their producers and to fill this area in order to prevent customers from leaving me as a producer.

Competitors - My fellow farmers are able to produce the same product as I and some to better value.
If I want my farm to stay in business I have to predict the trends in farming to stay ahead of other famers so that I am able to differentiate my product.

For my product to become preferable, it must stand out in the market, either as value added, or good value.
Suppliers - The supplier sets the price of good sold to farmers.

I must find the cheapest supplier selling the best quality goods to get the most benefit from them.
However, should stocks of certain products become low, the suppliers know that they have the monopoly over the farmer and are able to charge hideously high prices at the farmers loss.

Labour Market - If the labour market is full, I get greater choice over staff at lower wages.
However, if the labour market is poor then staff chosen are of poor quality and at high expense.

This reduces the farm profit.
However this is less of an issue for myself as I already have two very good full time members of staff.

Yet when my son finishes university he will no longer be able to work seasonally for me and I will have to look for another student to fill his void.
Internal Environment

The Internal environment looks at work towards reaching goals and those who can aid the process, as is shown below:- Employees - The happiness of my employees has a big impact on the success of the farm.
A happy employee will work beyond the call of duty to do something that they think will help.

Unhappy employees can be a drag on resources, as not only will their presence be costing money, but also they may not use equipment in an efficient manner.
Culture - I must employ either a directional or ambiguous leadership style depending on where I feel the power balance should lie.

I personally prefer to use a directional style, but as my staff are experienced, I like to give them a little more freedom so that they feel in control.
Management - As the farm manager, I must look carefully at resources, particularly staff.

An understanding of my staff means that I can unite them to work as a team and increase success rates.
I must know their strengths and weaknesses.

I must also understand where other resources are best applied and have valid goals for the future of the farm.
Stakeholders

The traditional view of a stakeholder is "someone who temporarily holds money or property while its owner is being determined" (Wikipedia 2006).
Stakeholders can also be described as "interest groups." Within the farming business there are many stakeholders, the main ones of which are listed below: - Customers - These are people who are affected by the outcome of the business and who can influence it, but are not directly involved with doing any of the work on a day to day basis.

An example of a customer could be the bank should it have to loan the farm money.
Consumers are also part of the customer group as they have the power to sway the farms future.

Employees - These are people who are affected by any outcome taken by the farm.
Manager - I am an individual with an interest in the businesses success in delivering intended results and in maintaining the viability of the business.

Local Agricultural Society/Local NFU Branch - These two groups participate in a community mobilisation effort, representing our particular segment of society, which is farming.
Business Strategy

In order for the farm business to be successful, I must analyse the business in its current state regularly to identify its strengths and weaknesses.
This then allows me to set realistic targets for the business which will produce the most beneficial results.

The setting of targets involves a complicated process of studying, planning and organisation and the success of this relies on good strategic management.
SWOT Analysis

An accurate SWOT analysis is crucial for setting accurate targets that are achievable.
SWOT stands for the Strengths, Weaknesses, Opportunities and Threats of a business and it is a valuable method of identifying the key areas of business that need to be improved.

The strengths and weaknesses are both part of the internal environment of the business, whereas the threats and opportunities are influence by the external environment.
I conducted a SWOT analysis of my farm business and the findings are shown below:- Strengths - The farm is involved in three markets as it produces dairy products, lamb and cereal crops.

This reduces its vulnerability should the demand for one of these three products be reduced.
Staff are kept to a minimum, reducing the financial cost of wages and seasonal labour is guaranteed in my son, who also has the added benefit of already knowing the farm and its mechanics.

With only having two full-time men, I have good experience of how they work and where their skills lie.
This means that I can employ them efficiently and get the best performance out of them.

I am on the farm daily so I understand exactly how my current targets and strategies are working.
This should help me set new targets in the future.

Weaknesses - The farm is relatively small and although it is primarily a dairy farm the number of cows has been reduced so that it no longer specialises in the production of any one product.
If it was specialised towards one area we would find our production costs decreasing for that area.

Employing a small number of employees causes trouble for the farm should one of them become ill, as that leaves a lot, or possibly too much work for the others to take on.
Looking to the future, the seasonal labour that is currently used on the farm will no longer be available once they he has finished University in two years time.

Opportunities - Due to the farm's small size it would be easy for it to improve the quality of produce so that it can sell to a higher paying processor, such as Waitrose or Marks and Spencer.
Alternatively the farm could diversify into organic produce, or make use of other profitable assets (Lantra, 2006).

For example old farm buildings could be converted into offices or stables.
Threats - Because the farm aims to supply produce to three different areas, it does not produce a tremendous volume towards one particular area.

This leaves its position in the market as vulnerable.
It could easily be pushed out of the market by a larger competitor.

For example it is more economical and simplifies the business for a milk company to collect milk from just one large farm, as opposed to several small farms.
Other threats lie within the possibility of changes to government policies towards agricultural production and national laws concerning care of employees.

Demand for British agricultural produce could decrease, resulting in the farm receiving a lower price for its goods.
There could also be major technological advances in agriculture culminating in the farm having to lose a tremendous amount of capital in order to keep up with competitors, or risk being pushed out of the market within a short space of time.

Farm Achievement Goal
Taking into account the information that the SWOT analysis identified, I have formulated a goal for the farm business to achieve over the next two years.

That goal is to ensure its future by maintaining staffing levels, maximising profits from assets and by looking at changes for the future.
I feel that the goal is fully achievable, but will improve the business' efficiency My proposed strategy for the goal is as follows:-

To continue with the production of dairy, lamb and cereal crops, but to raise the quality of these products to a satisfactory level so they are then accepted by higher paying buyers.To look into the initial costs and benefits that would come from converting the farm from commercial to organic.
To renovate existing unoccupied farm buildings into stabling, as a basic livery yard is a cheap and easy way of earning extra money and will at least make the buildings pay for themselves.To find a replacement seasonal worker for when the two years are completed.

In order to implement my strategy, I must look at two different levels of strategy formulation; business level and functional level.
The corporate level is contained in my proposed strategy.

Business Level Strategy Formulation
Differentiation of product - I am researching organic farming, which if suitable for the farm would differentiate my product from that of many other farms, as it would form part of a niche market, making it more valuable.

Cost leadership - The farm will produce goods at a higher quality to a lower input price and will offer horse accommodation at a competitive price go gain more customers, maximising the profit made.
Focus - The farm is producing goods to the higher quality end of the market and also should it be suitable to be farmed organically, it will then be able to focus on the organic market, making it more specialised.

Partnership strategies - The farm may need to work with the bank in order to gain funding for the change to an organic system.
The farm will also need to work with existing organic farmers so that it can gain full research.

Functional Level Strategy Formulation
Marketing - The farm will need to advertise the newly renovated stables in suitable locations e.g. tack shop, feed merchant.

It can also advertise the need for a new seasonal worker at the local agricultural college.
Meetings will have to be organised with higher paying buyers to demonstrate the improved quality of the farm produce.

Production - The farm must study existing imputs into the dairy, lamb and cereal production systems that it already uses and see where efficiency can be improved.
By that I expect to look at how the feeds, housing, fertiliser, etc.

can be improved to meet demands more accurately without increasing costs.
Finance - The goal that I set is designed to require minimum extra finance.

However, in order to renovate the existing farm buildings to stabling, an initial cost will be required.
This will be negotiated with the bank or other shareholders in the business.

Improving the quality of the farm produce should not require any extra money.
Human Resources - The farm must engage the support of the bank, higher paying bankers and existing organic farmers, for research purposes.

The farm must also maintain the loyalty of its staff to increase the chance in success of its goal.
The largest chance of the farm securing a new seasonal worker is by forging links with the local agricultural college.

Research and Development - The farm must research organic systems suitable for its own through existing publications and the experience of farmers who have already changed to organic.
Above I have simply detailed strategy formulation, but I believe that it should not be much different from strategy implementation.

I am responsible for the implementation of the entire goal, but I personally will have to undertake the carrying out of much of the above as both the farmer and the manager.
I feel that I will be more than capable of carrying this out during the two year period ahead.

However the task will then change from strategic formulation to strategic management and I then have to direct resources for strategic outcomes.
Although I am sure of my own dedication to achieving the right outcomes I can only push my employees so far and I must just rely on their support in improving the business.

Motivational Leadership
As the farm manager I am automatically the leader for my employees.

Leadership will always involve a certain aspect of Machiavellianism, for a leader must manipulate others slightly for personal gain, as this maximises the profit of their business.
Manipulation can take various forms, but the most effective manipulation of employees must result in an increase in their motivation to work.

Motivational skills are one of the essential possessions of a good leader, along with an aptitude for communication and the ability to encourage teamwork.
I personally find the use of motivational skills to be very important in my industry, as to get the best out of employees, they have to want to fulfil the task set.

In order to improve my own personal motivational skills I found it useful to study various existing motivational theories.
House's Path-Goal Model

House's Path-Goal Model is based on the belief that the leader's function is to clear the path toward the goal of the group, by meeting the needs of subordinates (Wikipedia, 2006).
In order to achieve this I will guide my employees towards choosing the best path that will enable to job to be completed and it involves my engagement in different types of leadership depending on situation.

The different forms of leadership are summarised below:- Achievement Oriented - I expect the most out of my employees, especially when I consider them to be doing easy tasks, such as the washing off of a tractor.
Directive - I set my employees non-specific tasks, such as the mucking out of a cattle pen.

I expect them to achieve the desired results if I tell them what tools they can use to carry out the task and what the end product of their work should be.
Participative - If an employee isn't completing the task properly, for example they are power harrowing a field and the machine isn't cultivating the field to the correct depth, I ask for their opinion on the problem and together we solve it.

Supportive - If an employee is unconfident about the task that I have set them, for example running the drier during harvest, I use a friendly and approachable attitude towards them.
The sum of these different forms of leadership is that I have to be flexible in the way in which I deal with my employees.

Different situations call for different managerial styles.
Vroom and Yetton's Normative Model

Vroom and Yetton's Normative Model is based on the principal that there are 5 different decision procedures involved in leadership: 2 are autocratic, 2 are consultative and 1 is group based (Changing Minds, 2006).
By autocratic I mean that I am solely responsible for the decision making procedure, by consultative I mean that I discuss the problem with others, but ultimately still make the decision myself and by group based I mean that I share the problem with others and we make the decision together.

The first autocratic decision procedure is based on me taking known information and making the decision alone.
It is very similar to the second autocratic decision procedure, which only differs in the fact that I take the information on which I base my decision from my employees.

I would use the first autocratic procedure in situations where I am confident that my knowledge and use of it is better than that of my employees, for example when selling corn to a grain trader.
The second autocratic procedure is better for use in situations where I feel my employees may have a useful perspective on the situation, but due to my position as farm manager, I must ultimately make the decision.

This procedure may be called for when deciding on the cattle feed rations, or other such situations.
The first consultative decision procedure is based on me sharing problems with my employees individually and then deciding alone.

The second consultative decision procedure is different in that I share problems with my employees as a group.
I use the first consultative procedure in situations where the information that my employees give me may include personal opinions, such as the negotiation of wages.

I use the second consultative procedure to solve problems that I feel require group input, such as the allocation of jobs, as I feel that it is important to hear from my employees on which jobs they feel best suited too, or whether they have particular objections to certain tasks.
The group decision procedure is based on me not having sufficient knowledge, so I share the problem with my employees and encourage them to decide with me.

I personally have never used this method as I feel that as the farm manager it should be my responsibility to make the final decision.
Alderfer's ERG Theory

Alderfer's ERG Theory is an expansion of Maslow's hierarchy of needs, where ERG stands for Existence, Relatedness and Growth needs.
In order to keep my employees happy and motivated in their work it is important for me to ensure that all three needs are met.

My employees existence needs are satisfied by acknowledging their presence and importance in the workplace.
Sometimes this involves my complimenting their work and reassuring them that they are doing a good job.

My employees relatedness needs are satisfied by their having friendship and company and this has been easily accomplished by giving them fellow employees.
To the best of my knowledge they enjoy each others company.

My employees growth needs can be satisfied by aiding their personal development.
All of my employees have had to attend various courses and complete tests, such as the Sprayer Operative test, at different stages in their working lives and this will have enriched their progress as an individual.

McClelland's Acquired Needs Theory
McClelland's Acquired Needs Theory is based around the theory that people in general fall into three different categories; those that need achievement, those that need affiliation and those that need power (Business Knowledge Centre, 2006).

Employees with a high need for achievement seek to excel in general.
They avoid low risk situations with no danger or chance of failure as these are seen as not being genuine achievement.

They also need regular feedback to monitor progress and they prefer to work alone, or with other high achievers.
I personally prefer employees with a high need for achievement because I feel that they produce the best results in the working environment of the farm.

The farm is best suited to confident staff as it offers employees the chance to take on high levels of responsibility and manage tasks individually so they can see their own results.
Employees with a high need for affiliation like to feel accepted.

They wish to conform to the norms of a work group and prefer work that provides significant personal interaction.
I don't feel that this group of employees are particularly suited to working on a farm as farm staff often have to spend many hours working alone, especially when working arable land.

However, employees with a high need for affiliation can sometimes enjoy working with stock as the company of the cattle is a slight substititute to that of humans.
Also working with stock generally requires the resources of more than one employee.

I am not particularly keen on employees with a high need for power and they certainly would not fit in with working on my farm.
Employees with a high need for power need to be put in managerial positions to get the best results and as I am the farm manager on my farm it really would not work as I could not put them in the position that they would enjoy most.

Herzberg's Two-Factor Theory
The Two-Factors involved in Herzberg's theory are those that lead to dissatisfaction and those that lead to satisfaction.

The factors that lead to dissatisfaction are company policy, supervision, relationship with myself, work conditions, salary and relationship with fellow workers.
Most of these factors should be at least satisfactory, but even the provision of these will not make staff happy and will only motivate them extrinsically.

To make my employees happy in their work and motivated to complete it I must ensure that they are fulfilled towards the factors that lead to satisfaction.
These factors are achievement, recognition, work itself, responsibility, advancement and growth.

Because the factors leading to satisfication are not just opposites of those leading to dissatisfication, I have to work especially hard to make sure that my employees feel secure in both areas.
Financial and Budgetary Analysis

As the farm manager it is important that I take full control of the farm accounts and employ a full analysis, as this is yet another area in which I can discover new methods of making the farm more efficient.
Although I may already have suspicions over which areas of my business are leading to profit and which are losing money, a thorough financial analysis allows me to accurately determine which areas these may be and I can then make changes to my business to either continue efficiency, or improve it.

I require both a financial and a budgetary analysis to identify the areas that need change and then calculate how this could be achieved.
The financial analysis is the first step in this process.

Financial Analysis
The first task for me when conducting a financial analysis is assess the farms goals in performance, solvency (whether or not a bank or supplier can safely grant credit) and potential value (whether an investment will be positive).

The main way in which I do this is by comparing financial ratios, but first I have to calculate them.
There are three main financial ratios that I study and these are that of return on equity, return on assets and Price to Earnings ratio (Wikipedia, 2006).

They are calculated in the following ways: - FORMULA FORMULA FORMULA The figures I use are taken from my annual financial balance sheet.
The balance sheet includes the physical resources that the business has, such as tractors and also the financial assets that the business has gained, such as those from share holders.

When comparing the farms financial ratios I can do this in a variety of different ways.
The first method is to compare the financial ratios across historical time periods, such as 5 years to see how the business has developed, or regressed against itself.

I then compare the ratios to the forecasts of the business' anticipated performance to see if these are still attainable.
The next step is to compare my financial ratios to those of similar business'.

It is easiest to compare them to neighbouring farms, preferably those of a similar size, as these will have to deal with the same limiting factors as myself and therefore we should get similar results.
However, the comparison of financial ratios is not always completely accurate, for they fail to account for consumer behaviour and as this differs, it can lead to anomalous results in some ratios.

Also there is also the problem that seasonal factors can influence results, so I find it more useful to use averages, as this lessens the effect.
Another form of improving accuracy when conducting the comparison is to combine several related ratios to form a more comprehensive picture.

The use of a financial analysis only demonstrates to me how my business has performed in the past year and how it is proceeding in terms of targets.
This does not help me tremendously in deciding how I will conduct my business in the coming year.

In order to determine this I must employ a budgetary analysis.
Budgetary Analysis

By conducting a budgetary analysis, I am able to determine how to efficiently distribute my limited financial resources (Bureau of Labour Statistics, 2005).
Through my role as farm manager, one of the areas of the farm that I am continually working to increase is that of profits in general and my budgetary analysis should assist this.

The main methods in which I carry out a budgetary analysis are explained below: - Preparation of the annual budget - This will include proposed funding for the coming year, any new initiatives that I wish to put into practice, estimated costs and expenses for the year and capital expenditures that need to be invested in.
Examination of the budget for conformance with organisational objectives - The budget must conform with organisational objectives for the farm to achieve its mission statement.

Employment of cost-benefit analysis to examine alternative funding incase this proves to have more financial gains.
Following on from these main methods, which are part of the initial review, I will also look at departmental budgets, such as those given to my dairy unit, or to my arable unit and I will then consolidate these into operational and capital budget summaries.

Throughout the year I also periodically monitor the budget every 3 months and analyse deviation.
If deviation occurs I may have to implement new, or revised budget procedures.

Should I wish to diagnose the farm business' financial condition, I would probably implement a financial analysis, but that would only illustrate fully, the business' situation when coupled with a budgetary analysis, as that puts the amount of money available to be invested within the business into context.
In addition to financial and budgetary analysis' there are various other methods available to me to use for measuring and controlling the organisational performance of the farm, the main ones of which are by analysis of the Cash Flow Statement, the Profit and Loss Statement and by looking at Share values.

Cash Flow Statement - Cash Flow Statement states where the cash received has come from and what the business has spent it on.
Ideally the cash received should be greater than the amount of cash spent in order for the business to be efficient.

Should the farm be proved as efficient, then my intention would be to reinvest the cash surplus into the farm again, hopefully to double the cash surplus produced in the next financial year.
Profit and Loss Statement - Profit and Loss Statement is based on business transactions.

It is particularly useful as it shows the profit I made following taxation.
When the cost of goods is deducted from the profit, the purchase of goods that need to be made in the next financial year are also deducted to give a more realistic view of the finances that will be available to me.

In addition to this, depreciation is calculated.
Share Values - The analysis of Share Values is relatively simplistic.

However, it can only be conducted if the business sells shares on the internet.
The change in price of the share of the business is a very good indication of how well the business is doing at that point, but the share price is very sensitive to change to does not give a fair view of how the business is doing over the course of a year.

Between the above three methods, I would find the Profit and Loss Statement the most useful, for the Cash Flow Statement is too simplistic and would be easily distorted by trivial matters.
Share Values certainly have no use to me, for my business is not large enough to sell shares for it over the internet.

Therefore that leaves the Profit and Loss Statement.
I cannot find any major problems with this method that would prevent me from employing it, however I feel that the financial and budgetary analysis' are the best two methods for a business, such as my own to use, for they work so well together.

Summary
A farm business is possibly not the easiest business to apply management theories to, for it is ever changing and no two jobs are ever the same.

That is why it is of high importance that I review my management strategy frequently and that it remains flexible in the mean time to allow for these alterations.
Although I am the farm manager, I find that there are many situations where I value the opinion of my employees.

All three of my employees have worked for me for such a long period of time that I believe that in some cases they know more than me about the land and the livestock that they are working with.
This is mainly a result of the increase in paperwork that has meant I now spend more time in the office than I do in the field.

However, this has meant that I now feel able to participate in a more consultative management style and I think that this has benefited the team.
Currently there are many farmers leaving agriculture.

One main reason for this is their inability to keep up with the changes in technology and consumer demand.
To me, conducting a SWOT analysis is of particular importance because of this.

I am also glad that I take the time to look at the external environment of my business.
Because of this I am able to keep abreast of the changes that occur and plan how I will my business to fit in around them.

I am particularly pleased with the goal that I have set the farm business to achieve, as I feel that it is attainable and having studied the current trends in consumerism, I feel it should guarantee my business a place in the future agricultural market.
Although there are big management tasks that must be completed, such as the setting of annual targets, the small tasks are of equal importance.

As manager I must support my team on a daily basis to ensure that they are motivated and to check that they are conducting tasks as I expect them too.
For although I trust their judgement and their ability, I cannot afford to have any slight weaknesses in my business.

Even if I had the resources to employ a separate farm manager, I don't know if I would wish to as I prefer to know myself, for sure how my business is working in terms of resources, production and competition.
If the farm is working efficiently towards all three of these goals then I am employing good management.

Manydown is a large mixed farm made up of 12 farmsteads surrounding the village of Wootton St Lawrence, near Basingstoke.
The farm is owned by the Oliver-Bellasis family, but they employ a farm director, Richard Stirling, to see to the day to day running of the farm.

In addition to the production of of meat and arable products, the farm also owns 83 private dwelling houses.
The rent from these buildings pays for the maintenance of the estate, but a considerable profit has been made from converting buildings into accommodation.

The farm was initially purchased in 1871 and has survived three "Farming Depressions," but Manydown has now diversified and developed its own Farm Shop in order to improve the profit made from its meat.
My main aim in this report will be to highlight the changes that Manydown has made over the years to gurantee the future of their business.

Improvements in Efficiency
One method of securing the future of a farm business is to expand in size and since 1994 Manydown has increased its arable land by 1, 045 acres and almost doubled the number of cattle finished per year.

This decision by the farm should stand them in good stead at a time when the UK Government is pushing farmers towards entering more competitive markets (Defra, 2005).
Manydown has also increased the range of produce yielded by the farm, as under the current farm director, pigs, sheep and chickens have been introduced to the farm.

Within the pig unit only 6 sows and 2 boars are kept from breeding.
I personally believe that to maximise profit made from use of the boars, more sows would be needed.

Manydown's argument against this is that should they keep more pigs than they currently do, they would be seen as an intensive pig unit by the public and this is not the image that they wish to put across.
Each of the sows at Manydown has two litters of 10 piglets per year, this is an average production rate compared to the rest of the UK (Nix, J., 2006).

Although Manydown has increased its acreage farmed, it has decreased its number of staff at the same time, which means that the men still employed often have to specialise in more than one area of farming.
For many years Manydown supported its own team of builders to maintain the buildings owned by the estate, but recently it had to terminate the builders' contract as they had become too much of an extravagance to spend finances on.

The farm also reduced its number of gamekeepers from 3 down to 1.
Yet despite this the farm does still employ its own carpenter and decorator to assist in the maintenance of the estate.

I am not convinced that this is an effective use of resources, for although the accommodation side of the estate brings in a large amount of money, I think it could still be more efficient for the farm to employ outside workers to complete these jobs only when necessary.
Satisfying the Consumer

Manydown opened its own farm shop in 1994, however, their farm director believes that it would no longer be possible for a farm business to open its own shop with the resources that they had available to them in 1994, such are the changes in agriculture that have occurred since then.
Manydown's farm shop has been very successful and has led to the estimated annual turnover of the business as a whole as being £1 million by 2008.

One of the reasons why Manydown made the decision to open the own shop was as a method of marketing their beef produce.
By processing and selling the meat themselves, Manydown was able to cut out many of the areas of food production that reduce the profit of farmers so drastically.

In 2005 the amount of money spent by consumers on food was £112 billion, whereas farmers only received £14 billion for that.
The rest of the money was lost into the retail and food service industry (Ashbridge, I., 2006).

These are precisely the losses that Manydown are wishing to eliminate.
Manydown does not produce organic meat, but the meat that it does produce is very high quality free-range.

Indeed it is fully accredited by RSPCA Freedom Foods.
Because of this, Manydown's produce should appeal to customers who want to eat ethically, but who cannot afford the high costs of most organic produce and this is currently seen as being a large number of consumers (Just Food, 2006).

Therefore Manydown has accessed a consumer market that is relatively uncatered for in the UK food industry.
Another bonus for Manydown is the location of its farm shop.

Situated directly next to a main road, the farm shop is in an ideal location for attracting customers.
The site of the farm shop also has an old barn built on it which has been renovated and rented out as part of Manydown's diversification.

The barn has been rented by a smart gym company and this may serve as another advantage for Manydown, as its customers will be interested in eating good quality, healthy food and so therefore should prefer the meat produced by Manydown.
However, Manydown has already expanded the building that they currently use as their shop and if their business continues to grow they will run out of space, as it is not particularly big.

This may lead to the problem for Manydown, of whether to extend the building again, or find a new plot.
Finding a new plot may result in the loss of customers, due to new location and would also require a considerable investment.

Conclusion
In conclusion I consider Manydown to be an efficient business.

The farm has experimented with the production of livestock and gained very positive results.
By marketing their own produce in the shop, the farm has identified the key area in which UK farmers are losing out most drastically on profit and should be able to prevent that for themselves.

In addition to this, it has made its enterprise more efficient, due to staff cuts, and it also has sustainability, due to the rent it receives on buildings.
Although the money the farm receives due to rent will not alter, regardless to the changes in farming, it will not be enough to support the farm should there be a severe problem.

The farm shop is a major asset for Manydown, but for it to maximise profit and security still further it needs to find new ways of enlarging and developing this business.
Throughout the many breeds of horse in the world today, the nobility of the Cleveland Bay horse highlights the injustice in its status as a rare breed.

According the 2006 Watchlist (Rare Breeds Survival Trust) the Cleveland Bay is in category 1, which means that its survival is critical.
To those who know the breed well, this must come as a surprise.

The Cleveland Bay is a very versatile, strong horse which can be used in a variety of different ways.
It also has a tremendous amount of history, which should not be ignored.

As a measure of the value of this horse, the Patron of the Cleveland Bay Society is Her Majesty The Queen.
In order to understand The Queen's support for this breed we should learn more about this most worthy of breeds.

History
The Cleveland Bay, as the name suggests comes from an area called Cleveland along the north eastern coastline of Great Britain.

The Cleveland Bay is the oldest breed of horse to be bred in Great Britain.
The first compilation of the Cleveland Bay Horse Society stud book was produced at the end of the 19 th century, however it is believed that the breed originated long before this time.

During the Middle Ages it is thought that the initial Cleveland Bay horses were bred in monasteries, to be used as pack horses between the abbeys and monasteries.
At this point the wider community realised the talent of the horse in this area and so it started to be used by many other people.

These horses that were primarily used as pack horses were known as Chapman horses.
Chapmen were packmen seen in the Yorkshire Dales at the time.

The Cleveland Bay horses of today were bred when Chapman mares were paired with Barb stallions imported into the harbour town of Whitby.
This produced a larger horse which could then be used to meet agricultural needs.

Further development of the Cleveland Bay breed led to another sort known as the Yorkshire Coach Horse.
The Yorkshire Coach Horse was produced to meet a demand for faster coach horses.

The Cleveland Bay was crossed with Thoroughbred blood making it possibly the most desirable horse at the time.
The combination of the Thoroughbred's speed and elegance with the Cleveland Bay's strength and power meant that it was not long before Yorkshire Coach Horses were seen throughout London, pulling carriages and many were even exported abroad.

However, with the era of mechanisation, the place of the Cleveland Bay was lost People turned to tractors and cars to supply the power they needed instead of horses.
Unlike the Thoroughbred, the Cleveland Bay could not be raced and it did not have the widespread appeal of the Shire, which has been kept by many as a pet.

Slowly the numbers of Cleveland Bays fell until it reached the level that it is at now.
A recent estimate has put its numbers at less than 500 worldwide (Cleveland Bay Horse Society of North America).

The Future
As a breed the Cleveland Bay has an enormous amount of versatility.

At the moment it is mainly used as a driving horse, police horse and as a hunter and it is very successful at each of these occupations.
Yet it is also thought that as an event horse, the Cleveland Bay will come into its own and will reach its full potential, at which point it can be commercially exploited.

An indication of this has already been noted, as one of the breeds pure bred stallions has already been tested by the National Stallion Association in the disciplines of dressage, showjumping and cross country and it scored very highly.
Both the physical and mental attributes of the Cleveland Bay will contribute towards its certain success in the field of eventing.

Some of the key areas in which the Cleveland Bay excels are listed below: - Long life-span lasting up to an average age of 20 will help it to gain experience whilst still being competed.
Strong, muscular body with plenty of bone should ensure that it has the endurance to last the 3 days of hard competition.

Bold and honest instinct means that it will follow the riders instructions without fear.
Intelligent and sensible nature confirms that the horse is unlikely to get itself into trouble, but should that happen it will be able escape from it again.

Hoofs of only blue horn give its feet strength and the famous saying of "no hoof, no horse" indicates the importance of this quality.
Many of the most successful horses in eventing at the moment are European Warmbloods and many of them contain some Cleveland blood.

The horse has the height, the intelligence and the temperament to flourish in eventing.
Eventing is a difficult sport, but for those bold enough to try it on a Cleveland Bay, the rewards will be many.

The Cleveland Bay is already described as "a valuable horse" (Cleveland Bay Horse Society) and its worth will be vastly improved after its success in the field of eventing.
Eventing horses are worth many thousands of pounds and are very much in demand.

Although a great many successful British event riders at the moment are using another great British breed, the Thoroughbred, as their mounts, surely it would give the British public greater pride to see them winning on the oldest of British breeds.
So far there are no conservation measures in place to protect the Cleveland Bay, apart from the obvious propaganda of the breed supporters club.

Yet no measures will need to be imparted, should the full potential of this breed be realised.
If the public learns to value the Cleveland Bay as a serious competition horse, the future of the breed will be safe.

In conclusion, it would be a national tragedy should the Cleveland Bay not be conserved.
The mere fact that Her Majesty The Queen stands as the patron of the breed society demonstrates how important the breed is to our country.

The Cleveland Bay Society states that "the Cleveland Bay is our heritage" and they are quite correct.
The Cleveland Bay is a breed that was highly important to our ancestors and surely it should be important to us too.

We cannot afford to lose the brilliant genes that lie within this prestigious breed.
Recently the population of Wild Boar in the UK has risen significantly; this is due in parts to the reintroduction of a wild community and also due to increasing consumer interest in the meat.

Historically, Wild Boar have been a native species to our country, but currently there is controversy whether it is good for them to be present again.
Although the greatest numbers of Wild Boar remain on farms and therefore in captivity, there is always the danger that they could escape, or be released.

Surely Wild Boar are not called "Wild" for nothing.
My objective in writing this report is to study whether the production of Wild Boar in the UK is a positive thing and also to investigate whether it has a sustainable future or not.

The Wild Boar Itself
The Wild Boar has a very similar appearance to that of the domestic pig.

The Wild Boar grows to a maximum height of 1m and has a thick brown, bristly coat with a ridge of longer hair along the spine.
They have a long snout, which assists them during rooting and after 2 years they grow tusks.

Male Wild Boar grow two sets, one from the top incisor and the other from the lower incisor, whereas female Wild Boar only grow one set, from the lower incisor.
However, it is this set which is the sharp dangerous set and so is of more use to the Boar.

The Wild Boar are seasonal breeders mate through October and November, as their breeding pattern is triggered by decreasing day length.
Farrowing occurs in April.

Litter size is typically 4-6 piglets, but can be larger in captivity.
However, their seasonal breeding pattern can be a problem when farmed as it means that the sows can only produce 1 litter per year (British Wild Boar, 2006) and this is seen as inefficient.

If sows are kept indoors all year round, they can be induced to believe that the seasons are changing by careful control of lighting.
However this form of high input system is not viewed favourably by the public.

Wild Boar typically feed on plant matter, such as roots, bulbs, tubers and fruit and this accounts for 90% of their diet.
Animal matter takes up the remaining 10% in the form of mice, birds, eggs, worms and carion (British Wild Boar, 2006).

To obtain the majority of the plant matter, the Boar must root.
This can damage the ground and although this is not a major issue for those Boar kept within captivity, it can cause disagreements where Boar kept in the wild root on agricultural land.

Throughout the rest of Europe, there are Government based compensation schemes in place for damage caused to farmland, however this is not the case in the UK.
Methods of Production

The growth of Wild Boar is very fast, which makes them an economically feasible animal to produce.
Wild Boar can either be produced in an extensive, free-range system, or produced in an intensive, semi-housed system.

For both systems the farmer must hold a Wild Animal Licence, which costs between £100 - £150 per annum and depends on the number of Wild Boar held on the premises (Basildon Council, 2006).
The Wild Animal Licence has been part of the law since the Dangerous Wild Animals Act of 1976.

For Wild Boar produced in an extensive, free-range system, their only requirements are that they have enough land to roam and give them nutritional satisfication.
This can be quite a very large area, as in their natural wild habitat they enjoy moving over a km per day (British Wild Boar, 2006).

The area given over to the Boar must contain wood and scrubland as the Boar, in particular the females, enjoy the security that thick cover gives them.
Also during the farrowing period, the females form nests for themselves that they line with leaves.

A large benefit of employing the extensive, free-range system is that the amount of labour required will be lowered significantly.
Indeed 1 man per 70 sows is needed in an extensive system, compared to 1 man per 60 sows in an intensive system (Scottish Agricultural College, 2006).

Employing an intensive, semi-housed system for the production of Wild Boar gives much faster levels of production.
The Wild Boar are not given the same area of land to cover as those in an extensive system have and normally they have a lot less tree cover, however they are given access to indoor housing and so therefore can make the choice between spending time inside or outside.

It is possible to keep Wild Boar completely indoors, but at the moment there is very little consumer interest in this product (Harmony Herd, 2006).
When kept in an intensive system, Wild Boar sows are often fed on a mix of cereals and ad lib vegetables.

The young are weaned onto ad lib vegetables with a specialist grower ration.
When keeping Wild Boar in an intensive system, care has to be taken in the formation of each group of Boar kept together.

A breeding group can comprise of 5-20 sows and 1 mature male, but the addition of new sows can cause fighting.
Also it can sometimes be wise to castrate mature males if they are to be kept together, because sometimes they can fight and cause fatal injuries.

One aspect of the production of Wild Boar that is equal to both systems is the provision of adequate fencing.
In order for a farmer to acquire a Wild Animal Licence, the fencing of his land must reach a certain standard (Basildon Council, 2006).

Extra high tensile fencing must be used and must stand 1.8m heigh and 0.5m below ground level.
It is important for the fencing to stretch below ground, as the Boar like to root and can dig fencing up.

The fencing must be electrified and gates and access areas padlocked.
In terms of the financial returns made from the production of Wild Boar, these are often similar between both intensive and extensive production methods.

This is because although the intensive system will have a higher rate of production per year, its inputs are higher.
The extensive system will have a lower rate of production per year, but its finished product should reach a greater price due to the higher welfare experienced by the Boar before slaughter.

In many cases Wild Boar produced in extensive systems are categorised as organic (Harmony Herd, 2006) and this is very popular with the consumer.
Wild Boar are protected under the UK Biodiversity Action Plan and they agree with the extensive farming of Wild Boar.

Uses of Wild Boar
The ultimate use of Wild Boar is as meat, but there are two methods of reaching that point: by direct slaughter at an abattoir, or through hunting.

The hunting of Wild Boar is a sport that is increasing in popularity.
Before the numbers of Wild Boar in the UK began its recent climb, many keen hunters were travelling over to Europe to enjoy this pastime, however it is now possible to continue with it in England.

It is mainly Wild Boar raised in extensive systems that can be used for hunting, as those kept in intensive systems become diurnal and lose their natural wariness of humans.
Therefore hunting becomes easy and animal rights activists argue cruel, for the animal has lost his fight or flight instincts (League Against Cruel Sports, 2006).

Also many huntsmen enjoy hunting Wild Boar because of the thrill they get from arousing the Boar's aggressive nature.
Yet the future of Wild Boar hunting could be in jeopardy, as the League Against Cruel Sports and other such organisations are unhappy about its continuance and the banning of foxhunting with dogs may be seen as a warning sign.

However, the use of Wild Boar simply for meat has a definite future.
The production of Wild Boar meat currently occupies a niche market, but as long this market is not flooded by more farmers diversifying into Wild Boar it should be sustainable.

The meat of Wild Boar is a deep red, similar to the colour of venison and it has a gamey flavour.
Male Wild Boars are generally slaughtered up to the age of 2, as after that the flavour of their meat becomes too strong, however sows can be slaughtered up to the age of 8, which enables them to live a productive life up to that age, increasing the amount of profit that can be made from them.

Although the popularity of Wild Boar meat is relatively small at the moment, the market is currently worth £2 million per year (British Wild Boar Association, 2006).
One particular worry for the future of the Wild Boar's marketability is the possibility of an introduction of contraceptives into the wild herd of Wild Boar.

At the moment the UK Government is worried about the ever increasing size of the wild herd and wishes to maintain it at a sensible population density.
Therefore it is considering using contraceptives to prevent further growth and is currently running trials on how this would work (DEFRA, 2005).

The main fear for Wild Boar producers is what would happen should meat contaminated with contraceptive enter the food chain.
This could happen very easily, particularly with unregulated hunting often occurring, but would have a big result on the industry.

It is currently unknown whether the Wild Boar contraceptive would have an effect on humans, but if it should then consumer confidence in the product would drop enormously.
This is just one of the reasons behind the British Wild Boar Association launching its own quality assurance scheme.

Dangers of Production
One of the main problems of production is whether it is actually cost effective or not.

Certainly the gross margin ascertained from the production of Wild Boar is higher than that of low input domestic pig production, but it still remains lower than the gross margin that can be made from high input domestic pig production.
The gross margin of Wild Boar production is £355 per sow, whereas it is £381 per sow for high input pig production (Nix, J. 2006).

That is not the only cost implication of Wild Boar production either.
There is also the high insurance prices that are brought on by the animal's dangerous tendencies.

Many Wild Boar farmers fence their animals correctly, however the animals can still escape.
Sometimes the electric fencing may short and there are also reports of animal welfare activists cutting through fencing, as they believe that Wild Boar should be left to roam wild, instead of being limited by fencing.

During their escape these Boar may cause damage, either to people or property and even though it is not always the farmers fault, the price must be picked up through the farmers' insurance.
Wild Boar are also carriers of Bovine TB, Foot and Mouth Disease and Swine Fever (British Wild Boar, 2006).

Many of the UK veterinarians are very worried about the results that might emerge due to contact between Wild Boar and domestic livestock.
A Wild Boar can smell a domestic sow in heat from a great distance and will try its utmost to get to it, often breaking through fences.

This could lead to cross contamination, but also once having served the sow it will attack the domestic boar, leaving it badly, or fatally injured.
Attacks such as these are not particularly common, but they could become so, should the UK population of Wild Boar continue its rise.

A main factor in the production of Wild Boar that is often overlooked is that of stress.
Wild Boar have to be managed very carefully to avoid irritating them and provoking an attack.

This is particularly important during the breeding and farrowing period, which is when the farmer may wish to keep a closer eye on his stock.
It can not be easy looking after an animal that has the potential to seriously injure, or possibly even kill you.

Conclusion
On the surface it would appear that the production of Wild Boar in the UK is a positive thing, as historically, Wild Boar were a native species to our country, however, our country has changed dramatically since then and Wild Boar no longer fit in as we would like them to.

It would be nice to alter our farming to more ecologically friendly methods instead of the high input, high output ones that we are currently using, but the worry is that we would be unable to compete with the world food production market if we changed our methods.
The production of Wild Boar is not terribly efficient and only commands a small level of consumer interest.

This; coupled with the potential damage it could cause to land and humans, means that as an industry it is unlikely to grow enormously in the near future, due to the risk factor.
However, it should definitely be sustainable if kept at the levels of today, for it is not taking anything away from the land and it should maintain its market.

Chickens, quail and other domesticated birds continue to breed even if there is no male bird or mating activity present.
Chickens in particular also do not have a specific breeding season and generally keep on laying eggs all year round.

The continuous laying season is due to selection as the genetic trait has been a much-valued asset of any bird and to any stockman.
However, there are exceptions such as the Sumatras chicken, which only lays in spring and summer and the goose which only lays two clutches of eggs a year.

In the wild once a bird has laid its eggs and it wants to reproduce they must become broody so their eggs will be kept at the correct temperature and looked after in the appropriate manner.
The broodiness quality is also a genetic one, which is being bred out of domesticated birds.

However, there is no real substitute to a broody mother when rearing chicks.
When a hen is born it will already contain the maximum number of rudimentary eggs it can lay in its ovaries.

An egg is made up of the following components;
56% albumen or 'the white bit'32% yolk or 'the orange bit'12% shell

The diagram below shows the labelled inside of a typical poultry egg; If a hen is encouraged to lay more eggs, for example using artificial light to stimulate the summer season, it will still only be able to lay its set amount in its lifetime.
Chickens usually start to lay when they are five or six months old, at this time the bird is referred to as at point of lay.

Ducks and geese also begins to lay at about five or six months while Turkeys begin a bit earlier, however Turkeys only lay between ninety to one hundred and fifty eggs a year while Geese will only lay forty to seventy eggs a year.
Chickens and ducks are the most productive birds and can lay three hundreds egg a year.

When a hen starts laying her comb will become brighter and redder in colour than pink and she will begin to 'talk,' this refers to the mutterings the bird makes when laying an egg.
Male birds will also mature at the same time and will also be able to reproduce, as they will now be fertile.

However, a hen will not be mature enough to look after chicks until she is at least a year old.
Chickens will lay the most amount of eggs during their first year of lay, usually around three hundred eggs, and the amount will decline during the next seasons until it plateaus at around five or six years of age.

Ducks and chickens will both lay around 1000 eggs in their lifetimes while Turkeys and Geese will only lay around five hundred eggs in total.
Hormones control the release and production of the egg as well as controlling the hens' broody behaviour.

Hormones are released from the pituitary gland, which is located at the base of the brain.
Nervous stimulation causes the pituitary gland to release hormones into the blood stream.

The two hormones, which control the production and release of eggs, are Follicle Stimulating Hormone (FSH) and Luteinising Hormone (LH).
FSH stimulates the ovaries to enlarge and form egg yolks as well as secreting oestrogen to enlarge the oviduct and allowing it to become functional.

The egg follicles mature when LH is released and the follicles are released from the ovary.
LH also stimulates the release of progesterone, which allows the oviduct to release the albumen and the shell to complete the egg.

The levels of Calcium, Phosphorous and proteins also increase when LH is released into the blood stream.
The Gonadrophic hormones control the broodiness of a bird and Prolactin is the most significant.

Prolactin is released when a hen has laid a clutch of eggs that she would be able to hatch and makes the hen become broody.
Broodiness is the time in which a hen sits and incubates her eggs and broods her young, this is known as the 'broody' period.

Naturally a bird becomes broody when it has laid and stored the maximum clutch size for her breed.
Hens stop laying eggs and their ovaries regress during the broody period and they will also become aggressive around the nest.

A broody bird will stay broody for as long as it would take to incubate their eggs and will not lay any eggs in that time.
Broodiness is not common to all birds and due to artificial incubators in is no longer necessary in poultry.

The most persistent brooders are the Silkie chickens and the most restless are the Leghorn and Rhode Island Red chickens, the Leghorn and Rhode Island Red birds are mainly used for egg production so broodiness is an unwanted character trait.
A bird will also become broody after a period of high egg production.

Levels of a Prolactin increase and when they reach over a certain threshold broody behaviour will take place.
For the small holder a broody hen is a positive feature as small amounts of eggs can be hatched with little effort from the keeper.

The hen maintains the temperature and turning of her eggs and also rears the young.
It is impossible to make a hen broody, even though extensive trials have been carried out, by giving different feeds and hormones to a bird they make no difference.

However, for a large scale producer broody hens are a nuisance as they stop laying eggs and can only incubate small amounts of eggs.
A large-scale producer will tend to use an artificial incubator to rear chicks.

As broodiness is not seen as a good thing in large-scale production it is being bred out, as broodiness is hereditary.
Producers therefore have come up with ways to discourage the broodiness, this is known as 'breaking up' a broody hen.

The easiest preventative is to remove the eggs from the hen, as hens will become broody after laying a clutch of eggs to rear.
Putting birds in uncomfortable housing, for example, wired floored cages with drafts, also mean a bird is less likely to sit and brood.

Other physical solutions are to introduce another bird and put them together in a coop without eggs and to make sure food and water are clearly available as less persistent layers may be tempted off the nest with no trouble.
Placing a 'clutch' of ice cubes underneath a broody hen should cool her off and persuade her to leave the nest although two 'clutches' may be needed for the more relentless brooder.

For the most determined brooder gonadotropic hormones can be injected which stop the bird becoming broody quickly but does not quicken the return to egg laying.
These hormones inhibit the production Prolactin, which makes the hen broody.

Other than broodiness there are a number of factors, which stop egg production.
Photorefractoriness is the changes in day length, which stop the hen's egg laying pattern.

This is due to a decrease in Luteinising hormone as the amount of daylight decreases, which can stop the bird from laying for weeks or months.
The day length can be controlled if the birds are kept inside and this can lead to year round egg production.

The other main factor is moulting.
During the moulting season, which runs from July to October, the levels of gonadotropic hormones change and the oviduct and ovaries regress as the bird loses and re-grows new feathers, this also stops egg laying.

The tissues in the reproductive system are able to regenerate and egg production reduces and then stops for about ten days.
To quicken up a moult the birds can be kept in a hot, dark house and are often starved, however, this method is unethical and can cause the birds damage if they are deprived of food and water.

Although the broody hen is the most effective way of rearing chicks it is not the most cost worthy or easiest if producing large amounts of stock.
Artificial incubators come in many different forms and sizes; some hatcheries have rooms of incubators in the form of incubation trays.

Whatever the size the incubator needs to have a constant temperature and a turning system as well as some form of moisture, as the eggs will lose 12% of their total weight during the incubation period due to water lost.
The incubator also needs to have some form of ventilation so air can flow freely over the eggs.

These photographs demonstrate the difference in incubator size; Eggs can be incubated up to two weeks after they have been laid as long as they are kept between 45 and 65 degrees Fahrenheit.
90% of fertile eggs that are incubated by hens or artificially will hatch successfully.

The are many factors which control the hatchability of the eggs these include; the age of the parent stock, inheritance, infections, nutrition of the parents and correct turning and temperature of the eggs.
Eggs that are stored before incubation are also more likely to hatch late or not at all.

Domestic fowl take 21 days to hatch eggs while Ducks and Turkeys take 28 days and Geese take 30 days due to the size of the chicks at birth.
The quail's small frame means it only takes 16 days to hatch their eggs.

37.5 degrees Celsius is the optimum temperature for hatching eggs and if the temperature falls below 32 degrees Celsius it is fatal.
Eggs can be incubated at high altitudes however the higher the altitude the less chance of hatchability.

Hens that have been acclimatized at 3800 metres above sea level for generations still have trouble reproducing.
This is due to the oxygen tension and the strength of the eggshell as well as the lack of oxygen present and excessive loss of carbon dioxide and water.

Eggs can be incubated at high altitudes if the following is accomplished; The oxygen tension must be brought back to the level present on sea level of 149 torr and this is possible if an artificial incubator is used.
The gas tension in the gas space in the egg must be also be the same as that at sea level so that a normal gaseous exchange can take place between the gas space and the embryonic blood.

An experiment in Visschedijk in 1985 at an altitude of 5.5km proved that using the above guidelines a normal gaseous exchange was achievable and near normality hatchability occurred.
Technological advantages have allowed artificial poultry incubation to surpass natural incubation methods.

In the future the broody hen may well become extinct as technology exceeds natural methods.
On the other hand, nothing can replace the mothering nature of a broody hen and the true beauty of her raising her chicks so maybe the broody mother hen may just pull through!

Nitrogen leaching is currently a major economic and environmental concern.
A clear understanding of the Nitrogen cycle would help to make a profitable use of fertilisers and manure, and reduce the nitrogen content in the water cycle and atmosphere.

Below is a map of the areas in Europe currently at risk form nitrogen leaching.
Areas at risk from severe leaching are classified as NVZ's, Nitrogen Vulnerable Zones.

There are restrictions in the use of fertilisers in these zones.
DEFRA send out information to farms in these zones to help protect the water course and give help to farmers on controlling the amount of Nitrates in the soil and water.

The Nitrates Directive was set up in 1991 and is designed to, 'Help protect the water environment against Nitrate pollution from agricultural land'.
( URL ) A plant needs many elements to help it survive and grow; these include Nitrogen, Potassium, Phosphorus, Calcium, Sodium, Magnesium, Oxygen, Hydrogen and Carbon.

The three elements usually used in fertilisers are Nitrogen (N), Phosphorus (P) and Potassium (K).
N is essential for growth as it is needed to synthesis proteins, store energy and build and maintain the plant skeleton.

If a plant is deficient in N it usually has little grown, a thin, weak texture and a weed-like appearance.
N losses are more apparent on arable land than grassland but both systems need inputs of N to help the crops grow, the major input of N is from fertiliser but livestock also produce manure and urine which also add to the N content in the soil.

The nitrogen cycle is summarised below; The most common N fertilisers in 1980 were Urea (35%), Ammonium Nitrate (20%), Ammonium Sulphate (8%) and Calcium Ammonium Nitrate (6%) but the most common in the UK are Calcium Ammonium Nitrate and Urea.
Organic Nitrogen is the main form in the soil but to be taken up by the plant it needs to be mineralised into Ammonium Nitrate or Nitrate.

(P. Newbould) Every year around 3% of the N in the soil is mineralised so 80-100Kg/ha is available to the plants.
Rainfall and deposits can add 10-20Kg/Ha but grass needs between 100-400Kg/Ha to make it an economically viable yield, N fertilisers are essential.

The efficiency of N is only about 50% so about 200-800kg/Ha of N is needed every year.
(P. Newbould) This is due to Nitrogen leaching from the soil.

Since the introduction of chemical N fertilisers the yields of crops and the amount of livestock produced have increased steadily.
There has been a reduction in the amount of fertilisers used in recent years as plateaus are reached in the yield and production, however much N is used.

There have also been environmental concerns as Nitrates continue to pollute the watercourse.
Below is a graph showing the increase and decrease in the amount of N fertiliser used.

A study was conducted at the Royal agricultural College on Coates Farm to improve N leaching from land previously used as arable and being converted into grazing land for cattle.
The major input of N at first was fertiliser but once the cattle were brought in the amount of fertiliser used decreased due to N being present in their excrements.

The study was conducted between 2000 and 2003.
In the driest winter the fields lost less than 10Kg N ha -1 while in the wettest winter the lose was nearly 200Kg N ha -1 this was also when the ground was most heavily grazed by the cattle.

The average lose of N was 87 Kg N ha -1 over the four winters of the study.
(K. Goulding et al) The conclusion was that calculating correct amounts of fertiliser needed for different fields reduced leaching as well as efficiently using the recycled manure and amount of feed given to livestock.

The time of year N is applied heavily influences the amount leached from the soil.
Most N is applied to fields in spring and summer.

Applying in early spring is however the most dangerous time due to the low temperatures and the ground being saturated so the N is unable to reach the soil.
However, farmers want to apply N as soon as possible so to reduce the amount time cattle spend in housing and to lengthen the growing season of the grassland.

If temperatures are high in the autumn and the grassland has been heavily grazed an application of N will benefit the following years yield as long as heavy rainfall is not predicted.
This is due to more biomass accumulation and an increase in nitrogen uptake by plants; therefore leaching should be at a minimum.

The right time to apply N to grassland is therefore difficult to determine but a rough guide would be as soon as the grass begins to show growth again after the winter months.
There is a two-five week period when if N is applied at the amount lost will be minimal and the yield should be over 90% of its maximum, (R. J Stevens).

Another strategy for reducing N leaching is to remove the livestock from fields earlier in the growing season to reduce the amount of nitrates left in the soil that could be leached over the winter.
The manure produced from the livestock in housing can also be used as a fertiliser in the spring after the periods of heavy rainfall.

This therefore recycles the N efficiently and potentially reduces the amount of N in the watercourse.
Feeding cereals to livestock that have low N concentrations such as maize silage will also reduce the amount of N excreted into the soil when they are kept on the grassland.

Less intensive systems do also reduce the amount of leaching but also require more space, which is not always possible or economically efficient.
Currently the recommended amount of Nitrates present in water is to be less than 50mg/l as set by the World Health Organisation and the European Union so it is safe for consumption.

(C. Stopes) Nitrates are soluble in water so therefore become present in drainage and ground water.
This then becomes an environmental concern as it reaches rivers and lakes and pollutes the water and creating eutrophication.

A way of reapplying the N leached away in the surface run-off water would be to irrigate it back on to the grassland this also stops some of it being present in drinking water and the watercourse.
To reduce the amount of N available a ban or a tax increase could be introduced by the government.

This has already been put in place by Sweden who imposed a tax on N fertilisers of 10%.
It has had no major effect on the sales of N fertilisers as economists have predicted a rise of up to 200% for it to have any significant effect on the sales of N fertilisers.

(P. Newbould) Currently a ban would not help work hunger problems, as N is needed to meet the high levels of production needed.
In tropical areas the El Nino and La Nina effects affect the amount of N leaching.

In El Nino years sea levels rise and weather conditions change causing a lower biomass accumulation in the soil.
This means that in El Nino years N leaching levels are higher.

The amount of fertiliser should therefore be altered accordingly to suit the climatic conditions.
Soil type affects the amount of leaching.

Soils, which have a low drainage rate, obviously have a higher rate of leaching.
Leaching is also higher in soils with a low cation exchange and soils with a higher pH.

Area with these types of soils should then be more aware of N leaching and plant crops, which take up N efficiently and quickly.
Grass types also vary the rate of leaching as they absorb nitrates at different rates.

In organic conversion farms red clover is planted for three years to maximise the N in the soil, as clover is a quick and efficient N fixer.
Clover can be used in conjunction with grass for grazing but it does not establish itself well and too much can lead to bloat in ruminants.

Crop rotation in arable farms and set-a-side land can also preserve and retain the N in the soil meaning less fertiliser needs to be used.
Although leaching is the main cause of Nitrate loss, soil erosion can also be a factor.

Loses from erosion are not generally a problem in Western Europe but can be a big problem on peaty soils especially in upland areas.
Heavily leached soils also have a tendency to be easily eroded as the water washes away soil particles quickly.

The use of N fertilisers should be proportional to the climate and soil type and be more readily available in areas with favourable climates and retentive soils.
Sensitive areas need to have more responsive crops such as grasslands and maize as they utilise nitrates efficiently.

If guidelines were followed then yields and productivity would be able to reach a healthy rate without damaging habitats and environments.
The results of a 2006 Food Frequency Questionnaire, filled out by 86 eager first years at, show remarkably modest levels of alcohol consumption.

This Questionnaire was part of a Dietary Survey used to establish the nutritional components of these students' diet over the previous 7 days.
The results showed that all 86 subjects - 23 males and 63 females - had relatively well-proportioned, nutrient-rich diets with few significant deviations.

Yet surprisingly it also revealed that they had an average alcohol consumption of only 17.2g per day.
Based on Government figures this converts to 2 units per day - equivalent to a pint of ordinary strength larger such as Fosters.

This would probably shock most of us as few would suspect that drinking more than one Fosters would be classed as "going on a binge".
Binge or no binge, drinking is still thought to be a major part of a students' life.

Yet, according to these recent figures, female students at consume on average 2 units a night and males only 3 - both a unit below the national maximum recommendations.
You would only need to go out once with a student to see that this level of alcohol consumption is grossly under-par.

So what does it mean?
Are students truly as angelic as the figures imply?

The truth is that there are many major inaccuracies associated with questionnaires such as these.
Food frequency questionnaires are no exception.

They are suitable for large-scale surveys and can focus on specific nutrients in the diet, yet they often have over-estimation of nutrients and under-estimation of unhealthy foods.
Whether they are accurate or not these students still show alcohol consumption levels below the maximum recommended amount.

Yet, compared with a 2003 National Diet and Nutrition Survey (NDNS), these students have daily alcohol intakes far above the national means.
This 2003 study recorded daily intakes of 21.9g of alcohol per day for men and a shockingly low 9.3g for woman.

Comparing the University's 2006 figures for female consumption with the NDNS' 2003, it shows that students knock back the equivalent of a whopping 11 bottles of 40% vodka, or 141 pints of lager more per year.
The best explanation for these differences is that this study is nearly 4 years out of date and doesn't take into account the nation-wide increase in alcohol consumption (especially in females) since then.

This increase in alcohol consumption requires an increase in nutrient intake.
This is due to alcohol's 'empty calories' - energy without nutrients.

But are 's students doing this?
Alcohol, as well as not giving any nutrients, also takes them away.

God's gift to hangovers, Vitamin B1 (Thiamine), one of the many vitamins depleted by alcohol, was found in relatively high levels in students' diet - almost twice the RNI (Reference Nutrient Intake) of it in males.
RNIs are part of the government's Dietary Reference Values (DRV).

These values are compiled to get an idea of an individual's nutrient requirement.
RNIs represent the amount that meets the nutritional needs of most people.

's levels of Vitamin B2 (for those bloodshot eyes), Zinc (for alcohol metabolism) and Vitamin C (anti-oxidant fighting the baddies in alcohol) are all well above the RNIs.
However the questionnaire also picked up on Potassium and Iron deficiencies (only in females) that would no doubt hamper the body in combating the effects of alcohol.

A new area of research is investigating the effectiveness of Phosphorus in treating alcohol withdrawal symptoms.
Funnily enough students have abnormally high levels of Phosphorus in their diet.

All subjects have more than double the RNI and males are even over the safe upper limit!
Could there be a hidden message behind this strange discovery?

Whether Students are in fact recovering alcoholics or just misunderstood youths remains to be seen.
Nonetheless, students appear to be consuming more alcohol than the national means (for 2003), while still managing to keep their units per day below the maximum recommended amount.

This paradoxical set of results leaves us questioning their accuracy - which, with the national trends showing huge increases in alcohol consumption, would not be a bad thing to do!
Published November 25 2006

Procedure
This practical involved weighing all food items consumed over a period of 3 days in order to estimate the individual's intake of energy and nutrients.

A set of weighing scales was provided and all foods consumed were recorded and weighed separately so as to facilitate component analysis when entered into the data processing program: Food Base 3.1.
This program utilised McCance and Widdowson's composition of food tables in The Composition of Foods (6th Ed., 2002: 5th 1978 and 5th Ed: 1991) to derive the energy and selected nutrients' compositions of each item of food.

The resultant data was compared to the DRVs (Dietary Reference Values) for each food.
Results

Discussion
The results of my data analysis showed an interesting pattern of nutrient deficiencies.

When my daily intakes were compared with the DRVs my diet showed deficiencies in all the given minerals and even deficiencies in protein and fibre.
However, my vitamin levels were all above or equivalent to the DRVs - without having taken any vitamin supplements.

This set of results denotes that my diet is sufficiently diverse to supply me with the right amount of vitamins yet is somehow extremely inefficient when it comes to minerals.
Most minerals were in fact found to be almost half the DRVs.

Iron and Magnesium - two of the most important minerals - when compared to the RNIs (Reference Nutrient Intake) were shown to be far below even the LRNI (Lower RNI) of 8g and 190g respectively (my intakes: Iron: 5.9g, Magnesium: 114.9g).
However, although my vitamin intake would appear to only be border-line for my needs (using DRVs), it is in fact far in excess in many vitamins when looked at in relation to RNIs.

Although most of the vitamins and minerals show the same value for their DRV and their RNI, vitamins such as Thiamin and Niacin Equivs are exceptions.
In both cases the RNIs are half that of the DRVs (Thiamine: 0.8mg vs. 0.4mg and Niacin Equivs: 13.0mg vs. 6.6mg), thus making my vitamin intake appear far greater.

However, such a high level of deficiencies (mainly in minerals) obviously demands a drastic change of my diet.
I need to eat far more fruit and vegetables.

It would be of use to consider levels of a particular nutrient upon buying a product - in order to buy a variety of food that would cater to my nutritional needs.
Buying spinach, for example, would be good in order to increase my magnesium intake.

Likewise, drinking more milk will increase my potassium and calcium intake, while eating more mushrooms and meat will improve my intake of Selenium and Zinc respectively.
Thus, by varying my diet more I should manage to incorporate higher levels of all vitamins and minerals into my diet and so help eradicate my present deficiencies.

In terms of energy, protein and fibre I'm far below recommended levels (DRVs and RNIs).
However, energy and protein intakes can vary without being detrimental to health as they only need to surpass my energy expenditure - which in this case they do as on those particular days I did not exercise much.

Yet, in terms of where I get my energy from might be relevant.
I am in fact getting more energy from fat than should be the case, where in contrast I'm getting less energy from sugar than I should be getting.

In fact, on these particular days an explanation as to why I had so many deficiencies may have been due to my only getting up at midday.
This will no doubt have affected my results and so in reality it might be best to view theses intakes in terms of two days as opposed to 3 in order to imitate the normal daily pattern.

If this where done then I'm sure my levels of deficiencies would be far lower.
This can be seen as one of the potential inaccuracies that could arise from this method of diary assessment.

Other such problems include subjects not bothering or not willing to weigh all the food products, the unavailability of certain products in the Food Base (2001) archive and lastly the difficulty and impracticality involved when faced with weighing foods - especially outside of the home environment.
In comparison with other methods, the Weighed Diet Diary has its pros and cons.

Alternatives such as the Retrospective Dietary Assessments: Dietary History, 24 Hour Recall and Food Frequency Questionnaire, all have the advantage of not requiring a set of scales everywhere you go, yet they fall short of the accuracy of the Weighed method.
This is due to the fact that subjects are likely to forget some food items consumed as it relies heavily on memory.

When compared to methods that involve estimating the food consumed - such as a Non-Weighing Diet Diary or a Photographic Record, one can see that these method are simpler yet can still lead to inaccuracies due to the subjects altering their diets.
The methods considered the most accurate (deemed the 'gold standard') are the Weighed Food Intake diets.

These include the Duplicate Diet, PETRA (Personal Electronic Tape Recording scAles) and the method used in this case - the Weighed Diet Diary.
Thus, one would be tempted to say that the Weighed Diet Diary is one of the best, if not the best, method for assessing a subjects' diet.

However, as with virtually all the other method this too may involve the subject altering their diet.
1. Introduction

Ongoing research in the Severn Estuary has been investigating the interaction of Mesolithic people with their environment.
The project has been particularly interested in the identification of human agency and other disturbance factors, at a time when sea-level rise was producing a constantly changing environment (Bell et al.

2004).
Work at the site of Goldcliff East (Fig. 1) has been particularly useful due to the amount and quality of environmental evidence preserved under peat and estuarine silt (Bell et al.

2003; Bell et al.
2004).

The micromorphological work at this site, undertaken by Virgil Yendell, has focused on the evolution of the buried land surface at Goldcliff in respect to climatic change and the interplay of these factors with human lifeways in the Mesolithic (Yendell 2004).
This project utilises one sample from Goldcliff Site B, slide 5525, to address questions about site formation and human activity, post-depositional processes, and the use of micromorphology on sites of this kind.

1.1 Previous research
The application of micromorphology to archaeological soils and sediments has been steadily increasing in the UK in recent years (French 2003).

Early examples of its use in the study of palaeosols are Westward Ho!
(Balaam et al.

1987) and Brean Down (Bell 1990), while micromorphology undertaken at the Experimental Earthworks at Overton Down and Wareham has increased our understanding of site formation processes and the effects of burial on soils (Bell et al.
1996).

However, there are still those who doubt whether micromorphological analysis contributes significantly to the understanding of a site or landscape on an archaeological timescale to warrant the time and expense of undertaking thin section analysis (French 2003).
A further question to be investigated in this report, therefore, is whether the study of soil thin-sections from Goldcliff contributes significant information that was not available from other archaeological techniques.

Archaeologists have also been debating the nature of settlement and land-use in the Neolithic in the last decade.
This has been prompted in particular by the discovery of microscopic charcoal at numerous Mesolithic sites in England and Scotland (Simmons & Innes 1996; Edwards 1990; Dark 2004), leading to suggestions that Mesolithic people deliberately burnt woodland and reed swamps to enhance the plant resources available and draw herbivores to the site (Simmons 1996).

The extent and nature of this burning is not yet clear, however.
These questions also relate to a wider issue of environmental reconstruction.

There are concerns in the archaeological community about the spatial resolution of pollen studies and the difficulties in discriminating between natural and anthropogenic disturbances (Bell & Walker 2005:183-6), and a growing recognition that Holocene woodland vegetation may have been much more patchy than has often been assumed (Vera 1997).
The importance of understanding how humans interacted with environmental change cannot be overstated, and micromorphology may present a useful approach in dealing with this question.

1.2 Research aims and objectives
There were three main objectives for the micromorphological work at Goldcliff East.

They were:
Understanding the formation and evolution of the old land surface at various stages of climatic changeInvestigation of the presence of any buried soils at the old land surfaceInvestigation of human presence and interaction with the old land surface and the wider environment (Yendell 2004).

The slide on which this project focuses is taken from the old land surface and occupation horizon from Site B (Fig. 3, Fig. 5).
The specific questions that this project was intended to answer were: What is the nature of the sediments comprising the old land surface, and what processes led to their formation and deposition?

What evidence for human activity is there on the old land surface?
What post-depositional processes have acted on the sediments and what does this suggest about climatic changes at the site?

Is micromorphology a useful tool for assessing these questions in relation to the Mesolithic period in Britain?
2. Methodology

2.1 Site selection
The area at Goldcliff called Site B (Fig. 2) was first recorded and partially excavated in 2001 and 2002 (Bell et al.

2003), and a further excavation was undertaken in 2003 by Martin Bell in association with 'Time Team', all as part of the ongoing research of the Severn Estuary Levels Research Committee (Bell et al.
2004).

Site B was re-excavated because in 2001 and 2002 Mesolithic artefacts and substantial environmental evidence had been recovered, and the erosion of the site was happening at a rapid rate.
Due to the short tidal window available for archaeological work (2 hours 40 minutes exposure at spring tides, none at neap tide), a block-lifting methodology was adopted.

0.25m 2 blocks of sediment were lifted in tins and excavated on dry land (Bell et al.
2004).

This methodology allowed more careful excavation and recording, particularly of small finds which might have been missed in the difficult conditions on-site.
The micromorphology samples were taken by Virgil Yendell as part of his MSc dissertation research on exposed sections of the site left by the block-lifting (Yendell 2004; Bell et al.

2004).
The section drawing shows the location of the micromorphological samples taken (Fig. 5).

2.2 Thin section preparation and analysis
The thin sections were prepared according to Murphy (1986).

Due to the waterlogged nature of the estuarine sediments, the acetone replacement method (Murphy 1985) was utilised to prevent shrinkage of the sediment blocks.
The samples were then impregnated with resin, cured in an oven and the slides prepared to a standard thickness of 30μm.

The surfaces were consolidated using Araldite 2020.
Further details of the specific procedure followed can be found in Yendell (2004).

Although the slide preparation was generally successful, there are several areas on slide 5525 (Fig. 4) where coarse grains have been dislodged or removed during sample preparation.
The slide has also been ground slightly too thin, causing the birefringence colours, particularly of quartz, to appear darker (J.R.L.

Allen, pers.comm.).
3. Results

The thin section was described using criteria from Stoops (2003), with additional use of Bullock et al.
(1985).

The laboratory sheets are attached in Appendix A.
3.1 Observations

Slide 5525 contains one main context from Site B, context 321, which is the original land surface, and whose characteristics are described below (Table 1).
At the top of the slide, incipient peat formation can be observed (context 319).

The boundary to this context is diffuse, and represented only by an increase in organic matter from 5% to 10% in the upper 2 cm of the slide.
The sediment of context 321 is a sandy clay loam to sandy clay, with a coarse/fine ratio of between 20:80 and 30:70.

The coarse fraction consists primarily of medium to fine quartz sand, both single crystals and quartz aggregate (70:30), with some flint and small amounts of plagioclase, microcline, biotite and olivine also present.
The fine fraction is fine silt and clay in approximately 50:50 ratio.

The overall colour is pale brown to grey in plane-polarised light (PPL), and under cross-polarised light (XPL) the b-fabric is crystallitic speckled with grey and white 1 st order birefringence, resulting mainly from the quartzite nature of the fine-grained silt, and shows some grano-striation of clays.
The microstructure is primarily massive, with the grains closely embedded in the fine material, with a few channel and chamber voids.

The sediment is poorly sorted with particles of all sizes represented.
There is no orientation pattern in the sediment, and no obvious distribution pattern, apart from the organic material which tends to be concentrated in patches and lenses in the upper third of the slide (visible as areas with slightly darker brown colouring in PPL).

There is a large (13mm) flint present near the bottom of the slide.
The sample has significant organic material present (Table 2), including charred, water-logged and siliceous remains.

The phytoliths could not be identified to any particular species or genus, but include long cells and short, probable epidermal cells.
Also present are various species of diatom.

The most common type is shown in Figure 6, and a preliminary identification made to the Diploneidaceae genus, which is commonly found in marine environments (Round et al.
1990).

The diatoms were clustered near the water-logged organic matter.
Several post-depositional features are apparent (Table 3).

Small black framboids of opaque material were present in many areas of the slide (Figure 6), particularly associated with organic matter.
These were identified under oblique incident light (OIL) as pyrite framboids through their characteristic reflective properties.

Gypsum is also present in small amounts (Figure 7).
Clay laminations can be observed lining voids and in thin bands throughout the slide.

The laminations are weakly birefringent with striated fabric showing alignment of clay particles.
Some of the channel voids also contain fine silt in circular structures.

This material is weakly birefringent from fine silt quartz particles and similar to the fine material of the main unit.
Where these features occur in the same void, the silt appears to cut through the clay deposition (Fig. 8).

3.2 Interpretations
3.2.1 Formation processes

The sediment is a poorly sorted sandy clay loam with very limited channel and chamber voids, many of which contain clay linings.
The presence of clay laminations within the massive sediment suggests that the structure was once more open.

Charred and waterlogged organic material is present in significant amounts, particularly in the upper third of the slide forming the diffuse boundary to the reed peat above.
These features suggest that incipient pedogenesis may have occurred in this sediment.

Yendell (2004) suggests that there is a palaeosol preserved at Site B at Goldcliff East, but the compression and post-depositional alteration of the sediments appears to have destroyed any evidence of peds.
The sand component of the sediment may result from the underlying parent material, which is Ipswichian beach sand/gravel and head deposits (Bell et al.

2004).
The finer material may have been brought in by fluvial processes, and mixed with the underlying sands and some organic material.

This mechanism would explain the poor sorting of the sediment and the sub-rounded shape of the coarse grains.
3.2.2 Organic remains

The plant remains, though not well-preserved, display a generally fibrous structure with no annual banding.
Therefore, it is likely that most of the organic matter comes from reeds and grasses growing at the edge of the estuary.

This interpretation is supported by the presence of reed peat directly above this deposit (Fig. 5).
Although none of the phytoliths could be identified, their presence also suggests grassy vegetation, as trees do not tend to produce phytoliths (Rapp & Hill 2006: 173).

The diatoms found in the slide suggest a marine environment, which is not surprising given the location of the site.
Their association with the water-logged organic matter suggests that they may have come on to the site attached to reeds rather than being splashed or swept onto the site by the tide.

3.2.3 Post-depositional processes
Several different post-depositional processes have acted on this sediment.

The channels appear to be polyphasic, with organic material in some channels, clay laminations lining these and later channels with silt inwash.
Where these separate channel fills occur together (Fig. 8), it appears that the silt-filled channel penetrates through the clay and organic material.

This suggests that roots and burrowing organisms have followed previous channels as a path of least resistance into the sediment.
The repeated nature of the burrowing suggests that the sediment has been subjected to significant bioturbation, which may be obscuring the original character of the sediment.

Pyrite is prevalent in the slide.
This mineral forms in reducing conditions in marine environments in the presence of sulphates.

In some cases, the pyrite partially or completely replaces organic matter.
The reducing conditions imply that the pyrite formed when the site was waterlogged and peat formation had begun.

Subsequently, a limited amount of gypsum has formed.
Gypsum is a calcium sulphate which is usually found in arid soils, but can also form through the oxidation of pyrite reacting with calcium carbonate (Miedemaal.

1974).
Although glauconite, another iron compound, is present in the sediment, the lack of intergrowth and rounded shape of the crystals suggests that it is likely to be detrital sediment rather than a post-depositional formation.

Apart from the iron compounds discussed, the sediment itself is relatively depleted in iron content, except in the boundary to the overlying peat.
Finally, the diffuse nature of the boundary to the peat above this unit, evidenced both here and other areas of the site, suggests that there may have been trampling across the site by people or animals.

4. Discussion of results
The sediments identified in this slide suggest that at Goldcliff East Site B, a soil may have begun to develop in the early Holocene on the underlying Ipswichian sand and head deposits.

This soil formation is seen from the presence of organic matter and clay movement through the sediment, and may have exhibited more pore structure before compression by overlying sediments.
The pale colour also suggests that there is low iron content resulting from mobilisation of the iron and redeposition as pyrite.

The sediment appears to have been compressed with little evidence of soil structure remaining.
Yendell suggests that the palaeosol can be classified as a saline gley (Yendell 2004), similar to that found at Westward Ho!, Devon, with its weakly developed soil characteristics, massive structure and root channels with organic and silty infill (Balaam et al.

1987).
The numerous episodes of channel formation by roots and burrowing organisms may originate during the period when the site was still vegetated before marine transgression occurred.

Once the site was waterlogged by rising sea-levels, the conditions became reducing and pyrite formation occurred.
At a later stage, some of this pyrite was oxidised to gypsum, but it is not possible to determine when this occurred.

The only evidence for human activity at the site from this slide comes from the presence of charred plant material in higher quantities than at other parts of Goldcliff East (Yendell 2004).
Although charcoal can occur naturally, elevated levels in an estuarine environment suggest some human activity, either from small fires at occupation sites for cooking or warmth, or more general burning of reeds and woodland.

Such burning has also been identified at other sites, such as Star Carr (Dark 2004), and may have been intended to encourage new growth of food plants or grazing for herbivores.
The trampling of the upper boundary of the land surface may also represent human activity, but could also result from animal trampling - footprints of humans and animals have been found at Goldcliff East Sites C and E (Scales 2003).

5. Evaluation
This study has been able to draw some conclusions about the nature and origin of the sediments at Site B at Goldcliff East, and the interaction of humans with environmental change at an estuarine site.

In particular: The land surface was found to be a sandy clay with soil development occurring.
There is little direct evidence for human activity from this slide, but charred material suggests possible anthropogenic burning of the reed swamp.

Numerous post-depositional processes have acted on the sediments, including burrowing roots from vegetations, burrowing organisms and waterlogging of the site as a result of rising sea-level in the Holocene.
The study of this site using thin-section micromorphology has enabled a very detailed understanding of the sedimentary history of the site, which would be difficult to fully understand without this technique.

Although analytical techniques can be used to characterise the particle size and organic content of a sediment, it is only when viewing the sediments in thin section that the relationships of the various components to each other can be properly identified.
In addition, the complex post-depositional history of this sediment makes micromorphology the best approach for interpreting the sequence of events correctly.

6. Future research
Although palaeosols have been identified at Goldcliff East, both in this section and in other areas, the exact mechanism of soil development has not yet been fully established (Yendell 2004).

Further study should be undertaken to determine when and how this soil may have formed, as micromorphological techniques are not able to date these layers to a specific period.
Radiocarbon dating of the organic material should assist understanding of the formation of the palaeosol.

Additionally, the order in which the post-depositional disturbance occurred should be confirmed.
There are also questions about the presence of glauconite in the sediment, which may be detrital or could have weathered from the pyrite.

The iron distribution and exact composition of the iron minerals could be determined using electron microprobe analysis (Courty et al.
1989: 51-2).

This work will further our understanding of the environmental history of the Severn Estuary and its relationship to human activity and changing climate through time.
As traditional views of predictable ecological succession are challenged by dynamic theories of patchy development (Bell & Walker 2005: 182), there has been increasing debate about the character of the vegetation of northwest Europe before human interference.

The nature of forest environments in the past has implications both for the conservation and management of woodland today, and for our understanding of the human/environment interactions during the Holocene.
The debate has centred around Vera's hypothesis that the natural vegetation of Europe was not closed forest, as previously assumed, but an open, park-like landscape managed by large herbivore grazing (Vera 1997).

This hypothesis was based on the fact that oak and hazel do not survive well in modern closed forests but are continually present in the pollen record of temperate periods in north-west Europe.
Background

Jens-Christian Svenning is a biologist by training and wrote this paper while completing post-doctoral work at the Smithsonian Tropical Research Institute in the USA.
The journal of publication, Biological Conservation, is focused primarily on ecological conservation and the influence of man on the biosphere, and it is therefore no surprise that the main objective of the article is to aid conservation agencies in their management of present woodland (Svenning 2002: 134).

However, this ecological focus is not detrimental to the usefulness of this paper to environmental archaeologists.
Svenning focuses his efforts on understanding the vegetation patterns in the temperate interglacials, particularly the last Eemian interglacial and the early Holocene, as analogues for the natural vegetation in the present if humans had not interfered (the present-natural vegetation).

His conclusion is that the vegetation was predominantly closed forest, but that some areas such as floodplains and chalklands had more open vegetation (ibid: 140).
Svenning suggests that light-dependent species such as oak and hazel would have been able to maintain themselves in closed forests through occurrences of fire, windthrows and certain edaphic-topographic conditions which favour these species (ibid: 140).

Research methodology
The primary method Svenning uses is a literature review of palaeoecological studies of the temperate interglacials and early Holocene.

It is not clear from the paper exactly how many sources were included in the review, but there are over 160 references, suggesting a fairly thorough coverage of approximately the last 40 years.
Unfortunately there is no discussion of the methods used in the source material, and the reader cannot be certain that all the evidence can be directly compared.

Although pollen methodologies have been established for some time, there is no clear indication from the paper as to the range of methods and analyses used in the source materials, or whether any criteria were used in selecting the papers to be included in the review.
Svenning uses the non-arboreal pollen (NAP) percentage to estimate the amount of tree cover in an area.

There has been some debate about the use of this measure for estimating tree cover (Sugita et al.
1999), but Svenning makes a comparison of NAP percentages with estimates of tree cover from beetles, molluscs and plant macrofossils, and shows that pollen can present a reasonable measure of forest openness (Svenning 2002: Fig.1).

He admits that, wherever possible, supplementary evidence should be used to confirm pollen results, and throughout the paper he draws together evidence from beetles, molluscan remains, plant macrofossils and vertebrate fauna.
This multi-disciplinary approach ensures that the conclusions Svenning draws do not stand or fall solely on the pollen data, and lend greater credibility to his arguments.

Results and interpretation
Having established his methods and the range of evidence used, Svenning presents the results gathered from the literature for vegetation patterns in five landscape types and assesses the overall picture they present.

For example, the section on 'normal' uplands begins with a definition of the environmental parameters being investigated, then describes a regional view, with evidence from pollen data, beetle assemblages and plant macrofossils, followed by localised evidence from small hollows (Svenning 2002: 135-136).
This logical progression makes it easy for the reader to follow Svenning's thought process, and the extensive references provide ample opportunity to follow up on his conclusions.

He makes good use of case studies and presents a balanced view where the evidence points in two directions or where more than one interpretation is possible (e.g. the possible reasons for the presence of Pteridium aquilinum and Melampyrum - ibid: 137).
As well as analysing five landscape settings separately, Svenning also provides an overview of the vegetational succession in the pollen record of northwest Europe as a whole (Svenning 2002: 138), and then moves into a discussion of the reasons why oak, hazel and pine were able to regenerate in the closed forests that he suggests were present on 'normal' uplands (ibid: 138).

This section is one of the major strengths of the paper; Svenning looks at a wide range of factors that could improve the survival of oak, including fire, tree throw and edaphic-topographic variability (ibid: 138).
The only factors not mentioned are disease and storm patterns, although as tree throw is one of the main results of storms, the possible effects of this are covered in Svenning's discussion.

The flow of argument and interpretation throughout the whole section is well structured and easy to follow, drawing on the evidence already presented and bringing in additional sources such as faunal material (ibid: 139-140).
By discussing a range of contingent disturbance factors, a picture of a dynamic mosaic environment emerges which could have been used by humans in a more flexible and spontaneous manner.

Svenning does fail to discuss the nature and scope of anthropogenic impacts (their potential role in fire regimes is briefly mentioned; ibid: 139), but considering the stated aims of the paper and Svenning's non-archaeological background, this is hardly surprising and does not detract from the value of his analysis as a whole.
Especially important is his recognition that modern regenerating forests are dominated by young, densely packed trees which do not provide a good analogy for the old forests which would have been present before human intervention (ibid: 139).

This point is rarely recognised by archaeologists, yet is vital if modern growth patterns are to be used to infer past vegetation, as Vera did (Vera 1997).
Svenning sets out a clear summary of his conclusions in Section 6 (Svenning 2002: 140), and then discusses the implications of his findings for nature conservation and suggests future research (ibid: 141).

Svenning is aware that the conclusions he draws require further supporting evidence, particularly in understanding herbivore-vegetation interactions and the effects of fire, and his own evaluation of his work shows that he has carefully considered the limitations of his methodology.
Having drawn together previous evidence so effectively, Svenning's paper is a good basis for such future research.

Because of the cross-disciplinary importance of the character of past vegetation, it would be beneficial for both biologists and archaeologists to contribute, giving a balanced view of the anthropogenic and natural causes.
Style and presentation

The paper is well written and presented, but there are a few points that would have made the paper more readable for a wider audience.
One in particular is the use of binomial nomenclature for plants and animals rather than common names, which are rarely indicated.

This poses potential problems for readers not familiar with these terms, and a table or appendix giving the scientific and common names of the most important species would have been helpful here.
Of the two appendices, the first is very useful, comparing the fauna present in the Eemian and Holocene (Svenning 2002: 142).

However, the purpose of Appendix B ibid.
142-44) is less clear, though the information it contains may be more meaningful to the audience of conservation biologists that the paper was originally aimed at.

These issues suggest that Svenning may have failed to consider the wider application of his research for the archaeological community.
Impact on the debate

This paper was published four years ago, and has been followed by several others that support and build on its conclusions (e.g. Bradshaw et al.
2003; Mitchell 2005).

This paper presents a good case for considering past vegetation in a more dynamic way, taking into account that there are many factors which can contribute to the character of forested areas.
Svenning's work has been particularly significant in providing a wide-ranging review of the available evidence, which others can now build on.

If further research continues to support Svenning's conclusions, the conservation strategies of English Nature and similar organisations may need to be adjusted to take his recommendations into account.
For archaeologists, the character of the wildwood is a significant topic in understanding the interaction between humans and environment in the past, and this paper may help them consider these issues in new ways.

Svenning has provided a significant and well-written contribution to the debate on the character of north-west European vegetation, and despite small deficiencies, his work succeeds in its aims and draws some important conclusions which are refreshingly non-anthropocentric.
It is likely to become a standard reference for archaeologists assessing palaeoecological evidence in Europe.

Exercise 1: Descriptive statistics (L1)
Table 1 contains the data used in this exercise, which consists of five assemblages of Upper Palaeolithic endscrapers recovered from the rockshelter site of No'kulchaear.

Table 2 shows the summary statistics for each assemblage (mean, median, mode, standard error, standard deviation, sum, sample size).
The mean lengths of the endscrapers in each assemblage have been compared in Figure 1.

Exercise 2: Normality of data (L1)
The five assemblages were tested for normality in three ways - production of a histogram for each assemblage (Figures 2-6), and two calculations, which are summarised in Table 3.

In the skewness/standard error and kurtosis/standard error tests, the data is normal if the values for both tests lies between -1.99 and +1.99.
As Table 3 shows, all the assemblages have values in this range and are therefore normal.

The histograms show that all the data have a fairly normal distribution, although Assemblage D has an anomalous peak at 45-50mm, and B and C have no data in one range.
Exercise 3: ANOVA (L1, L2)

The ANOVA (analysis of variance) test will compare the mean lengths of all five assemblages to see if the differences between the assemblages are statistically significant.
The significance level is set at 0.05.

Two hypotheses may be established.
Hypotheses

H 0 - the differences between the mean lengths of the endscrapers of the five assemblages are not statistically significant.
H 1 - the differences between the mean lengths of the endscrapers of the five assemblages are statistically significant.

The ANOVA test produced a P-value of 9.09 x10 -6 (Table 4).
This exceeds the significance value (i.e. is less than 0.05), and therefore the null hypothesis (H 0) can be rejected in favour of H 1, that the difference in the mean lengths of the five assemblages is statistically significant.

The ANOVA test was also applied to test the significance of the differences between mean lengths of Assemblages X and B, again at 0.05 significance.
The hypotheses are given below, and the results are in Table 5.

Hypotheses
H 0 - the differences between the mean lengths of the endscrapers of Assemblages X and B are not statistically significant H 1 - the differences between the mean lengths of the endscrapers of Assemblages X and B are statistically significant The P-value exceeds the significance value (0.05) and therefore the null hypothesis is rejected in favour of the hypothesis that the differences in mean endscraper length between the two assemblages are significant (H 1).

The same test was carried out to compare Assemblages X and E.
The same parameters were used, and the results are in Table 6.

Hypotheses
H 0 - the differences between the mean lengths of the endscrapers of Assemblages X and E are not statistically significant.

H 1 - the differences between the mean lengths of the endscrapers of Assemblages X and E are statistically significant.
The P-value for these assemblages does not exceed the significance value (0.05), therefore the null hypothesis is upheld.

There is no statistically significant difference between the mean length of the endscrapers in assemblages X and E.
Exercise 4: Chi-squared test (L1, L2)

The chi-squared test is used to test the distribution of the endscrapers in relation to the distribution of animal bones on the site.
This test was performed on Assemblage A, and the null and alternative hypotheses are given below.

The observed values are given in Table 7 and the expected values calculated in Table 8 by multiplying the sum for the row with the sum for the column and dividing by the total number of observations.
The significance level of this test is 0.05, and there is 1 degree of freedom.

Hypotheses
H 0 = the observed distribution of the endscrapers and animal bones in Assemblage A is not statistically significant.

H 1 = the observed distribution of the endscrapers and animal bones in Assemblage A is statistically significant.
For Assemblage A, the calculated X 2 value exceeds the X 2 value of 3.84 at 1 degree of freedom at the 0.05 significance level (Table 9).

Therefore, the null hypothesis is rejected in favour of H 1, that there is a statistical significance to the observed distribution of endscrapers and animal bones in this assemblage.
The same calculations were performed for Assemblage C, for which the observed values are given in Table 10 and the expected values in Table 11.

Hypotheses
H 0 = the observed distribution of the endscrapers and animal bones in Assemblage C is not statistically significant.

H 1 = the observed distribution of the endscrapers and animal bones in Assemblage C is statistically significant.
For Assemblage C, the calculated X 2 value does not exceed the X 2 value of 3.84 at 1 degree of freedom at 0.05 significance level (Table 12).

Therefore, the null hypothesis is upheld.
Exercise 5: Correlation analysis (L1, L2)

This test establishes whether there is a correlation between two variables, in this case the tools:waste flake ratio and the distance to the flint source.
This data is presented in Table 14, and the two variables are plotted against each other in Figure 7.

The correlation value is calculated in Table 15, and shows a very strong correlation (>0.8) between the two variables.
The tools:waste flake ratio and the mean endscraper length were also tested for correlation.

A correlation value of - 0.14 (Table 16) suggests that there is no apparent correlation between the two variables, and this is supported by the appearance of the scattergram (Figure 8).
Exercise 6: Regression (L2)

The linear regression equation is FORMULA The regression equation for the correlation between tools:waste flake ratio and distance to the source (Figure 7) is FORMULA Using this equation, the number of waste flakes in a new set of assemblages (F-J) can be predicted from the distance to the source of the raw material.
Once the ratio is calculated, the actual number of waste flakes can be predicted using the formula: FORMULA The results of these calculations for the new assemblages are presented in Table 17.

Lowland heath is a key habitat of England supporting a wide range of species, it consists mainly of heather plants but there are also grass and gorse species present as well as some trees.
Lowland heath land has always been a regular feature of the English countryside, but decline has occurred here and worldwide and is now a globally rare habitat, 20% of which can be found in England.

Lowland heath grows in poor soils, these are often sandy gravely soils that are slightly acidic, the factors that create good soil conditions for heather are shallow soil, acidic, and low nutrient level.
On these types of soils it is believed that heath land is the climax vegetation.

All areas of heath located below the Pennines are classed as lowland heath, diagram one shows the line that the Pennines forms across the UK, all heath land below this line is classed as lowland heath.
Diagram one.

Grazing by animals has occurred on lowland heaths for centuries, and without this grazing most areas of heath would have been lost.
Grazing by domestic animals used to be common practise in England, and most areas of common land were heath land areas.

They were important areas for grazing of cattle, ponies, and sheep, with areas of grazed lowland heath becoming "a distinctive part of the cultural landscape in many southern counties" (Hampshire Council).
The livestock were essential to creating the habitats on the heath that encouraged such a large amount of diversity on the heaths, when the animals were removed it caused a disturbance in the balance which also led to the loss of the amount of species diversity.

Since the domestic animals have been removed from the land the area of heath land has decreased, control of succession is still needed and is a constant threat and the heather needs to be grazed to kept it in the building stage, this is the stage of the heathers life where most growth occurs, the plants growth quickly and cover the ground often causing other plant species to disappear.
If the heather is not kept in this stage it will reach the mature and then degenerate stage and then eventually die back.

Using grazing livestock of lowland heath has many benefits, such as the control of invasive species, the creation of diverse habitats, which helps to increase species diversity of a site, grazing animals also help to control scrub encroachment.
Fourteen thousand years ago the UK was coming to the end of an ice age.

The ice sheets that covered the country were beginning to melt.
Tundra vegetation began to grow which included heather plants.

The climate continued to warm and this allowed trees to establish.
Between 10,000 and 6,000 before present the heather may have only survived in open areas in the woodland, or on the edge of woodlands.

Neolithic man began to make its impact on the countryside 5,000 years ago, plants and animals were being domesticated, this was the birth of agriculture.
Areas of woodland were cleared to grow crops.

When all the nutrients had been used from a piece of land the people moved on, this allowed heather to start to grow, grazing from both wild and domestic animals prevented the tress to re-establish.
This meant that by the Bronze Age, 3,000 years before present, large areas of England were covered in heath land.

Heath lands were used by man for many things such as bracken for animal bedding and gorse for fodder.
Trees were cut for fuel and potash, grazing and peat cultivation also took place on the heath land.

These actions maintained the heath land.
During the 18 th century new agricultural technology was developed that allowed 'waste areas' to be cultivated, common lands became enclosed and field systems were introduced and arable farming increased, this all led to the downfall of heath land.

In the 19 th century the heath land areas declined further, many of the traditional activities ceased, as did the reliance on local resources, this was due to the availability of new cheap and easy transport.
This meant the heaths were no longer maintained and so reverted to woodland.

As time progressed so did technology, larger more efficient machines were introduced capable of cultivating more land, producing more timber and extracting more sand and gravel.
The increasing population needed more housing and this resulted in urban encroachment.

All these factors have led to the decline of heath lands.
Almost all remaining heath land is artificial, it has been created on sites that were once wild wood that were cleared and the regeneration has been prevented, this is continued for two reasons to create a habitat that would otherwise not exist, and for use as grazing land in agriculture.

The threats to lowland heath are succession, invasive species, over grazing, conversion to agriculture or forestry, urban encroachment, and change to recreational land.
Picture one shows an area of lowland heath in Danbury Essex, this is what most areas of heath land now look like in the UK, small areas of heath surrounded by encouraging trees and scrub.

Lowland heath can only survive if certain factors are present, these are, exposure, poor soils, grazing, and human activity, if any of these are missing succession will occur.
Succession is the process where other trees and plants grow on the heath and out compete the lowland heath plants, this results in a loss of the heath and creates a new habitat, succession will occur from plants and trees such as Scots pine, Birch, Rhododendron, Wavy Hair Grass and Purple Moor Grass.

Grazing has declined as the drive towards more intensive agriculture increased, and now most lowland heath sites have been totally abandoned as a site of animal grazing.
This has meant that the only remaining grazers of these sites are rabbits and deer, but there are not large enough numbers to continue the amount of grazing that the domestic animals carried out, this has meant that the heaths were allowed to be encroached or put into other uses.

Lowland heath first established itself and the end of the last ice age and the amount present has increased as man started to clear areas of woodland and use that heather that grew in its place.
As agriculture became more intensive and grazing ceased the heath land was either ploughed and put into agriculture or left and succession occurred.

Without the grazing of domestic animals heath land is never likely to survive.
Shinfield farm is situated outside of Reading and is owned by the University of Reading; the farm covers an area of 1200 acres of farmable land, the main enterprise of the farm is the dairy unit, which comprises of at present 550 Holstein Friesian dairy cows.

The other enterprises on the farm are a commercial flock of Scottish Greyface sheep, and also the research that is carried out on the farm in the centre for dairy research and the animal production research unit.
The cows are feed on a 50/50 mix of maize and grass, both of which comes from the farm, the farm grows 400 acres of maize and 150 acres of wheat which is used as whole crop wheat silage in times of shortages, otherwise it is combined, with the grain being fed to the cows and the straw being used for bedding.

The cows are also fed on concentrates which are delivered to the farm on a lorry and stored in bays, it contains a mix of Soya, Rape, Barley, Kale and Wheat, and it also contains minerals supplements.
The cows are fed 12kg of concentrate a day, the concentrate is mixed with the silage using a mixer wagon, the mix is then tipped on the floor in front of the cows, a mix of 9 tonnes will feed 180 cows The feeding system on the farm could be modified to help increase milk yield, currently all the cows are fed the same rations throughout the year, changing to a Complete diet or Traditional methods may help to increase the milk yield, in these systems the cows are feed different amounts of concentrates depending on their stage of lactation and the yield being produced.

The complete diet could be implemented on the farm by splitting the cows into groups depending on their stage of lactation, and then the correct amount of concentrates can be fed to each group to achieve the maximum milk yield.
Calving starts in July and will continue through until December, with there being around 100 calves born in July, August and September and less in remaining months.

Once the cows have calved they are kept inside until the spring, then those producing under 30 litres a day will be turned out to the grass while those over 30 litres will stay in.
The milk is sold at 19 pence a litre, the farm is in the high hygiene bracket and has a volume bonus, this helps to increase the milk price, the farm also receives a London premium of 1.75p as it is situated near the London market.

The milk buyer encourages the farm to produce more milk in autumn and will give a higher price for it then this is one reason for summer calving, the milk is all sold to the Dairy Farmers of Britain and is collected every other day by a tanker.
The milking is carried out in a newly built 50 point rotary parlour, this has increased efficiency, the old parlour only had the capacity for 400 cows and it took four hours to complete all milking, the new parlour has the capacity for 600 cows, and can operate at 240 cows an hour, at present the parlour averages 150 cows per hour.

The milking is done in 9 groups, milking is carried out twice a day, and will occasionally take place three times a day for research purposes.
The dairy unit is operated by three people, two carry out the milking, one prepares the teat while the other fits the teat cups, the third person marshals the cows to the parlour and ensures they are constantly moving in and out of the parlour, there are also automatic gates and a teat sprayer to help with this process.

The average cell count is 199, and is currently at 251, the current milk output is 8750 litres per cow per year with fat at 4% and protein at 3.3%.
The milk yield for this herd is in the high to very high bracket (source: Farm Management Pocketbook), and the butterfat and protein levels of the milk are exactly on what the standard litre is expected to be, (source: Farm Management Pocketbook), this means that the farm is going to receive a good price for its milk due to its excellent butterfat and protein levels.

Sand is used in the cubicles of the milking cows instead of straw, this should help to reduce the chances of mastitis, the disadvantage of this is that sand will wear the cubicles faster; also the scraper cannot cope with large amounts of sand as it is too heavy.
The sand is spread using a machine fitted to the front of a teleporter, 35 tonnes of sand is used a week, at a cost of £17.50 a tonne which is around the same cost as saw dust.

The dairy system being operated on this farm is a high input high output system.
Its is due to the large inputs that the farm is putting into the system that substantial outputs are being received, the diagram below shows the inputs into the farm how they are processed and the outputs that come from them: There are also 900 ewes on the farm, the flock is run for commercial purposes, although they are also used for research, the flock consists of Scottish Greyfaced ewes, and Texel sired lambs.

The flock grazes the permanent grassland on the flood plains, as well as on cereal volunteers and on cattle grazing land in the winter to control the grass growth.
The tupping date is fixed so that all the ewes lamb at Easter, the ewes are tupped with Texel and Suffolk rams.

In June and July between three and four hundred of the lambs are sold, the rest go into store and are fattened through to January when they are then sold.
The whole flock is managed by one worker.

The research facility at the farm conducts applied and strategic research at the Centre for Dairy Research; the research department receives no university funding and is totally reliant on contracts.
The work undertaken includes milk composition, nutrition, reproduction, health and the environmental impact of diary cows.

The cows can all be individually fed using a gate system at the feeding troughs which will only allow the designated cow in.
The animal production research unit looks at cattle and also other animals including sheep, goats, broilers and turkeys, with the research looking at improving carcass and meat quality.

Sheepdrove organic farm is located in Lambourne, Berkshire, it is an organic farm with several enterprises including pigs, cattle, sheep, broilers turkeys and crops.
The farm was established thirty years ago with the aim of running a farm that was self-sufficient.

The farm has since grown and now covers an area of 2,250 acres and employing 40 people.
There is an eco friendly conference centre located on the farm which is used by various organisations such as Vodafone, Waitrose and Landrover, and is used to promote the farm produce.

The farm has shops located in Bristol and London and on the farm, there is also an internet site, all the meat and vegetables that are produced on the farm are available from shops and the internet site.
There are many enterprises on the farm which could not all be covered in this report so I am going to talk about the free range poultry enterprise.

In total there are 25,000 chickens on the farm, the chicks are bought onto the farm at a day old and are stored in house for the first three weeks of their lives, the chicks have access to conservatories so they can get used to natural daylight and sounds of the outside world, such as tractors, cows and birds are played into the shed.
This gives the chickens sold from Sheepdrove a unique selling point as they can claim that the chickens although reared inside to start with have always had an experience of outside life, and also they are probably one of the only farms in the country to play music to their chicks.

The chickens are transferred to outdoor mobile houses after three weeks.
The houses are located in grassy paddocks with each one having around 1000 chickens in each shed.

The sheds are on skids and so can be moved around to different paddocks using a tractor, all the sheds have mains water and the power is provided by solar panels on the roofs of the sheds.
The chickens are feed a mix of Wheat, Beans and Soya in pellet form and are also able to roam the paddock to forage for other food.

The feed is distributed using a hopper system that can hold 10 tonnes and is filled once a week using a tractor and feeder wagon.
The feed ratio for these slower growing birds is around 2:2.5, commercial birds which are faster growing will have a feed ratio closer to 2:1.

The paddocks are also surrounded by hedgerows with herb strips containing birds foot trefoil, yarrow, marigold, salad burnet, sheeps parsley and vetches, the chickens use this for foraging and for self medicating.
The chickens have to be kept on the farm for at least 70 days under soil association rules to qualify as organic, are slaughtered at 77 days.

When the birds are slaughtered they weigh 2.7 kg, and 1.8 kg after processing.
These weights are very different to intensive poultry farming where the birds are slaughtered at 41 days, with a live weight of 2-3 kg.

The longest most birds are kept in intensive broiler production is 60 days, after this time the birds would weigh 3.5kg live weight, 2.2kg after processing, (Agriculture Notebook, 2003), this is very different to the free range chickens who are 17 days older and 0.4 kg lighter when killed.
This means there is greater expense to produce free range poultry, not only do the birds have to be kept for longer, they also weigh less.

This means the farm is reliant on receiving a good price for the organic free range meat.
This can cause problems though with diseases, as the farm is organic it cannot vaccinate its chickens, so when they do contract diseases medicines have to be administered and then a withdrawal period is needed, this can mean that chickens are kept on the farm longer than necessary just so they kept their organic status.

In total around 40p is spent of each medication for each chick with the main reason for its administration being infectious bronchitis.
All the chickens are slaughtered at the on site processing plant, the farm slaughters 2000 of its own birds a week, with a further 5000 finished birds being brought in from local farms.

Around 80% of the meat is sold to Waitrose with the rest being sold through the farm shops and internet site.
The unit is run by one poultry foreman, and there are also four general farm workers who assist with the foreman.

At present the farm is having problems with the transition from the store to the field, when the birds are first put outside there is a loss in body weight which is resulting in a loss of 20-30g on a finished bird, this is costing the farm around £200,000 a year.
This needs to be resolved to stop this major loss in income, one option is to wait longer until the chicken is not at such a vital stage in its growing phase, this will help to reduce the amount of weight lost, the problem with this is that the chickens need to be housed for longer and therefore another store will be needed to hold the new chicks that will arrive during this longer period.

The inputs, processes and outputs for the system are shown in table one.
As can be seen from the table there is quite a high amount of input needed to achieve the output.

The output is a good that will fetch a high price and so therefore these high inputs are worth it.
Prices for the organic free range meat are never likely to fall in the foreseeable future as demand is still increasing as the public move away from meat that has been produced in intensively housed units, this is supported by The Agricultural Notebook (2003) which states, "The demand for poultry meat produced from drug-free fed birds will continue and expand.

Growth in organically produced meat and eggs from chickens housed and managed under semi-natural and total extensive management systems is forecast to double within the next decade".
If prices did decline, or the market for them disappear then the farm would be at risk of serious financial loss, the broilers are the most profitable enterprise on the farm so a large loss of income would occur, the high investment in the housing for and the processing plant could also be lost if the enterprise had to cease.

The poultry system is efficient and has been created to supply an existing and expanding market, this is reflected by the fact that is it the most profitable enterprise on the farm.
The only improvement that needs to be made is creating a new transition system which does not cause the chickens to lose weight when they are moved outside, and therefore increase the profit of the enterprise.

Introduction
This report is based on a hypothetical case study based around a Berkshire farm, the specification is given below: "You are the owner and manager for a 180 ha farm in the county of Berkshire.

The basis of its farming activities has been milk production, and at one time over 300 cows were milked.
Latterly the number of cows have been reduced, but it remains a dairy farm.

In addition to the dairy cows, there are also some sheep and some cereal production.
Staff include a dairy herdsman, tractor driver, and your 19 year old son, currently at university, who helps out during his vacation".

In this report I have made the following assumptions; that the manager and two workers are not in a position where they will be retiring soon, and the son will be returning to work on the farm from university in the next year.
The farm is owned outright and the manager is working on the land as well as carrying out office work.

My final assumption is that the farm is in a financially secure situation but is looking to increase profits.
This report covers five main areas of the business, these are: The roles of the manager, and how they link to management tasks In this section I will look at the technical, conceptual and human skills required by a manager in the agricultural sector, I will also describe the informational, interpersonal and decisional roles and how they link to the management tasks.

The external environment that affects the business I shall identify the main aspects of the external environment and identify the main figures in the task environment.
I will also use the PEST framework to examine other forces that affect the business.

The stakeholders of the business are also identified in this section.
The strengths, weaknesses, opportunities and threats of the business, and its goal over the next two years I will perform a SWOT analysis for the business, and use this to decide on a goal for the business to achieve over the next two years.

I will then write a strategy for how the goal will be achieved and a statement on how the strategy will be implemented.
The communication within the business This section explores the role of communication within the business, the types of communication and the problems with communication.

The business financial, and organisational performance I will use this section to explain the differences between budgeting and financial analysis and why they are important to the business, and I will detail the types of analysis that can be used to measure financial and organisational performance.
Section one

A person in this position will need to fulfil all the management parts, as they will be the functional, general, line, staff and most probably the task manager although this type of management is unlikely to be needed on the farm.
As the farm is self owned there is no one to report to, or whose requests and targets have to be meet, so it would be up to the manager to run the business on a daily basis and then to review the business on a regular basis and check that the management that is being implemented is being successful.

The manger could delegate certain sectors of the farm to employees, such as having the herdsman responsible for the dairy unit, and he would then need to report to the manger as he would be the herdsman general and line manager, as well as his employer.
The manager in this role would need to have many conceptual skills as they are responsible for the entire business, the conceptual skills needed are thinking, information processing, planning and decision making.

The planning will be needed to effectively manage the crops and livestock, factors such as when to drill, apply fertilisers and chemical sprays and when harvest will take place.
This will need to be planned to ensure there is the staff to carry out all the tasks, and that the materials are on site when the job needs to be carried out.

The ability to process information is also important, what is grown and the livestock kept will depend on current and predicted prices so the ability to read the information on these issues is essential to choosing a profitable venture.
The ability to make decisions will help with choosing what crops will be grown and what livestock keep and how much time should be spent on certain tasks, but these decisions can only be made in conjunction with the other conceptual skills of thinking about what tasks need to be done, planning them and processing information to make informed decisions.

As the sole manager of the business the person in this role would need to lead the workforce, this will involve directing them on what work needs to be done and when and most probably leading by example as the manager of this business will probably also have to work on the farm.
This is linked to communication, it is essential that the person in this role has good communication skills as they will need to set the goals of what is trying to be achieved by the work carried out, and they will also need to convey work and tasks to employees clearly, this is especially important in this working environment, as carrying out a task wrong can have very large consequences that may affect the survival of the business such as the spraying off, or cutting of the wrong crop.

Other human skills will also be needed, such as the ability to co-ordinate, this will include the co-ordination of staff, the manager would have to ensure that he has the man power to carry out all the tasks and that the business is never under or over staffed, this may mean that extra workers will need to hired during busy times such as harvest.
As the only manager, the person in this role would also be responsible for resolving any issues the workforce have, this could be difficult in this situation because it is such a small workforce the manager is likely to friends with all the employees, it will also be difficult if his son was involved and what have to act as though he had no connection to any of the employees.

The person in this role would also need to be self motivated as they will have no one to motivate them, they would also be responsible for motivating the staff, so it is essential that they never loose sight of the end goal or it could end up affecting the workforce as well.
The third management skill that would be needed by someone in this position is that of technical skill, this means having the ability to understand the work of the business and how it is carried out and be proficient in the skills required.

In a farming business this is vitally important, as the manager will probably have to do manual labour as well as doing the office work.
They are also most likely to be the main source of relief staff to cover the workers when they are ill or on holiday, it is therefore essential that they poses some basic knowledge of all the farming activities so that cover can be provided.

The person in this position would be responsible for carrying out many informational, interpersonal and decisional roles due to the fact that they are the only manager and the owner of the business.
The informational roles that will apply are, monitoring information, being able to find the information they need and read and understand it, this is relevant to this job as information on the industry will constantly need reviewing, such as the price of inputs and outputs, predicted changes in the industry and new techniques and technology that is developed that could increase efficiency.

The farm accounts will also need reviewing regularly to check that the business is not going to make a loss, budgets will also need to be set and this will not be possible unless information is known on predicted price for inputs and outputs, and what new machines or equipment will need to be bought.
The role of a disseminator is not particularly relevant to this role as it is a small business, and there is no owner to present too as the owner and manager are the same person.

However if any information was to be told to the employees it what be the responsibility of this person.
The role of spokesperson is the responsibility of the person in this role; they would be the one to represent the business to other companies and organisations.

The person is the position will have to lead and liaise with people, these are the interpersonal roles.
As the manager and owner the person would be responsible for directing and motivating the staff, and also motivating themselves, as the person has no boss there is no one to provide motivation for them, so they must provide themselves.

Liaising with suppliers and buyers, and also possibly financial institutions would be the responsibility of the manager, using the information gathered while liaising with these people should help the person in this role to manage the business more effectively.
The decisional roles the person in this role will have to make are very important as they will affect the success of the business and there is no one to over see the decisions or to liaise with on important issues, so the manager must be entrepreneurial to be able to spot and exploit opportunities in the market, or to see future threats and change the business so that it isn't caught out when the threat appears.

They must also be able to deal with any problems and resolve them, whether this be staffing issues or unexpected supply or sale issues.
The ability to allocate resources is also essential, the resources available have to allocated in a way that will maximize there returns.

This is where a good business mind is needed and also the use of the informational roles will help to make these decisions.
In this job the ability to negotiate will also be an important part of this job, if the person in this role can effectively negotiate then better prices may be obtained for inputs and outputs.

Problems could also be resolved faster if the person is able to negotiate with the people involved.
All these roles can be linked to the management tasks of production, marketing, financial and personal management.

The following list shows which roles and tasks are linked:
Monitor- marketing, financialDisseminator - personnelSpokesperson - marketing, financialLeader - personnel, productionLiaison - marketing, financial Entrepreneurial - production, marketing, financial Disturbance handler - personnel, financial Resource locator- production, marketing, financial, personnel Negotiator - personnel, financial

As the manager of this farm I would have to cover many management roles as I am the only person in a managerial position in the business.
As the manager I would be responsible for day to day running of the farm and implementing plans, as the owner I would have to review the business to check that the management that is taken place is effective.

As the manager of this business I would need good conceptual skills to process information, plan and make decisions, I would also need good human skills to lead and motivate the workforce and to co-ordinate their work.
The technical skills are also very important for this role as help may be required from me, but also so I know what I am asking my workforce to do and know if it is achievable.

I would also have to represent the business and monitor the business in the internal and external environment.
In the next section I will go on to look at home the business is affected by the external environment.

Section two
This business will be largely affected by the external environment, as it will determine what is farmed and how it is farmed.

The external factors that influence the business fall into four main sectors; these are the political sector, the economic sector, the socio-cultural sector, and the technological sector.
Within each sector there are many different factors that will affect the business is various ways, some of the main factors are set out below.

Political -
Regulations:

A farming business can be affected by regulations from four main areas; these are local regulations, national regulations, EU regulations and international regulations.
These regulations will govern what can be grown and how it can be grown, these regulations are often very strict so have to be adhered to otherwise it could have severe consequences for the farmer and the business.

Quotas:
A quota will determine how much of a product a farmer can produce, if he produces over this quota he cannot guarantee that his product will be bought.

This will dictate how the land is farmed and what production occurs.
For example it is inefficient for the business to produce over its milk quota as we will be paying for the production but may not get a return on the product.

Pressure groups :
The pressure groups will affect what demand there is for certain products, for example if a large scale campaign was launched by a vegetarian group trying to promote vegetarianism then the demand for meat may fall affecting farmers prices and may meaning they have to change what is produced.

Also the status of the business in the local and national community can be affected by pressure groups, if a farm is exposed as miss treating animals or using chemicals on crops that could endanger human health consumers are highly unlikely to buy from this source.
Trade barriers:

If trade barriers are established by either UK or International governments then it may affect the import of UK produce to other countries and therefore lower the demand for the product, and this will also lead to a decline in the price of the product.
An example of this is the EU ban on UK beef exports which was established after the BSE crisis in the UK.

This saw the UK beef exports fall from £1bn before the ban to £20m in 2004 during the time of the ban.
Economics -

Imports:
Imports into a country will affect the price of the home produced goods; if the import of goods is too high then the producers in the U.K.

will end up with no market for their products.
Purchasing power:

The purchasing power of the consumers will affect what the business can produce.
For example if the consumers have a low purchasing power then they will not be able to afford to purchase higher costing goods such as organic food or specialist food.

Exchange rates:
The exchange rate between countries will affect the price of the product and also the amount of import and export that occurs.

For example businesses in Europe are more likely to trade to those who use the Euro as there is no need to exchange currency.
Government:

Most economic factors are shaped by government or EU policies
Socio- cultural -

Religious beliefs:
The beliefs of the consumers will affect what is produced and how the goods are produced.

For example Halal meat which is consumed by Jews and Muslims and is killed in a specific way.
Direct selling:

Farmers markets and farm shops have become more popular as people become more aware of the impacts of buying food that has travelled hundreds of miles and are starting to buy more local products.
There is also an increase in novelty shopping occurring, this means that more people want something different from their shopping experience and so visiting the farm shop or farmers market makes the shopping experience different and more enjoyable.

Technological -
New machines and techniques:

The development of new machines and techniques will help to make the farm more efficient, such as more precision farming that will increase farm yields.
This will probably increase the profit of the business.

G.M. food:
The use of G.M.

crops will most likely help to increase yields, and cut input costs.
This will help the business as it will increase the profit that it is making.

A stakeholder is an individual or group that is either within or outside the organisation that has a stake in the business.
There are two types of stakeholders, they are primary and secondary, the primary stakeholders are directly affected by the business, secondary stakeholders are not directly affected by the business but have an interest in it.

The primary stakeholders in this business are: The owner - they are looking for a return on their investment, in this situation if the business fails then the farmer is likely to lose everything The employees - they will want job security, job satisfaction, paying and benefits such as health care.
They will also be looking for a safe environment to work in; this is a major factor on farms as they are a very dangerous work place.

Suppliers - suppliers want to know that they will be paid for the products that they sell to the farm, as most sales are done on an invoice basis the stakeholders want the farm to succeed so they get paid.
Customers - the customers are looking at the availability of the product and its quality.

If availability or quality is low then customers are unlikely to return.
The secondary stakeholders for this industry are: The government - the export value the farms produce affects the government and the economy, by producing goods to export the farm is helping to improve the economy.

The farm is also creating jobs in the rural sector where there is a lack of employment; this is helping to reduce unemployment.
The local community - the farm is providing jobs in the local community, it is also helping the local economy as it will use other local services.

In conclusion the farm will be affected by four forces in the external environment; these are the political, economic, socio-cultural and technological.
As the manager it would be my responsibility to ensure the business is able to survive the pressure each of these forces puts on the business.

The primary stakeholders are all very important to the business and without them the business would not be able to exist, it is therefore essential they are kept happy, the business is also important to its secondary stakeholders as it helps the national and local economy.
In the next section I will look at the other environment that affects the business, this is known as the internal environment.

Section three
As the manager of the farm it would be one of my tasks to asses the business in different environments and see how capable the business is to respond to the environment.

The two environments that the business needs to be assed in are the external and internal environment.
To do this a SWOT analysis can be carried out, this involves looking at the strengths, weaknesses, opportunities and threats the business has.

Diagram one shows some the strengths, weaknesses, opportunities and threats the farm has, some of which are explored below.
The internal strengths and weaknesses of the business can be measured by reviewing how efficient the business is at using the resources available to it, the use of technology, the structure of the organisation and the people that work within the organisation.

The farm has changed what it produces to make use of more resources, it has changed from being a solely dairy farm to a farm that has dairy, sheep and cereals, this means that the farm is spreading its risk as well, if one enterprise was to prove not viable then it could be shut down and the extra resources than allocated to the remaining enterprises.
The farm is also making good use of technology as it uses up to date technology in the form of tractors and the parlour.

The organisation is also structured well, as there are few employees there cannot be a large structure system, but they way it works with the dairy herdsman being responsible for the dairy herd and the tractor driver being responsible for the arable work, it means the manager has more time to concentrate on running the farm as a whole and looking at ways to increase its productivity and profits in the future.
As the manager is also the owner it means that decisions can be made quickly and the interest of the owner will also be the interests of the manager which is the farms success, this is a major benefit of this type of organisation structure.

Porters five forces are a measure of the external environment and how they can affect the business; they are used in a SWOT analysis along with the PEST factors that are detailed in the previous section.
They can be used to asses the threats to the business which is part of the SWOT analysis.

Porters five forces are:
existence of competitorsexistence of substitute products or servicesthe ability of new competition to enter the industrystrength of buyersstrength of suppliers

The farm would face competition from local farms, larger farms, imports, goods at lower prices, and higher quality goods.
Local farms will be competing for the local contracts, if they can offer a better quality or cheaper good or more of the good then they may gain the contract, the farms needs to choose whether it is going to go into competition with the local farms or whether it would be the weaker farm in the market so it would be more beneficial to produce a different product.

Larger farms will probably be producing products on a larger scale than this farm is able to so it is more likely to get contracts from big buyers such as supermarkets as all the produce can come from one location cutting down transport and logistics.
To compete the farm could over a niche market good such as organic produce, or the farm could aim to compete with the larger farms by producing the same good at a superior quality.

Imports are hard to compete against as they often able to produce goods a lot cheaper often due to cheaper labour costs.
Exploiting niche markets could help to solve this problem, examples of how this could be done are setting up a farm shop or selling produce at farmers markets, these are also examples of the opportunities available to the farm.

The farm should be able to effectively compete with the lower priced and higher quality goods that some farms may produce.
Although one reason may be to high investment, it could also be due to a good farm system and if this farm were to adapt it should be able to produce goods to the same standards and therefore compete in the market.

Substitute products in the farming industry are goods such as free range or organic products, they are almost identical to the other goods accept are produced in a different way which may be more appealing to some customers.
The farm can compete by either joining this market or by producing high quality goods and low prices that will hopefully make customers buy them.

It would be difficult for new competition to enter the agricultural industry as it requires a large amount of finance firstly to acquire the land and then to but the machinery and plant needed.
To gain contracts can also be difficult as most buyers will stick with trusted suppliers, so it may take a long while for a new farm to find a consistent contract.

The buyers in this situation have a large amount of strength; there are many farms which signify that there is a large amount of choice.
If the farm is not producing high quality goods or is not fulfilling its contract then buyers can quite easily go elsewhere.

The strength of the suppliers depends on which good is being bought, for small goods such as tools and small equipment there are many suppliers so the strength is with the buyer, in this case the farm.
However for larger goods, such as a tractor, there is a large amount of choice before it is bought as there are many makes, but once it has been bought there are a limited number of suppliers for parts and spares so the strength is then with the supplier.

The goal of the farm over the next two years is to be a farm that is making a profit and producing high quality goods.
This will be done in a way that cares for the environment, employees and the local community while also providing food for the UK and international market.

This will all be done using sustainable practises that will not exploit or damage the land, and the aim is to increase and improve the farming practices implemented.
This goal can be shown to be realistic and achievable using the smart system, Specific, Measurable, Agreed, Realistic, and Time, this reviews the aim of the goal (S), how it can be measured (M), how agreeable it is (A), how realistic it is (R) and the time and resources it will take to carry it out (T).

The smart system is set out below for this farm business: S - The aim of this goal is to increase profits and farm in a sustainable way.
M - Both these goals are measurable; a monthly and yearly review will show how the profits have changed.

A wildlife or species diversity review carried out once a year, or recording of wildlife sightings will show if the sustainable farming is helping to improve the wildlife numbers.
A yearly soil examination will also see if the new techniques are helping to benefit the soil, one way of doing this is to carry out an earthworm survey.

A - The workers should agree to these new aims and the new techniques that they bring as it will benefit them.
There may be some initial concerns that the business will fail to make a profit and so they will therefore loose their job, but by talking with employees and explaining the benefits they should agreeable with the plans.

It is essential that they do agree in a business like this as each worker is responsible for a whole enterprise on the farm, and if the lose motivation it will most likely result in the failure of that enterprise.
R - These goals are realistic and can quite easily be achieved, it will require some additional work from the employees to learn the new regulations and techniques but it not an unattainable goal.

T - The time limit for this goal is two years, by the end of these two years there should be some indication as to whether these goals are going to be successful, but it will probably take many years before it will be seen exactly how successful it will be.
The advantages of these goals is that they can be carried out using current resources and staff and that additional workers are unlikely to be needed, accept the son who could return to the farm after university to run the sheep unit and help to expand this enterprise, increasing profits.

The goal can be implemented by modifying the use of the current resources; the farm currently has dairy, cereal and sheep enterprises.
The farm has always been a dairy farm and I believe it should stay this way, the farm already has a good dairying systems and a dairy herdsman so it makes sense to maintain this enterprise, to meet the goal of having more sustainable farming while also producing a profitable and quality product I believe the dairy enterprise should become organic.

There will be no additional costs as the parlour system is already in place, all that will need changing is the grass system that is used as this will also have to become organic to supply the cows with food.
There is likely to be increased costs from concentrates and a lower milk yield but this will be outweighed as the organic milk will fetch a higher price in the market as it is worth 26.5 pence per litre whereas conventional ilk is worth 18.5 pence per litre (farmers weekly interactive), the calves can also be kept and stored and then sold as organic beef, this will also fetch a high price.

The sheep enterprise should also be changed to organic for three reasons, the higher prices that will be achieved for the meat, the increased benefit to the environment and also convenience.
If both animal enterprises are operating organically it will make it easier for working and there won't be issues of the organic animals eating non-organic food or becoming infected with diseases that the non-organic animals have been vaccinated against.

I don't believe that all the cereals should become organic as this has a high risk factor, in an idealistic state the whole farm would be able to become organic, but this increases the risk of a whole year's harvest being lost, and if this occurred it is unlikely the farm could recover.
Some organic cereals should be grown as the grain will be needed to feed the animals, and the straw will be needed for bedding.

These goals can be achieved by changing the current land on some areas of the farm into woodland or permanent pasture; this will help improve biodiversity on the farm and will be mean the land is no longer damaged by cultivations.
These areas could be managed with the emphasis being towards environmental sustainability rather than agriculture; this could be done by reducing stocking rates on the land or by reducing the amount of time livestock is allowed on these areas.

These changes will be implemented over the whole farm at the same time but it will need to be done in stages, the farm cannot change from being conventional to organic straight away, there is a transition period, and to start with none of the milk will be classed as organic so although the livestock being kept organically, the organic status will not be achieved until after nine months, but they may never be sold as organic cows, the calves that are born after the change to organic will be the fully organic generation.
This will be the same with the lambs, it will be the second generation of lambs that are organic, the current stock can not be sold as organic as they have not eaten organic food for all of their life.

The crops that are going to be grown organically will have to be kept separate from the conventional crops and it is important that they come into contact as this will cause their organic status to be lost.
The organic status will take two years to obtain and in this time the crops grown on the conversion land will not have organic status, so the farm may be making a loss during this time but it will be recovered when the status is gained.

To do this a cropping system will need to be developed so that all the organic crops can be harvested first, stored and preferably moved off the farm before the conventional crops are harvested.
The SWOT analysis is used to measure the business strengths, weakness, opportunities and threats; it is used by using the PEST framework and Porters five forces.

I have shown in this section that the business has many options available to overcome its threats and there are also many opportunities open to it.
I have also set a goal for the farm that it can achieve in the next two years, and I have assed the goal using the SMART system and have shown how the goal will be implemented.

This is linked to communication, which is explored in the next section, as the goal will have to be communicated to the workforce.
Section four

There are three functions of leadership; these are motivation, communication and encouraging teamwork.
Communication is a vital part leadership as the ability to communicate will affect motivation and the success of the business.

A manager will spend 80% of their time communicating this can be done through a variety of channels such as face to face conversations, over the telephone, via letters and emails, or through items on newsletters or on notice boards.
Communication is a part of every function of management and without communication the manager would not be able to explain plans, share visions or implement changes.

Communication as defined by Daft (2002) is "the process by which information is exchanged and understood by two or more people, usually with the intention to motivate or influence behaviour".
This means that communication is more than just a manager talking to employees and telling them what is expected, it is about interaction and the sharing of information and ideas.

Managers need to interact with the employees as this will give them a better idea of the state of the business and how it can be improved.
The ability to affectively communicate is a difficult skill to master, many times people end up conveying the wrong message to people, whether this be at work or in our every day lives.

The communication process is made of five parts: encode, message, channel, decode, and feedback.
The sender of a message encodes a messages sends it via a channel such as letter or email, the receiver then decodes the message and then delivers feedback.

It is during encoding and decoding that mistakes in what is being communicated can occur, the knowledge, attitude and background of the receiver maybe different to that of the sender so they may interpret the message in a different way.
There are many ways of communicating within an organisation and each one is used to convey different information, it is important that the manger selects the right type to use in the situation.

The commutations can be found on a scale that runs from low media richness an example of this is reports to high media richness which is face to face talks.
Low media rich channels should be used for information that is already agreed or understood, or when a record of what is said is needed.

Media rich sources should be used when the message is likely to be misunderstood, or when a quick response is needed, media rich sources are also more personal.
Communication is not solely about talking, listening is also a key part and without it communication is not taking place as 75% of effective communication is listening, but most are unable to does this, so mistakes in communication occur.

There are 10 key skills that are essential to listening, these are:
listen activelyfind areas of interestresist distractionscapitalise on thought being faster than speechbe responsivejudge content, not deliveryhold one's firelisten for ideaswork at listeningexercise one's mind

A good listener is able to do all of these and therefore understand and engage with the speaker, the definition of a good listener in Daft (2002) is someone who "finds areas of interest, is flexible, works hard at listening, and uses thought speed to mentally summarise, weigh, and anticipate what the speaker says".
There are barriers to communication that can cause messages to become distorted; they exist in two areas; individual barriers and organisational barriers.

In each there are several barriers that have to be overcome, they are shown below: Individual Interpersonal - problems with peoples emotions and perceptions Media - using the wrong channel to communicate with employees can lead to confusion and mixed messages.
Semantics - this means understanding the meaning of words and how they are used.

Different words may mean different things to people.
Inconsistent cues- the verbal and nonverbal communications need to be the same so that confusion does not occur.

Organisational Status or power differences - low level workers may be unwilling to pass up bad news creating a false image of the business.
Also high status workers may ignore low level employees thinking they have nothing to contribute Different needs and goals - each department will have different needs and goals, the departments to work together and compromise for the benefit of the business.

Lack of formal channels - without formal channels of communication the organisation cannot communicate effectively and therefore cannot operate at its best.
Unsuited communication flow - if wrong communication channels are used then there maybe insufficient information provided, similarly it is a bad use of resources to use media rich channels for low level information.

Poor coordination - departments may become isolated, or management will not know what is happening in each department.
This means the organisation is not operating as a whole as is therefore not working at its best.

Communication at the farm is very important as without it work may not be completed or tasks maybe carried out the wrong way, this will all lead to a lack in motivation.
As each enterprise is being run by one person it is important that their views are listened to and that they have input into how the business is run, they are the ones that are running the enterprise every day so they therefore know it the best.

By listening to the workers it can improve the farming techniques that are used which will help to increase the profits of the business.
At the farm it is best, and probably most practical to use, media rich channels to communicate, the manager is likely to see the employees every day so can talk to them in formal and informal settings about his ideas and the ideas of his workers.

The telephone is really the only alternative method on the farm as the workers will be carrying out practical work most of the time so will be unable to regularly check emails or read letters or memos.
A notice board can be used for less important information but this will probably need following up with a personal meeting.

The barriers to communication at the farm are, +Problems with people's perception of how tasks should be carried out or how systems should be run; everyone needs to agree to make the business work +Using inappropriate channels to communicate with employees what work needs to be one, could lead to mistakes +Difference in knowledge, the son will have learnt about agriculture today and in the future, whereas the manager and workers may be more inclined to stick with traditional practises and not listen to new ideas +The workers may be afraid to report mistakes or failings to the manager; this will mean that the manager assumes his business is working fine when really it may be struggling +The different enterprises need to work together so that they all benefit, if they all just work for their own benefits then the business will suffer +If the different enterprises do not know what each other is doing then opportunities could be missed, or extra expense laid out Communication is a vital part of the business, and without it the business would not be able to operate.
There are many ways to communicate as I have shown in this section so it is vital that the correct method is used to communicate or it could cause problems and misunderstanding.

On the farm that can be many barriers to communication one of the main ones being different peoples knowledge or techniques for carrying out work.
It is essential that everyone understands each other so the business can work as a unit.

Section five
Budgetary control and financial control are both used as management controls, the definition of budgetary control is "the process of setting targets for an organisations expenditures, monitoring results and comparing them to the budget, and making changes as needed" (Daft, 2002).

This means that the budget is used as a report to monitor planned and actual expenditures, they are used to look at costs, assets, investments and salaries, the budget will often set out expected costs and then actual costs so a comparison can easily be made between the two.
Financial control is used to monitor the financial performance of an organisation using financial statements; they tell managers where the organisation stands financially and indicate possible future problems.

The definition of a financial statement is "the basic information used for financial control of an organisation" (Daft, 2002).
Financial analysis is linked to financial control; once finical statements have been prepared they need to be interpreted.

This can mean comparing the organisations performance with previous years or with competitors or industry leaders.
By creating these comparisons it allows the organisation to see if it is improving and how competitive it is.

When carrying out budgeting analysis there are four different budgets that can be used, these are expense budgets, revenue budgets, cash budgets and capital budgets.
The expense budget is used to look at anticipated and actual expenses, for each department and for the organisation as a whole.

It can show all types of expenses or just focus on particular expenses such as research.
The revenue budget lists the revenue that was forecasted and then the actual revenue received.

If the revenues comes in below the expected amount then it means investigation is needed, investigation may also be needed if it is higher than expected.
Cash budgets "estimates the receipts and expenditures on a daily or weekly basis to ensure that an organisation has sufficient cash to meet its obligations" (Daft, 2002).

If the budget show the organisation has an excess then it may be able to invest to earn interest, if it is going to be low then arrangements have to be made to borrow money.
Capital budget records investment plans that will be long lasting, often over a year, for assets such as plant and buildings.

They will have great effect on future budgets, but will hopefully they will cause a rise in profits, if not then the investments are probably not worth while.
The reason the cash budget is necessary is "to plan the impact of these expenditures on cash flow and profitability" (Daft, 2002).

There are three measures that can be used to carry out financial analysis these are the balance sheet, this shows the organisations financial situation at a specific point in time.
It is concerned with the assets the organisation has and the liabilities it has.

It gives three pieces of information, assets, liabilities and owner's equity.
The second financial measure is the profit and loss account; this shows the wealth created by a business in a given period of time.

On the profit and loss account there is information on revenues and expenses, which includes tax and depreciation.
At the bottom there is the net income, this is the profit or loss for the given time.

The third measure is the cash flow statement; this has details of receipts and payments, its shows when money will be coming into the organisation and when it will be going out.
Other techniques that can be used to measure and control the organisational performance are the TQM (total quality management) techniques.

Some of the techniques are listed below: Quality circles - this is a group of between 6 and 12 volunteers who meet to help solve problems that affect the quality of their work.
It is an ongoing process and meetings are usually held once a week.

The members of the group may use existing data or they may collect their own.
The idea of a quality circle is that the people who are doing the job can have an input to how it can be improved.

This technique would not really work on the farm due to the low number of employees; the manager could operate an open communication system where employees feel confident to express their ideas on task and how to improve them.
Benchmarking - the definition of benchmarking is "the continuous process of measuring products, services, and practices against major competitors or industry leaders" (Daft, 2002).

It is important that the company is honest with its self when it analysis itself, and that it picks a company with a similar goal and mission statement.
The farm would be able to use this type of analysis, as it could compare itself to successful local or national farms.

Continuous improvement - this is the approach of changing a little bit constantly rather large scale changes infrequently.
This type of change has the highest probability of success as it is a never ending system so the company can adjust to markets and demand in small stages.

This method of control can be used on the farm as each worker can improve how they do their job and run their enterprise over time, this will mean that the farm will not have large costs at once, and it should also keep them at the front of their industry.
Quality standards are also a way of measuring the performance of the business, an international example is the quality assurance scheme called ISO 9000.

This is a set of standards that are created by the International Organisation for Standardisation, it is a globally recognised certificate that many businesses require and organisation to have before they will enter into business with them.
An agricultural example of quality standard schemes are the Quality Assurance Schemes that operate for all sectors of the industry such as the Assured British Meat, who's aim is "To maintain and develop credible Standards within the red meat industry covering food safety, animal welfare and environmental protection and to promote integrated beef and lamb assurance to provide consumers and retailers with confidence about product safety" (ABM, date unknown).

By complying with these standards the farm can be sure it is producing the highest quality meat as only the best will be allowed to enter the scheme.
Another form of assessing a business performance is to take into account finances, markets, customers and employees so that the whole of the business can be evaluated rather than just the financial sector.

An example of this method is the balance scorecard which is shown in diagram two.
The balance scorecard looks at four key areas, financial performance, customer service, internal business processes, and the capacity to learn and grow.

The farm can use this to view how its activities affect financial performance, how the farm is viewed by customers and its potential to gain more employees, how the farm operates and if it is meeting orders and costs, and the potential the farm has to improve and grow, this is mainly focussed towards employee development for the future.
The son is an example of the development of an employee, by sending him to university he will gain a better understanding of modern agriculture and can then input his knowledge to the farm, and then one day take over being competent for the job due to previous investment from the farm in his education.

FORMULA A business needs to carry out analysis to establish where it stands compared to its competitors; it is done using a budget and financial analysis.
The budget analysis is used to set out expected expenditures and then to review what actually happens, it is done using four budgets these are expense, revenue, cash and capital budgets.

Financial analysis looks at the businesses financial performance; this is done using three methods, the balance sheet, the profit and loss account and the cash flow statement.
Other ways a business can analysis it self is by organisational analysis using TQM techniques such as benchmarking, and by using industry and quality standards.

Conclusion
In section one I set out to look at the roles the manager of this farm must perform and the technical, conceptual, and human skills that they must have.

I have shown that the manager must be able to fill all the roles of management as he is the only one in a management position in the business.
The owner and manager of this business is the same person so they must manage the farm on a daily basis and also review their management looking at it the eyes of the owner and implement changes that are necessary.

The manager also needs good technical, conceptual, and human skills in order to process information, make decisions, and plan how they will be implemented.
Also to lead, motivate, co-ordinate the workforce and to have knowledge of the work being carried out so assistance can be offered and proper planning can be achieved.

In this section I also looked at the informational, interpersonal and decisional roles that apply to the manager and how these link to the management tasks of production, marketing, financial and personal management.
To be able to find, understand, and use information is an important skill for the management, it relates to the financial side because it is important they are able to interpret financial accounts and important for production to ensure the farm is up to date with the latest techniques and is aware of the machines available to it.

To have good interpersonal skills is essential as the manager will have to deal with employees as well as the external environment, he will have to be able to motivate the staff, while also be a spokesperson for the farm and liaise with customers and suppliers.
As this is a small farm business having an entrepreneurial manager is a distinctive advantage as it means they will be able to exploit opportunities in the market and increase profits, this is part of the decisional roles that the manger must perform, which as I have shown in this section are a vital part of the skills the manger in this position must have.

Section two looked at the main aspects of the external environment using the PEST framework, from this I was able to show that the farm will be influenced by every sector of the PEST framework, some of which have more detrimental affects than others, but all can ever be over come or worked around so that the business can continue to make a profit.
This section also contained the main stakeholders, the main ones to the farm are the owner, the employees, the suppliers, and the customers they all have a very high interest in the business although it is only the owner who has high amounts of power.

The farm is also important to the local and national economy as it provides jobs and brings money to the country and also to the local community.
The SWOT analysis was used in section three; this used the PEST framework from the previous section as well as Porters 5 forces to create an image of the business internal and external environment.

This section should that the business does have several threats mainly in the form of larger and local farms and from imports, but I have shown how these can be overcome but entering into niche markets such as organic, or to start new enterprises such as farm shops.
I decided on a goal for the farm over the next two years, which was to become an organic farm.

I have then used the SMART system to show that this is achievable; I then showed how the farm will change to an organic farm over a period of time which is mainly determined by the Soil Association.
In section four communication within the business was examined, why it was important, the types used and problems that could occur.

I showed how important communication is and how it is more than just talking; I showed how it is a vital part of the farm business but is often a difficult thing to get right.
I showed the ways that could be used to communicate and explained why the face to face method would be the best to use on the farm.

I showed the importance of using the correct delivery method when communicating and how it can cause problems if the incorrect method is used, I also listed the other barriers that occur and why.
Section five was concerned with budgeting and financial analysis; I demonstrated how they are both important to a business so that it is able to analysis its financial state.

I showed the types of analysis that could be used and what each one showed, and how it was useful to the business.
I then looked at the organisational analysis that could be used to compare the business to industry leaders and competition, and how other areas of the business can be monitored other than just finance.

The Manydown Company is an estate located in the village of Wootton St Lawerance, near Basingstoke, it is made of three tenant farms, the main enterprises across the three farms are arable, beef and dairy.
The estate is family owned, and the family along with a director make up the farm board.

The board also has many top advisers who help to carefully manage the farm to ensure its survival for the next generation and only really interested in the long term plans of the farm.
The director is responsible for implementing the family's plans for the farm.

The farm then has eight workers who are involved in the daily running and operation of the farm.
Although each person is employed due to a particular skill in a certain area they also work all over the farm and are never solely working on their enterprise, for example the arable staff work in the slaughter house once a week killing chickens.

It is due to these workers being able to, and also being willing to work in many areas that this farm has become such a success.
The estates mission is "To operate and effective country estate, supplying quality farm produce to the food industry.

Caring for our staff, our animals, the environment, soils and buildings.
Trying to maintain a long term future for everyone within the company.

Providing a quality range of services, both commercial and charitable, to the local community." As can be seem from their mission statement, the management of the estate is very much based of preserving what they already have and working towards enhancing this.
The public are also a major part of the farming policy, "to be the "preferred supplier"".

The estate has set up a farm shop where it sells products grown on the farm and from local products.
The shop was originally set up for the sale of beef but now sells many products and has been the reason for many introductions on the farm, including the chickens, pigs and Angus beef.

There was a demand in the shop for quality Angus beef and the farm decided that it could meet the demand so introduced Aberdeen Angus cattle to the farm seven years ago.
The chicken enterprise was also introduced due to a demand from the shop.

The original supply of chickens to the farm was not of good quality so it was decided that chickens would be reared in a free range environment on the farm.
The chicks arrive at a day old and are kept outside, after three weeks they are allowed to roam outside during the day.

After ten to eleven weeks the birds weigh 2-2.5kg and are ready for slaughter at the onsite slaughter facility, after hanging the chickens are packaged and sold in the shop for £1.90 a pound.
The slaughtering of the chickens is also based around customer demand; the chickens are killed on a Thursday, hung over the weekend and are then ready for the shop for Wednesday.

This is because the biggest demand for chicken is on Wednesday through to Saturday.
The farm also has a flock of 800 ewes, 100 of these are poll Dorset ewes, the rest are North Country Mules which are served by Hampshire or Texsel rams.

The sheep enterprise is also set up to meet the demands of the shop.
As the shop wishes to supply lamb all year round the farm has four lambing periods at Christmas, late February, April, and May.

This ensures that there are lambs available all year round so the farm is able to supply its customers with the goods they want.
The farm also has a herd of large black pigs which consists of 6 sows and 2 boars, each sow has 20 piglets a year in 2 litters they are weaned at 6 weeks, and are kept outside in grassed paddocks and fed on cereal based roll as well as foraging for themselves.

They are slaughtered at 9 months; the pork is then used to make bacon, sausages and ham that are being sold through the farm shop.
The pigs are proving to be a viable enterprise, while also helping to maintain the numbers of the breed which is listed as critical on the rare breed trust website.

The breed was first introduced to the farm because there was a shop already in place to sell the meat, and also because it gives the shop an edge over its competition.
The shop is able to advertise the fact that it has a rare breed of meat for sale that is not widely available, and hopefully then draw in more business.

If there was not already a shop to sell it through or if the farm had no competition then the pigs probably wouldn't be kept.
The farm shop is on the site of an old contractor business, which used to sell frozen beef as a side enterprise from a small shop.

The shop now sells beef, lamb, free range chicken and pork all from the farm as well as other local produce such as pickles, cheeses and juices.
In 2004 the shop was awarded Hampshire Meat Producer/Retailer of the Year; this has helped to draw customers to the shop.

Manydown estate took over the shop in 1994 when it was making £50,000 per annum, the shop has since grown and has had three extensions and now makes £780,000 per annum with most weeks seeing 700 customers a week.
Over the last 10 years the shop has managed to achieve growth of 10% per year, with word of mouth being the main advertiser, but the shop now needs to look at other ways to advertise itself to more people so it can continue growing.

The reason the farm shop was first created was because the farm could no longer make as much money due to the increased supply and very small increase in demand which came about after the UK joined the EU, subsidies were used to help farmers but they were reduced in the 1990s and this meant farmers either had to diversify, expand, downsize or quit.
Manydown decided that it would diversify, there were many options open to them such as use the land for forestry or leisure, rent out buildings for leisure or work, go into the agricultural inputs sector, or go in to the process and retail sector.

They choose the retail sector and hence why there is now a farm shop.
The shop and the farm have a close relationship that has meant that the farm can supply the shop with exactly what it needs, meaning that its customers are delivered with exact the products they want all year round in most cases.

The shop is able to adapt quickly to customer demands as the shop has a direct link to its suppliers and can work with them to get exactly the products they want and often at short notice.
This has meant that the shop has seen a growth in its customer numbers.

Theory:
In the same power range 3-phase is typically 150% more efficient than single-phase.

In a single-phase system the power falls to zero three times during each cycle, in 3-phase it never drops to zero.
The power delivered to the load is the same at any instant.

3-phase power is relayed using four wires while a single-phase system uses two.
To transmit the same power using a 3-phase system instead of a single-phase system requires less than a third of the conducting material.

A phasor diagram shows that the three phases of voltage/current are displaced by 120.
The concept was originally conceived by Nikola Tesla and was proven that 3-phase was far superior to single-phase power.

FORMULA There are two types of connection configurations, star '' (or wye) and delta ''.
These configurations reduce the amount of conductors needed to convey power (generator to end-user) from six to four.

Star connection.
The end of each stator coil is connected to form the letter wye.

The central point is called neutral and may be brought out to the end-users and may be used for earthing.
FORMULA FORMULA FORMULA FORMULA FORMULA FORMULA FORMULA FORMULA

Delta Connection.
The delta connection again saves on conductors as the output terminals of each coil are connected to the input terminals of the following coil.

FORMULA FORMULA FORMULA
Impedance.

Impedance 'Z', expressed in Ohms, is the ratio of the voltage applied across a pair of terminals to the current flow between those terminals.
In AC circuits, impedance is a function of resistance, inductance and capacitance.

Inductors and capacitors build up voltages that oppose the flow of current.
This opposition, called reactance 'X', must be combined with resistance to find the impedance.

The reactance produced by the inductance is proportional to the frequency of the alternating current, whereas the reactance produced by capacitors is inversely proportional to the frequency.
FORMULA Star connection gives a higher terminal voltage than delta connection but a correspondingly smaller output current.

Comparison of the above equations shows that, to arrive at the same voltage and current levels, the phase impedance in delta must be three times higher than the corresponding impedance in star.
FORMULA

Equivalent Circuit Method:
The equivalent circuit method is used to simplify calculations.

Below is the 'initial' equivalent circuit (Thevenin) for the induction motor.
It shows per-phase, so the phase voltage and current are used.

Any power considerations need to be multiplied by three to obtain output power or total power loss.
If the current through the magnetising branch (R c & jX m) is significantly low compared to I ph and I 2', then the equivalent circuit can be simplified further.

The magnetising branch is moved to the terminals.
Also the reactances are combined.

Moving the magnetising branch should not affect the accuracy because the voltage across it is so low and most of the current should flow around the rotor loop.
Set Up:

FORMULA FORMULA
Running Light Test:

FORMULA
Core Loss Resistance RC

FORMULA
Magnetising Reactance Xm

FORMULA FORMULA
Locked Rotor Test:

Winding Resistances
FORMULA

Leakage Reactances
FORMULA FORMULA

Stator Resistance Measurement:
Phase to phase Resistance R ph-ph=64.1 FORMULA

Load Tests:
Peak Torque and Starting Torque and Current

FORMULA FORMULA FORMULA FORMULA
Computer Simulation:

Graphs 1, 2 & 3 show the theoretical results.
Graph 1 shows the torque/ speed curve.

It shows a starting torque of approximately 0.775Nm, compared to the measured 0.85Nm.
The peak torque of 1.44Nm occurs at 1145rpm, whereas the measure peak torque is 1.15Nm at 1100rpm.

The synchronous speed is 1500rpm.
Synchronous speed is a theoretical value that could never be achieved because the motor will always have some load however small it is.

The running light test doesn't give actual no load results.
This is because there is a slight load from the measuring equipment, for instance the dynamometer.

Graph 2 shows the line current/ speed curve.
The starting current matches that of the measured starting current.

Generally that starting current is 5 to 10 times higher than the full load current.
In this case it is just about five times higher.

This over-current, even though it only lasts for a few seconds, can be a problem.
The solution of this is to use a starter or wound-rotor motor.

The measured results are close to the theoretical, showing that speed decreases with current.
Graph 3 shows the efficiency/ speed curve.

The motor is most efficient (68%) at 1410rpm.
Beyond this speed efficiency drops off rapidly to zero at 1500rpm.

The measured values are quite close to the theoretical.
Any differences between actual and theoretical results are due to friction within the motor.

1.0 Introduction:
1.1 Compression Ignition vs. Spark Ignition:

A Petters ACI four-stroke diesel engine is used.
The four-stroke cycle, as it suggests, consists of four stages; air intake, compression, fuel injection & combustion and exhaust.

Unlike the petrol four-stroke cycle that compresses a fuel and air mixture, the diesel cycle compresses only the air.
The heat from this compression ignites the diesel upon injection, rather than providing a spark to ignite the fuel as in a petrol engine.

For this reason a diesel engine has a higher compression ratio than a petrol engine, as high as 25:1 compared to 12:1 of a petrol engine.
This higher compression ratio also enables the diesel engine to have a higher efficiency than a petrol engine.

1.2 The Diesel Cycle:
Figure 1 shows a graphical representation of the processes that occur during the Diesel cycle.

Notice how heat addition occurs at constant pressure instead of constant volume as in the Otto cycle.
1 2 Air compressed isentropically through compression ratio FORMULA 2 3 Heat is added while the air expands at constant pressure to volume V 3.

At state 3 the heat supply is cut off and the volume ratio V 3/V 2 is called the cut-off ratio r C. 3 4 The air is expanded isentropically to the original volume.
4 1 Heat is rejected at constant volume until the cycle is complete.

1.3 Experimental Procedure:
Mineral Diesel and Bio-Diesel were used as fuels for this engine.

The Bio-Diesel in this case was Rape Methyl Ester (RME).
In order to compare the effectiveness of each fuel, a number of measurements were taken.

These measurements were then used to calculate some parameters that indicate the level of performance of the engine.
The engine was run at a constant speed throughout the experiment by adjusting the throttle control.

The table below shows the running speed.
On a few of the graphs one of the results was over-looked as it was deemed to be anomalous.

This can be seen from Graph 24 (Appendix 2) where the second point of time taken for 20ml of fuel consumption at 3N load on the engine was too low.
The recorded time was 103 seconds when it should have been more in the region of 118 seconds.

This inaccuracy was most probably down to observer vigilance.
A table of the measured and calculated data can be found in Appendix 1.

2.0 Data Measurement & Calculation:
2.1 Oscilloscope Read-Outs:

The indicated power is the sum of the brake power and the friction power.
It can be obtained through a process of measuring the pressure and volume inside the cylinder.

Graph 1 (Page 5) shows the different stages of the four-stroke cycle:
Points 1 2, Air intakePoints 2 3, Compression of airPoints 3 4, CombustionPoints 4 1, Exhaust

The larger area of the diagram represents the useful work done by the engine (compression and combustion) and the small region represents the work lost from the intake of air and exhaust of gases.
The total integral of this graph equals the total work done.

The indicated power equals the total work done per cycle multiplied by the number of cycles per second.
Graphs 4-15 (Appendix 3) show the p-V diagrams for a range of loads with the engine running on mineral and bio diesel.

Notice the area under the graph gets bigger for bigger loads.
Graph 16 (Appendix 3) shows a general upwards trend of indicated power against load for both fuels.

This correlates with the analytical calculation of indicated power shown in Appendix 1.
and Graph 17 (Page 6).

2.2 Indicated, Brake and Friction Power:
Graph 17 shows that as the load on the engine increased the brake power increased.

The friction power was assumed to be constant over the applied load range for each fuel therefore the indicated power increased at the same rate as the brake power, (I.P.
= B.P.

+ F.P.).
Graph 18 (Page 7) shows that, as the brake power increased the fuel consumption increased.

This was true for both the fuels, however the bio-diesel fuel consumption rate was greater than the mineral diesel.
This is because the bio-diesel has a lower L.C.V.

than the mineral diesel.
By extrapolating the graphs back to the x-axis the intercept gave the friction power (aka friction loss) within the engine.

The friction power within the engine for mineral diesel was 1.7kW and 1.5kW for bio-diesel.
These friction powers are shown on Graph 17 represented within the indicated power.

The reason that mineral diesel had more frictional losses than the bio-diesel is because bio-diesel has better lubricating properties.
2.3 Fuel Consumption & Air/Fuel Ratio:

Graph 19 (Page 8) shows that for lower B.M.E.P.'s (loads) relatively more fuel is required than at higher B.M.E.P.'s.
This is because it takes x amount of fuel to 'turn' the engine and x to turn it with a bit more force.

Both fuels followed the same trend but the bio-diesel used more fuel in doing so.
This again is because of the relatively low L.C.V.

of bio-diesel.
The air fuel ratio decreased as the B.M.E.P.

increased (Graph 20, Page 9).
This is because more fuel was required to drive the engine at higher B.M.E.P.'s while the air intake remained approximately constant.

2.4 Energy Balance & Usage Throughout the System:
Air cooling losses increased linearly with B.M.E.P.

(Graph 21, Page 10).
This was because, as more fuel was being burnt more heat was being produced.

The bio-diesel gave more air cooling losses because more energy was being supplied (see dashed lines on graph).
Graph 22 (Page 11) shows the energy usage throughout the system as a % of the heat supplied by the fuel.

The vast majority of the supplied heat was wasted in the exhaust, air cooling and other energy losses (other losses included noise, vibration etc.).
This was very similar for both fuels.

This wastage of energy is expressed in the efficiencies as discussed below.
2.5 Engine Efficiencies:

Graph 23 (Page 12) shows the brake thermal, indicated thermal and mechanical efficiencies of the engine running on both fuels.
The mechanical efficiencies for both the mineral and bio-diesel rose from zero to approximately 70% over their full working range.

The bio-diesel constantly had a slightly better efficiency, about 2%, over its entire range.
However the bio-diesel couldn't work under as high a load as the mineral diesel.

The reason for bio-diesel having a better mechanical efficiency is because of its better lubricating properties, not as much energy was used to overcome the friction within the engine.
The indicated thermal efficiency for both fuels was very similar reaching a maximum of approximately 20% with the mineral diesel being slightly higher over the complete range (about 4-6% higher).

Both fuels started to decrease at their maximum working loads.
Mineral diesel had an overall higher indicated efficiency because this efficiency measure ignores the mechanical (frictional) losses within the engine.

So because mineral diesel had a higher friction power (thus used more of the supplied energy to drive the engine) it also had a higher indicated efficiency.
The brake thermal efficiency of both fuels was almost identical up to about 300kPa of B.M.E.P.

where the mineral diesel continued to rise and the bio-diesel levelled off.
Both fuels rose from zero percent at 0kPa with mineral diesel hitting a peak of 23% and bio-diesel 19% both at approximately 500kPa.

The reason for mineral diesel having a slightly better efficiency is because it converted a higher % of the heat supplied into brake power.
3.0 Conclusions:

Overall bio-diesel performed very well compared to mineral diesel.
Positives for bio-diesel were its lower friction power due to its superior lubricating properties and its higher levels of mechanical efficiency.

On the down side it could not perform at high levels of load and it had a lower indicated thermal efficiency.
However indicated thermal efficiency is not a very useful gauge of performance as it ignores mechanical losses.

Where brake thermal efficiency was concerned it performed to just about an equal level as mineral diesel.
With all the above points in mind and the environmental benefits of bio-diesel it could substitute for mineral diesel as a transport fuel very well.

Experimental Procedure:
The beams' dimensions (breadth and depth) shall be measured using a micrometer.

This shall aid the calculation of the second moment of area of the beam.
Four readings shall be taken at different positions along the beam and averages calculated.

The equation used for calculating 2 nd moment of area is: FORMULA Below is a table of the data used to calculate the 2 nd moment of area: The test rig shall be set up with the three following configurations: (i) (ii) (iii) FORMULA Each time a 1kg mass shall be used which equates to approximately 10N.
Setup one has a central load, two has 'quarter length' load and three has a 'quarter length' load with a central simple support.

All the end supports are simple with setup three having an additional 'simple clamp'.
This 'simple clamp' is an effective 'knife edge' and should not affect the curvature of the beam, it simply prevents the right hand end breaking contact with the support.

For setup one and two a rule shall be used for measuring.
The rule shall be clamped into the slider on the rig and the initial profile of the beam with hanger obtained.

Next the 1kg mass shall be added and the new profile of the beam obtained.
Measurements shall be taken at 10cm intervals.

Once all the data has been collected the 'new' profile shall be subtracted from the 'initial' to give a deflection profile.
For setup three a dial gauge shall be used instead of a rule, because of this a direct deflection reading can be taken.

Three sets of measurements shall be taken and an average calculated.
Throughout the procedure the slide track should be kept free from obstacles to prevent inaccuracies in measurements.

Setup One:
Graph one shows the deflection profile for experimental and computer calculated results.

The maximum deflection is in the centre of the beam where the load is applied.
There is slight variation between the experimental and computer calculated deflection profiles.

Overall the experimental profile shows greater deflection.
This could be because of fatigue in the beam.

It has been used in previous experiments therefore shall be 'weakened'.
Also it could be due to inaccuracy of the Young's Modulus of the steel.

Setup Two:
This setup also requires prediction of the deflection profile via the integration method.

Below is the process of obtaining the general deflection equation.
FORMULA FORMULA FORMULA FORMULA when FORMULA FORMULA FORMULA FORMULA FORMULA FORMULA Graph two shows the experimental, computer calculated and integration method deflection profiles.

The maximum deflection is at 40cm from the left hand end.
The computer calculated and integration method profiles are exactly the same.

This means that the computer program uses the integration method to obtain its results.
The experimental profile is again slightly greater at the extreme.

This is probably due to fatigue or an inaccurate value of the Young's Modulus.
Setup Three:

This setup creates some complication for predicting the deflection profile using the computer.
Two external loads have to be used, firstly the actual load of 10N at 250mm from the left and secondly the reaction force (Q) of the middle support.

This reaction force is working upwards so it should be input as a negative load at 500mm from the left.
Setup two is similar to three accept it doesn't have the central support.

The mid deflection of two is 20.9mm.
If there were a support in the centre of two it would not deflect here.

In setup one the load is central, this can be thought of as the reaction force of the central support in setup three.
The 10N load should be thought of as working upwards, but this gives a deflection of 30.4mm.

The beam would be bent upwards by approximately 10mm in the centre.
Clearly this is not the reaction force required to give a deflection of zero.

Therefore a ratio of the two deflections multiplied by the 10N load gives the required reaction force.
FORMULA Graph three shows little variation between the experimental and computer calculated deflection profiles.

The computer profile has slightly greater deflection at the two extremes.
In practise the central support is not a knife-edge or 'true' point load as the computer simulation takes it to be.

Therefore the surface area of the support reduces the overall deflection in practise.
Introduction:

Bread is an important element of a healthy balanced diet.
Bread is part of the 'fibrous' food group, which includes bread, potatoes, pasta, rice, noodles and breakfast cereals.

These foods are high in fibre and rich in vitamins from the B complex, they should be the main part of every meal.
Bread is mass-produced commercially for the domestic consumer giving a relatively low final cost.

Many people prefer bakery bread either because of the fresh traditional taste or because of its low additive concentration.
With the use of a domestic breadmaker the consumer can decide exactly what goes into their bread and have a freshly baked loaf to greet them and their family in the morning.

There is a variety of domestic breadmakers, with a great range of price, features and design.
The six breadmakers that shall be focused on in this assignment are:

Breville Home Bake Breadmaker BR2Breville Fan Assisted Breadmaker BR3Cookworks Breadmaker B0906Kenwood BM200Morphy Richards Fastbake Coolwall 48280Panasonic SD253
Performance:

Four of the breadmakers were used in this section, with the Kenwood being used in two different modes.
Each mixture used was brand recommended.

Kenwood (traditional, slow process):
2hrs 30mins baking timeRectangular pan with circular sweeping motion of the paddle.

This gave the bread a non-uniform texture, as the corners were not properly mixed.The bread had a fluffy sticky texture.The paddle was connected to the pan leaving a hole in the bread when removed.There was a concave depression in the top of the bread, which coincided with the window.
This is due to heat transmission through the glass leaving a region of cooler air inside the pan.

Kenwood (quick process)
1hr baking timeThe exterior of the bread had a smooth appearance and non-uniform colouration.The bread gave the impression to be denser than the bread from the slow process.

Also it resembled a 'sponge'.The taste was stodgy and sticky, with underdone areas around the paddle hole.
Breville (large, fan assisted)

3hrs 25mins baking timeLoaf was difficult to remove from pan, shouldn't use metal utensils because may damage non-stick coating.Paddle stays in bread so has to be cut out, must take care as it remains hot.Uneven mixing of the dough, swirls are present in the loaf.The loaf has a light and fluffy texture.
Breville (small)

Again there is uneven mixing with swirls.Paddle remains in bread, leaving a big hole when removed.Soft and fluffy appearance.
Cookworks

3hrs 45mins baking timePaddle stays in bread.Thick crust.Small 'bubbles', relatively dense.Loaf is small in size.Stodgy and rubbery texture.
Testing of the bread could have been quantitative rather than qualitative.

This would have given scientific comparisons between the different breads.
The following criteria could have been followed: Moisture content, the loaf could be weighed once it came out of the pan, then dried out and re-weighed.

Then the moisture percentage could be calculated.
Colour, a "colourometer" or "reflectometer" could be used to determine the colour of the crust.

This would probably correspond to the breadmakers temperature.
Crustiness, a hardness test such as Rockwell or Brinell could be performed on the crust.

Density, the loaf fresh out of the maker could be weighed and then the volume and finally density calculated.
The volume could be obtained by measuring the displacement in water.

The bread would have to be wrapped in a non-porous material such as cling-film to prevent water absorption.
Instead of using the brand recommended recipes in each machine, the same recipes could be used.

This would give a fairer comparison for quantitative criteria.
Safety :

The Cookworks, Morphy Richards and Kenwood have no safety warnings on them.
The Panasonic has a safety sticker warning about steam and hot surfaces (Figure 2).

The large Breville has a safety warning printed on the lid (Figure 1).
The small Breville has an embossed warning on the lid and bread pan (Figures 3 & 4).

The small Breville's pan has an integrated heat element.
It is located at the top of the pan that gives better heat distribution than a low element.

As a performance improvement this is good but for a safety point of view this is bad.
The heat element will remain extremely hot and shall put the person at risk that is trying to remove the bread from the pan.

Every breadmaker has vents for the release of hot air/vapour.
None of the breadmakers apart from the Panasonic had warnings about vent 'exhaust'.

The temperature of this exhaust is 90c as measured on the Kenwood.
Steam at this temperature can cause harmful burns.

Also the window temperature is 70c, this can also cause burns.
Using more warnings and colour-coding the vents could improve this.

The vents could be coloured red to signify danger.
Ergonomics & Aesthetics:

The Morphy Richards has the best button layout with nice large buttons.
The buttons are easy to read and are arranged in an attractive way (Figure 5).

All the control panels are relatively easy to use they all have numerical displays apart from the small Breville.
This used LED's to display the amount of time remaining of a cycle.

This method is not as affective as the others because it only displays the time to the nearest half hour (Figure 6).
Overall the large Breville has the best display panel.

It uses both a numeric and LED display to convey information (Figure 7).
The most aesthetically pleasing model is the large Breville (Figure 8).

It has curved corners, a dome like section for the bread pan and a viewing window.
The Panasonic is quite the opposite (Figure 9).

It looks more like a storage box or bin.
The best ergonomically designed model is the small Breville (Figure 10).

This is the only model with carrying handles that makes it easier to move around.
Noise & Energy Consumption:

This graph shows the noise level of each breadmaker.
They all give out a similar result, the Panasonic being the quietest and the large Breville the noisiest.

This is as expected because the Breville has a fan which adds to its noise output.
There are no energy consumption figures available for all models.

The fast bake modes consume less energy because the program doesn't run as long.
The smaller Breville consumes more energy than the larger Breville.

This doesn't seem as it should, because the larger model has a fan to run whereas the smaller doesn't.
Type of Motor:

Only four of the breadmakers were taken apart for inspection.
Three of them, the Cookworks, Kenwood and Morphy Richards, use an induction motor with a belt to drive the mixing paddle.

The small Breville uses a universal motor with gears (Figure 11).
Induction motors are robust giving good reliability.

Universal motors are not as reliable and they are noisier than induction motors.
They run at relatively high speeds, this speed is dependent on loading.

So in theory a zero load would give infinite speed, therefore speed control is needed.
The positive for using a universal motor is that they start with an initial high torque compared to the induction motor.

Also they are cheaper, lighter and smaller than induction motors.
The gearing on the small Breville uses a worm gear connected to the motor shaft that actuates a 'dual' gear that in turn actuates a single cog gear (Figure 12).

The overall gear ratio is 1:149.
Therefore the gears that transmit the motion to the mixing paddle step down the speed of the motor.

Use of Materials:
All the models have metal surrounds except the small Breville.

The small Breville has a plastic surround.
This is more pleasant to touch as it has an 'ambient' temperature.

All the lids have a metal heat shield apart from the large Breville.
This is because the large Breville is fan assisted so the heat is evenly regulated.

All the windows are made from glass, again apart from the large Breville.
The large Breville's window is plastic, this is an advantage, as it has a relatively low heat conductivity compared to glass.

Therefore it shall not get as hot to touch, so it is safer.
Four of the bread pans are pressed mild steel whereas the two Breville's have cast aluminum pans.

The gears used by the small Breville are made of glass-reinforced nylon, whereas all the belts are made from felt-backed rubber.
The belts are toothed so they can engage the cogs.

Cleaver Features:
The Panasonic has an integral nut/raison dispenser.

A good nut or raison loaf would have even distributed nuts or raisons.
This feature was not tested but it seems that it wouldn't work as effectively as it should.

The contents of the dispenser are released all at once so it seems that mixing wouldn't be uniform (Figures 13 & 14).
A better design would release the contents gradually.

The small Breville bread pan has shallow indentations around rivets head, whereas all the others have deep indentations.
The shallow indentations enable efficient cleaning, therefore good hygiene.

Price Comparison:
Overall the price of each breadmaker has fallen.

This is as expected because of the release of newer more advanced models.
The Panasonic is the most expensive each time prices were obtained.

Next most expensive is the large Breville, then at the high end of the budget market are the Morphy Richards, Kenwood and small Breville.
The most basic of all the models, the Cookworks, is the cheapest.

Conclusions:
The best value for money model in my opinion is the small Breville.

It is averagely priced, produces a decent loaf and has sufficient safety warnings.
It also has the added bonus of a viewing window, although this is not essential it is a nice feature.

The best overall model I think is the large Breville.
It is not extortionately priced, it produces a very good loaf and has good safety warnings, all like it's 'younger brother'.

The extra pros of the large Breville are its aesthetics and its display/control panel.
This uses both LCD and LED displays, an effective way to convey information.

Overall both Brevilles are the 'pick of the bunch'.
Theory:

There are three methods of heat transfer, these are radiation, conduction and convection.
These methods can work simultaneously or alone.

Radiation- is the transfer of energy via electromagnetic waves.
This requires no medium therefore can work in a vacuum.

This is the main transfer of energy emitted from stars (and all other bodies above absolute zero) through space.
Conduction- works via the physical interaction between molecules.

All molecules above absolute zero vibrate to some degree, these vibrations transfer energy from one molecule to all the adjacent molecules.
Fourier's law governs heat transfer via conduction.

Convection- takes place through the motion of fluid.
On a macro scale level the flow of fluid transfers energy from/to a surface.

Conduction slightly contributes to this method at the micro scale level.
When more than one of these methods contributes to heat transfer the total heat transferred can be split into the component amounts.

For the purpose of this experiment each method will take place but it is possible to calculate the amounts for which each contributes.
Firstly conduction can be neglected as it only acts over the leads that suspend the element.

As these leads are long and thin the heat transfer via conduction is minimal.
This is because; FORMULA The surface area of the leads, A, is so small that conduction is neglected.

The ideal method for calculating the heat transfer via radiation is to create a vacuum inside the vessel.
This would totally restrict any heat transfer via convection as there is no medium for it to occur in.

The trouble is that a complete vacuum cannot be obtained with this equipment so a slightly different method has to be used.
This involves calculating the emissivity of the element by plotting element and vessel temperature against varying pressure (to the power of four).

Then extrapolating the graphs back to zero pressure.
The corresponding temperatures can then be used in the following equation to calculate the emissivity; FORMULA.

In order to calculate the value of C in the relationship between the dimensionless numbers at ambient pressure, convection must be taken into account.
At ambient pressure (atmospheric) the temperatures of the element and vessel are taken.

These are then used in conjunction with the previously calculated emissivity and electrical power input to the element to calculate the heat transfer via convection.
This can then be used to calculate the Nusselt number and Rayleigh number (for which the film temperature is also needed).

Once these two dimensionless numbers are calculated the value of C can be obtained from the following equation; FORMULA (where n=1/4).
Pirani Effect:

During this experiment we are using varying pressures.
These varying pressures alter the amount of "air particles" in the vessel.

The higher the pressure the more particles there are.
The Parani effect relates the pressure to the efficiency of heat transfer.

The air particles in contact with the surface of the element receive energy from the element.
This energy excites the particles causing them to move at higher velocities.

The heat energy from the element is converted to kinetic energy of the particle.
The particle moves away from the element and collides with other particles and the vessel wall.

On collision energy is transferred from the particle to whatever it hits.
At low pressure there are relatively few particles in the vessel so any excited particle has the ability to bounce back and forth from the element to vessel wall with little obstruction of other particles.

This results in a very efficient heat transfer from the element to the vessel.
At high pressure there are relatively many particles in the vessel.

This means that any excited particle will collide with many other particles on its journey to the vessel wall.
This slows down the heat transfer from the element to vessel as the energy is dissipated over many more particles.

Calculations
Radiation:

FORMULA FORMULA
Natural Convection:

Film Temperature at ambient pressure- FORMULA Rayleigh Number- FORMULA Where; FORMULA FORMULA FORMULA FORMULA FORMULA Heat Transfer via Radiation- FORMULA Heat Transfer via Convection- FORMULA Nusselt Number- FORMULA Where; FORMULA Dimensional Analysis- FORMULA
Conclusions:

The element is constructed from a copper tube with the heating element (Nickel-Chrome) mounted in alumina.
The emissivity of 0.581 indicates that the surface of the copper has oxidised.

The ratio of heat transfer at ambient pressure via convection to radiation is approximately 3:2.
This is expected because an effective convection current system was set up in the ambient pressure atmosphere.

From page 34 of the Heat Transfer notes it states that for horizontal cylinders with Raleigh numbers between 10 4 and 10 9 that the C value should be approximately 0.53.
The Rayleigh number calculated here is only in the order of 10 3 so a C value of 0.385 seems to be appropriate.

The experimental procedure has some slight inaccuracies.
The main one is the measurement of pressure via the manometer.

The measurements are taken by eye from a wooden ruler with 1mm intervals.
Therefore the readings are only accurate to the nearest millimetre, this inaccuracy then multiplied by two as two readings are taken for either side of the manometer tube.

The wooden ruler shall also expand/contract with varying humidity.
These inaccuracies where overcome for pressures ranging from 0.1-100 Torr as they could be measured accurately using the McLeod gauge.

1.0 Magnetising Curve & Running Light Equivalent Circuit Parameters:
1.1 Measured Data:

1.2 Calculation of Equivalent Circuit Parameters:
The running light test enabled the calculation of the total volt-amp reaction (Q total), the magnetising reactance (X m) and core loss resistance (R c).

FORMULA FORMULA FORMULA The lower section of the graph (0.75 - 1.5A) shows a linear relationship between current and voltage.
At 200V (the saturation point) the current-voltage relationships starts to level off.

For an 'x' increase in current above the saturation point, not as big a voltage increase occurs as below the saturation point.
The locked rotor test enabled the calculation of R 1 and R 2' (the stator coil and rotor coil resistances respectively) and the leakage reactances (X 1 + X 2').

FORMULA Eq.
7 where R ph-ph was measured directly as 7.6Ω FORMULA Eq.

8 where FORMULA Eq.
9

1.3 Graphs from MatLAb:
See Appendix 1 for graphs of generator efficiency against machine speed, power against machine speed and line current against machine speed.

The generator efficiency started at about 15% just above 1500rpm then shot up to its maximum of 75% at about 1590rpm.
After this point it decreased steadily to just over 20% at 2250rpm.

2.0 Load Test (Grid connection):
2.1 Measured Data:

Graph 2 shows that the induction machine started to act as a generator and produce power at 1507rpm.
(A negative power indicates that power was being generated.) This was slightly above the synchronous speed.

The synchronous speed is the speed at which the rotor speed exactly matches the stator speed (magnetic field).
As no flux is being cut no current is induced, and hence no power.

Graph 3 shows that above the synchronous speed (approx 1500rpm) the current induced increased linearly with machine speed.
Nb.

The reason for the 'kick backs' at the beginning of both graphs is because these readings where taken when there was no water pressure so they can be ignored.
2.2 Efficiencies:

The table below shows the efficiency of the induction machine whilst acting as a generator.
This was achieved from the interpretation of Graph 1 in Appendix 1.

The measured pressure had to be corrected by subtracting 30% from it.
This represented the pressure loss in the pipes.

This corrected pressure was then used to attain the input power from the jet to the turbine by using graph A.5 from the lab script.
At this stage it was possible to calculate the overall efficiency and turbine efficiency by using equations 10 & 11.

The calculated data is shown in the table and graph 4 below.
FORMULA Eq.

10 FORMULA Eq.
11 The turbine efficiency peaked at 1537rpm, this was due to the blade geometry.

Above 1537rpm the rate at which torque was produced by the blades started to reduce hence the reduction in efficiency.
The generator efficiency rose steadily from 45% to just above 70% over the range of tested speeds.

However the graph shows that it was beginning to level off at the higher speeds, so if even higher speeds had been tested the optimum efficiency may have been observed.
The product of the turbine and generator efficiencies gave the overall efficiency.

Therefore as expected it rose from just above 30% at a rate just lower than the generator and then levelled of at a rate significantly faster than the generator because of the dropping off of the turbine efficiency at the higher speeds.
The maximum overall efficiency was 48% at the highest tested speed.

3.0 Three-Phase Stand Alone Operation:
3.1 Capacitor Sizing and Frequency & Synchronous Speed:

As induction machines have no internal magnetic excitation a capacitor is needed to provide this excitation.
The capacitor generates the required volt-amp reaction for the magnetising reactances.

Equation 12 demonstrates the required size of capacitor to bring up the voltage to 220V at 50Hz.
FORMULA Eq.

12 During the stand-alone operation of the induction machine it was not required to run at the grid frequency of 50Hz.
Equation 13 was used to calculate the frequency at which the stand-alone operation ran at.

FORMULA Eq.
13 This frequency was then used to calculate the synchronous speed of the induction machine running at 62Hz.

FORMULA Eq.
14 With the synchronous speed of the machine being 1860rpm instead of 1500rpm the generator would have been operating at a higher efficiency (as discussed in section 2.2).

Graphs 2 & 3 in Appendix 1 have the power and current points from this stand-alone operation plotted on them.
Notice how none of the points sit on the curves.

This is because the curves are from grid connected operation.
This highlights the unstable operation of a stand-alone system.

3.2 Efficiency:
The table below shows the measured and calculated data from the stand-alone test.

Graph 5 overleaf shows the overall efficiency against the load setting.
The efficiency rose from zero at zero load to a maximum of 56% at maximum load setting of 3.

This is as expected because as the load on the induction machine increased the current induced and hence power produced increased.
4.0 Single-Phase Ballast Operation:

As seen above stand-alone operation is quite unstable.
One way to stablise the operation is to use a ballast controller.

With varying load, instead of letting the generator respond to the load, the ballast controller responds.
The generator operates at constant output (current, voltage, power), the load draws what it requires and the excess power is 'dumped'.

This is all managed by the ballast controller.
The table below shows how the ballast controller reacted under varying load (set of lights).

Graph 6 overleaf shows the efficiency of the system as a function of the load.
It is perfectly linear as the load drew what ever power it needed and the ballast controller reacted accordingly.

Quite obviously at zero load the efficiency was zero as all the power was being dumped into a resistor bank.
As the load increased the efficiency increased in hand.

So if a load of 350W was activated the efficiency of the system would be 100%.
The ballast control system regulated the voltage very effectively as it was held at 220V regardless of the load.

Whereas during the stand-alone operation in section 3 the voltage 'jumped' around as load varied.
It took some time for the voltage to stabilise upon load adjustment.

5.0 Conclusions:
With a ballast control system, an induction machine micro-hydro scheme would be very effective as a stand alone single-phase system.

This is because of the ease of voltage regulation.
An induction machine micro-hydro scheme would also be effective when ran as a three-phase grid connected system.

When running as a stand-alone single-phase ballast controlled system it would be valuable to use the excess power instead of just dumping it.
There are a number of possible uses for this excess power, for example using the energy from a resistor bank to heat water for district heating or storage in a heat sink.

In the future, when and if the hydrogen economy takes off, the excess power could be used to generate hydrogen.
What is an intelligent building?

Many people in the construction industry have different opinions on the exact definition of 'intelligent buildings'.
Some of these definitions include: "A building that is fully leased." So according to this, any feature that helps to lease a building fully could be thought of as intelligent.

"An intelligent building combines innovations, technological or not, with skillful management, to maximize return on investment." This definition resulted from the International Symposium on the Intelligent Building, May 28 and 29, 1985 in Toronto.
The Intelligent Building Institute proposed: "an intelligent building is one that provides a productive and cost-effective environment through optimization of its four basic elements - structure, systems, services and management - and the interrelationships between them.

Intelligent buildings help business owners, property managers and occupants to realize their goals in the areas of cost, comfort, convenience, safety, long-term flexibility and marketability." "Intelligent buildings use electronics extensively and are high-technology related." An intelligent building "...
provides a responsive, effective and supportive environment within which the organisation can achieve its business objectives".

(Worthington, 1997:89) "An intelligent building is one that creates an environment that maximizes the efficiency of the occupants while allowing effective management of resources with minimum lifetime costs." The Intelligent Building Institute.
All these definitions point in the same direction.

An intelligent building uses a high level of technology that enables it to be proficiently managed while maintaining an energy efficient existence.
Who uses intelligent buildings?

Due to the relatively high cost of manufacture, these buildings are mainly constructed for leasing agencies.
The buildings are leased to companies generally as offices.

Some companies have them build for themselves mainly large internationals and multinationals.
Large establishments such as schools, universities, councils and governments also use intelligent buildings.

It is very rare to have intelligent buildings as domestic properties but some do have a certain degree of intelligent features.
Systems within an intelligent building:

As already mentioned intelligent buildings use a high level of technology, this relies heavily on electronics.
With regard to the use of electronic technologies we can split them into four groups:

Energy EfficiencyLifesafety SystemsTelecommunication SystemsWorkplace automation
In an ideal world all the systems would be part of one integral system.

This system could be controlled and managed from a single computer.
All the hardware and software of the systems would be from a single source that uses the same interfaces so that all systems are compatible.

Breakdown of the four groups:
1. Energy Efficiency

In the present state of world energy is used very carelessly.
The aim of some of the intelligent systems is to keep energy use to a minimum.

Not only does this save the building occupier money on their fuel bills, it is also good for the environment.
The schemes used to implement energy efficiency include;

Programmed start/stopOptimal start/stopDuty cyclingSetpoint resetElectric demand limitingAdaptive controlChiller optimizationBoiler optimizationOptimal energy sourcing
An example of a programmed start/stop system would be an escalator system.

The escalators in a shopping centre do not want to be run continuously, they only need to operate during the hours of trade.
Therefore they would be on a timer that activates them at the beginning of the day and turns them off at the end.

Optimal start/stop is used in lighting systems.
Motion sensors are used to detect when someone enters a room then the lights are turned on automatically.

Duty cycling means to change or control the duty cycle (ie. The ratio of on-period to total cycle time) of on/off controlled equipment.
For example instead of simply turning heating on and off, a thermostat is used to keep the temperature at a certain level.

These schemes are primarily part of control systems.
Such control systems are known as;

Building Automation System (BAS)Building Energy Management System (BEMS)Energy Management and Control System (EMCS)Central Control and Monitoring System (CCMS)Facilities Management System (FMS)
2. Lifesafety Systems

These are basically, systems that uphold the health and safety of the buildings' occupiers.
The systems involved in this are;

Closed-circuit televisionCard access controlSmoke detectionIntrusion alarmsEmergency control of elevators, HVAC/R systems, doorsUninterruptible power supply (UPS)
These systems reduce the dependence on manpower.

For example only a single or pair of security guards is needed to protect a building instead of a team.
This is because of intrusion alarms and the ability to observe the whole building via CCTV.

These can also provide solid evidence for use in investigations.
The HVAC/R system controls the Heating, Ventilation, Air Conditioning and Refrigeration.

The UPS system provides power protection to entire buildings served by a single source and protects power sensitive equipment from the detrimental effects of power disturbances such as voltage sags, surges, transients, momentary disruptions, and complete outages.
3. Telecommunication Systems

Communication and information is an important part of everybody's life.
For businesses it will affect performance and in turn profits if this is not quick and easily accessible.

These systems enable rapid communication and put information at people's fingertips.
PBX telephone systemCablevisionVideotextEthernet

A Private Branch Exchange (PBX) is a telephone network with a building/s.
A PBX will contain a number of outside lines for making external calls.

PBX's are used because it is cheaper than connecting an external line to every phone in the building/s.
Also it is quick and easy to contact other users within the PBX by simply dialling a three or four digit number.

A Public Announcement (PA) system may also be integral with a PBX.
Cablevision is essentially high speed Internet, cable television and digital voice service.

The Internet is an important information and communication tool.
Information can be obtained from websites and communication via email, instant messaging services and video conferencing.

A videotext system displays information as text and simple pictures.
The London Stock Exchange uses this system to display up to date information about stocks and shares.

Ethernet is a network of computers and other devices linked by cables and/or wireless transmitters and receivers.
This enables uses to send and receive and/or access information of other computers on the network.

There are two types of Ethernet, Local Area Network (LAN) and Wide Area Network (WAN).
LAN would be within a single building whereas WAN would connect several different buildings maybe miles apart.

4. Workplace automation
This mainly consists of software based intelligent features.

Centralized Data ProcessingWord ProcessingComputer Aided DesignInformation Services
Word processing includes the electronic creation, revision, storage, retrieval, and transmission of correspondence documents.

These documents can be transferred in a number of different ways, either over the buildings Ethernet, the Internet or in physical form by courier or post.
Computer Aided Design (CAD) is an important tool that allows the users to make adjustments and modifications to a design without having to redraft.

This is a great time and money saving.
My Intelligent Building:

The type of building I choose to build shall be a domestic property.
The location of the building shall be very important.

Firstly I shall want it to be a rural location but this might conflict with the second factor.
The second factor is the area must have broadband for high-speed Internet access.

Currently rural areas in the UK have limited broadband availability.
Therefore a few miles outside of a major town shall have to suffice.

Ideally an old farm barn to be converted into a dwelling shall make a good start.
Also a few aches of land preferably with woodland.

I want the house to use a high level of technology for comfort, ease of living and entertainment.
But I don't want to compromise my main objective.

That is to an energy efficient home that uses 'Green' energy and resources and as much recycled materials as possible.
Structure of building

The main living area shall be large and so the main source of light for this room shall be natural.
An external wall of this room shall be glass so that as much light as possible could be utilised.

The top floor rooms in the house shall use roof light windows, as this shall also maximise natural light.
All the windows throughout the house shall be triple glazed windows that are self-cleaning.

Triple glazed windows have a low U-value so heat loss is kept to a minimum.
Also they have good sound reduction and added security.

Self-cleaning windows have a transparent titanium dioxide coating that is chemically fused to the glass.
This reacts with UV rays in natural light resulting in a photocatalytic reaction that oxidizes organic dirt and loosens it from the surface.

Water is needed to rinse this loosened dirt away so there is a built in sprinkler system that does this.
But of course being in the UK the precipitation level is relatively high so the sprinkler system shall only be needed in the height of summer or during drought periods.

Any additional timbers needed shall be from a reclaimed source.
This shall prevent the use of new wood from ever dwindling global forests.

Also bricks and roof tiles shall be from reclaimed sources so their age shall match that of the style of the building.
Insulation throughout the entire house shall be AIR-CELL insulation.

This includes walls, ceilings, roof and floor.
AIR-CELL uses an advanced thermo cellular reflective structure that is only 7mm thick.

Technology for Comfort
A HVAC system shall be used so in summer the house shall be cool and in winter it shall be warm.

Also humidity levels shall be kept at a healthy level throughout the whole year.
There shall be hand held portable thermostats for adjusting the temperature.

In rooms with tiled floors (ie. Bathrooms and kitchen) there shall be under-floor heating.
This uses electrical heating elements that keep the floor at a certain temperature.

The internal wall coverings shall be mainly lighter colours to aid light dissipation, lighter colours also tend to induce 'happy' moods in people.
There shall be a wet and dry sauna room, hot tub/whirlpool bath and up and down stairs shower rooms, also on-suite for the master bedroom.

Easy of Living
The kitchen shall be equipped with all the modern day appliances, fridge/freezer, dishwasher, washing machine, tumble dryer, microwave, oven and halogen hob.

All the appliances shall be selected by the EU energy label system.
They shall ideally all be 'A Rated' appliances (A++ for the fridge).

Entertainment
There shall be an Ethernet with sockets in the main rooms of the house.

The router used shall also have wireless 802.11g technology for roaming by laptops and other wireless enabled devices.
Rooms with hifi systems shall be equipped with an Apple Airport Express for streaming Airtunes.

Also the main rooms shall have cable television enabled sockets.
Energy and Resource Efficiency

Solar power.
The roof shall be tiled using Solar Roof Tiles.

As for this project money is no object so the whole roof shall be tiled using these.
If money were a factor then only a portion of the south facing side would be done as the north facing side typically produces only 60% of the power of a south side.

Wind Energy.
As mentioned earlier the house shall be situated within a few aches of land.

If the location has sufficient wind speed and power then a small wind turbine farm shall be erected.
On average the money saved on electricity bills over 15years shall recoup the cost of these turbines.

Any extra electricity generated can also be sold back to the energy supplier.
Water Usage.

The waste water from bath, showers, sinks and dishwasher, and rainwater can be collected in a tank.
It is filtered to remove 'sludge' and soap scum, and then used to flush toilets.

This saves the drinking quality water that is usually used to flush toilets.
Not only does it save money on utility bills but also energy and chemicals that go into producing clean water.

Integration of all systems
All these systems used shall be controlled via a 'central' computer.

Not only shall it make the management of the systems easier, it shall also enable data to be collected so optimum configurations can be obtained.
1.0 Geometry of Sheltered Play Area:

1.1 Optimum Design:
In using the preliminary design section of PVSyst it became apparent that the proposed design for the shelter was not the optimum.

The proposed design had the canopy angled at 20 degrees to the horizontal and orientated 30 degrees West of South, however the optimum design was angled at 30 degrees to the horizontal and facing directly South.
This preliminary design stage used monocrystalline PV and assumed the array to be ventilated by natural air flows.

System losses were not taken into account and unobstructed irradiance was also assumed.
Therefore the following results only give a rough guide to the amount of power generated but give an accurate comparison between differing geometries.

Graph 1 below shows the yearly electrical output of an 8x5m array against varying tilt angle for a select number of orientations.
As can be seen from the graph the optimum design had canopy slope angle of 30 degrees to the horizontal with the shelter facing directly South.

However this optimum design gave only a 100kWh/yr increase over the proposed design.
Varying the orientation of the shelter had a greater impact on the energy generation compared to varying the slope angle.

For example a range of acceptable slope angles, say 5-45 degrees, had less of a range of electrical outputs than changing the orientation of the shelter from North to South facing.
The summery table below shows the key performance indicators for both the proposed and optimum designs.

Notice how there is little variation between the two.
The optimum design gave only 3% extra energy per m 2 of array and again only 3% extra energy as a function of the peak power.

Something to note however is that if the shelter was to cover a ground area of 40m 2 then the optimum design at 30º as opposed to 20º to the horizontal would have a larger PV array and therefore even more power generation.
The table below compares the two.

The energy output from the maximum sized optimum design gave 500kWh per year more than the maximum sized proposed design.
This equates to an increase of approximately 12%.

1.2 Comparison with Twinned School in Benin, Nigeria:
The twinned school in Benin, Nigeria wants a similar sheltered play area to the proposed one in Reading.

Graph 2 overleaf shows the yearly total array output for a similar design (20º slope, 30º azimuth) along with a range of other designs.
The optimum design for Benin was again orientated directly South but this time with slope angle of 10º.

The summery table below shows the important values.
The optimum design generates just below 2% more than the proposed design.

But this time the slope angle of the optimum was less than the proposed.
The reason for this is illustrated in the Figure 1.

In order to maximise the 'view' of the sun's rays the array must be perpendicular to the rays.
The optimum slope angle for Benin is 10º because of it position on the globe (latitude 6.33º N), whereas for Reading the optimum slope angle is 30º as its latitude is 51.4º N. Graph 3 overleaf shows the variation of electrical output from the arrays in both Reading and Benin on a monthly basis.

The output in Benin is more equal throughout the year compared to Reading.
Benin has a dip in output from May to October because this is during the monsoon season when there is cloud cover.

Reading has a large range of output throughout the year.
The rise and fall of output is quite steady through the year with high output during the summer months and low output during the winter months.

2.0 Optimal Type of PV:
There are several different types of photovoltaic cells, two that were explored during this study were monocrystalline and amorphous.

The table below shows the annual electrical output for both types using the proposed design in Reading.
Monocrystalline PV gave 100% more output than the amorphous PV, this was due to monocrystalline having double the nominal power.

3.0 Financial Analysis:
3.1 Capital Costs:

Amorphous PV is generally cheaper than monocrystalline PV, but this lower price also means lower efficiency as seen above.
Overleaf is a brake down of the costs for each type of PV installation.

Even though the amorphous PV has lower capital cost and lower running cost it costs more for the energy produced.
This higher £/kWh is due to the lower efficiency of amorphous PV.

3.2 Financial Savings:
The system is grid connected therefore during peak generation times energy can be sold back to the grid and during low generation times energy can be purchased from the grid.

The selling/ buying cost from the grid was estimated to be 10p per kWh.
So for amorphous PV just under £200 can be saved (or earned) per year whereas for monocrystalline just under £400 can be saved (or earned) per year.

Not only does the energy from monocrystalline cost less, but also more income can be generated per year.
More sophisticated methods could be used, such as Levelised production cost or Internal rate of return, to work out the 'true' economic benefits of the PV installation but here it is outside the scope of this report.

4.0 Government & Private Grant Schemes:
The main grant scheme available for school based PV installations is a government scheme called the 'Low Carbon Buildings Programme' which is ran by the Dept of Trade & Industry.

It will provide grants for a maximum of £3000 or 50% of the capital costs.
For more information visit the following web site, ' URL '.

5.0 Detailed Project Design:
5.1 Module & Inverter Selection:

In choosing appropriate modules for the canopy there was one main constraint to take into consideration.
That constraint was finding a group of PVs that fit the canopy, as close to 40m 2 as possible.

The next task was to select the specific make and model that gave the highest nominal power.
Once the nominal power had been attained the next step was to figure out an appropriate power rating for the inverter.

The standard practise for selecting an inverter is to size it approximately 10% less than the nominal power of the array.
The penultimate stage was to choose the ratio of series:parallel connection.

In other words, how many strings (parallel) in the array with a certain number of modules (series) in the string.
The more modules per string the higher the voltage.

It is better to have more than one string in the array so losses from shading or damaged modules can be minimised.
Generally a common way of minimising these losses regardless of the number of strings is to use bypass diodes.

This diverts any generated current around shaded or damaged modules.
Finally a specific inverter was selected.

This depended on the V mpp and V OC, the voltage at the max power point and open circuit voltage.
These two values gave the appropriate voltage range for the inverter.

Many different combinations of modules and inverters were assessed and the one giving the maximum annual energy chosen.
The table below shows the modules and inverter chosen with all the important parameters shown.

The report from PVSyst can be found in Appendix 1.
5.2 PV Array Layout:

The physical layout of the PV array is shown in schematic form by figure 2 below.
6.0 Affect of Tree Shading:

PVSyst has the ability of calculating the array output when there is known shading on the array.
Figures 3 & 4 shown the sheltered play area with a tree 4m taller than it and 2m in front of it.

The graph in the bottom left of each figure displays the linear beam shading loss on a clear day.
In winter 20% of the irradiance is lost per day whereas in summer only 3% is lost.

The reason that more is lost in winter, is because the sun is low in the sky so the tree has more of a shading affect as shown in Figure 3.
Notice how in Figure 4 the sun is much higher therefore not behind the tree as much.

Over the whole year the array with shading produced 3991 kWh.
That equates to almost a 9% dip in energy per annum.

To remedy this problem the tree could be removed altogether or trimmed down to below the level of the array.
7.0 Rival Computational Modelling:

RETScreen PV project was also used for a comparison between the performances of different simulation programmes.
However the chosen modules from PVSyst were not one of the listed modules in RETScreen.

Therefore modules that featured in both programmes were used, these modules were BP Solar 5170.
PVSyst gave an energy production of 4074 kWh/yr and RETScreen 3586 kWh/yr.

This was a difference of about 0.5MWh/yr, which is quite a substantial difference.
A full version of the data from RETScreen can be found in Appendix 2.

8.0 Pro's & Con's of Computational Modelling:
PVSyst is an easy to use programme with some good initial and detailed simulations methods.

It has a simple CAD package built in for the simulation of shading effects.
A couple of down points are the awkward multiple-window user interface and inability to quickly and easily compare various inverters and modules.

RETScreen is also relatively easy to use but seems rather primitive compared to PVSyst.
It doesn't have an extensive PV module database like PVSyst and there is no ability to select an inverter.

Another drawback of RETScreen is the lack of data analysis methods, ie. Irradiance and array performance data-tables and graphs.
Theory:

Normalised means that the steel has been heated to approximately 800-900ºc then left to cool 'normally' in air.
This is a very slow cooling rate so the austenite transforms fully into ferrite and pearlite.

Pearlite is a lamellar structure of alternating layers of ferrite and cementite.
Pearlite is hard and it gives the steel strength.

Ferrite is soft, and gives the steel ductility and toughness.
If the amount of carbon in the steel is increased, this increases the amount of pearlite in the steel.

This has the effect of increasing the strength, but it also decreases the ductility and toughness.
Tempering is a process that involves reheating the already quenched steel.

Quenching steel transforms the austenite to a partially or completely martensitic microstructure.
The martensite is extremely hard and brittle.

To regain some ductility the steel is reheated to a temperature in the range of 200 to 600ºc.
This allows some of the body centred cubic of martensite to transform to pearlite and/or ferrite.

Tempered steel is harder and stronger than the normalised steel, but has less ductility.
Normalised Samples Results:

A general trend (of EN6A samples) shows that as the test temperature increases the fracture energy increases.
From observation of the broken samples it can be said that brittle fracture occurred (Figure 1).

Brittle fracture takes place when failure develops along the cleavage planes of individual crystals.
This gives the fracture surface its 'glittery' appearance.

The sample of EN1A took almost five times the energy of EN6A to fracture.
This is because EN1A has a smaller carbon content (0.1% compared to 0.4%) than EN6A.

As mentioned in the theory an increase in carbon corresponds to an increase of pearlite and therefore a decrease in ferrite.
Pearlite is hard and brittle, whereas ferrite is soft and ductile.

So EN6A is more brittle than EN1A, brittle materials are more prone to fracture.
Ductile materials will have some resistance to fracture.

Upon failure they don't leave a 'clean' break whereas brittle materials do.
Ductile materials have the appearance of layers being 'ripped' apart (Figure 2), the surface is said to be 'fibrous'.

Plastic deformation is associated with ductile failure.
The material is taken past its 'limit of proportionality' and into its plastic region.

This deformation is permanent and unrecoverable after load is released.
Tempered Samples Results:

Series 1&2 are the results obtained during this laboratory session, the rest are from previous sessions.
There is a lot of variation between the results of corresponding temperatures.

This is due to the difficulty of heat treatment.
Also the samples used in previous sessions might be from different batches.

There may be discrepancy of re-quench time and/or furnace reheat time.
Re-quench was done it batches of three samples, therefore the middle sample may have cooled slightly slower giving it a higher pearlitic structure and slightly more ductility.

The graph shows increasing fracture energy with increasing tempering temperature.
The higher the tempering temperature the higher the proportion of martensite that gets transformed back to austenite.

On re-quench (dependant upon cooling rate) this austenite shall be transformed into a proportion of pearlite and ferrite.
This will give the tempered steel a higher proportion of pearlite/ferrite than the original quenched steel.

Therefore it shall have more ductility and a greater ability to withstand fracture.
So...higher temp more austenite more pearlite/ferrite greater ductility Increased fracture energy.

Sketches of Fracture Zone:
FORMULA This graph is consistent with the previous results.

The higher the tempering temperature the lower the crystalline percentage.
Again this is because, higher tempering temperature gives more pearlite/ferrite that in turn gives greater ductility and resistance to fracture.

Crystalline appearance is associated with brittle steels of low temperature.
The results show the correct trend but the 200ºc result is extremely low.

The 400ºc is just about matches the theoretical and the 600ºc is slightly too high.
Series four samples would probably give the best crystalline percentage results.

This is because its 200ºc fracture energy is less than half of the used sample, so it is more brittle giving a higher crystalline structure.
Its 400ºc is approximately the same as the used sample and its 600ºc's fracture energy is about 10% higher than the used sample.

Therefore it would be more ductile and should give a completely fibrous fracture.
Introduction:

Computational Fluid Dynamics (CFD) is a technique used to solve fluid flow problems.
Rather than using experimental methods, that are expensive and time consuming, CFD can be used effectively to predict the real life solution.

CFD is carried out on digital computers that use algorithms to solve a set of indeterminate equations called the Navier-Stokes equations.
These partial differential equations where derived in the early nineteenth century and are used to describe the processes of momentum, heat and mass transfer.

In the 1960's CFD was born, even though it was a very primitive form.
It was not until the mid 1970's that the complex algorithms needed for CFD began to be understood.

By the early 1980's CFD programs where available but they required a high amount of knowledge of fluid dynamics and time to set up the simulations.
They also required very powerful computers.

The CFD industry rapidly expanded in the 1990's and through to the 21 st century.
CFD programs can now import complex models from CAD programs.

CFD is used by many different sectors of industry, these include automotive, building services, electronics, energy, medical and numerous others.
It aids the design of comfortable and safe working environments, efficient equipment, high performance vehicles and maximum yields from chemical reactions.

Solving The Differential Equations:
CFD can use one of or a combination of three different methods for solving the partial differential equations.

The three methods are;
Finite Difference Method (FDM)Finite Element Method (FEM)Finite Volume Method (FVM)

FDM is the simplest of the three methods.
It is based on a grid of nodes dependent on time and distance.

It uses the Taylor series to predict the condition at a node based on the adjacent nodes one time-step behind it.
FEM is based on the same principles as FDM but instead of a uniform grid being used it uses a non-regular grid.

This gives a higher accuracy as complex boundary conditions can be modelled.
FVM is a combination of both FDM and FEM.

It simulates a small volume at each node then uses divergence theorem to convert them to a surface integral.
It is able to replicate the complex boundary conditions while applying the finite difference technique to calculate values at discrete places on the mesh.

CFX from Start to Finish:
The whole process of CFX follows five main steps;

Creation of the Geometry in Design Modeller.Generation of the Mesh in CFX-Mesh.Region setupMesh setupSurface Mesh creationInflation setupVolume Mesh creationDefinition of specific problem/ conditions in CFX-PreSimulation creationSetup of inlets/ outlets/ fluids/ turbulence models etcRunning the simulation in CFX-SolverWriting the solver fileRunning the simulationViewing & Analysing the Results in CFX-PostSetup planes/ streamlines/ contours/ animations etc.
CFX File Extensions and What They Mean:

When revisiting a partially or fully completed project you are confronted with the project page.
This shows all the 'workable' files associated with the project.

If you want to redo part of the project, for example change the flow conditions, you need to know which file to start from.
Below is a list of the file extensions and what they mean..wbdb- Workbench database file, this is the project file.

It links all the files that are associated with project..agdb- this is the design modeller geometry file.
It contains the shape, dimensions and constrains of the model..gtm- this is the CFX-Pre mesh file.

It contains the information about the mesh..cmbd- this is also a CFX-Mesh file..cfx- this is the CFX-Pre case file, it contains information about the surface and volume mesh, the inflation and other details..def- CFX-Solver definition file.
It defines the conditions for the simulation..res- CFX-Solver results file.

Once the simulation is complete a file is written that contains all the results..cst- CFX-Post state file.
This contains all the analysis tools used, for example streamlines.

Static Mixer Tutorial:
The first step of this tutorial is to create the geometry of the static mixer in design modeller.

Once design modeller opens it asks for the desired length unit to be used throughout the project, metre is chosen here.
Making sure that the correct plane is selected (ZX plane) a new sketch is made with the settings, show 2D grid and snap to grid.

This enables sketching to be simple as only specific points on the grid can be used.
Using the polyline function a basic shape is sketch then revolved 360º around the Z-axis to give a 3D body.

Now the two inlets are formed by sketching circles on the original 2D sketch then extruding each to 3m in length.
One should be in the 'normal' direction and the other in the 'reversed' direction, this means that the inlets are on opposite sides of the mixer (Fig 1).

The next step is to create the mesh.
Regions have to be defined this shows where on the body the inlets and outlet are.

The default body spacing of the mesh is set to maximum of 0.3m, this is a relatively course mesh.
Next the surface and volume meshes are generated, then the mesh file is saved (.gtm) (Fig 2).

A simulation can now be defined in CFX-Pre.
This sets up the domain, inlet and outlet conditions and turbulence model (Fig 3).

Now a solver file is written and the simulation started.
Once the simulation is finished CFX-Post can be initiated.

In CFX-Post a streamline starting from a point is created that shows varying temperature (Fig 4).
A slice plane is also created that again shows temperature and can be 'picked' to different positions (Fig 5).

Contours are added to the slice plane and an animated movie is created.
Refining The Static Mixer Mesh Tutorial:

In this tutorial inflated boundaries are used, this increases the accuracy by generating prismatic elements from the surface.
This provides better resolution of the velocity field near the wall, where it changes rapidly.

The default body spacing is reduced from 0.3 to 0.2m.
An inflated boundary is added using five layers with a maximum thickness of 0.2m.

The surface and volume meshes are generated.
Next the simulation has to be redefined, the advection scheme is changed to high resolution from upwind, the physical timescale reduced and the number of iterations increased.

The slice plane shows the inflated boundary layers around the edge of the mixer, also there are more lines and they are smoother than the original mesh (Fig 6).
This is because the refined mesh is finer and represents the true geometry of the mixer better.

The temperature variable slice plane shows a smoother colour variation and finer inlet regions (the red and blue areas are thinner) (Fig 7).
In order to view the inflated elements volumes must be created.

The red and orange volumes show elements at different locations and radii (Fig 8).
Figure 9 shows the layers of inflated elements on the mixer walls.

Modifying the Geometry Tutorial:
In this tutorial the mixer inlet pipes are reduced in diameter from 0.5 to 0.4m.

The outlet pipe is extended and curved.
This involves adding two new planes and carrying out a revolution and extrusion.

Process Injection Mixing Pipe Tutorial:
This tutorial also includes creating the geometry of the body.

The process of creating the geometry includes making new planes, using the arc tool and sweep function (Fig 11).
Next is setting up the mesh, this involves applying regions of inlets and outlet and also inflating.

Once the maximum default body spacing has been set to 0.25m and new face spacing constraint needs to be added to the side inlet.
Now generate surface and volume mesh and inflation (Fig 12).

In CFX-Pre the properties of water have to be modified, this involves setting up new expressions.
The expression can be plotted and intermediate values evaluated.

The domain is now setup using thermal energy and k-Epsilon.
Region must be setup for the inlets and outlet, in order to do this profile data must be imported.

This profile data allows for the main inlet to have non-uniform flow (Fig 13).
In CFX-Post streamlines are added, they show the ranges of velocities.

The range can be changed between global, local or user specified.
The plane technique can be used to show other properties such as turbulent kinetic energy dissipation or temperature (Figs 16 & 17).

Blunt Body Tutorial:
This tutorial involves creating a solid body surrounded by a fluid body.

The solid body uses rectangle, fillet, chamfer and extrude techniques along with modifying the dimensions using vertical, horizontal, angle and length tools.
In the next part polyline, equal length, arc by tangent and extrude are used.

The extrusion involves removing material instead of adding (Fig 18).
The second part of the geometry is adding the fluid body then removing the solid body (Fig 19).

The solid body has to be frozen before adding the fluid body to enable this.
In CFX-Mesh regions are created, these include, inlet, outlet, freewall 1, freewall 2, symmetry plane and body.

Next surface mesh, volume mesh and inflation are generated (Fig 20).
Here surface proximity is used with number of elements across gap set to three.

In CFX-Pre the domain is configured, a composite region created and boundary conditions setup (Fig 21).
Initial values have to be set using the global initialisation tool.

In solver control physical timescale is set to two seconds and iterations to 60.
Once CFX-Solver is complete the results can be visualised in CFX-Post.

Above is a wireframe of the fluid body with a mirror across the symmetry plane (Fig 22) and a vector plot of the velocity around the exit end of the solid body (Fig 23).
Below is a streamline plot of the velocity over the solid body (Fig 24).

CAD Cleanup and Meshing Tutorial:
The first step of this tutorial is identifying errors within the geometry, the Verify Geometry function must be used to do this.

The next step is to insert a Virtual Face, this combines several faces to make a single face (Fig 25).
The mesher only needs to mesh the virtual face and not the separate constituent faces.

Once a surface mesh is produced another 'erroneous' area is highlighted, therefore another virtual face must be inserted (Fig 26).
Note how the fine strip 'disappears' enabling a more accurate surface mesh to be generated (Fig 27).

On this geometry are also 'short edges', these can be dealt with in the same way as the faces by creating Virtual Edges.
Once the five virtual edges are created the new surface mesh created is a lot smoother.

The Remove Short Edge function is also used to remove the remaining one short edge.
Finally a finer mesh, inflation and volume mesh are setup.

Figures 28, 29 & 30 show the original surface mesh, surface mesh once errors are resolved and final refined mesh respectively.
Notice how the areas of dense meshing disappear enabling the mesh to become more uniform over the whole geometry.

Introduction
This essay is based on Grand Challenge 2/4: Ubiquitous Computing: experience, design and science.

In summary, this Grand Challenge [1] envisages a future where computers are everywhere.
Ubiquitous computing is a broad field encompassing other fields such as distributed computing, mobile computing, sensor networks, human-computer interaction, and artificial intelligence.

In this context, the definition of computer is far greater than simply the humble desktop PC.
The definition includes the many "invisible" (embedded) computers that will/already exist, such as in our vehicles, in our homes, in our buildings, in our communications devices, perhaps even worn on our bodies and many other places besides.

In the future these embedded computers will be even more powerful than they are now.
Through the use of a multitude of sensors they will become inseparably interweaved with every aspect of our lives, able to detect, analyse and respond to changes/events in their surroundings.

These computers will all be networked and interconnected and collectively form "a single Global Ubiquitous Computer".
It would be impossible to cover every aspect of ubiquitous computing (or ubicomp) in detail so instead, this essay aims to give an overview of the field and explore what it could all mean for us.

The essay will focus on the applications of ubiquitous computing and the ways it may be implemented.
Also, it will briefly speculate on the possible problems associated with ubiquitous computing.

Background
The late Mark Weiser (July 23, 1952-April 27, 1999) is widely regarded as the father of ubiquitous computing [2].

He first proposed the concept of ubiquitous computing in 1988 while leading the Computer Science Laboratory at Xerox PARC (Palo Alto Research Centre) [3].
The following quote sums up the vision of ubiquitous computing: "Ubiquitous computing names the third wave in computing, just now beginning.

First were mainframes, each shared by lots of people.
Now we are in the personal computing era, person and machine staring uneasily at each other across the desktop.

Next comes ubiquitous computing, or the age of calm technology, when technology recedes into the background of our lives." --Mark Weiser [4] Here the definition of the computer is broadened to include anything with computational capability.
Weiser envisioned a future where we would be surrounded by computers that were so much a part of our environment and so well blended that we can just take them for granted not even notice them.

Computers will no longer just be those which sit on desktops, they will also be worn and embedded into everyday things Thus, ubiquitous computing has concepts more in common with an augmented reality than with virtual reality.
It involves moving towards a reality where the devices/computers/things we use in everyday life are context-aware and/or location-aware and operate as appropriate with regard to this new awareness.

This awareness could be facilitated by a network of sensors in combination with RFID tags.
The many changes in the way computers are used may also necessitate new ways of interacting with them so new HCI (Human-Computer Interaction) methods will be needed.

In a talk on the "Computer Science challenges of the next 10 years" given in 1996, Weiser describes the principles of ubiquitous computing [5]: The purpose of a computer is to help you do something else.
The best computer is a quiet, invisible servant.

The more you can do by intuition the smarter you are; the computer should extend your unconscious.
Technology should create calm.

State of the Art
While developments are being made, there is still some way to go before Weiser's vision of ubiquitous computing is fully realised.

A somewhat crude and rudimentary ubiquitous computer already exists, in the form of the Internet and all the devices and computers connected to it.
So we already possess a means for achieving the interconnectivity necessary for ubiquitous computing.

The extent of this connectivity is further extended by other technologies such as 3G mobile phone networks (i.e. for mobile internet) and even shorter range methods like Bluetooth.
However, the Internet is not without problems.

Its greatest strengths can also be one of its weaknesses.
It is the problem of governance.

The Internet does not have an administrator as such- so who is responsible when something goes wrong?
The issue of net governance [6] has been up for debate for some time.

Currently, the main barrier to true ubiquitous computing lies in ubiquity and the extent of current connectivity.
Computers are not everywhere and there exists a clear separation between humans and the machines.

Computer technology and human society are still distinctly separate entities.
This is demonstrated in the apathy expressed by some people towards computers.

The vision of ubiquitous computing entails apathy of a different kind.
It entails apathy where we don't think about computers not because we don't care but because we don't have to care.

Whether for reasons of finance or practicality, not everyone has access to computing.
For the vision to become a reality any Global Ubiquitous Computer must allow everyone in the world free and equal access- another hot topic of debate is net neutrality [7].

There is also the issue of interoperability.
Our computers are not immediately compatible.

This needs to change so that all computers are able to be connected to each other seamlessly.
Computers are currently isolating in nature.

This is in part due to the way we interact with our computers.
Currently in this "personal computing era", our main means of interacting with computers is via physical hardware-based input/control devices such as mouse, keyboard and/or possibly graphics tablets.

These input tools mean that computers are generally only used by one person at a time.
Smartcards are already in everyday use, such as the Oyster card used for paying fares on London's public transport network [8].

The use of such electronic systems will probably only increase.
No discussion about interconnected computer systems would be complete without the inclusion of the topic of privacy, given that at the most basic level, the purpose of any computer is to process data.

In the UK, data protection laws govern how long data can be kept and how it is used [9]:
Fairly and lawfully processed Processed for limited purposes Adequate, relevant and not excessive Accurate and up to date Not kept for longer than is necessary Processed in line with your rights Secure Not transferred to other countries without adequate protection

With increasing interconnectivity, potentially increasing amounts information about us is shared between systems so data protection becomes increasingly more important.
Future Possibilities

Advances in technology and falling hardware prices may in time solve some of the problems of ubiquity and connectivity.
Assuming these barriers can be overcome, the future has in store potentially wondrous possibilities.

Some may be realised sooner than others.
Healthcare can be enhanced if patients wear sensors, perhaps woven into clothing which are linked with a computer to constantly monitor readings- as the University of Technology in Sydney is investigating [10].

For patients with heart disease, such a system can monitor for signs of an impending heart attack and warn the patient, doctors and hospitals.
In his article "Ubiquitous Computing", Weiser notes that one of the challenges of ubiquitous computing is dealing with mobility.

People using computers will not be bound to a single location and their computer use may transfer between devices.
"The X window system protocol [(a window system designed for networking)], for instance, makes it very difficult to migrate the window of a running application from one screen to another." Also, current networking protocols are based on the assumption that a computer's name and network address is constant.

This will no longer be the case when devices are able to switch from network to network as users move from place to place.
[11].

With ubiquity and having computers everywhere all around us, new HCI techniques may be needed.
Futuristic interfaces similar to that seen in the Spielberg movie Minority Report could be the answer.

These user interfaces would allow a more hands-on approach and operate based either on direct contact or hand gestures.
These input methods would make computers quicker and more natural to use and may allow a computer or interactive wall to be used by more than one person at a time.

This is a major improvement over existing methods of input.
An example of this is Multi-Touch Interaction being pioneered by Jefferson Han of New York University [12], a "low-cost...

very scaleable...
and intuitive" touchscreen system which unlike existing touchscreens allow for multiple points of contact.

Another example of this is the FlipTouch project [13] from the Office of Tomorrow Project of Digital Media which is based on an interactive transparent display and IR stereo tracking- essentially two infra-red cameras which in conjunction with computer vision algorithms allow the computer to 'see' what hand gestures the user is making on the screen.
Intelligent spaces will become commonplace such as in smart homes.

Smart homes are being built even today [14] and are part of the evolution of home automation concepts.
In the future, potentially all homes may be smart homes.

These intelligent spaces will learn the preferences of their users and customise the environment as appropriate.
Problems may occur for homes or buildings which are not 'smart' to begin but are instead being upgraded to become 'smart'.

In the paper "At home with ubiquitous computing: seven challenges",[15] Edwards & Grinter investigate the possible problems that ubiquitous computing may result in, in the context of smart homes.
In it, the concept of "The "Accidentally" Smart Home" is discussed.

The issue here is that where system designed to interoperate with other systems may do so but in an unwanted way.
Ideally they should be "plug and play" but in practice the user will have to set up the device so that the correct network connections are made.

Also, with such an array of different devices, it may not be a certainty that the interoperability that is required will occur at all [15]- therefore standard protocols will need to be defined.
Reliability is also important.

If there is some central computer co-ordinating all the other embedded computers within the home, that central computer must not crash- or at least crash only extremely rarely.
So such a computer should have built in redundancies so that should anything wrong, it can continue to operate.

Finally, computers operate in a binary fashion whereas the real world is analogue in nature [15].
Programming must be robust enough so that the system can cope with ambiguities.

Even more mundane problems can benefit from an ubicomp approach.
For example, the simple issue of what your "Online Status" is in an instant messaging program.

The most basic implementations of this feature simply toggle the status between Online and Offline.
Other implementations may allow selection from a number of preset statuses or alternatively allow you to type your own.

"Online Status" is not typically an issue but now with the potential for instant messaging on the move from highly mobile devices such as phones, PDAs, Pocket PCs, the actual location of the user is now much more relevant.
LUCI, the Laboratory for Ubiquitous Computing and Interaction of UC Irvine, has a project called Nomatic*Gaim [16].

Nomatic*Gaim is an instant messaging program which via sensors and a central will automatically determine the location of the user and the name of the location and display this in the program for other users to see.
Another mundane application may be to ensure we (almost) never lose anything ever again.

The Ubicomp Lab of National Taiwan University has a "Object location tracker" project [17].
This is a simple yet innovative use of RFID tags.

Various objects are tagged and can be later located using an RFID reader with antenna.
It makes use of another of their projects called Zigbee which is an indoor localization system based on radio sensor network.

This allows any tagged object to be located.
Ubiquitous computing will result in all systems being context aware.

For this to work they will collect data about us, process it and share it with each other.
This poses major privacy concerns.

As long as this information is used only in the provision of the user experience, it is not a problem.
The information collected can be highly personal and/or confidential but in using a ubiquitous computing system, there may be unwanted sharing of this information.

Ideally all data about users should be erased as soon as its purpose is served but therein lies the problem, the information may be required for the ubiquitous computing to operate for us in the manner we require of it so it may be retained in the form of personal "profiles".
The privacy issues will need to be resolved before broad uptake of ubiquitous computing is feasible.

With greater use of smartcard systems, smartcard chips which allow for electronic payment may soon be widely incorporated into common devices- mobile phones in particular, as in Japan where smartcard chips in mobile phones can also be used for travel.
Increased use of smartcards also raises privacy issues.

If used as payment on transport systems, such as in the case of the Oyster card, the users travel patterns can be logged and analysed [18].
This has incurred the displeasure of civil liberties groups and new smartcard systems which operate similarly will no doubt do the same.

Nevertheless, police in the UK are finding the information collected by Oyster cards useful in tracking criminals [19].
Conclusions

Ubiquitous computing will make computing so much a part of our lives it will become as natural as reading a sign on a wall and feel just as effortless.
It will allow us access to information seamlessly wherever we are.

The promises of ubiquitous computing are great but there are also potential problems.
These problems will need to be resolved before ubiquitous computing can be truly realised.

Something like ubiquitous computing spans is so complex in nature and spans so many fields that I think it is difficult to say when this grand challenge will be fulfilled- though the Grand Challenges document cites a time frame of 15 years.
However, seeing the beginnings of the global ubiquitous around me I am fairly certain that it will be fulfilled eventually.

1. Introductionand informal statement of requirements
In the course of playing the video game Ratchet and Clank, the player will encounter various (often useful) objects.

Thus, one of the objectives of the game is to collect as many of these objects as possible.
For any given objective (or problem) for which there exists a computable solution, there may be one or more possible solutions which will produce the desired result.

This coursework aims to develop one such solution- i.e. to attempt to discover the best way to collect as many objects as possible.
Assumptions

For any model, assumptions about the operating environment must be made.
I assume the following: In the game, there exist infinite instances of each of the types of objects (e.g. in a crate containing only apples and pears, there are infinite apples and infinite pears).

This allows that at least one of each object can be collected.
Also, this assumption is made because the exact numbers are unknown.

Objects will be encountered randomly- i.e. randomly distributed within the universal set.
Finally, it is assumed that as listed on the exercise sheet, there are 11 distinct objects.

Requirements :
The algorithm must produce a set containing a single instance of each object, with no duplicates.

When the set contains one of each type of object, the set is complete and the algorithm will terminate.
2. The Algorithm

Initial conditions:
The universal set U contains infinite instances of all the objects.

The set C which will hold the collected objects is initially empty.
The constant d is defined as the number of distinct elements in set U, i.e. the number of different objects.

For this example, d is assumed to be 11.
The variable s is defined as the number of elements in C.

It is initially equal to zero.
2a. Expressed informally as a sequence of steps

FORMULA
2b. Expressed as Pseudocode

In this section I make comments in italics.
Arrays are defined using the zero-based convention, thus the first element of array U is U[0].

NULL will be used to signify an empty element- when an object is picked up, it is no longer a part of the universal set and the position it was in is now empty.
FORMULA Next I define the subroutine for the PickUp operation, first the object is "picked up" and added to the collection then the cardinality of C is checked (the number of distinct elements).

If newly added object already exists then s+1 will be greater than cardinality of C or else if new object is unique, s+1 = |C|: FORMULA I define the subroutine for the Drop operation.
The element of the array is set to NULL to represent that object being removed from collection.

The removed object U[i] is a duplicate instance of an already existing object in the set C.
This is verified by finding the set complement between the set X and C. X contains a single element: the single removed object.

If the removed object exists in both X and C then the set complement is an empty set : FORMULA
3. Algorithm Properties:

Input: The Universal set U containing at least one of each type of object, the number of distinct objects (represented by the constant d).Output: The set of collected objects, C. Definiteness: The main function Play uses a while loop to go through the set U and uses the PickUp subroutine to collect objects and Drop to discard duplicates.
Finiteness: When s = d, the set C is complete and contains one of each type of object- at this point the program terminates.Effectiveness: Each step of the algorithm is either a comparison or an assignment operation so will take a finite amount of time.Corrrectness: Yes, algorithm results in a set containing all distinct elements.

Tested using d=2 with U containing 5 elements.
4. Algorithm Efficiency:

The program is controlled by a while loop so the time need to execute the algorithm is ultimately dependent on the nature of the universal set U as this governs the time taken for the while loop (controlling the program) to complete.
A greater number of distinct elements in U results in a longer execution time.

The actual execution time is affected by the distribution of the elements within U.
For example, when attempting to obtain a set containing the elements {3,1, 4, 2}, the algorithm would achieve this faster from a set such as {2,3,4,1,4,3,1,2,1,4} rather than (2,2,2,2,2,2,2,3,1,4}.

On the basis of the model used in developing this algorithm, the execution time would vary due across runs due to the random distribution of the elements contained within the set U (see Assumptions).
If the number of distinct objects is given by the constant d, the ideal U set would be one where there is no repetition within the first d elements.

Whatever comes after is irrelevant.
Using the exercise example of d=11, taking into account 11 runs of the While loop, I estimate that at least 78 (11 x 7 steps per completed While + 1 last check of While condition to terminate) steps are required to obtain a set of 11 distinct objects.

This assumes that calling a subroutine is only counted once, as a single step, ignoring their internal steps.
5. Application of Sets:

A common application of sets could be in programs that generate random sets of numbers for lottery tickets, bingo, etc.
So for example, with the National Lottery, there may be set U = {1,2,3,4...

48,49}.
The program must then generate subsets which contain 6 elements with no repetitions within the sets.

1. Introductionand informal statement of requirements
In this coursework the problem to be solved is based on the analogy of a rat navigating a maze, trying to get from the entrance to the exit.

Essentially, the aim is to develop a solution to the maze- i.e. find the path from the entrance to exit: A LIFO stack can be used to store the path taken.
It will model the movements of the rat and allow retracing of steps in the case that a wall or "dead end" is encountered.

On the assignment handout sheets, the maze is depicted in a form such that it appears that grid references are given in (Y, X) form rather than the conventional (X, Y).
Also, the grid references are zero-based, i.e. the first cell is (0,0).

Both these features indicate that the maze should be modelled using a two-dimensional array (as in C programming) because such an array has exactly the same properties.
Assumptions

For any model, assumptions must be made.
The assumptions provided on the Exercise sheet shall be used: Mazes consist of walls and paths.

Walls cannot be crossed and it is unknown what is on the other side.
Movement is possible in four directions- North (N), South (S), East (E), West (W).

At a junction, N,S,E,W directions are tried in that order.
The entry and exit co-ordinates are given.

Other I make a further assumption and assume a Peek() function is available to read the last pushed value without removing it from the stack.
It is assumed that functions/methods for finding the number of elements in an array and the sum of an array are already defined.

Requirements :
The algorithm must solve the maze and the solution should involve the Pop(), Push() and IsEmpty() operations.

When this is achieved, the algorithm is complete and should terminate.
Ideally, the algorithm should avoid revisiting squares unless a dead end has been encountered and it is attempting to find a valid path.

When a dead end is encountered, Pop() should be used until it arrives back at a position that offers an alternate path.
2. The Algorithm

Initial conditions:
The two-dimensional input array M models the maze.

"Wall" elements will be assigned the value of 1, while "Path" elements have the value zero: The LIFO stack P will store the path, it is initially empty.
Positions are stored in the form (I,J, dir).

Where [I][J] is the array reference of the current element that the algorithm has arrived at.
The LIFO stack D to store possible directions is initially empty..

The variable DSize will be used to store the number of elements in stack D dir will be initialised to 0.
During run-time, it will take one of four values 1,2,3,4- corresponding to the 4 possible directions North, South, East and West.

The Entry co-ordinate is A(I A,J A) and the Exit co-ordinate is B(I B,J B).
2a. Expressed informally as a sequence of steps

The following is designed as a general solution to work for any maze.
However, the more complex the maze is, the longer the running time.

1) First there must be a LOOK function to obtain information about the spaces to the North, South, East and West so that the algorithm can determine which direction to move to.
It also ensures that the algorithm will never try to go through walls/obstacles.

The LOOK takes Peek(P) as input and returns a stack D indicating which directions are valid moves (i.e. moves which do not mean trying to move through a wall) and the number of possible valid moves (the variable DSize).Given that it has been specified that movements at junctions must be tried in the order N,S,E,W, these directions are "Looked" in reverse order so that when stack D is Popped, the directions are tried in the order specified on the exercise sheet.2) The algorithm runs until the exit co-ordinate is reached- i.e. until Peek(P) == B.
The algorithm begins at the entry co-ordinates, which is Pushed into stack P.3) Use LOOK with Peek(P) as input.

4a) If there is only one valid move, that move is made.
Or if there is more than one possible direction then the algorithm is on a junction.

The first available move is made based on the order NSEW by Pop(D).
Then, depending on direction, the appropriate position is Pushed into P and algorithm moves into that direction.

The array element representing the cell that the algorithm is leaving is marked by changing its value from 0 to 1.
This results in it being impassable so that the algorithm will not revisit it.4b) If there are no possible moves, Pop(P) and Look(Peek(P)) until DSize > 1.6) Repeat steps 3-5.7) When Peek(P) == B, the sum of array M is equal to the number of elements because every one of its elements will equal 1.8) Output Stack P and terminate algorithm.

2b. Expressed as Pseudocode
In this section I make comments in italics.

Here I refer to the I or J parts of the co-ordinates in the form A.I and A.J, like this: Peek(P).I and Peek(P).J, where A is the co-ordinate.
Assume the starting values: FORMULA In an actual program, the co-ordinates would probably be represented by a pair of linked-lists.

LOOK function defined to provide information about surroundings: FORMULA For easier reading I will refer to the lengthy LOOK function calls as follows: FORMULA Each cycle of while loop updates stack so that top of the stack shows the co-ordinates and direction of next square.
Pseudocode program begins here: FORMULA End of Algorithm Pseudocode program ends.

3. Algorithm Properties:
Input: The array M containing the maze to be navigated, the entry co-ordinate and the exit co-ordinate.Output: A LIFO stack containing the solution to the maze or an error message.Definiteness: Each step is precisely defined using only assignment, conditionals and loop statements.

Finiteness: If the maze has a solution then the algorithm terminates when this solution is found, i.e. the exit co-ordinate is reached.
If there is no solution, it will terminate once the entire maze has been processed.

Therefore the algorithm has a finite number of steps.Effectiveness: Each step of the algorithm is either a comparison or an assignment operation so will take a finite amount of time.Correctness: Yes, providing the maze is solvable, algorithm outputs a stack containing the solution to the maze.
If the maze is not solvable, an error message is output instead.

4. Trace
For the example given:

The starting square is given by the position (3,0,0).
The following is a trace of the "state" space of the algorithm showing all the positions that are recorded in the LIFO from start to finish.

Reminder, the third figures mean as follows: 1=North, 2=South, 3=East, 4=West.
Note that for my own maze in particular, the table is a complete transcript of the actual route taken.

The P stack that the program produces will be more concise and will not have repeated co-ordinates.
5. The Space Required

Calculation of the space required
For the maze:

For a maze X columns by Y rows, there is a border of padding cells around it which will be filled with 1s.For an array to store such a maze, it must be able to store (total height * total width) number of elements.The total height of each column is Y + 2 padding cells.The total width of each row is X + 2 padding cells.Therefore the total number of elements required is (Y+2)(X+2).So for example, the maze I have created requires (10+2)(5+2) = 84 elements.
For the stacks:

The main stack P which stores the path as it is being found.
The stack must have enough space to accommodate all possible routes.Movement is permitted in four directions and the maze itself (excluding borders) is X*Y.Therefore, to ensure sufficient storage space, the stack should be able to accommodate X*Y*4 positions.So for my maze, this equates to 200 spaces.

1. Introduction
The tree data model is used in computer science for things like file systems and search algorithms.

Trees are also used in modelling chemical structures and biological systems.
This coursework will investigate tree models as applied to binary search and also look at the procedure for inserting data into such a tree.

This coursework will result in a better understanding of how to model a problem based on the tree model.
The degree of the tree that will be used is the maximum number of child nodes that any particular node can have.

In this case, where a binary tree will be used, the degree is 2.
An algorithm is to be devised to insert data into a binary tree and thus construct trees for the data.

The following datasets will be involved:
List of all SSE degree courses A subset of counties in England.

Informal statement of requirements
The insertion algorithm should result in a tree containing all elements of the dataset in ascending alphanumeric order (i.e. A to Z, 0 to 9).

The tree will consist of nodes such that every node other that the root node will have one parent node and either:
Zero child nodesOne Left child nodeOne Right child nodeBoth a Left and a Right child node.

There should be no repetition- i.e. all nodes in the tree must have unique values.
Therefore the algorithm must check if adding the current item duplicates an existing item and discard if this is the case.

The value of all nodes in the left sub-trees must be less than the value of the node it descends from.
The value of all nodes in the right sub-trees must be greater than the value of the node it descends from.

2. The Algorithm
As an Informal Series of Steps

If a non-existing node is referenced, its value is automatically NULL.
The algorithm will start building tree from the root node.

FORMULA
As Pseudocode

The Pseudocode will be designed with the following assumptions: The data set D that the algorithm will operate on is a finite set with N elements and D[x] refers to element x of the dataset.
It will be modelled as a single-dimension array.

R and C are nodes.
Nodes may possibly have child C nodes.

There may be 0, 1 or 2 C nodes.
A parent node may have either one child of either type left[R] or right[R] or have both child nodes.

The type of the node will be determined by whether the value of C is less than or greater than the value of R.
If less, it is a left child (left[R]) else it is a (right[R]).

Empty or non-existent nodes return the value NULL when referenced The data will be stored into the tree T.
The Pseudocode for this algorithm is as follows.

FORMULA
Testing

The two sample datasets that the algorithm is to run on are:
List of all SSE degree courses A subset of counties in England.

Here the algorithm will be used for each in turn.
List of all SSE degree courses

This is a list of SSE degree courses (copied directly from the various SSE pages, for CS, Cyb, EE, IT).
For the ease of display of the tree, the algorithm shall run on the UCAS codes of the degree programmes.

The convention for sorting used is numbers before letters, i.e. : 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ The resulting tree is shown on the next page.
It is an unbalanced tree because the root used would not be the middle item if the elements were ordered alphabetically.

In an alphabetical list, the root would be located near the end of the list, this is why the tree left-heavy.
Subset of Counties in England

The following shows the worst possible use of a binary tree.
As the dataset is already in alphabetical order, the resulting tree is nothing more than a list and offers no additional advantages compared to a normal list.

Steps to Insert an Item
For the first tree (SSE degrees), the longest path from root to leaf is 11 elements including the root.

Therefore for that tree, up to 11 iterations of the algorithm loop would be required to insert a new item.
The inefficiency of the second tree (subset of English counties) is demonstrated by the fact that despite being built from a smaller dataset (18 counties versus 24 SSE degrees), it would take up to 18 iterations to insert a new item.

3. Algorithm Properties:
Input: The dataset containing the data that is to be inserted into a binary tree.Output: The binary search tree T. Definiteness: The algorithm exhibits definiteness in that all data that will be inserted into the tree will one and only one of the following: be equal to the top node, greater than the top node or less than the top node.

Comparison statements identify which of these is the case for each data item to be inserted so that it can be insert to the correct location by an assignment statement.
A loop ensures that every data item is dealt with.

Finiteness: The algorithm terminates when all data has been inserted.Effectiveness: Each step of the algorithm is either a comparison or an assignment operation so will take a finite amount of time.Corrrectness: Yes, algorithm results in a binary search tree of the data.
NP-Complete problems

General Procedure:
In solving of TSPs, we are essentially finding Hamiltonian Circuits - i.e. finding that path that that will visit each vertex (for this exercise, these happen to be cities) and then return to the point of origin.

For solving TSPs, there is the additional condition that we are seeking the circuit with the minimum possible weight (i.e. shortest distance travelled): Start from a city and proceed to the nearest city that has not yet been visited.
Stop when last city is reached and return to the starting city.

Repeat with a different starting city and compare total weights to find to find the minimum possible weight.
Here are the distances of some of the various possible circuits:

Use of Permutations and Verification of Result
The number of possible circuits is equivalent to the number of possible permutations.

From initial testing, the shortest circuit has a distance of 237 and there are multiple possible circuits which result in this total distance.
For the example problem, there are 5!

= 120 possible circuits, where half of these are the other half in reverse.
The 120 combinations consist of 24 circuits for each starting city (12 circuits and their reverses).

The value of 120 was obtained via application of the permutation formula: FORMULA where n is the total number of items and r is the number of items required in each set.
Since n = r in this exercise (permutations of 5 cities from a set of 5 cities), the denominator is 0!

= 1.
So the formula is reduced to n!.

120 permutations is a small enough number to "brute force" and hence verify whether or not the shortest circuit has indeed been found.
An Excel spreadsheet and lookup functions were used for this.

First, a list of permutations need to be found, this was done using the Permutation Generator at URL This was put onto the spreadsheet.
The table giving relative distances was pasted into the spreadsheet to use as a lookup table.

With the help of the INDEX spreadsheet function, this allowed the distances travelled to be generated using the list of permutations.
The INDEX spreadsheet function is as follows: INDEX(reference,row_num,column_num).

The function will use pairs of two consecutive numbers in the permutations to select the appropriate distance value from the lookup table.
A sample of the data generated is shown here: Data for all 120 permutations (circuits) were generated and the shortest possible distance is confirmed to be 237.

There are 10 circuits that will result in the total distance of 237- this consists of 5 circuits and their reverse paths.
For brevity, only one direction will be shown- their reverse paths are equally valid:

Questions
If no.

of cities = 30 and if it takes 1 nanosecond (10-9 seconds) to find one circuit (i.e. Start to finish and back to start) on a Dell Pentium PC, how long would it take to find the shortest circuit?
FORMULA

Is exhaustive search of all possible paths the only way to solve the problem?
Reference: URL There is not yet a general solution for the problem but using "heuristic algorithms" it is possible to obtain a "best guess" solution which is usually sufficient.

For small finite number of cities, exact solutions can be obtained using "branch and bound" algorithms and "linear programming".
What are NP-Complete problems and why TSP is an NP-Complete problem?

NP-Complete problems are problems which cannot be easily solved by computation.
TSP is NP-Complete because it the time taken to solve it increases exponentially as the complexity of the problem increases- i.e. the time taken increases exponentially as the number of cities increases.

Halting Problem
Translate the following pseudo-code into a 'C' program and test the program by executing it for the following inputs: n = 3, 21, 25, 27.

Check if there is an input for which the program doesn't halt.
FORMULA Output values for each of the inputs for the above program: FORMULA The problem with this particular implementation is that it does not check validity of input and so does NOT halt when input other than integers are used.

As long as input is an INTEGER, the program will halt.
Significance of "halting problem".

The halting problem, as it exists in computability theory, is deciding whether a program will finish or continue running forever, given a particular input.
To solve the halting problem is to develop a general solution for determining finiteness.

While this can in general be determined for programs individually, it is another matter to In 1936, Alan Turing proved that a general algorithm (one that works for any program) to solve the halting problem for all possible inputs cannot exist.
1. Introduction

'Biomass' is the broad term given for matter of biological origin.
Biomass is defined as 'The total mass of all the organisms of a given type and/or in a given area; for example the biomass of trees' (6).

Biomass in all versatile forms represents an important contribution to global primary energy use.
The World derives 14% of its primary energy from biomass (1993).

It is an important energy source (35%) for ¾ of the world's population in developing countries.
Biomass forms an important role in the natural carbon cycle (fig.1).

Mankind's intervention of the natural biomass system can interrupt the carbon cycle, which has been linked to climate change.
2. Biomass effecting climate change.

We are using biomass in an unsustainable way, for example using up wood fuel faster than new trees can grow.
Besides slope instability, landslides and mudflows this can also lead to desertification - less land for biomass energy crops, or land for re-forestation.

In Indonesia we have cleared 9 million ha of tropical rain forests to grow palm oil 40% of which is for bio-diesel production.
This hampers the fight against climate change, the very problem bio fuels are supposed to overcome (11).

2.1 Deforestation/Land Clearance.
Deforestation is a major cause of climate change.

When trees are cut down and put under the plough they cease to absorb atmospheric CO 2.
They decay, and along with disturbed or bare soils re-emit stored CO 2 into the atmosphere (1).

The rate at which is estimated to be 1.6 Gigatonnes CO 2/yr, equal to 20-25% of the total annual human-induced CO 2 emissions (12).
2.2 Fertiliser use.

Energy intensive land use leads to land / soil degradation which requires the use of artificial fertilisers to maintain production levels.
Fertiliser production requires the energy intensive inefficient 'Haber process' to fix Nitrogen into NO 2 and Ammonia (10).

Fertilisers account for 67% of human induced NO 2 emissions (1).
Animal Fertiliser requires land clearance to graze animals, which also produce methane.

2.3 Bio fuels
look like an attractive alternative to our dependence on fossil fuels.

They can appear 'Carbon neutral' by recycling CO 2 back into the atmosphere.
Bio-fuels include; Solids - Burning wood, animal dung and crop residues for traditional cooking and heating, involves the incomplete combustion of fuel in non-optimum conditions.

Results in significant quantities of greenhouse gases.
These include methane (contributes 5% atmospheric methane), un-burnt hydrocarbons (10%), di-Nitrogen oxide (8%) and CO 2 (3%)(2)(1).

We could reduce emissions by utilising cleaner burning fuels such as charcoal, or biomass-sourced methane or ethanol in stoves instead of wood (2).
Liquids - Transport fuels from Sugar cane derived Ethanol ('Pro-Alcool' Brazil)(1) and Vegetable Oil Bio diesel (USA/Indonesia)(11).

CO 2/NO 2 emissions vary according to source, process, and whether fuels are burnt at 100% or blended with fossil fuels.
Gases - Animal wastes (Biogas) and municipal waste (landfill gas) can provide useful energy that would have otherwise been lost to atmosphere.

For example 12% of methane in atmosphere is from landfill (1).
Methane has a global warming potential over 100 years, and averaged over this time each kg of CH 4 warms the earth 23 times more than the same mass of CO 2 (14).

Combustion produces CO 2 + water, so it's better to burn Methane than to release it into the atmosphere.
2.4 Transportation and processing

account for the conventional energy used in intensive crop production, irrigation, machinery fuel, labour, processing energy and chemical use, and biomass becomes a less environmentally sound alternative.
Unless processes optimise energy sourced from locally sourced biomass or power generated from renewables.

For example, for Bio-diesel, process waste oils using energy from renewable sources, and process/distribute close to source reducing transportation.
3. Climate change effects on Biomass

3.1 Increasing temperatures
will alter global weather features such as the intensity of the El Nino phenomenon, the distribution and intensity of pressure systems and associated precipitation, storms, flooding and drought events.

These will affect biomass in both its natural environment and for commercial operations.
Increased evapo-transpiration by plants will condense into more cloud cover - blocking valuable solar radiation for plants to photosynthesise.

Increased summer drying decreases crop yields, water resource quantity and quality.
Warmer climates increase the risks of pests, disease and forest fires.

Ice caps and glaciers melt increasing sea levels.
Increased sea temperatures will expand the oceans further, flooding biomass production on low coastal land.

Increasing vegetation decay releasing methane, and in the long term reducing productive land area.
3.2 Increasing atmospheric CO2 levels.

This will increase crop yields.
This increase would be counteracted by the risk of water shortages for example in Southern and Eastern Europe, and by shortening of the duration of growth in many crops.

Northern Europe is likely to experience overall positive effects, whereas crop production in Southern Europe may be threatened.
4. Conclusion; Biomass management.

Sustainability, education, management, and efficient use of biomass are vital factors in helping to reduce our effects of climate change.
Biomass fuel crops (wood/bio fuels) need to be grown in a sustainable way so that we replenish crops as we use them.

Producing and distributing biomass close to their source to reduce transport requirements.
Recycling production by-products, for example where sugar cane fibres are burnt for electricity generation their ashes and washings can be returned to the soil as a nitrogen rich fertiliser (10).

4.1 Managed fuel crops.
For example using fast growing willows or poplars as an important source of fuel for either cooking/heating or electricity generation.

A practice known as 'Short rotation arable coppicing' (7).
4.2 Efficient use of bio fuels.

In developing countries greenhouse gas emissions could be significantly reduced by the use of more efficient stoves that burn either biomass-sourced methane or ethanol fuels (2).
Using charcoal to replace wood as fuel.

4.3 Re-forestation.
Re-planting millions of ha with new trees would be an attempt to reduce atmospheric CO 2, trees act as a vegetation based 'carbon sink' (3).

Reforestation and grassland restoration would help put CO 2 back into soil storage too.
These processes are collectively termed 'Carbon sequestration'.

For example to sequester all the UK's continuing carbon emissions in trees would require new forests to be planted over an area the size of Devon and Cornwall each year (7).
To conclude, our production, processing and use of biomass can emit greenhouse gasses such as methane and carbon dioxide, which contribute to global warming and climate change.

It's our responsibility to ensure our practices reduce these emissions to a minimum.
Introduction

A Petters ACI 7.2 bhp (5.4 kW) single cylinder diesel engine was tested under laboratory conditions running on both standard Diesel, and an alternative Rape Methyl Ester (R.M.E) 'Bio-Diesel' fuel.
Various key parameters of the engine, run on these different fuels, such as mechanical efficiency, indicated thermal efficiency, brake thermal efficiency, specific fuel consumption and brake mean effective pressure were deduced by taking a series of measurements over a range of loads in order to then plot values on graphs.

Methodology
Theory

To compare the relative performance of these fuels we need to calculate three main efficiencies: The brake thermal efficiency FORMULA, where: FORMULA Which includes all energy losses from the engine.
The mechanical efficiency FORMULA where: FORMULA Which includes only mechanical losses from the engine.

The Indicated thermal efficiency FORMULA where: FORMULA Which ignores the mechanical losses in the engine.
Method

The mathematical equations used to work out these efficiencies (see 6.
Appendix) require the measured values of engine load, engine rpm, fuel and airflow rates, and exhaust, air and inlet temperature readings.

The parameters were measured as follows:
Load.

The engine drove an electric dynamometer through a shaft to a torque meter consisting of a torque arm and a spring balance.
The electrical power was dissipated through a resistance bank cooled by a forced air draft.

The amount of power dissipated; therefore the load on the engine could be adjusted using a rotary rheostat on the load bank.
A voltmeter and ammeter measured the power being absorbed by the load bank.

Engine speed.
Under each load applied, the engine speed in rotations per minute (rpm) was set at 2,800 rpm.

This was done by adjusting the engine throttle and measuring the speed using a digital optical tachometer in conjunction with a reflective plate placed onto the rear surface of the flywheel.
Torque.

Measured by adjusting tension on the spring balance by turning a knurled edged disk on a threaded suspension bar until a pointer on the balance pointed to a fixed mark on the frame.
The Torque value in Newton's could then be read off the spring balance needle position.

Fuel Consumption.
Measured by turning off the fuel from the header tank so that the fuel (Diesel or Bio diesel) was supplied by a graduated tube.

The time taken for the engine to consume 20ml of fuel was measured using a stopwatch.
Temperatures

were taken via digital temperature sensors located on the engine components to measure ambient air temp, Inlet Temp and Exhaust temp in degrees Celsius (C) then converting to Kelvin (K).
Air Pressure Drop.

A manometer measured the air pressure drop across a calibrated orifice plate hole located inside the engine air box.
The manometer was set to an inclined position to make reading subtle pressure drop measurements easier.

This inclination angle was taken into account when equating formulas by using the sin tangent of this angle, which was 12deg.
Multiplying the manometer reading in mm by a factor of 0.2 (sin 12deg = 0.2) gave an accurate reading of the pressure difference.

Indicator Diagrams.
Sensors were connected between the engine block and an oscilloscope then to a computer data logger program, which measured both cylinder pressure and volume.

Plots of pressure against volume gives an 'indicator diagram'.
The area enclosed by this diagram gives a measure of the 'indicated power', the work done by the fuel in moving the piston up and down the cylinder every stroke.

All of this indicated power is not available as useful work in engine 'brake power' as some is lost in friction driving things like pumps, valves etc.
Order of which measurements were taken:

Set Load using rotary rheostat on the load bank.Set engine speed to 2,800 rpm under each load applied, using a combination of engine throttle adjustment, and the digital optical tachometer.
Measure Torque by adjusting tension on spring balance by turning knurled edged disk until the pointer on the balance points to a fixed mark on the frame.

The Torque value in Newton's can then be read off the spring balance needle position.Measure Fuel Consumption in Seconds Measure Temperatures in KelvinMeasure Air Pressure Drop in mm.Repeat all steps 1-6 for each desired load.
The graphs shown in figures 1 to 6 were generated using the raw data obtained on both the lab sheets and Microsoft Excel spreadsheets in the attached Appendices.

Results
Discussion of findings

4.1 The Mechanical Efficiency
graph (fig.1) shows that for both fuels as the load increased, mechanical efficiency decreased.

At a higher load than tested the engine would therefore have stalled due to lack of power.
The Biodiesel appears to have a higher mechanical efficiency over Diesel.

4.2 The Indicated Thermal Efficiency
graph (fig.2) shows that for Diesel fuel the thermal efficiency rose as the load increased from 0 to around 15N.

From 15N the thermal efficiency rose only slightly to a peak at around 36 Newton's load.
The engine therefore would have been at its most thermally efficient at this load and temperature.

From 36 to 50 Newton's load the thermal efficiency decreased, as more of the energy in the fuel was lost as heat as the engine became too hot.
Biodiesel followed a similar trend to Diesel but burnt at a higher temperature at every load and so was overall less thermally efficient.

Biodiesel appeared most thermally efficient at 25 Newton's load, with thermal efficiency tailing off rapidly above this load.
4.3 The Brake Thermal Efficiency

graph (fig.3) shows that for both fuels as the load increased to approx 7 Newton's, the brake thermal efficiency increased at similar rates.
Above 7 Newton's load however, both fuels continued to increase brake thermal efficiency but at a lesser rate with Diesel performing better overall Compared to Biodiesel.

Both fuels seemed to give maximum brake thermal efficiencies at around 360-380 N m-2 of Brake Mean Effective Pressure, above which their brake thermal efficiencies either dropped significantly (Biodiesel) or remained constant (Diesel).
4.4 The power loss due to Friction

(fig.4) shows that when the engine burnt Diesel fuel on average 1.624kW of power was lost due to internal friction.
Biodiesel faired better in that only 1.221kW of power on average was lost this way.

Biodiesel having a good natural lubricating property over Diesel could explain less friction.
The Air/Fuel Ratio

for both fuels decreased as the loads increased for both fuels.
Diesel ranged from 50:1 at 0 load to 15:1 at 50N load.

Biodiesel ranged from 43:1 to 11:1 at 47N load.
This higher Air/Fuel ratio could be explained by the presence of excess Oxygen on the Biodiesel molecules.

More Oxygen means the fuel requires less air with which to burn.
(2)

Fuel Consumption
rates for Biodiesel were around 10% higher than Diesel possibly due to the fact that Vegetable oils have a lower cetane number (40) compared to Diesel (45-50) and overall lower energy density of 37-40 MJ kg-1 compared to 45.9 MJ kg-1 for Diesel.

(2)
Heat Losses.

(Fig.5) Shows that when the engine was burning Diesel at loads between 0 and 15N a greater amount of power in kW was being lost through heat out of the exhaust system than the engine was producing in kW!
At loads above 15N the engine only just managed to produce slightly more power (max of 0.35kW) than the exhaust power loss!

The greatest heat loss was through the air-cooling system (air cooled finned engine block).
At maximum engine power output (3.64kW) a massive 10.21kW was being lost to heat the air.

The total heat power loss was therefore 13.85kW - a frightening fact..
Biodiesel performed in a similar way to Diesel though its total loss was higher at 17.70kW.

Indicator Diagrams
of pressure VS crank angle show the 4-stroke cycle very clearly.

Both figures 7 & 8 demonstrate graphically that as the piston is pulled down and the new fuel/air mix is drawn into the cylinder, volume increases and pressure remains constant.
When the piston travels back up the cylinder to compress the mixture, the volume decreases and pressure increases.

When ignition takes place the pressure decreases and volume increases as the piston is pushed downwards.
On the exhaust stroke volume decreases and pressure stays the same, as the exhaust value remains open.

The line at constant pressure represents the pumping losses like the air/fuel mix being sucked in, and exhaust blowing out.
The area enclosed by the PV curves represents the Indicated power - the cyclic work done.

(Figure 7) shows a typical P/V plot trace for when the engine was running at a low load.
(Figure 8) shows that as the load was increased and the engine was working hard, the area within the cyclic trace became greater.

The exact loads for each figure are unknown as there was some uncertainty as to which P/V plot corresponded to which load from the data logger.
Some data produced returned zero values.

4.8
To conclude Diesel appears to perform better than it's renewable counterpart being more thermally efficient with overall engine efficiency higher than Biodiesel.

Diesel fuel consumption was lower, and less energy was lost as heat.
Biodiesel did perform well loosing less brake power to friction than Diesel.

Introduction
The aim of this experiment was to run a series of set measured tests on a 3-phase induction motor in order to understand its properties both as a motor and as a generator under both grid-connected and stand-alone environments.

Measurements such as Voltages, Currents and Motor Rotation speeds were taken in order to calculate the various parameters of component reactance's and efficiency curves.
This allowed us to deduce which circuit configuration would be best suited to be used in both a stand-alone and grid-connected generating environment.

Details regarding how induction generators work, the equipment used within the test and a list of aims and equations used are given in the accompanying lab sheet (1) in the Appendix.
2. Magnetising Curve and Running-Light Equivalent Circuit Parameters

Method:
In this experiment we ran the induction motor with no loads attached, either to its rotor shaft, or within the circuit.

The circuit used for testing is shown in figure.1.
The 3-phase, 400V, 50Hz supply was connected through a 0-400V variable voltage output autotransformer allowing us to vary the voltage supply to the induction motor.

The measured parameters of line current, voltage, watts and motor rpm are shown in figure.2.
From our results obtained in table figure.2, we can work out the calculations for the Volt Amp Reaction, Input (Apparent) Power, and the Magnetising components losses.

Volt Amp Reaction
The total 'Volt Amp' (VAr) reaction (Reactive power) can be obtained from: FORMULA 220V, 1.85A, 110W (Active Power Total), 36.67W (Power in each phase) FORMULA

Magnetising Components
The magnetising components X m and R c represent the reactance of the magnetic field (X m), and the resistance due to losses in the magnetic core (R c).

FORMULA (Reactance generating the magnetic field) FORMULA 260V, 3A, 250W (Power Total), 83.33W (Phase Power) FORMULA
Magnetising Components

FORMULA FORMULA Figure.3 Shows that as we increase the line voltage up to the motors rated 200-220V the line current drawn increases from 0.75 to 1.4Amps.
At 260V the current drawn by the motor increases to 3.3Amps.

Applying a voltage and current through the motors 4-pole stator coils generates a magnetic field within the ferromagnetic core material that the coils are wrapped around.
The potential effect of this magnetic field causes a rotating magnetic flux that flows through each of the stator cores (set at 120 deg in 3 phase motor) and it's this rotating flux that induces the magnetised 4-pole rotor inside the 3 coils to react and hence rotate.

The ferromagnetic cores can only generate so much magnetic field strength and hence flux.
We reach a point whereby the ferromagnetic core will not generate any more magnetic fields no matter how much we increase the current.

This is known as the 'saturation point' or 'sweet spot' of the magnetising curve and can be seen on figure.3 at approx 200V at 1.5Amps.
Incidentally this is the roughly the rated Voltage for our motor.

The motor is efficiently converting current into useful magnetic flux, we have to apply a lot more current to increase the magnetic flux by only a very small amount and therefore beyond the 'saturation point' the motor draws too much current and becomes inefficient.
The reason the motor speed (rpm) stays constant no matter what the voltage/current is, is due to the supply frequency of 50Hz.

When the motor is connected to a 50Hz grid supply, the synchronised speed of a 4-poled machine will always be 1500rpm.
3. Locked Rotor Test

The circuit for this test was the same as that seen in figure.1.
The line voltage was set back down to zero to ensure motor had stopped.

The rotor shaft fan was locked with a screwdriver, and then the line voltage was turned up slowly until the current matched the motors rated full-load current of 3A.
Figure.4 Shows the measurements taken: So.....

FORMULA FORMULA Most of the 3.3Amp current was flowing through the rotor circuit.
This was because the rotor was locked and not free to react to the rotating magnetic flux in the stator, so the flux induced current flow in the rotor circuit.

The Voltage supplied to the motor was 55V.
This measured Voltage is much reduced from the normal (rated) operating voltage of 220V at 3Amps.

The line voltage (Potential difference) dropped significantly during the test because a large majority of the current energy flowing through the stator coils was not being used to rotate the rotor via the rotor inductors cutting the stator coils flux.
So the potential difference (V Supply) was less (55V) than when the rotor rotates at the full speed of near 1500rpm at 220V.

At this speed the rotor would be extracting a lot of induced energy out of the flux, hence more current energy is consumed in the stator coils and the potential difference is therefore greater (220V).
Calculations

Volt Amp reaction for Locked Rotor test:
FORMULA

Power loss due to Stator (R1) and Rotor (R2) windings resistance is given by:
FORMULA The Stator (R 1) and Rotor (R 2) winding resistances have to be split.

This is done by measuring the 'phase-to-phase resistance', the D.C resistance between 2 phases.
The motor was unplugged from the autotransformer to measure the phase-to-phase resistance.

FORMULA
4. Load Test (Synchronised grid connected motor test)

The circuit was connected to the 3-phase 50Hz grid supply as shown in figure.5.
The supply voltage to the motor was set to 220V allowing the motor to rotate at no load.

Two water pumps were turned on and valves opened to allow an increasing water flow to hit the 'Turgo' turbine attached to the spinning rotor.
As the water pressure was increased, the water pressure, line voltage, current, electric power and motor speed were measured.

The results can be seen in figure.6.
Figure.6 Shows that at 'no load' the motor is running at a rotation speed of 1440rpm, which is just below its grid connected synchronous speed of 1500rpm.

This could be due to either some electrical circuit losses, the turbine causing drag slowing the rotor speed, or the motor being rated to run at 1500rpm at 240V (we are running at 220V).
As the water pressure increased on the turbine, this induced more torque power to the rotor giving more electric power in watts per phase.

At a water pressure of 10psi the total power in watts was decreasing as the motor started to generate power due to the increasing torque on the rotor from the water pressure reacting on the turbine.
Above water pressures of 10 to 15psi the motor stopped acting as a motor and generated an output measured on the Wattmeter's in negative total Watts.

This was because the power (electrical energy) had changed flow direction within the circuit.
Figure.7 Shows that when the power was flowing from the supply to the motor, this gave positive Watts Power.

When the motor started to generate power to the grid and act as a generator, the output power became negative.
Figure.8 Shows that whilst the motor was drawing a current from the grid supply, it drew a steady 1.8Amps.

When the water pressure was above 10 to 15psi causing the rotor turbine to rotate at higher speeds above the grid synchronised (and rated) speed of 1500rpm, the current output increased to 3.1 at 1560rpm.
This was because the motor was generating power above 1500rpm.

This is motor was now generating in a 'Supersyncronous' or 'Oversyncronous' mode.
By using our results data in figure.6 we can see that the generator was generating the most current (3.1Amps) at 25psi of water pressure.

There were many losses between the water pressure meter and the water hitting the turbine such as pipe length causing flow restriction, valves, pipe couplings etc.
We assumed a total loss of energy between the meter and the turbine of approx 30%

Calculations
FORMULA Turbine Input power can be obtained by reading off the power/pressure graph as provided in the labsheet.

17.5 psi gives 1000 Watts of Turbine Pressure Input.
In order to calculate the generator efficiency we would use the equation: FORMULA We know the Electrical Power Output but we do not know the Mechanical Power Input, so we can use the computer simulated performance graphs provided for this motor in this experiment.

The computer simulated Machine (Generator) Efficiency/speed graph seen in the Appendix shows that at 1560rpm the Generator efficiency was at its optimum point which was roughly 78% efficient.
FORMULA Overall System efficiency = 47.5% Also..

FORMULA By re-arranging this we can work out the Turbine Efficiency: FORMULA
5. Three-Phase Stand Alone Operation

As induction motors do not have any internal source of magnetism except for a little residual magnetism, capacitors have to be used to 'excite' the machine in order to generate power in a stand-alone system.
Capacitors can absorb the residual magnetic leakage flux from the stator coils when the motor was last running.

Capacitors can store this energy as reactive power to then create a voltage across the 'exciter' circuit within the stator coils when the motor is required as a generator.
When the rotor is turned by means of a turbine via input energy from hydro power for example, the voltage released by the capacitors flows through the exciter circuit and this helps to kick start the current flow in the coils to generate an output.

Figure.9 Shows how we require 3 capacitors to be connected in delta across the 3-phase delta connected stator coils (1 capacitor for each coil).
To calculate the capacitor size required for this motor we use the following equation: FORMULA (Using Q total value calculated from the no load running light test) FORMULA So using 18 microfarads capacitors are more than adequate as they will not become fully charged or electrically overloaded in the case of using capacitors of less than 15 microfarads.

To calculate the frequency (and hence synchronous speed) that this motor will be running at using 18 microfarad capacitors with a line Voltage of 260V we use the following equations: FORMULA FORMULA Theoretical frequency at which our generator should operate at = 63.6 Hz Method: Figure.10 Shows the circuit connected for this test.
Here we can see the three capacitors wired in delta across the stator coils in the motor.

Instead of being connected to the grid supply, the motor is connected to a 3-phase load, this was just load resistor bank connected in 'Delta' configuration inside a 'black box'.
Each time we increased the load we increased the water pressure accordingly until the output line voltage read 220V, then took readings of the current, voltage, power, water pressure and generator (motor) speed (Figure.11).

The voltage was raised to 220V to simulate the voltage of the grid, as any consumers effectively connected to this system would require a 220V supply.
Our results as seen in Figure.11 show that this system would not work well for a micro hydro power set up because the system is too dependant on load demand.

As the load required increases so the hydropower water pressure to the turbine needs to increase in order to speed up the generator to generate more power output.
The computer simulated Machine Efficiency/speed graph (see Appendix) shows that a generator needs to run at an optimum fixed speed to obtain the best electrical output efficiency.

So increasing and decreasing the generator speed according to load demand is a very inefficient way of generating a power supply (Figure.14).
If the load demand were to suddenly fall (by consumers switching electrical items off), this system would become very unstable as the turbine would still be spinning the rotor at 1716rpm and the electrical power produced would have no where to go....

As this stand alone system is not connected to the grid OR have any means with which to 'offload' excess power, this could seriously damage the equipment!
6. Single Phase Ballast Operation test.

Single-phase operation is possible by attaching a single-phase load across two phases of the Induction motor and the capacitors.
Method:

Figure.15 shows the circuit connections for the test.
One phase of the three-phase resistor bank was used as the Ballast load resistor.

The single-phase load consisted of an array of parallel-connected lamps that could be connected in and out of the circuit via individual switches.
Initially these would be switched off.

In order to excite the machine the ballast resister was kept at zero.
The water was turned on to bring the machine up to speed (1580rpm).

The water flow was turned up further before switching in a ballast resistance of 200W.
The water pressure was increased so the voltage was a constant 220V.

The usual measurements of Voltage, Current, Wattmeters, and rpm, including the percentage of load going into the ballast resister as individual lights were turned on were recorded as shown in table (Figure.16).
The Voltage, Current, Wattmeters and Motor speed remained constant regardless of how many lights were turned on.

Figure.16 Shows that as lights were turned on (load increased), the percentage of power being absorbed by the load resistor was less.
Energy was being used to light bulbs instead of it being absorbed by the load resistor.

This system set up would work well for a stand-alone micro-generation scheme such as micro-hydro as long as the water pressure could be controlled using a governing system in order to maintain the required voltage.
The circuit ensured a set of stable operating conditions for both the generator and the load.

The generator was running at its rated voltage and near-enough current, so therefore could run efficiently.
The load had the required voltage and current to suit its needs, until of course the load exceeded the total power output from the generator.

The major drawback with this system set up though is the huge energy losses being absorbed and dissipated out through the load resistor when the load does not require the energy.
7. Conclusions Effectiveness of system as a micro-hydro scheme.

As discussed before in section.6 the single-phase ballast system set up could work well for a stand-alone micro-generation scheme such as micro-hydro but only if the water pressure could be controlled in order to maintain the required voltage.
Advantages (if the water flow and power electronics regulations were automated): System would be fairly rugged and robust as the only moving parts would be the turbine rotor shaft and a few valves therefore low maintenance.

System would be relatively cheap to install and once installed have few operating costs.
The system is running in stable operating conditions no matter what the load requirement is.

The generator is continuously running at 1580rpm, which according to the Machine efficiency/speed curve is the most efficient speed for this motor to run at.
The system is relatively efficient if either all the electrical power being generated is being used by consumers, OR if not, the heat produced due to excess load being dissipated into the ballast resister bank is utilised for example to heat consumers homes.

If this waste heat is not utilised then the ballast resistor bank contributes the greatest inefficiency to the whole system.
Disadvantages: Loads would be strictly limited to the power output of the generator.

If the system were to maximise energy generated by the 3-phases, careful monitoring of load balances between the 3 phases would be required.
Energy not used by the load would be lost into the resistor banks, though this energy could be put to good use - for example running either power station system or domestic heating water through heat exchangers in the resistor banks to heat it up for required usage.

Energy could also be diverted into either batteries, or to generate hydrogen to store energy via the electrolysis of water.
If the single generator failed the system would go down.

The system would either need to have two generators running simultaneously so that one could continue (at a reduced total output) whilst the failed generator was being fixed, or be able to switch the load supply over to the grid network in the event of a failure.
In the case of a non self-regulating system, someone would have to be paid to oversee supply and demand by operating machine and electronic controls.

The stability of the system voltage and output would be threatened if there were a problem with the water flow.
Optimum Canopy Orientation for maximum PV power output

The optimum position for a 'Monocrystalline' type PV installation at your Reading Primary School installed on a tilted canopy roof with an area of 8m x 5m (40m -2) would be when PV modules are orientated at 30 degrees from the Horizontal and facing directly South (Azimuth bearing of 0 degrees South).
(See Fig.1 below)

Power Output Sensitivity
Azimuth Sensitivity

Calculated with PV slope angled at Optimum 30 degrees from horizontal (+ OR - 4 degrees).
FORMULA There are similar percentage losses at equal degrees east of south.

Slope Angle Sensitivity
(Degrees from horizontal showing % power loss) FORMULA

Electrical Output
The maximum power output per year when using 40m -2 monocrystalline PV installation at the optimum slope and azimuth orientation would be approximately 4116 kWh/yr (4.1MWh/yr).

This equates to 102.9kWh per year per m-2 of canopy roof.
The Energy Cost would be £0.57p/kWh.

The nominal power for this installation would be 4.8 kWp (Peak)
PV system orientation on the schools proposed canopy roof.

The maximum electrical power output per year when using 40m -2 monocrystalline PV installation on the proposed canopy roof (fig.2) where the slope is 20 degrees with an azimuth orientation of 30 degrees West of South would be as follows: Maximum Power Output Approx 3996 kWh/yr (4.0 MWh/yr) This equates to 99 kWh per year per m-2 of canopy roof.
The Energy Cost would be £0.59p/kWh.

The nominal power for this installation would be 4.8 kWp (Peak)
Proposed canopy compared with the Optimised Orientation

Installing a monocrystalline PV system on your proposed canopy roof would generate approximately 3% less power in kWh per year when compared to the same PV installation at the optimum orientation as discussed in section 1.
Comparisons with your linked school in Benin, Nigeria, Africa.

An identical monocrystalline PV system the same size 8m x 5m (40m -2) at the same orientation as your proposed canopy roof though built at a school in Benin, Nigeria, Africa would give the following electrical outputs: The maximum electrical power output per year would be approximately 6141 kWh/yr (6.1 MWh/yr), This equates to 154 kWh per year per m-2 of canopy roof.
The Energy Cost would be £0.38p/kWh.

The nominal power for this installation would be 4.8 kWp (Peak) The greater total electrical power output is mostly due to the difference in annual solar irradiation (energy from the sun) between Reading and Benin City.
Figure.3 Shows a graphical representation of the monthly solar irradiation Benin City receives over the course of the year.

We can see that on average Benin City receives 4.5 kWh of energy from the sun per m -2 per day for at least 9 months of the year.
Benin City is located at 6 degrees North of the equator and so receives up to 5 kWh per m -2 per day peak solar irradiation though this is only during November.

Figure.4 Shows the Annual Solar paths for Benin City.
The suns daily path rises quickly up to between 60 and 90 degrees from the horizon (fig.4).

As the suns position is mostly directly overhead during the core daytime hours, this means there is a lot more solar energy available to utilise.
Figure.5 Shows that Reading receives on average only 2.5 kWh of solar energy from the sun per m -2 per day during the course of the year.

This is due to the higher Northerly latitude of Reading (51 deg N) compared to the more equatorial location of Benin City (6 deg N).
Reading receives most of its solar radiation during the summer months April to August where solar irradiation can reach as high as 4.5 kWh per m -2 per day when the sun is at between 45 and 60 degrees above the horizon (fig.6).

During the winter months September to May the solar irradiation energy falls down to just 0.5 - 3 kWh per m -2 per day.
This is due to the low angle of the sun in the winter sky being at just 15 degrees above the horizon (fig.6).

Choice of PV type
Here are two different types of PV technology.

Although both types use Silicon wafers, they have physical, efficiency and most importantly price differences.
Monocrystalline (Expensive yet most efficient)

The maximum electrical power output per year when using 40m -2 monocrystalline PV installation on the proposed canopy roof (fig.2) where the slope is 20 degrees with an azimuth orientation of 30 degrees West of South would be approximately 3996 kWh/yr (4.0 MWh/yr).
This equates to 99 kWh per year per m-2 of canopy roof.

The Energy Cost would be £0.59p/kWh.
The nominal power for this installation would be 4.8 kWp (Peak)

Amorphous thin film (Cheapest yet less efficient)
The maximum electrical power output per year when using 40m -2 amorphous PV installation on the proposed canopy roof where the slope is 20 degrees with an azimuth orientation of 30 degrees West of South would be approximately 1998 kWh/yr (2.0 MWh/yr).

This equates to 50 kWh per year per m-2 of canopy roof.
The Energy Cost would be £0.68p/kWh.

The nominal power for this installation would be 2.4 kWp (Peak)
Capital Costs and Income Assessment

Sources of Funding
There are a number of funding sources available to schools interested in installing renewable energy technologies.

The Department of Trade and Industry could provide government funding through their 'Low Carbon Buildings Programme'.
Grants will be up to a maximum of £30,000 or 50% of the capital and installation cost of the micro generation technologies installed.

These grants will be awarded on a rolling first-come-first-served basis.
Details of how to apply can be found through their website: URL (5) More details of other sources of funding can be found on.pdf pages available through the Dti website at URL (3)

BIG Lottery Fund
There may be grants available from the 'BIGlottery' fund from the national lottery.

Although exact fund details are not published, your school could apply telephoning telephone 0845 410 20 30.
Details at URL or (4)

Utility Companies Funding
Schools can also go to utilities companies for extra funding.

EDF Energy, Npower, Powergen and Scottish power are all major community investment funders.
Npower, for example, provided £30,000 to Nidderdale high school and community college in Harrogate, for the installation of a wind turbine; Scottish Power gave St James' Catholic high school in Barnet £5,666 for its solar panel installation.

(3)
Project Design

Modules
suitable for your installation requirements would be of Monocrystalline Silicon type PV modules manufactured by Schueco (model S330-PM-2) each rated at 330Wp 62V.

There would be 14 modules required in an array to nearly match the potential nominal power of 4.8kWp as calculated for the size of the proposed canopy roof.
The nominal power for the array would be 4.6kWp, this is just under the power required for this size roof.

Inverters
suitable for your installation would be 1x 4.4kW, 350-600V (Max input Voltage = 800V), 50Hz model: Powador 4000xi manufactured by Kako.

The total nominal power rating for the inverter is 4.4kWp.
This is approximately 10% less than the array nominal power of 4.6kWp.

This is deliberate because the inverter will work more efficiently if it is working flat out at maximum power output most of the time.
On the very few occasions during the summer months when the total array output exceeds 4.4kWp the inverter will either shutdown temporarily or safely ensure excess power from the array is dealt with by power electronics in order to continue operation.

The 14 modules should require an installation area of 37.7m2 and would be wired to the inverters as follows: FORMULA My software assures me that the inverter can take this array output.
Layout Benefits

There are 2 parallel strings of 7 modules in series.
If one half of the roof suffers from either shading influences, or module problems then only half of the circuit suffers.

The other half of the roof will still produce electricity at full power.
The voltages in the circuit are high at 434V within the series strings and 868V in the parallel wires of the circuit.

High voltages reduce the current load (5.3Amps) hence allowing the use of thinner core wires to transmit the same overall power to the inverter.
Lower voltages transmitting currents at high Amps would require larger diameter wire cores to reduce the risk of high currents heating the wires.

High currents heat wires by converting electrical energy into heat, a process that wastes energy.
The estimated annual energy output available at the inverter after all losses within the system have been deducted is 3627kWh (3.6MW)

6. Power losses due to nearby tree.
A tree stands 4m high and 2m in front (south of) the proposed canopy roof.

At different times of the day throughout the year the sun will cast a shadow of the tree onto the PV system installed on the proposed canopy roof.
On most days this particular tree location forms shadows across the roof starting around midday and then on throughout the afternoon.

Winter time.
The sun arcs low in the sky at just 15 degrees above the horizon at midday.

This low sun will cast long shadows of the tree trunk upon the roof throughout the afternoon.
This time of year the PV will the receiving mostly diffuse light due to likely cloud cover and less direct sunlight.

Shadows will make little difference to low energy output levels.
Autumn/Spring time.

The sun arcs across the sky at 40 degrees above the horizon at midday.
This will cause a greater % of decreased power output due to the bulk of the tree causing a large shadow which could cover up to one third of the roof.

Summer time around midday.
The sun arcs high in the sky at 60 degrees above the horizon.

From this position the sun forms less of a shadow, which covers a smaller % of the roof.
The suns irradiance is at its greatest this time of day and year so the shadow should not make a great deal of difference in power output.

Summer time mid afternoon.
During the afternoon the shadow increases and is at its greatest around 3pm after which the shadow lessens in size.

The output at this time will be affected more than the shadows position at midday.
The total annual energy loss due to shading from this tree will be 273kWh.

The total annual energy output with shading from this tree will be 3353kWh
Conclusion - Two options:

1. Ideally the proposed canopy roof would be built away from this tree to avoid the effects on the annual electrical output of the PV system through shading losses.2.
Remove the tree.

7.Power output for un-shaded Canopy roof using RETSCREEN (10)
The system output predicted using RETscreen software suggests that at a London location using mono-silicon PV at a nominal rated output of 4.6kWh we could generate 3904kWh.

This is 280kWh more than PVSYST, though to be fair we cannot directly compare, as we cannot choose the same modules or inverter in RETscreen as in PVSYST.
8. Comparing PVSYST with RETscreen software.

(2)(10)
PVSYST is a comprehensive PV project design package that has lots of potential.

It has great graphical representations of sun path diagrams, data histograms, energy losses.
It has the ability to design and view your systems orientation with nearby shading objects such as trees and buildings in 3-D.

The data from these shading factors can then be seen to affect your PV systems power output.
The software is by no means perfect and could be further improved by addition of more features and better navigation throughout pages.

One annoyance is the fact you can only specify your array size in the 'Preliminary' and not the 'Project' sections of the software.
This would be immensely helpful when choosing your module types, numbers of and hence size of area taken up by modules.

RETscreen is based on Microsoft excel spreadsheets and is therefore a case of plug in your values and see how this affects the final values.
The spreadsheet trends to over calculate your power output compared to PVSYST even though it assumes some losses.

You are unable to enter the PV area required in m 2, only the nominal PV array power that then automatically predicts the array area in m 2.
There is less choice in PV module manufacturers to choose from.

You cannot choose a specific inverter, only the number of inverters but it does allow you to enter the inverter efficiency if you are lucky enough to know it.
RETscreen feels and looks more like a 'system costs model' for business, rather than a piece of software designed specifically for PV design such as PVSYST.

RETscreen lacks any graphical representations of data or design, and lacks the broader overall features of PVSYST.
The only real advantage of RETscreen over PVSYST is that the software is free!

Introduction
The main objective of this experiment will be to show the performance characteristics of a 'Rutland Wind Charger' turbine (fig.1).

The turbine will be assessed under set laboratory conditions in order to produce qualitative comparisons that will determine the forms of both its Coefficient of Performance vs. Tip Speed Ratio (C p-λ), and Torque Coefficient vs. Tip Speed Ratio (C Q-λ) curves plotted on a graph.
We will also examine the generator characteristics represented as a graph plot of the relationship between I (current delivered to the load), and ω (the rate of rotation in rpm) for constant voltages E.

2. Methodology
2.1 Theory

The efficiency of a wind turbine in extracting energy from the wind can be represented by a coefficient of performance, C p.
The coefficient of performance is given by the formula: FORMULA where FORMULA We cannot measure the power extracted from the wind directly but the electrical power output should be proportional to the power extracted from the wind.

P w is proportional to EI (Where E is The Voltage and I is the Current supplied to the load) therefore FORMULA the wind velocity V is proportional to the square root of pressure difference value ΔP.
So V is proportional to √∆p.

therefore FORMULA FORMULA Therefore if we substitute V with √∆p we get FORMULA
2.2 Method

Turbine Position
The turbine was fixed upon a support frame, within a safety cage so that it interacted nicely with the exhaust flow of the laboratory 'Plint' variable speed wind tunnel.

The turbine rotor diameter was 503mm, this was significantly larger than the tunnel working section (305x305mm), so would therefore not fit inside.
The turbine position outside the tunnel had the advantage of making it easier to take measurements within this controlled environment (fig.2).

Wind speed / Velocity (measured in m/s).
By adjusting the main wind tunnel speed control dial, we were able to control the velocity of exhaust air exiting the tunnel.

This allowed us to take a set of measurements at 5 different wind velocities.
The wind velocity was measured in metres per second (m/s) using a digital electrical probe.

This was placed in the exhaust flow at several positions across the 'face' of the turbine blades.
Measurements were taken in the centre of the exhaust flow at the same distance from the blades for all wind speed tests.

Rate of Rotation (measured in rpm).
A digital optical tachometer in conjunction with a reflective plate placed onto the rear surface of a blade was used to measure the rate of turbine blade rotation in revolutions per minute (rpm).

Measurements were taken at a stationary position at a set distance from the reflector plate until a steady rpm was measured.
Pressure Readings (ΔP).

A pitot-static tube was set within the working section of the tunnel facing into the flow and connected to an electronic digital manometer.
This device automatically read the difference between the static P and stagnation (total) pressure P0 to give us the pressure difference value ΔP.

Voltage and Current Readings
To measure the Voltage (E) and Current (I) equipment included both an analogue and digital Amp meter wired in series with the turbine, with a series of variable resister banks rated at 35.6 ohms in the circuit allowing us to vary the electrical 'load'.

An analogue and digital analogue and digital Voltmeter were wired in parallel across the leads from the turbine to measure the potential difference in Volts across the circuit (fig.3).
Analogue and digital meters were used as digital meters are quick to respond to sudden changes in electrical flow and can therefore readings fluctuate rapidly.

Analogue meters are slower to respond and 'iron' out rapid fluctuations, they can be easier to read once they settle.
Order of which measurements were taken:

Set wind speed.
Set the wind tunnel control dial to a fixed setting initially just enough to start rotation of turbine blades.

Increase setting for other desired wind speeds.Take pressure reading ΔP.
Measure wind velocity.

Set the variable resistors to give desired Voltage (e.g. 2.5V) then take Current reading.Measure rpm of blades.
Repeat steps 4-5 for each desired Voltage increment for any given wind speed.

Repeat steps 1-5 at each change of wind speed.
Discussion of Findings

Electrical Generator Performance.
(Figure 5) shows that each series of Voltages from our recorded data roughly follow the form of a general Cp / λ curve.

The optimal position of the curve for this turbine cannot be accurately plotted.
Some voltages at different wind speeds lie outside of an ideal Cp / λ curve, this could indicate that these combinations of Voltage and wind speed are not as efficient in producing electrical power as those values that lie closer to the Cp / λ curve.

The peak Coefficient of Performance value of 0.0028 (fig.5) was obtained when the turbine was rotating at 870 RPM in 11.63m/s of wind generating 0.92 amps at 12.5V producing 11.5 Watts.
(Figure 7) shows that voltages as low as 2.5V could be generated over a range of speeds of rotation from as low as 250rpm upwards, though at 250rpm the generator could only generate 0.4 Amps.

To generate more current at these low voltages we needed to increase the speed of rotation, for example 2.5 Amps at 750rpm.
This was achieved by increasing the wind speed.

Higher voltages were generated at higher speeds of rotation too.
For example 20V was only generated above 1000rpm and at this rotation speed the current was only 0.2 Amps.

To generate more current at 20V we needed to increase the speed of rotation above 1000rpm, again by increasing the wind speed.
As we increase the wind speed velocity the current supplied to the load increased, the rate of rotation increased but the voltage decreased.

The relationship between Current (I) and the speed of rotation (rpm) when drawing different loads at the same voltages clearly appears to simultaneously increase in linear fashion.
Torque Performance

(Figure 6) shows clearly that at each fixed wind speed (DP) the speed of turbine rotation increases as the Voltage supplied to the load increases, it is the current supplied to the load that varies.
For example when the difference in pressure (∆p) value was either 133,188 or 253, and the voltage supplied to the load was between 2.5 & 5V the current supplied to the load increased slightly before decreasing at Voltages above 5V.

When the (∆p) value was either 90 or 363, and the voltage supplied to the load was between 2.5 & 5V the current supplied to the load remained the same, before decreasing at Voltages above 5V.
At (∆p) values above 441, the current supplied to the load decreased at each Voltage increase.

Although there are these minor variations, the general trend is that at each wind speed (∆p), as the Voltage supplied to the load increases, so the rate of rotation increases, but the current supplied to the load decreases.
Figure 4 shows that at a wind velocity of 10m/s (or equivalent ∆p of 13.71) at 15V we generated 0.43Amps so the resistance was 34 ohms.

Increasing the voltage to 17.5V and we generated 0.3Amps but the resistance increased to 58 ohms.
This is because we are demanding more energy from the generator by increasing the resistance at the load.

Increasing resistance means less current can be delivered to the load.
The only way to generate an increased current at higher voltages is to increase the wind speed hence increasing the available Kinetic energy.

This proves that there is only so much energy we can extract from the wind at any particular velocity before both the torque and performance coefficient decreases.
Maximum Power Output Comparison

The maximum power output in Amps was plotted against the wind speed (fig.8) where wind speed was converted into knots for comparison against the manufacturer's performance specification graph (fig.9).
The trend line generated from our data plots suggests that our best power output in Amps for every wind speed was in fact lower than that suggested by the manufacturer.

For example we were generating 1 Amp at 20 knots, where the manufacturer claims the turbine would do this at 15 knots.
Likewise at almost 30 knots we generated 2.5 Amps whereas the manufacturer suggests we should generate 3.7 Amps.

A more realistic Cp / λ curve?
In our mathematical equations we worked out the coefficient of performance (Cp) and the Tip speed Ratio (λ), using the square root difference in pressure (√∆p) as the wind velocity as the velocity 'V' is proportional to √∆p.

We assumed using the Continuity Equation (A 1V 1 = A 2V 2) that the wind velocity at the turbine (V 2) was equal to the Velocity inside the working section of the tunnel (V 1) where our pitot-static measurements (∆p) were taken.
The area inside the working section (A 1) was 305 x 305mm = 93,025 mm 2.

The cross sectional area at the turbine (V 2) could not be measured as the turbine was in a position outside of the tunnel in free air.
Therefore because A 2 could not be measured the continuity equation would not work.

If equations were recalculated substituting √∆p with the true measured wind velocity at the face in front of the turbine, then our plots may have compared better to the manufacturers specification as shown in (fig.9).
Interestingly if we substitute √∆p with the true velocity when plotting our Cp / λ Curves (fig.5), the Coefficient of Performance gets much better with a peak Cp value going from 0.0028 at 870 RPM, 0.92 amps at 12.5V generating 11.5 Watts (fig.5), to 0.0074 (fig.10).

This proves that by increasing the wind speed we get a higher Cp value, a higher proportion of energy extracted at the blades.
The individual Voltage Cp / λ curve plots (fig.10) also display a more chaotic nature than those smoother forms seen on our original (fig.5).

This in essence could be caused by the true disturbed variable wind velocities the turbine was experiencing (fig.11), due to turbulent flow mixing in the tunnel exhaust.
Discrepancies.

Discrepancies that could affect results include:
Airflow Turbulence

The exhaust airflow from the tunnel exited through a circular nozzle opening 490mm in diameter.
This was narrower than the Rotor diameter of 503mm.

The jet of air leaving the nozzle would entrain with the air outside causing it to slow and mix via turbulent wind eddies.
Although the jet is likely to have expanded to a larger diameter over the distance between the nozzle and the turbine to effectively influence the whole swept area, turbulent eddies within the flow could explain varying wind velocities measured across the face of the blades (fig.11).

Yawing Issues
The turbine was allowed to 'self yaw' itself into the airflow via means of its vertical tail fin.

The motion of the fin was allowed to move to an extent via a hole cut into the rear section of the cage.
On observation the tail wedged itself against one side of this hole and therefore kept the blades at a position slightly angled away from the airflow.

Generator efficiency
Due to lack of details from the turbine manufacturers we could not take into account the efficiency of the generator within our calculations.

Circuitry
The numerous wiring connections throughout the electrical circuits could have influenced the true power output from the generator by adding more resistance.

Meter discrepancies.
Large fluctuations were seen in the digital circuit meters, a more steady reading was observed on the analogue meters, although the readings on these appeared slightly lower than the digital.

Most readings were taken from an average fluctuation from the digital meters.
PART A:

i - See diagram 1
ii) 00:00 UTC chart

The dry bulb and wet bulb temperatures would have been used to calculate the frontal position using air mass properties.
Behind the cold front there are station readings of both low dry and wet bulb temperatures, for example T dry = 5, T wet=4 (expected for a polar maritime air mass).

In the warm sector, they are both much warmer - T dry = 13, T wet=12 as moist air has been advected from the south.
06:00 UTC chart: Western cold front: A clockwise veer in wind direction and an increase in wind speed are expected across a cold front.

This was observed between stations 2C (33-37 kts from the west) and 2D (23-27 kts from the south west).
It was otherwise hard to observe as a low pressure centre was placed near the front with tight isobars - strong winds.

Falling pressure normally precedes a cold front shifting to rising pressure as it passes.
This can be observed in the kink in isobars along the front and the shift in pressure tendency from falling to rising (falling at 2D, 2E, 2F, 2G and rising at 2A, 2B and 2C).

The front follows the trailing edge of cloud and the heaviest rain: This can be seen in the satellite and radar and also in the weather reports from stations 2C, 2E, 2H and 2I.
The stations along the front also have T dry = T wet which corresponds to this rain.

Central occluded front and triple point: An occluded front generally 'wraps around' closed isobars at the low pressure centre and has been drawn as such.
Conditions around an occluded front are similar to cold fronts.

The occluded front follows the line of heaviest rainfall (radar and reports from stations 2J and 2K).
There is also a small change in pressure tendency across the front (falling at station 2L and rising at station 2J).

However it is hard to distinguish this due to the intensity of the nearby low pressure centre.
The front also marks the boundary between cool polar air behind the system and warmer mixed air in the occlusion.

Central warm front: The warm sector is warmer and moister than ahead of the warm front.
For stations behind the front, 2M, 2N and 2O, the wet bulb temperature is 9-10 oC while at station 2P ahead of the front, the wet bulb temperature is only 6 oC.

The air is also drier at this station.
There also seems to be an increase in wind speed across the warm front (stations 2M, 2N, 2O and 2P report winds of above 30 kts while the stations to the south of the front report maximum speeds of 20kts.

Eastern frontal system: As the system is older, it is much more occluded and so the occluded front is much longer than in the main system.
To the north of the occlusion, the air is drier compared to the mixed air inside the 'hook'.

(c.f. station 2Q has T dry = 6, T wet=3, while 2R has T dry = 9, T wet=0).
The front also follows the line of heaviest rainfall (satellite and weather reports from stations along the front, i.e. station 2S).

There is also less cloud inside the 'hook' of the occlusion (satellite and station 2Q).
The warm front is placed due to greater amounts of cloud ahead of the front and an increase in temperature from T dry ~ 7, T wet=8 ahead of the front to T dry = 11, T wet=8 behind it.

12:00 UTC chart: The system has now become much more occluded as it develops.
The low pressure centre has also moved to the NW.

Cold front: Placed through small kink in isobars (more pronounced near the low pressure centre).
The pressure tendency is also falling ahead of the front (stations 3E, 3F, 3G and 3K) compared to riding tendencies behind it (stations 3D,3H, 3I and 3J).

There is also a clockwise veer in wind direction across the front (stations behind the front, 3A, 3B, 3C and 3D report W, WSW winds while stations 3E, EF and 3G ahead of the front report S, SSW directions).
Note the increase in wind speed can't be seen as the wind is amplified as it flows along the nearby English Channel.

The front also follows the line of heaviest cloud and rainfall, (radar, satellite and stations 3B, 3C and 3E weather reports).
Occluded front: This was placed due the satellite showing the distinctive 'hook' shape in this area and the wind speed / isobars.

It also follows the line of heaviest rain.
Warm front: This was placed along the line of heaviest rain (stations 3L and 3M).

There is also an increase in wet bulb temperature behind the front (stations 3L and 3M show T wet~ 9-10 behind the front while stations 3O and 3P show T wet ~ 6-8 ahead of it).
There is also a slight kink in isobars and clockwise wind shift (Easterly wind at 3O while Southerly at 3L and 3M).

PART B:
i)

Please see calculation 1 for full workings.
The geostrophic wind was found to be 37 2 ms -1 blowing from the South west and parallel to the isobars at that point.

The surface wind was measured to be 18-22 kts ( 9 - 11 ms -1) blowing from SSE.
This difference is due to friction in the boundary layer: From notes, over land, the surface wind is approximately 30% of the geostrophic wind flowing about 30 o to the isobars.

This corresponds with the results found.
ii)

Please see calculation 2 for full workings.
The geostrophic wind was found to be 67 5 ms -1 (error analysis in calculation 2) blowing from the west, parallel to the isobars at that point.

This is compared to a surface wind measured as 95 kts (~50ms -1) from the WSW.
The difference is again due to friction in the boundary layer.

However, as the measurement is over the ocean the friction is much less (no terrain to create friction).
Therefore the surface wind would flow at approximately 10 o to isobars and at about 70% of the strength of the geostrophic wind.

This again corresponds well to the results found.
iii)

FORMULA FORMULA FORMULA This shows the vorticity increasing between 00:00 and 06:00 while decreasing slightly between 06:00 and 12:00.
This corresponds to the development of the cyclone.

At 00:00, the system had just started to build and therefore wouldn't be expected to have much relative vorticity.
However, by 06:00, the system had developed around an intense low pressure coupled with high geostrophic winds blowing around tightly packed isobars.

This would be expected to have much higher vorticity reaching a maximum around the point that the occluded front started to form (as that's the point with the most energy in the system).
The fast development of the cyclone and the speed the fronts moved over the UK also suggests high vorticity.

The vorticity was still high at 12:00 although it had dropped slightly, suggesting that the frontal system was starting to lose energy and so lose vorticity.
The vorticity is still an order of magnitude higher than normal however, so fast winds are still expected.

(these seem much faster around the low in the 12:00 charts due to the centre of the low pressure now being over the ocean, so recorded wind speeds are much closer to the geostrophic wind value).
The higher vorticity, the more vortex stretching of the cyclone would be expected to occur so air is pushed aloft and more rainfall would be expected.

Heavy rainfall was recorded at the developing low at all 3 timescales however.
The higher the vorticity, the faster the fronts would also be expected to move over the UK as the geostrophic winds would be faster.

The relative vorticity is normally ~0.1 planetary vorticity (10 -4) so these values are of the right order of magnitude (it would be expected to get higher values as the storm progresses, especially due to the intensity of the system).
There are many errors with this method, especially as it assumes a smooth pressure gradient and a perfectly symmetrical grid.

PART A:
1)

See diagrams P5.1 and P5.2
2)

Surface analysis:
A developed low pressure centre can be seen at 60 oN, 10 oW.

The start of a developing low pressure system is also found at 53 oN, 7 oW, which can be seen by the 'kink' in the trailing cold front at this point.
500 hPa analysis:

A trough can be seen centred at 60 oN, 15 oW.
The geopotential height contours are closer together to the south of the trough, which corresponds to a jet stream:

Comparison
The upper level trough lies slightly to the east of the surface low.

This is to be expected as the low is nearly developed, so the upper levels and surface should line up vertically.
However, the upper trough lies to the north west of the developing surface low.

This, combined with the baroclinic instability at the surface at that point (i.e. the kink in the trailing cold front), suggests that the low is going to develop.
In the vicinity of the southern entrance to a jet stream, there is upper level divergence and lower level convergence.

This results in a rising air mass, which encourages cyclone development.
However, the jet stream would need to be further north for this to be an important factor at 0000UTC (the method may have become more important later in the cyclone's development however).

3)
Please see calculation 1 for full workings.

The geostrophic wind was found to be 33 4 ms -1 blowing from the south west and parallel to the isobars at that point.
The surface wind was measured to be 65 kts ( 33 ms -1) blowing from the same direction.

This result is expected as the wind is calculated at the 500hPa level, so it's well above the boundary layer.
Therefore there would be no friction which could cause ageostrophic winds.

However, an ageostrophic wind component is normally found in the vicinity of a jet exit region.
This is because air parcels decelerate in jet exit regions, so they aren't in geostrophic balance.

This leads to ageostrophic motion in jet exit regions towards the area of higher pressure.
The reason this isn't observed in the calculated/measured wind may be because the station is situated too far from the exit of the jet to be noticeable (or I've drawn my contours wrongly!)

PART B:
1)

Please see diagram P5.3
2)

Please see diagram P5.4.
How the configuration has change from 00:00UTC: The system has been accelerated along the jet stream and has moved towards the North East (so that the surface low is now positioned ~ 55 oN, 5 oE at 12:00UTC).

The jet stream is now positioned directly behind the cold front.
This is expected as air is being pulled down from the top of the troposphere at this point and has more momentum, therefore faster winds would be expected (jet streaks).

Also, we know from the synoptic chart analysis that the strongest winds blow across the cold front.
The upper level trough has also moved slightly towards the south east ~at 57 oN, 2 oW).

The cyclone has also developed, as can be seen by the occluded front curling around the surface low.
3)

I believe the system is still developing as the upper level trough is still situated to the north west of the surface low.
However, the upper levels and surface are much more lined up (i.e. the upper trough is much less further west than the surface feature).

This suggests that the system is becoming fully developed/mature.
Also, the geopotential height contours are still crossing the pressure contours (so cyclogenesis).

This is more relevant to the initial stages of a developing cyclone however.
The cyclone is also now situated near the northern exit of the jet stream; therefore it may be developing through the process discussed in question A2.

It's more likely however that this process happened slightly earlier in the cyclone development (i.e. about 06:00UTC when the explosive low developed).
Therefore, I believe the system is still developing but is about to reach maturity.

4)
Please see diagram P5.5

1. Introduction
The 'Experiencing the Weather' field course took place during the weekend of 28 th - 29 th October 2006.

It was based at Leeson House field studies centre at Langton Matravers, Dorset: The weekend's activities included:
Launching several radiosondes and plotting their ascents on tephigrams.Taking part in instrumented walks in the surrounding area throughout the period (maps are shown in Appendix 2).Monitoring both Met Office synoptic charts and satellite dataAnalysing data from a fixed observing station at Leeson House and a meteorological mast at Durlston Head.Using all of this data to produce presentations and forecasts for the period

Please note that there is also some automatically recorded data in the days preceding the period from the mast at Durlston Head.
2. Aims

This report aims to both describe and explain the observed weather during the period 27 th to 29 th October 2006.
It should link together observations from different sources to provide a coherent picture of the weather's evolution during this time.

3. Analysis of Saturday 28th October (00:00 - 18:00 UTC)
3.1.

General situation and surface observations
The weather on Friday 27 th October started out calm and dry, as the country was sitting in a ridge of high pressure (figure 1 A1).

However, by the end of the day a rain shower was recorded at Leeson House.
This was possibly caused by an occluded front which may have moved across the UK (In figure 2, A1, the front is stretched between Belgium and Norway).

Evidence for the rain is seen as a negative spike in potential gradient (figure 10 AC) and a rise in relative humidity (figure 11, AC).
Initially on Saturday, there was a warm moist air mass over the South Coast with associated mild temperatures and high relative humidities (Table 1, AB).

A tephigram launch at 0845UTC showed a layer of dry air at approximately 900hPa, which corresponds to early observations of a high cloud base which fell as the day progressed (due to water vapour falling into the dry layer until it became more saturated).
As there was no synoptic change in pressure over the UK, this provided an ideal time in which to look at the relationship between atmospheric pressure and height.

The results are shown in figure 8.
If the relationship is given by FORMULA, where FORMULA, then the scale height was found from the curve fit to be 7492m.

There may be substantial error in this however as the equipment may not have been properly calibrated.
At approximately 1400UTC, a weak warm front passed over the region, bringing in exceptionally warm moist air (figure 2).

As the air was already warm and moist, the front was too weak to cause precipitation.
However, the relative humidity increased enough for fog to be recorded on one of the instrumented walks (Table 1, AB).

The walk data also shows a drop in temperature and a peak in the wind speed at 1400UTC.
However, geographical location needs to be taken into account, the 1400UT measurement was made on an exposed ridge at the highest elevation during the walk whilst the 1300UTC and 1500UTC measurements were made in more sheltered locations.

Therefore it is more likely that local effects caused the readings rather than mesoscale events.
However, the data at Leeson House and Durlston Head also support the theory that a front passed at 1400UTC.

A comparison of Leeson house and Durlston Head dry bulb temperatures for this time is given in figure 14 (AC).
The correlation between the two measurements shows that the variations in temperature must be mesoscale in origin and cannot be down to local effects.

Note, the temperatures at Durlston Head are damped compared to those at Leeson House due to Durlston Head's close proximity to the sea.
The sea has a greater specific heat than land and therefore heats and cools more slowly.

The relative humidity also increased at both Leeson House and Durlston Head; this would be expected if a warm front passed through the region (figure 11, AC and figure 15, AD).
The potential gradient shows the passage of the front clearly (figure 10, AC).

It shows a classic fog profile, which corresponds to the observations on the instrumented walks: The solar flux also decreased sharply at 1400UTC, which could signify thick low cloud or fog (figure 12, AC).
Finally, the pressure was at a minimum at approximately 1400UTC (figure 15, AD).

3.2 Radiosonde Launches
A radiosonde launched at 1503UTC on Saturday 28 th shows a thick layer of high ice cloud, with a cloud base of 500hPa.

This corresponds to satellite pictures for Saturday afternoon in which a layer of cirrus can be seen following the jet stream (figure 20, AF).
This also corresponds to observations made on instrumented walks.

There is a thick layer of cloud extending to the ground that corresponds to the fog reported by the other sources of data.
In addition, the warm front can be seen in the tephigram, as seen in the figure below: After the front had passed, the region was in an exceptionally warm and moist air mass, which can be seen by the high relative humidity measurements in figure 11 AC and figure 15 AD.

3.3.
Weather phenomena: The polar jet stream.

This can be observed as a striking feature in the satellite images for Saturday (figure 20, AF).
Cirrus clouds are carried along by the jet stream, so a defined edge is seen in the cirrus at the edges of the stream.

The strength of the jet stream can be seen in the 500hPa geopotential and pressure charts.
Earlier on in the week, the jet stream was weak due to the large depression over the UK (figure 21, AF).

However, by Saturday the jet had significantly increased in strength and caused the edge to the cirrus (figure 22, AF).
4. Analysis of the evening of Saturday 28th October (1800UTC on 28/10/06 - 0600UTC on 29/10/06)

During the early hours of Sunday morning, a cold front passed over the region.
This is the more northerly front lying across the South coast in figure 3 (A1).

It is a trailing cold front from a developed cyclone, which moved in from the West during the previous days.
The passage of the front was observed as a sharp dip followed by a rise in pressure, recorded at 0230UTC at Leeson House observatory (figure 15, AD).

The temperature also started to fall at approximately this time, suggesting a cooler air mass may have moved into the region (figure 15, AD).
2.5mm of rain also fell at Leeson House although the time is unknown.

This is unlikely to be a local effect as rain was recorded at Bournemouth between 0000 and 0600UTC.
A sharp change in wind direction was recorded at Durlston Head at approximately 0230UTC (figure 13 AC), this is consistent with the passage of a cold front.

There is also a sharp dip in the potential gradient at this time (figure 10, AC).
However as rain drops are negatively charged, the potential gradient should have dropped below zero if it had rained at Durlston Head.

Therefore this is an indication that it might have been a weak front or that it didn't rain at Durlston Head.
5. Analysis of Sunday 29th October (0600UTC - 1800UTC)

5.1.
General Situation

By the morning of Sunday 29 th the cold front had passed over, leaving the region in a cooler air mass.
This can be seen in the charts for 1200UTC Sunday (figure 4, A1) and for 0000UTC Monday 30 th (figure 5, A1).

Note, figure 4 is a forecast MSL pressure and thickness chart while the other figures shown are analysis charts.
A large anticyclone can also be seen building over the entire region.

This is associated with a stable air mass undergoing large scale sinking, which generally produces fine settled weather.
The system is also causing the front to pivot clockwise off a point over Cornwall rather than following a conventional path.

5.2.
Surface Observations

The situation is validated by the walk data, as seen in table 2 (A2).
The pressure had risen noticeably since Saturday (seen in figure 9 (A2)) as would be expected in an anticyclone.

Note, this pressure was measured with a digital watch, so the readings may not be as accurate as with specialist equipment.
There are also errors in the altitude estimates used to calculate mean sea level pressure.

The wind had also changed direction from South West to North East and its maximum speed during the morning was 3kt.
This may be explained by the wind being drawn around the centre of the high pressure area over the UK, which would produce slack northerly winds (figure 4, A1).

Note, local effects could have affected wind measurements as some readings were taken in more sheltered areas than others.
The temperature was comparable to Saturday's measurements even though the region was in the cooler air mass behind the cold front.

This is because there was much less cloud so more solar radiation could reach the surface.
There was some stratus cloud reported, but this was at a higher level; cloud temperatures were approximately 8 oC compared to 14 oC on Saturday's walk.

A layer of cirrostratus was also reported, as a halo was seen around the sun.
The visibility had increased to over 20km as the Isle of Wight could be seen, an indication that the air mass originated over the ocean.

5.3.
Radiosonde launches

Two radiosondes were launched on Sunday 29 th at 0930UTC and 1145UTC.
The tephigrams for the 1145 launch can be seen in figure 17 (AE).

This shows a layer cirrus cloud above approximately 400hPa (7200ft) but little lower cloud, corresponding well with the discussed observations.
The profile is also exceptionally stable which would be expected in an anticyclone.

As can be seen in the analysis charts for Sunday (figures 4 and 5, A1), the cold front which passed through the region is almost stationary over the English channel and by 0000UTC Monday 30 th, has been suppressed completely.
As cold fronts slope backwards with height, the front can be seen on figure 17 as the sonde passed through the 2 air masses: A closer view of the tephigram is shown below, compared to one showing a typical cold front: More quantitatively, the front can be seen in figure 17 by a jump in wet bulb potential temperature.

This occurred between 600hPa and 700hPa, suggesting the sonde rose into a different air mass at this point (figure 19).
The profile for an anticyclone is one of stable large scale sinking.

As the atmosphere would sink approximately adiabatically, the mixing ratio is conserved and a tephigram profile would get 'drier' as an anticyclone developed, producing a subsidence inversion.
This effect can be seen if the Sunday's two launches are compared (figure 18, AE).

This is not an ideal comparison, as the sondes were launched only a few hours apart.
For the effect to be clearly seen, a sonde would also have to have been launched late Sunday afternoon.

The relative humidity measured by the met mast also started to drop on Sunday (figure 11, AC).
6. Summary

Saturday was characterised with warm moist air that was advected from the South West.
This caused mild temperatures, high relative humidities and low visibilities.

A warm front passed through the area at approximately 1400UTC.
The polar jet stream was also strong and characterised by distinctive high level cirrus formations.

Sunday had much clearer stable conditions due to an anticyclone building over Western Europe and a cold front passing through the region early Sunday morning.
This led to reduced cloud and excellent visibilities.

The front stayed over the English Channel before eventually becoming suppressed by the anticyclone.
1.

See code
2.

Test A: Set = 0 for all x.
The solution is zero as expected.

Test B: Set the initial condition to the square wave and check the solution after 0 time steps.
This showed the square wave as expected.

Test C: If FORMULA and FORMULA from the lab script where D is the total distance travelled and N is the number of time steps, then FORMULA If the x domain is 1m with 40 grid points and the velocity u is 1ms -1, then it should take the model 400 time steps to complete one cycle (i.e. because of periodic boundary conditions the solution should be in exactly the same place on the grid as before).
The solution completed one cycle at 400 and 800 time steps as expected.

Test D: From the formula above, if the velocity changed then this shouldn't affect the distance travelled per time step.
When u was 2ms -1, the model still completed one cycle in 400 time steps as expected.

Test E: Set N to be a negative number.
The program returns no values as expected (it shouldn't work for a negative number of grid points!).

Test F: Set the velocity to be negative.
This again shouldn't affect the direction travelled or the distance travelled per time step.

The solution propagates 'forward' as expected.
3.

See graph 1
4.

A: The numerical model is moving more slowly than the theoretical model.
This is expected because the numerical phase speed is always underestimated compared to the real speed for the CTCS scheme.

The more time steps the model is run for, the more the numerical solution 'lags behind' the theoretical solution (this can observed in the results from runs for 50, 100 and 200 time steps).
B: There are also oscillations in the numerical solution in the space following the initial function i.e. on the left hand side of the function on the graph.

These are due to the computational mode of the numerical solution, introduced through truncation and round-off errors at each time step and because the CTCS scheme is designed to conserve.
The amplitude of these oscillations increases for increasing time step as expected.

The oscillations are also lagging behind the initial function, as numerical speed is dependant on wave number.
Therefore, different Fourier components of the solution travel at different speeds, so waves with a shorter wavelength (i.e. the computational mode) travel more slowly.

This is known as a diffusion error.
C: The amplitude of the numerical solution is greater than the theoretical one for the leading edge of the initial function (this can be seen at x=0.95 for 200 time steps)

5.
The numerical solution is unphysical if represents the concentration of a dye as it has some negative values in the oscillation following the initial function (i.e. you can't have a negative concentration)

6.
This error could cause problems for any physical quantity in a climate model.

For example modelling a temperature field, or the concentration of a green house gas such as CO 2 or Ozone.
Simply underestimating either of these would prove bad for a climate model, as well as the model proving unphysical if they went negative.

7.
The numerical solution to the CTCS scheme is unstable if the courant number is less than 1.

The courant number is the fraction of a grid point travelled in one time step: FORMULA.
If FORMULA, then c = 1.2.

Therefore the numerical solution is oscillating and unstable (amplitudes had increased to ~50000 by 50 time steps).
8.

See graphs
9.

If the smooth initial function was split into it's Fourier components, it is only made of a few sine waves of relatively long wavelength compared to the grid size.
However, a square wave is made of a large amount of Fourier terms, many of which have a small wavelength.

As numerical speed is dependant on wave number, these move more slowly than the long wavelengths.
Therefore there are many more slow moving oscillations for the square wave than for the smooth function.

10.
See graphs

11.
The errors are principally associated with the computational mode as the solution begins to oscillate in sign with time step.

This is because of the - 1 n in the general solution: FORMULA The 1 st term is the physical mode and the second term is the computational mode.
The equation also shows that the computational mode also propagates in the opposite direction to the function.

However, as the two graphs are only 1 time step apart, movement is hard to distinguish.
The solution needs to conserve (mass), so there is also a wave formed upstream due to the negative in the computational mode.

12.
The function was expected to move as before, but with its amplitude would be reduced exponentially with increasing time (i.e. the solution was being damped).

The theoretical solution looks like FORMULA For small timescales, the numerical solution follows the theoretical solution closely.
However, as the time approaches 0.5 seconds (200 time steps), the amplitude of the numerical oscillation increases compared to the theoretical solution and short wave oscillations are seen.

See extra graph for clarity.
13.

Oscillations can be seen because the amplitude of the computational mode has increased to greater than the physical mode.
The physical mode is decreasing with time, but the computational mode is increasing with time.

200 time steps (or 0.5s) is approximately the time it takes for the amplitude of the computational mode to be of the same order of magnitude as the physical mode.
14.

You could make the scheme implicit (i.e. put in FORMULA on both sides of the equation).
For example FORMULA I'm not sure if this would model the same thing though.

Introduction
A diesel engine can be run on either mineral oil or a vegetable oil fuel, however the properties of the two fuels are different and so engine performance will differ depending on the fuel used.

An experiment was set up to investigate the engine power output and efficiency of a diesel engine, run on diesel and RME (rape methyl ester) - a bio-fuel made from transesterification of rape seed oil.
Several different parameters are used to asses the engine performance:

Engine brake power, P
Engine brake power is a function of the torque of the engine and the engine rotation rate: FORMULA (1) Where brake power P is in Watts, T is torque (Nm), and ω is engine angular velocity (rads -1), which is related to engine speed as follows: FORMULA (2) Where N s is the engine speed in r.p.m.

divided by 60.
Brake mean Effective Pressure, P b

This is equivalent to the mean pressure as experienced by the piston that is needed to produce the same brake pressure as calculated by (1).
It is obtained from FORMULA (3) Where P b is in Nm -2, L is the length of the pistol stroke, A is the area of the piston, and n is the number of working stokes per second.

n= N s/2 for the four stroke cycle of the diesel engine.
Brake specific Fuel Consumption, b.s.f.c.

This is equivalent to the fuel consumption per unit of useful power converted by the engine.
It is given by: FORMULA (4) FORMULA is the fuel consumption rate (kg s -1), and b.s.f.c.

is in kg kJ -1.
Engine Efficiency, η

Engine efficiency or brake thermal efficiency is a measure of the overall engine efficiency.
It is the mechanical energy produced by the engine divided by the energy in the fuel input.

FORMULA (5) Where L.C.V.
is the lower heating value of the fuel.

A table of the fuel properties can be found in appendix 1.
Air/Fuel Ratio, A.F.R.

This indicates the proportion of air to fuel that is used in the engine.
FORMULA (6) Where FORMULA is the air flow rate in kg s -1.

Friction Power
This is the power needed to overcome friction in the engine at a particular speed, and can be obtained from the graph of fuel consumption vs. brake power.

Indicated Power, P i
This is the brake power plus the friction power.

Indicated Efficiency, ηi
This is the engine efficiency calculated using indicated power instead of brake power.

FORMULA (7)
Exhaust Gas Losses, FORMULA

This is the energy loss of the engine through heat in exhaust gases.
FORMULA (8) Where FORMULA is in W, FORMULA (sum of the air flow and fuel flow rates), C p is the specific heat of the exhaust gas (kJ kg -1 K -1), T e is the exhaust temperature (K) and T a is the ambient air temperature (K).

Air Cooling Losses, FORMULA
This is the energy loss through air cooling of the engine, calculated through use of energy balancing: chemical power in through the fuel must equal the sum of all power dissipated in the engine through mechanical work, heat, and friction losses.

FORMULA (9) FORMULA is in W.
Methodology

A Petters ACI 7.2 bhp (5.4 kW) diesel engine was used to drive an electric dynamometer and a torque meter.
Power from the dynamometer was dissipated in an air cooled resistance bank, which could be used to alter the load on the engine.

The performance of the engine at several different loads was investigated for both fuels.
At each load, the engine speed was set at 2800 rpm, and readings of the fuel consumption rate, ambient air temperature, inlet temperature, exhaust temperature, engine torque output, and air intake rate were made.

Fuel consumption rate measurements were made by recording the length of time needed for the engine to consume a set volume of fuel: 2×10 -5 m 3 (20ml).
The fuel consumption rate FORMULA was then calculated with: FORMULA (10) Where ρ is the density of the fuel (kg m -3) [see appendix 1], and t was the time taken (s).

The three air temperature measurements were made by use of temperature sensors in appropriate locations.
Engine torque was measured using a spring balance attached to a torque arm.

The force delivered by the torque arm was calibrated by: FORMULA (11) Where F m is the measured force (N).
The engine torque was then obtained through FORMULA (12) Where r is the length of the torque arm.

In this experimental set up, r=0.248m.
For the calculation of (3), L×A is equivalent to the volume swept by the piston.

For the engine used, this was 3.04×10 -4 m -3.
The air intake rate was measured using a calibrated orifice plate.

The air passing through the plate orifice into the engine causes a pressure drop which is related to the volumetric flow rate by the calibrated equation: FORMULA (13) Where FORMULA is the air flow rate (kg s -1), P a is the ambient air pressure in mmHg, T a is the ambient air temperature (K), and Δp is the pressure drop across the plate orifice (mm H 2O).
The pressure drop is multiplied by a factor of 0.2 to account for the incline of the manometer used to measure Δp.

Measurements of cylinder pressures and piston crank angle (a function of cylinder volume) were also made over several cycles at each load, and data-logged.
Plots of pressure vs. volume can be used to analyse the indicated power of the engine, as this is the area under the pressure-volume curve.

Results
All graphs show the various engine parameters against brake mean effective pressure, which is a function of the load on the engine and increases linearly with increasing load (increasing engine torque),which can be seen by manipulating equations (1) (2) (3) (11) and (12).

P b is therefore a suitable dependant variable.
At low loads, the engine has a high fuel consumption proportional to the power that is being output (figure 1).

The engine requires more fuel to produce a unit of power at lower loads when it is running on diesel than it does when using RME, however this is only true for small loads.
The fuel consumption per kJ output falls to a near-steady level at higher loads, and more RME fuel is needed than diesel to produce the same engine power at higher engine loads.

The average amount more of RME needed per kJ output is ~4×10 -5 kg.
Engine efficiency (or brake thermal efficiency) as shown in fig. 2 compares the mechanical power output to the chemical power input.

It is clear that the engine is more efficient when run on diesel.
The lower brake specific fuel consumption of RME at low loads does not have much of an impact on efficiency, and the engine efficiency is similar for both fuels at low loads.

At high loads, the engine efficiency appears to peak for both fuels.
For diesel, this is at about 3kW of output, at 21% efficiency.

For RME, the peak occurs at 2.7kW with an efficiency of 16%.
Higher air/fuel ratios mean that the engine requires more air to burn the same amount of fuel.

Figure 3 shows that RME requires less air to burn.
This is because vegetable oil esters comprise compounds which already contain oxygen with which to burn (HBLVA, 1996).

The air fuel ratio for both fuels drops as the engine load increases, at roughly the same rate.
The difference between the two ratios is roughly constant at 33.

The friction power is obtained from the plot of FORMULA against brake power (fig. 4).
The value of P where the regression line fit to the data crosses the x-axis is the friction power.

For diesel, the friction power at 2800 rpm is 1.89 kW.
For RME, it is 1.08 kW; the engine expends less energy overcoming friction when RME is used.

The indicated power, P i, is the brake power plus this friction power.
Since friction power is about constant at one engine speed, this translates to a constant difference between P i for diesel and RME, equal to friction power diesel - friction power RME = 1.89-1.08 = 0.81kW.

So, the power converted by the engine on diesel is always greater by about 0.81 kW than the power converted by the engine on RME at any one load.
Indicated power is also shown by pressure versus crank angle graphs, as the area within the curves is equal to the indicated power.

The volume of the cylinder is a function of crank angle, and so crank angle represents the changing cylinder volume in the four-stroke engine.
Figures 5 and 6 show that the area within curves is increasing as the load is increased; the indicated power is increasing with load.

The difference in indicated power between RME and diesel is clearly shown; the areas are much smaller for RME.
Note the small spike in run 16 near the compression stroke curve - this may be due to premature combustion in the cylinder.

The graph of indicated efficiency, ηi, (fig. 7) shows the efficiency of the engine when the work done to overcome engine friction is taken into account.
In comparison to figure 2, it can be seen that indicated efficiencies are higher than brake thermal efficiencies, due to the friction power inclusion.

The indicated engine efficiency at low loads does not approach zero, unlike brake thermal efficiency, again due to the inclusion of friction power.
ηi at low loads is high, as the fuel energy is being used to overcome friction.

This representation of efficiency therefore shows a smaller range of engine efficiencies, since work done overcoming engine friction is viewed as useful energy output.
As before, the engine is more efficient on diesel, with more useful power being extracted per unit mass of fuel.

The efficiencies for both fuels seem to peak at medium loads.
The range of efficiencies for diesel is 32 - 37%, and 19 - 25% for RME.

Figure 8 shows the exhaust losses for each fuel.
This is the heat energy that is lost through the exhaust.

RME has higher energy losses through heat than diesel, so RME must be producing higher exhaust temperatures than diesel, at a faster air flow rate.
The hotter the air, the higher its heat capacity, so more heat is lost at higher temperatures.

The other heat loss in the engine is through air cooling.
The amount of energy lost through air cooling is what remains after mechanical power and exhaust heat have been accounted for.

See equation (9).
RME has greater air cooling losses than diesel.

The air volume flow rate is also, in general, higher for RME, and so more heat can be transported away faster than for diesel.
The majority of total heat lost is through air cooling of the engine, and this amount increases as load increases.

At higher loads, the engine is producing more heat, and air heat capacity increases with temperature, so increasing amounts of energy are lost at higher temperatures.
Figures 10 and 11 show the energy balances in the engine.

Total heat losses are exhaust losses plus air cooling losses, and the figures show how most of the energy available from burning the fuel is converted into waste energy.
Only a small amount of the total energy in is converted into useful power.

At low loads, almost all of the energy available in the fuel is lost as waste heat.
The energy used in overcoming engine friction is dissipated as heat.

It may be that more accurate estimation of exhaust and air cooling losses can be made using the inlet air temperature instead of ambient air temperature.
However, since the inlet temperature is only a few degrees higher than the ambient, this only makes a slight difference to the exhaust and air cooling losses.

In general, the exhaust losses decrease by about 50 Watts, and the air cooling losses increase equivalently by 50 Watts.
Discussion

Each measurement was performed at one engine speed only (2800 rpm).
A full investigation would require measurements to be made at different engine speeds in order to fully compare engine performance with diesel and RME.

One thing to note is that the window of the lab was opened during the experiment, for the duration of the engine run using RME.
This reduced ambient air temperatures and inlet temperatures, which may have had some effect on the results.

RME has a lower Lower Heat Value than diesel (see appendix 1), meaning that there is less energy per unit mass of fuel.
This means that the engine has a higher fuel consumption of RME to achieve same power produced by diesel.

However, RME has lower engine friction, since RME is a better lubricant than diesel, meaning that less power converted in the engine is needed to counteract friction.
This may be shown in figure 1, where RME has lower brake specific fuel consumption at low loads.

RME loses more heat in exhaust losses and air cooling losses than diesel at higher brake mean effective pressures.
It is likely that the 'extra' energy from lower friction in the engine with RME is lost at higher loads as heat.

RME burns faster to provide same power output as diesel (since it has a lower fuel energy capacity), so fuel flow rates are faster for the engine running on RME, and this means faster air flow rates.
However, the air/fuel ratio of RME is lower than diesel for a given load - this may be due to the chemical composition of RME in comparison to diesel.

RME is an ester, which contains some oxygen.
It could be that the oxygen content already in the fuel means that less air is required for RME to burn.

RME has higher exhaust and air cooling losses.
It could be that RME burns at a higher temperature than diesel, and so more energy is lost as waste heat.

The higher heat losses could be due to the higher fuel consumption rates: more heat energy is being produced per second for RME.
Figures 8, 9, 10 and 11 must be viewed as only qualitative representations, since heat capacity of air was only roughly estimated from a table of figures.

Figure 7 shows the indicated efficiency of the engine on RME and diesel.
There appears to be a peak in efficiency at medium loads, indicating that the diesel engine is best run at medium loads.

In conclusion, diesel engine run on the biofuel RME will have a higher fuel consumption rate and a lower efficiency than the same engine run on mineral oil diesel.
However, fuels must also be considered in light of their larger environmental impacts, and it is in this area that RME has advantages over diesel.

Introduction
Induction generators are simpler, smaller and more robust than synchronous generators, and are not limited by fixed speed requirements.

As such, they are increasingly being used in the field of renewable energy.
In this experiment, an induction motor/generator was investigated in relation to being applied to hydro-power.

A 0.75 kW 4 pole motor with a synchronous speed of 1500 rpm was used in the experiment.
Firstly, the motor characteristics were investigated with the motor connected to an exterior power supply.

Secondly, the characteristics of the motor acting as a generator were investigated, after the motor was disconnected from the grid and a water turbine was used to power the generator.
Methodology

The motor used was hardwired in a three-phase delta connection, with a line current rating I L of 3.2 A and a line voltage V L of 220 V.
The system was set up as shown in figures 1 and 2.

The first parts of the experiment (a - b) were to determine the value of these circuit parameters.
a) Magnetising curve and Running Light Equivalent Circuit Parameter test

The motor was connected to a three-phase 400V, 50Hz supply, which was controlled with a 3-phase auto-transformer so that the supply voltage could be varied.
In the running light test, the slip of the motor is approximately zero, and so the motor circuit in figure 2 becomes simplified: With the motor running, the supply voltage was varied between 120 to 260 V, in increments of 20V.

At each line voltage V L (V), line current I L (A), total power P total (W), and the motor speed (rpm) were measured.
From these measurements, the magnetising curve could be obtained from a plot of V L against I L.

The total volt-amp reaction Q total (VAr) of the motor circuit was then obtained using FORMULA (1) Which could then be used to find the magnetisation reactance X m (Ω) of the motor: FORMULA (2) Where V ph is the phase voltage of the system: for a delta configuration, V ph=V L. Q ph is the phase volt-amp reaction of the system.
In a three-phase system, Q total=3×Q ph.

The voltage stabilisation of a stand alone system depends on the variation of the magnetisation reactance.
R c, the electrical loss in the magnetic core (represented as a resistance), can be obtained using: FORMULA (3) Where P ph, the power in each phase of the 3-phase system, is equated to the total power by FORMULA (4)

b) Locked Rotor test
This test involved locking the motor rotor, then passing a line current close to the rated value through the system, and recording the line voltage and total power in the system.

The circuit in figure 2 is now approximated by figure 4: The remaining circuit parameters can now be obtained: R 1, the stator coil resistance, is determined by FORMULA (5) Where R ph-to-ph is the d.c.
resistance between two phases.

The rotor resistance, R 2', is then obtained by: FORMULA (6) I ph is the phase current, determined by I L= √3 ×I ph for the delta configuration.
The stator and rotor leakage reactances, X 1 and X 2' are found from: FORMULA (7) These two values do not need to be split; they can be considered as one value.

A computer program was then used to produce power/speed, electrical machine efficiency/speed, and current/speed curves for the motor, using the obtained values of the circuit parameters.
c) Load test

The motor was set running at a fixed supply voltage of 220 V, but water was run into the turbine at increasing pressure.
The water pressure was increased from 0 - 25 psi in increments of 5 psi, and I L, P total, and motor speed were recorded at each stage.

At the point when the turbine forces the motor speed to become supersynchronous, the motor will become a generator, and P total will become negative.
The motor efficiency, turbine efficiency, and overall system efficiency can then be determined.

Motor efficiency can be found from the efficiency/speed curve.
The overall efficiency is the electrical power out divided by the power from the water pressure.

The turbine water pressure power is obtained from a graph of power/water pressure.
Turbine power is the mechanical power output divided by the water pressure power in.

d) Three-phase stand alone operation
The motor was then disconnected from the three-phase supply, and run as a generator connected to a three-phase load.

The system is now as follows: In order to start the motor running, some residual magnetism needs to present in the motor.
Capacitors in the circuit are used to excite the machine: resonance between the magnetising reactance X m of the system and the capacitative reactance amplifies the small residual magnetic field.

The volt-amp reactance X m obtained in (2) must therefore be generated by the capacitors if the motor is to run in a stand alone system (provided an external source of rotation - the turbine driven by water flow).
For capacitors in a delta configuration, the value of capacitance C (F) needed is given by: FORMULA (8) For a line voltage of 260V, the output frequency of the induction motor is given by: FORMULA (9) Where L is the inductance of the system, obtained from: FORMULA (10) and so the synchronous speed N sync (rpm) (the speed for no load on the motor) of the generator is: FORMULA (11) Where P is the number of poles in the generator.

For the machine used, P=4.
The generator is run as a stand alone system, with the load in the system being increased in steps.

For each load configuration, the water pressure is increased so that the line voltage output becomes 220 V.
The line current, power, water pressure and rotation rate were recorded.

The turbine efficiency and overall efficiency could then be obtained as in part c).
e) Single-Phase Ballast operation

The machine is operated stand-alone, and a load is connected across two-phases of the system: a single phase load.
This load consists of a ballast load controller connected to a variable load: a bank of parallel connected lamps.

Each lamp can be switched on individually, increasing the resistive load in increments.
The ballast load controller effectively maintains a constant load on the generator: if the load demand increases (more lamps are switched on) more of the power into the ballast load controller is diverted to the lamps, and less is dissipated by the ballast load controller's internal resistance.

If the load demand decreases, the ballast control increases the load from its internal resistance so that the total load on the generator is constant.
In the experiment, the water pressure was set so that the generator voltage output was 220V.

The number of lamps on was increased, and the percentage of the generator power into the ballast load was recorded.
Results

a) Magnetising curve and Running Light Equivalent Circuit Parameter test
Using equations (1), (2), and (3), Q total, X m, and R c were calculated for the motor at 220 and 260V line voltages.

The graph shows how the current in the system increases with increasing supply voltage.
For supply voltages 120 - 200V, the increase is small.

For voltages above 200V, the current increases greatly with small increments of supply voltages.
This is when the motor has reached its saturation point: the magnetic field will no longer increase in strength despite an increase in supply voltage.

b) Locked Rotor test
The circuit parameters measured in the locked rotor test are shown in table 3.

R phase-to-phase was measured as 7.6 Ω.
Using equations (5), (6), and (7), the stator and rotor resistances and reactances were found to be as follows: These parameters were put into a program in Matlab, and graphs of power/speed, electrical machine efficiency/speed, and current/speed curves for the motor were produced using the obtained values of the circuit parameters.

These graphs can be found in appendix 1.
The graphs are based on the assumption that the system is generating, and that the system is grid connected with a synchronous frequency of 50Hz.

c) Load test
The circuit parameters from the load test are shown in table 5.

Note how the power in the circuit becomes negative when the motor starts acting as a generator - when the water pressure is greater than 10 psi.
Up until this point the line current is roughly constant, and only increases after generation starts to occur.

The three sets of circuit values for the generator (at water pressure 15, 20, and 25 psi) were plotted on the power/speed and current/speed graphs.
From these it can be seen that the generator is not producing the expected powers and currents for the measured speed of the motor.

The generator is producing less power, but a greater current than expected based on the measured circuit resistances and reactances.
By using the measured line current, and reading off the expected motor speed from the current/speed graph, the generator efficiency can be found from the efficiency/speed graph, by using the motor speed values just obtained.

The turbine input power is found from a graph of turbine power vs. water pressure.
(Dorrell, 1997) The water pressure value from table 5 is reduced by 30% to account for pressure losses in the pipes.

This altered value is used to obtain the value of turbine power from the graph.
The overall efficiency is then the ratio of the electrical power (from table 5) to the turbine power.

A measure of the turbine efficiency is then the overall efficiency divided by the generator efficiency.
The efficiencies are shown in table 6.

The efficiencies increase with increasing water pressure.
d) Three-phase stand alone operation

From equations (10) and (8), the capacitor size needed for stand-alone operation of the system was found to be 15.26µF.
Three capacitors of this size in delta configuration would provide the reactive power needed for the magnetising branch of the circuit to become excited and the generator to turn, provided adequate water pressure into the turbine.

In reality, capacitors of size 18µF are used, as a slightly greater capacitance is needed to overcome losses in the circuit.
For a stand-alone system with 3×18μF capacitors in delta-configuration, the output frequency can be calculated using equation (9).

For a line voltage of 260V, this frequency is 56.5Hz, and equation (11) relates this frequency to a synchronous speed of 1695 rpm.
This is larger than the synchronous speed for the grid connected system.

The operating frequency of a stand alone system will drift more than that of a grid connected system, since this is linked to slip.
As can be seen in table 7, the rotational speed of the motor increases with water pressure, and so slip is increasing.

The generator was then run as a stand-alone operation.
Load in the system was increased using resistors; for each load configuration, water pressure was altered to maintain a line voltage of 220V.

Using the same method as in part c), overall efficiency can be found.
The results are in table 7.

The efficiencies from table 7 are shown in figures 7 and 8 plotted against motor speed.
No clear curves are visible for the turbine efficiency and overall efficiency since the data set is only four points.

It is possible that one of the data sets is anomalous.
e) Single-Phase Ballast operation

For fixed values as shown below, the load was increased in terms of number of lights switched on.
Table 9 shows the % of the power from the generator that is dissipated in the ballast resistor bank.

This shows a measure of the efficiency of the system.
The more power dissipated in the resistor bank, the less efficient the system is: since less power is being used 'usefully' in the lamps.

For a ballast controlled system, it is possible to have 100% inefficiency, if all the power were dissipated in the resistor bank.
Discussion

The synchronous speed of the motor is about 1500rpm, as can be seen from table 1.
The motor speed remains roughly the same while line voltage and line current increase: the power output increases due to the magnetic field strength increase.

Table 5 shows how power is generated when the motor is pushed supersynchronous: when the motor speed goes above 1500 rpm.
The overall efficiency of the system is not high, but increases as the motor runs faster.

Using ballast load control, the generator can be run at a constant load, however this decreases the efficiency of the system as there is a lot of waste power dissipated in the resistor bank.
The proposed PV site is a sloping roof sheltering an area of 5×8 m 2 at a school in Reading.

The sloping roof will have a slope angle of 20 o; this gives a total roof area of 42.6m 2.
The roof is also angled at 30 o west of south.

Analysis of a potential solar PV site was carried out using the PVSyst software, assuming that the roof is ventilated.
There existed no meteorological data for Reading, and so data for London (a location with a similar latitude) was used.

Optimum array vs. Proposed array
The sensitivity of a PV array output to changes in slope angle and azimuth is shown in figures 1 and 2.

These two figures only show how changing one parameter affects the output while the other parameter remains the same.
Changing the azimuth has an affect on the slope output curve, and vice versa.

As the slope angle increases from 0-30 o, the azimuth curve changes from a flat line to the curve shown in figure 2 Past 30 o, the azimuth curve retains its shape but drops further down, so that the peak does not reach the optimum output.
For the slope curve, it is the optimum at 0 o azimuth, as shown in figure 1.

As the azimuth angle increases (the array is turned away from the south in either a westwards or eastwards direction), the slope curve peak drops, and the curve flattens out.
To obtain the maximum electrical output for a PV array at this latitude, approximately 50 o N, the optimal slope angle is 30 o, and the array should face directly south.

The proposed array will therefore not produce the maximum possible output, as it will not be receiving the total possible amount of sunlight; however the difference in output of the optimum array and the proposed array are not large: the combined effects of the non-optimum tilt and orientation mean a 3% reduction in power from the optimum amount.
For a hypothetical array of mono-crystalline PV cells, the potential outputs of the optimum array and the proposed array are compared in table 1.

For the same identical array at the same latitude, the cost and nominal power output will be the same.
The difference in the array slope and orientation causes the proposed array to fall short of the maximum potential output by 0.1 MWh/yr, causing the cost of the energy to be greater.

The energy cost is an estimate of how much the array would be costing per unit output.
The potential profit is based upon a selling rate of 10 pence per kWh of the electricity to the grid.

From the estimated annual production rate, it can be seen that the school could earn about £430 if all the electricity were sold to the grid.
The value of kWh/year/m 2 shows how a unit area of the proposed array will receive less solar energy than the array at the optimum orientation, but that this difference is not that large.

The proposed array will give approximately 97% of the output of the optimum array.
The value of kWh year -1/kWp shows that the proposed array produces less energy per unit nominal power of the array than the optimum array.

For an identical array in Benin, Nigeria (also at 20 o slope angle and 30 o azimuth), the array output is summarised in table 2 below, in comparison to the proposed array in Reading.
Appendix 1 contains data on the monthly outputs for both Reading and Benin, for comparison.

Table 2 shows that an identical array located in Benin, Nigeria, will have a much greater annual yield.
This is due to the greater number of hours of daylight at this latitude.

Appendix 2 shows the solar paths at these two latitudes - Benin has a much higher number of hours of daylight per year, and the intensity of the incident sunlight (Wm -2) is also much greater nearer the equator.
The value of kWh/year/m 2 is accordingly much greater for Benin.

The monthly output patterns for both locations are quite different.
The graphs in appendix 2 show how the incident sunlight varies greatly over the year.

Reading sees a peak in summer and a trough in winter, while Benin has two small maximums and two small minimums - the minima occur during high summer and midwinter, when the sun is furthest from the equator.
Benin is only a few degrees north of the equator, and sees maxima at about the times of equinox.

The array costs are the same, but the cost of energy from the Nigerian PV array will be much lower due to the greater energy output.
The Benin array would therefore earn much more money, if all the electricity was sold to the grid at a price of 10p/kWh.

Cell Type and Funding
The previous analysis was based on an array comprising mono-crystalline PV cells.

This type of cell is more efficient than polycrystalline or amorphous silicon cells, however it is also more expensive.
Table 3 compares the outputs of the proposed array in Reading using different cell types.

Thin film cells are the cheapest, however they are not as efficient, and so the nominal power output and estimated annual yield is much less than for mono-crystalline cells, at 2128 kWh/yr.
Similarly, polycrystalline cells are also cheaper but less efficient.

The lower output of the thin film cells is not outweighed by their reduced cost, and so the cost of the electricity produced is much higher for thin film cells.
The lower energy output means that the potential money to be made from selling the electricity is also much lower.

If the budget can accommodate it, it would be a better option to use mono-crystalline cells.
Funding is available for this sort of project, from the government's Low Carbon Buildings Program.

The maximum grant available is £30,000, to cover 50% of the total cost of the scheme, however there is a monthly budget available for funding schemes of this type and so funds for a particular month may run out.
More information can be found at URL

Cell and Inverter Selection
Mono-crystalline silicon cells from BP Solar, type BP495, were chosen for the proposed array.

These modules have an individual power rating of 95Wp.
60 of these modules will cover an area of 37.8m 2, and if connected in 6 parallel strings of 10 series connected modules will give a nominal output of 5.7kWp, at a maximum current of 19A, and maximum voltage of 279V.

The inverter chosen to match these modules is the Fronius IG 60EI, which has an operating voltage range of 150-400V and a nominal power of 4.6 kWp.
This is less than the nominal power output of the cell array, since this means that the inverter is working at high efficiency for a larger proportion of time than if it were to match the array maximum output.

The array is only likely to produce maximum power on a few bright summer days, and so it is more efficient to have the inverter match a more average array power output.
More details of both the cells and the inverter can be found in appendix 3.

The cell layout would be as shown in figure 3.
A low number of parallel strings ensures that the currents in the system are low, and equivalently voltages are high.

A low current means that there is less resistive loss of power in the system.
However having a few parallel connections is beneficial, since a shaded cell will affect the output of the other cells in the string.

If there are a few separate strings, the total array output will not be affected by shading of one part of the array, since the output of unshaded strings will be unaffected.
The total annual output of the system is estimated at about 4.47 MWh/yr.

The loss diagram in appendix 3 shows how power losses occur at several stages in the PV system, mainly from irradiance levels being too low, and from the inverter.
Several other combinations of cell types and inverters were considered.

It is possible to achieve similar power output for a similar size area using other cell types and inverter types.
If budgeting allows/restricts, cell and inverter types could be changed to reflect this, as the more efficient and more powerful cells are likely to be more expensive, and vice-versa.

Another problem to consider is fitting the cells to the roof dimensions.
This would be easier to achieve with a greater number of smaller cells, and this is also something to consider.

Shading
The presence of a tree in front of the PV array will have some negative effect on the system output.

A simulation of the shading effect of the tree was run using PVSyst.
The tree top is 4m above the roof height, and the tree stands 2m in front of the PV array.

A model of this setup is shown in figure 4.
Depending on the time of day and time of year, the amount of shading caused by the tree will vary.

In the summer when the sun is high, shading is minimal.
In the winter, a large proportion of the array could be shaded.

For the cell and inverter choice as proposed, the annual power output with the shading is estimated at 4.09MWh/yr, a reduction of about 0.4 MWh/yr from the simulation without shading.
The shading effect is greater in the winter than the summer.

As well as the direct effect of shading (loss of irradiance on the cells) there is also a knock-on effect from the inverter - it will be receiving lower voltages, and so there will be greater power losses due to the inverter's operating efficiency decreasing with voltage.
RetScreen

The proposed scenario was also investigated using the RetScreen software.
This program allowed the choice of location, array slope and orientation, and choice of type and number of PV cells.

However the program did not allow choice of inverter or manipulation of series/parallel connections between the cells.
RetScreen did not have the same range of cell types offered by PVSyst.

The BP modules chosen were not available, so the next closest thing was chosen: BP Solar modules BP590F, with a rating of 90Wp (compared to the 95Wp used above).
60 of these modules give a nominal power output of 5.4kWp, and cover an area of 37.8m 2, same as before.

RetScreen estimated the annual array output to be 4.45 MWh/yr, similar to that estimated by PVSyst for the 95Wp modules - this would seem to indicate that RetScreen will tend to give higher estimates of annual yield than PVSyst.
RetScreen offers less choice when putting together a PV system, and does not produce graphical output.

It also does not offer the opportunity to investigate shading of the array by other objects.
However, RetScreen has far more comprehensive financial assessments, allowing costs to be studied in detail.

It also contains a section to estimate the possible reduction in greenhouse gases from converting to solar energy.
For the proposed array, there would be a possible reduction of 4.56 tonnes of CO 2 per year if the solar power were used instead of electricity from a coal powered power station.

The CO 2 savings are dependant on the original source of energy.
Summary

The proposed PV array to be erected at the Reading School will not produce the maximum amount of power possible at this latitude due to its slope and orientation, however the reduction in power is not large and the site is still a good choice for a PV array.
It is recommended that if the budget allows, that mono-crystalline PV cells be chosen.

These will give much higher power outputs, and so the cost of the energy produced by the array is lower than that from an amorphous cell array despite the higher cost of mono-crystalline cells.
1. Introduction

The power produced by a wind turbine is dependant upon several things: the wind speed incident on the turbine, the turbine characteristics, and the load on the turbine.
The efficiency of the turbine at extracting power from the wind can be described in terms of a coefficient of performance, C p; a dimensionless number.

The value of Cp of the turbine will vary with wind speed, but it is better to compare it to another dimensionless number, λ, tip speed ratio.
A graph of Cp against λ will show the performance of a wind turbine at a range of wind speeds.

FORMULA (1) Where P w is the power extracted from the wind (W), ρ is air density (kg m -3), A is the area swept out by the wind turbine blades (m 2), and V is the free stream velocity (ms -1).
FORMULA (2) Where ω is the rate of turbine rotation (Hz), R is the rotor radius (m), and V as before.

Another turbine characteristic is C q, which is coefficient of torque.
FORMULA (3) Where Q is the torque exerted by the rotor; ρ, R and V as before.

C q is equivalent to C p divided by λ.
The objective of the experiment is to investigate the characteristics of a small wind turbine at different wind speeds and for varying electrical loads (resistances) on the generator.

2. Methodology
A Rutland Wind Charger was positioned in the exhaust flow from a wind tunnel, and connected to a circuit containing variable resistors.

An ammeter was connected into the circuit, and a voltmeter across the resistors.
Measurements of current, I (A), and voltage E (V), could then be used to investigate the generator characteristics at different wind speeds and loads.

The rotation rate of the turbine was measured using an optical tachometer.
Free stream wind speed in the tunnel was measured using a pitot tube.

A hot wire anemometer was used to measure wind speeds at the turbine.
Using these measurements, C p was approximated by (4): FORMULA (4) Where ΔP is the pressure difference as recorded by the manometer connected to the pitot tube (Pa).

Tip speed ratio is approximated by (5): FORMULA (5) Plotting (4) against (5) will give a representation of the C p-λ curve of the turbine.
Since torque coefficient is equivalent to (1) divided by (2), an approximation to C q can be obtained from (4)/(5)= (6): FORMULA (6) The experiment is set up so that several measurements of I and ω can be made by varying the resistance and so the voltage across the circuit at a certain wind speed.

The wind speed is also altered several times, and a set of current and rotational speed readings taken at each different wind speed.
In this way, graphs equivalent to C q-λ curves can be made for each wind speed (each pressure difference), by plotting I against ω.

This graph will give an indication of how the turbine reacts to changing loads at constant wind speeds.
The pitot tube pressure difference is equated to wind speed by (7): FORMULA (7) A generator characteristic for the turbine can be obtained by plotting I against ω for each different voltage set, which gives an indication of how the generator responds to changing wind speeds at different loads.

3. Results
Appendix 1 contains tables of the data recorded.

The wind tunnel is designed for experiments to be performed within the square working section, however the wind turbine was too large to fit within it (Table 1 gives the exact measurements).
Instead, the turbine was placed within the exit air stream.

The air stream outside is less controlled, and so there will be more turbulence: the air flow reaching the turbine is likely to vary across the turbine cross section, and be slower than the air flow within the wind tunnel working section.
During the experiment recordings of wind speed at several places across the area of the rotor were made in order to investigate this.

Figure 1 shows two sets of readings taken at a low and a high wind speed at five places in front of the rotor.
Recordings of wind speed at the central part of the rotor were also made at each pressure difference in order to compare speeds inside and out of the wind tunnel.

Figure 2 shows how the wind speed measured at the turbine compared to the wind speed within the wind tunnel, as obtained from (7) using the pitot tube pressure difference.
The wind speed at the centre of the rotor increases linearly with the wind speed inside the wind tunnel, and so the wind tunnel pressure differences Δp, measured by the pitot tube, correspond to increasing wind speed very well for qualitative analysis.

Figures 3, 4 and 5 show the Cp-λ curve, generator characteristic, and torque coefficient obtained for the Rutland Windcharger turbine.
4. Discussion

The optical tachometer required positioning behind the turbine to obtain results.
The tachometer was handheld, and so there will have been some influencing of the air-stream near the turbine by the tachometer operator.

Figure 1 shows that wind speed across the rotor cross section varies by quite a large amount.
The two sets of data for the two different wind speeds both show the same sort of decrease in wind speed towards the edge of the rotor disk.

The bottom of the rotor appears to experience the greatest decrease in wind speed from the central value, while the top has the smallest decrease.
The left and right hand sides of the rotor both experience decreases in wind speed.

The reason for the different wind speeds at different locations is likely to be due to changes in the air flow as it exits the wind tunnel.
At room air temperatures and pressures, air flow is very turbulent, and so turbulent entrainment of ambient air from outside the wind tunnel exit region is likely to slow the outer regions of the air flow.

The continuity equation, (8), shows how the wind speed at the tunnel exit will be lower then the speed within the tunnel.
Past the tunnel exit, turbulence, entrainment, and further expansion of the stream tube of the air flow will further slow the air.

FORMULA (8) Using values from table 1 and tunnel wind speeds as used in figure 1: FORMULA FORMULA This is an interesting result, as the recorded wind speeds at the turbine are higher than these calculated values of wind speed at the tunnel exit.
It could be that there is some error, either in the measurements of ΔP or wind speed at the turbine.

A polynomial regression line of fourth order fitted to the curve in figure 3 gives an equation: C p = 2×10 -9 λ4 - 5×10 -7 λ3 + 4×10 -5 λ2 - 0.0012 λ + 0.0131, similar in form to other C p approximations for other wind turbines (Cockerill, 2006 (2)) This graph shows how C p peaks for a certain value of tip speed ratio: this is the Betz limit, a physical constraint which is a consequence of the balance of forces which must occur as the wind stream acts upon the turbine.
The exact shape and peak of the C p curve depends upon the free stream wind speed, and the characteristics of the turbine itself.

The maximum value of C p that can occur is 0.59.
Since we have used an equivalent to C p (4), we cannot obtain the true maximum value of C p for this turbine.

Figure 1 is proof, however, that the turbine being tested has a power coefficient response similar to that of other turbines.
Figures 4 and 5 are best considered in unison, as they show how the turbine responds to changes in free stream wind speed and electrical load.

Both wind speed and load have an affect on the current in the circuit and the rotational speed of the turbine.
From figure 5 it can be seen that increasing wind speeds lead to increased rotational speeds and an increased current.

Figure 4 shows that increasing voltages (higher loads in the circuit) lead to smaller currents in the circuit for the same rotational rate, following Ohm's Law: E=IR, where R is the circuit resistance.
This can be looked at alternatively; higher loads at the same wind speed lead to a decrease in current but an increase in rotational rate.

For a higher voltage across the resistors, a higher rotation rate is needed to produce the same current.
For a constant load, increasing wind speed means increasing rotational rate and so increasing current.

For a constant wind speed, increasing load means a decrease in current and increase in rotational rate: the faster the turbine must turn in order to provide the required voltage/current.
Figure 5 shows that higher wind speeds (greater values of ΔP=DP) give higher rotation rates of the turbine, and higher currents produced in the circuit, however the relationship between current and rotational speed is not linear for changing loads at the same wind speed.

As the load in the circuit is increased (voltage increases), current and rotational speed both increase at first for most of the wind speeds, but as load continues to increase, the current in the circuit drops while rotational speed continues to increase.
In order to produce the maximum power, a balance between load and wind speed must be found; the power output is equal to the current times the voltage, but current and voltage are also bound by Ohm's Law.

Increasing the circuit resistance causes a decrease in the current for a given voltage, and so an increase in the rotation rate of the turbine.
In practice, many wind turbines make use of this fact, and use resistance control to affect wind turbine speed so that the frequency output of the turbine generator remains constant.

Resistor control can also be used to ensure that the power output of the turbine is at a maximum for a given wind speed.
Figure 4 can be used to compare the measured performance of the turbine with its expected performance, as reported by the turbine manufacturer.

The manufacturer provides a curve of charge provided by the turbine into a 12V battery against wind speed.
The manufacturer presents results from 0 - 20 ms -1, however this experiment only had a range of wind speeds 7.25 - 15.4 ms -1 (as measured at the turbine), which is equivalent to the straight line portion of the manufacturer's curve.

As such, the experimental results match quite well except in terms of magnitude; the maximum current produced by the turbine at about 15ms -1 for 12V is about 4A from the manufacturer's graph, but only about 2A from the experiment.
However, the manufacturer states that the expected performance curve is for ideal, non-turbulent conditions, which are unlikely to have been achieved in the experiment.

The resistive load for charging a 12V battery could also have been smaller than the load imposed in the circuit during the experiment.
Anthropometry - Height, Weight and Circumstances

1. What are the limitations of a measurement of BMI?
What does it not tell you?

BMI is a simple method to indicate whether a person is underweight or overweight.
However the method can not tell us further information about other' body composition to make a further precise conclusion of healthy or not.

According to Ursula 2002, "BMI is an imprecise measurement of fat-free and fat mass".
It further explained "the weight changes are not related to the changes of the fat mass".

One of the good example from Halls 2003, is athletics increase the weight by muscle to over BMI 25 (suggested unhealthy), however, his body fat mass is low.
Furthermore, BMI is not precise in some areas such as different age groups and racism.

Teenagers usually have higher muscle ratio to fat; while aging people are in contrast, but the BMI remains a stable value during the age changing.
Asian people should have lower BMI value to achieve the "overweight" level, some would suggest BMI 23.

2. What does the waist: hip ratio not tell you about the distribution of body fat?
The storage of body fat mainly located at the abdomen and hip, therefore three fat body shapes, android (apple shape), gynoid (pear shape) and ovoid (fruit box shape) called.

However, fat tissue also store at other parts of body, like the thigh, the trunk and surround the internal organ.
Moreover, Waist: Hip ratio is a measure of the central fat distribution (centralized obesity), but it does not measure the blood lipid, (with blood glucose, insulin level) which now consider as a stronger indication of the health rather than using the waist: hip ratio.

Recent study from Lemos-Santos (2004) implied that "the fat location is still in controversies"; and correlation study among the waist: hip ratio, waist circumference and BMI to fatness, the WHR is "less dependent on fatness".
Skin-fold Measurement

3. Comment on the reproducibility of the measurements and what steps could be taken to improve reproducibility.
Skinfold measurement is a fair good reproducible method to predict the body fat except visceral fat.

It is because the instrument required is comparatively cheap and easily to train to use.
Furthermore, Wattanapenpaiboon N. (1998) implied skinfold measurement is slightly better than BIA in estimating % body fat in their study.

To improve the reproducibility, we could increase the number of measurements from five times to seven times, eliminate the largest and the smallest data, then take the average of the five remaining data.
Moreover, use two trained testers and each measure four times, remove the largest and smallest from eight measurements and take the average.

The relaxation of the patient, technique of the tester are also an advanced.
4. What would be the advantages and disadvantages of making skinfold measurements at four sites rather than the triceps measure only?

Four sites or even up to nine sites skinfold measurements greatly increases the accuracy, because fat tissue can varies in different parts of the body.
Imagine a labor who usually use his right hand to carry luggage, result the fat mass reduce.

Increase the sites of measurement should reduce the bias on one single site measurement.
However, skilled and trained tester is very important, because when the measurement is not accurate at four sites, the larger bias gained at last.

The formula to calculate the final fat mass is also important, it could compare the answers by using different formula.
Bioelectrical Impedance

5. How does a measurement of body fat by skinfolds differ from that measured by bioelectrical impedance?
The body content is simply divided into two main components, fat and non-fat content.

The non-fat content is mainly composed by water.
When electric flow through the body, the resistances of fat and non-fat content are different, through the processing, BIA calculates the fat mass content.

The non-fat content could be increased by high water content in body.
Therefore, intake or excreting water affects the water content in body, which influence the fat mass prediction.

On the other hand, skinfold is a prediction measurement, which mainly influenced by the tester technique; however, BIA is mainly affect by the water content in body.
Energy requirementsCalculate TEE and BMR (measured or prediceted) multiplied by your PAL.

FORMULA
6. How does your TEE (indicating your daily energy requirements if you are in balance), compare with your estimated daily energy intake?

My TEE data estimated by BIA is lower than formula predicted one.
A general estimation of male adult recommend consume 2000-2500 kcal daily, which varies between physical activities, racism and his basal metabolic rate.

Assume my TEE from BIA is more accurate; the TEE from formula prediction had a standard error 156 kcal of male group age 18-29.
Thus the formula is based on particular racism group, it could be generated from western diet people, therefore the bias occurs.

Moreover, the PAL value is important and it influences the final energy value.
In contrast, TEE from the BIA could be influenced due to the water content of user varies time by time, and finally estimate a inaccurate basal metabolic rate.

7. What particular adipose tissue deposit(s) is waist circumference measuring and how valid is the measure?
After meal, activation of insulin leads to synthesis of fatty acids and for further fat storage.

The adipose tissues for storage are mainly "white" form, which there are lots of fat cells located at the abdomen.
Waist circumference (WC) measuring is mainly at this site of white adipose tissue.

There are several studies suggesting and providing evidence to show the simple way of anthropometry - waist circumference is the best method to indicate the health or risk of disease(s) among waist circumference (WC), waist-to-hip ratio (WHR) and body mass index (BMI).
One of the study by Chan (2003) compared WC, WHR and BMI methods to investigate the intraperitoneal and posterior subcutaneous abdominal adipose tissue mass in men.

There are totally four sites to examine, which are IPATM, RPATM, ASAATM and PSAATM.
59 men were joined the study, 18 of them are over-weight and the other 41 are obese.

This study used Magnetic Resonance Imaging (MRI) to generate a group of data, as a standard, comparing the data with WC, WHR and BMI by statistic methods and generate the final results.
Chan (2003) stated "correlational analysis suggests that in men who are on average overweight-to-obese, waist circumference is a better predictor of the distribution of adipose tissue among several fat compartments in the abdominal region".

Thus WC had a stronger correlation in ASAATM and PSAATM than WHR; while WC also had a stronger correlation in IPATM and RPATM.
Finally, WC showed it is the best among three anthropometry methods by using MRI to make a standard reference for statistic calculation.

In addition, Chan (2003) claimed that "WC could be a surrogate marker of the distribution of adiposity in the abdominal region in men." WC is particularly providing a better result in overweight or obese groups, however, other age groups, even other racism groups require more study to give a certain result.
WC is a very simple and cheap method, which the bias is very low and does not require training to start the measurement; Chan (2003) also claimed that is "reliable".

has recently carried out a study on the daily diet of 86 students (23 males, 63 females; aged 19-38) majoring in nutrition by the Food Frequency Questionnaires (FFQ) method.
Students almost can consume food following the dietary reference values (DRV) of the UK.

However, there's not much difference in most of the dietary values between the students and the general UK residents (Hendersonal.
2003) except that the students have a less satisfactory result in the consumption of macronutrients (Table 1).

FORMULA ^ UK mean is from the NDNS 2003 report, the mean of the "total" age groups There are totally 36 diet intake items have been generated the mean and standard deviation values.
Those values are then categorized into male, female and total groups.

The diet intake items included macronutrients like total fats, carbohydrate; and micronutrients like calcium, iron and vitamin A-E.
Among all those diet intake items, male group usually have greater values than the females due to greater energy requirement and consumption of them.

The only exception is that men ingest (71.3mg; mean) slightly less vitamin C than women (72mg).
Macronutrients

Protein, carbohydrate (sugar, starch, fibers), total fats (saturated, monosaturated, polysaturated, cholesterol) and alcohol are the main contributions to the total energy intake (excluding fiber).
Ideally, around 55-60% and 30-35% of energy should come from carbohydrates and fats, while only 10% should come from protein.

The group of total expressed quite an ideally ratio in the energy intake with around 46%, 31%, 17% and 6% of energy from carbohydrate, total fat, protein and alcohol respectively.
High energy, fat, cholesterol, alcohol and protein intake in male group

From table 1, the mean of male energy intake is 2536 kcal while standard deviation is 464 kcal.
In other words, more than 50% male intake 2536 kcal daily; and 16% of them even intake more than 3000 kcal per day.

The energy intake of male students is higher that the recommended value for men - 2500 kcal per day.
When the input of energy is more that output, excess energy will be converted to fats for storage.

The increase in fat reservation will raise the risk in the incidence of obesity, diabetics and coronary heart disease (CHD).
The total fat intake of male (35.8% of total energy intake) exceeds the recommended values (<35% of total energy intake).

Therefore, the male students are advised to reduce the fat intake.
Furthermore, the male group consumed cholesterol (481mg per day) more than the recommended intake (<300mg per day) by the American Heart Association, which also suggests people with high blood total cholesterol level (>200 mg/dL) should ingest less than 200mg cholesterol per day.

High cholesterol intake is not the primary cause of high blood cholesterol (high fat and carbohydrate intake do) (NHLBI, 2005), but it does play a role in it and thus it's necessary to reduce the intake of food having high cholesterol level.
Besides, to reduce the total fat intake, male group could reduce cholesterol intake, also the saturated fat intake, from ~15% of total energy to ~10%.

Alcohol consumption of both male and female groups is about 6% of total energy intake, but male' daily intake is 24.5g which is above the daily recommended value - 20g (equal to 1.5 pint beer or 2.5 glasses (312ml) of wine).
Although alcohol' energy won't put on weight, young man should not drink too much because alcohol overdose can cause liver disease, gout and other diseases.

Moreover, wine is beneficial to middle aged people rather than young people in reducing CHD or mortality rate (Wright 2006).
Both male and female groups consumed more protein than the recommended ones.

Male had 108 gram per day which the RNI is 55.5g.
Female consumed 81.2g per day and the RNI is 45g.

Reducing meat intake could result in reducing protein consumption dramatically; also reduce the total energy intake.
Micronutrients

All multi minerals and vitamins except vitamin E are within the normal recommendation consumption level.
No deficiency or overdose occurs from the study.

Low vitamin E consumption
NDNS 2003 showed the vitamin E intake for UK male and female is 10.6mg and 8.1 mg respectively.

In contrast, our study showed that male only intake 4.7 mg (mean) while female only intake 3.3mg per day.
Both the level is merely more than the UK safe intake (male > 4mg; female > 3mg).

Although there is extremely low incidence of vitamin E deficiency in UK, students should consider to intake more vitamin E from their diet, such as through potato, wheatgerm, nuts.
Food Frequency Questionnaires method used in the study

FFQ is a retrospective survey that examine the past (our study is 7 days) food nutrient intake.
A food lists fill up by the subjects will be processed by the food database to generate the diet intake results for further interpretation.

FFQ is a quick nutrient intake examination method that it's also inexpensive and could assess current or pass diet (Grimble 2006).
However, FFQ' food list only covers the simple components of food products.

If a subject intakes a variety of food, it will be hardly to fill in the FFQ.
Error occurs also as subjects usually "underestimate of their intake, especially the macronutrients - carbohydrate and protein" (Paulal.

2005), also errors may come from poor memory.
On the contrary, (Paulal.

2005) also claimed "subjects with higher energy intake tended to over report their intakes of protein and fat".
Another error we may consider would be the number of the subjects.

In our study, male group only has 23 subjects, therefore, a larger variance may appear.
Thus "small number of subjects may not reliable enough" (Paulal.

2005) to represent the fact.
All in all, no matter the errors would pull up or down the figures in table 1, our subjects should pay attention to the items we discussed and to make a healthy eating habit.

calculate the iron content of the beverage using the calibration plot
The reading value of the diluted sample (100 ml) is 92.

According the formula y=41.291x, the concentration of iron in solution of diluted beverage is 2.23μg/ml, so when we calculated back to the original drink the concentration of iron is 4.46μg/ml.
calculate the iron content of the drink by the standard addition method

Due to the concentration of iron in sample is 4.46μg/ml by first method, in 50 ml sample should content 223μg iron.
So X=223μg/ 25μg/ml=8.9 ml.

so added 0, X, 2X, 3X ml of stock iron solution into A, B, C, D labeled flask respectively.
Using the ratio is constant to calculate the concentration of iron in drink The calculation formula: 95.55/Z= (184-94)/223, where Z is the iron concentration in beverage; Z=236.8 μg which means there is 236.8μg iron in 50 ml drink sample, so using the standard addition method, the iron concentration in beverage is 236.8μg/ 50ml = 4.73μg/ml.

Compare your results and comment on them.
How do they compare to the iron concentration declared on the label?

Comparing the two methods, the standard addition method is better than calibration plot.
Due to the fact that in the first method, we used standard iron contented solution to draw the calibration curve, and separately determine the sample reading of atomic absorption spectrophotometry which contents the matrix interference can affect result.

But when using the second method, standard addition method, in each flask contented same volume of sample (50 ml), so this method can minimum or very often eliminated the interference and get more reasonable answer.
On the label showing the concentration of Ammonium ferric citrate is 0.002% (0.002 g/100ml).

And the structure of Ammonium ferric citrate is C 6H 11FeNO 7.
The molecule weight of Ammonium ferric citrate is 265 and Fe molecule Molecule weight is 56, so in sample the concentration of iron is 4.23μg/ml.

Explain why atomic absorption methods are mainly using for the analysis of transition metals, while flame photometry is used for alkali metals.
Pomeranz (1987) stated the heat energy from flame can raise an electron in some atoms group jump to an excited state, if the excited electron drops down back to ground state from its original excited level in one jump, the radiation given off is called resonance line.

Low flame temperatures (900-1200 oC) are used so that only easily excited elements such the alkali and alkaline earth metals produce emissions.
This results in a simpler spectrum and reduces interference from other elements that may be present (Nielsen, 2003).

The definition of transition elements are the elements in the 'd-block' of the periodic table (Hill, 2000).
When determining transition metal need more energy provided for exciting electron from ground state to excited level.

In atomic absorption spectroscopy, the most commonly used fuel/ oxidant system is air-acetylene and over 30 elements can usefully be measured using this combination of gases, and this temperature of this flame is of the order of 2300 oC which is great higher than flame photometry (King, 1978), so the atomic absorption methods can excited the transition metals due to the wavelength from an extend source and can provide higher energy.
Yogurt as a kind of traditional food has a long development history.

Nowadays, with the evaporation of food technology, yogurt has been improved in many aspects.
The fertility of nutrient and special texture is the main reason for why yogurt becomes a popular food.

This essay will briefly introduce the processing manufacture and focus on the evaporation of microbiology of yogurt.
There is no available record about the time of origin yogurt, but it is believed that yogurt is an important fermentation food operated by human.

The name of yogurt came from a Turkish word "Jugurt" means "a product resulting from milk by fermentation with a mixed starter culture".
(N shah, 2003).

The processing of making yogurt can be divided into the follow steps:1) preparation of mix(standardization, fortification with skim milk power); 2)homogenization; 3)heat treatment; 4)cool to incubation; 5)inoculation with starter; 6)pack in retail container; 7)incubate; 8)cool.
In the following, this essay will give some details about the above steps.

Preparation of mix
Traditionally, people use boiling the mike to increase the viscosity of product.

(N shah, 2003) Due to the reason is that by heating the β-lactoglobulin structure opens and expose which is a suitable structure to combine with κ casein protein or αS2 protein to form a new structure which can hold water that is why milk can be fortified by heating.( A.Y.
Tamime,1985).

Nowadays, with the development of food industry technology, stabilizers can be added into milk to increase the viscosity.
Homogenization

The aim of homogenization is to avoid cream producing during the fermentation processing.
Due to the reason that the diameter of fat globules in milk ranges from 1 to 15 μm, with a average of 3-4μm.

After homogenization the diameter of fat globules can be 1-2μm by destroy the membrane.
In case of lipolysis, if homogenization before pasteurization, the milk must be pasteurized immediately.

(N shah, 2003)
Heat treatment

The purpose of heat treatment a) to kill the pathogenic micro-organisms which may be present; b) to kill the majority of other organisms in order to increase the keeping quality; c) to in activate naturally occurring enzymes.(J L. Rašic 1978)
Cool to inoculation with starter

This step is most important to yogurt manufacture processing.
Cool the temperature to 42° in which temperature is suitable for microbiology growth.

The first bacteriological study of yogurt was in 1905.
Until 1974, Bergey, D.H gave the classification of microbiology in yogurt in two separate families, the Streptococcaceae and the Lactobacillaceae.

With the development of research, the growth association between the two organisms (S. thermophilus and L. bulgaricus) of yogurt starter culture is termed a symbiosis.
Pette and Lolkema (1950) indicated the rate of acid development was greater when mixed yoghurt cultures of S. thermophilus and L. bulgaricus were used as compared with the single strains.(Pette,J.W.

1950) By Tamime (1977) indicated the rate of acid development % lactic acid of L. bulgaricus after 8 hours incubation is 0.55 % and the rate of S. thermophilus develop lactic acid reached 0.7 %.
But the mixed culture is more effectively which reached 1.2 %.

The discussion about the symbol lasts many years.
Bautista, Dahiya and Speck (1966) also investigated a theory about this phenomenon.

They hold the point that S. thermophilus and L. bulgaricus by releasing gliding and histidine in to the growth medium and the histidine played a more important role than valine.
The discussion did not stop, with the more research scientists did, the more theories about the symbiosis had been found.

People began to observe the active of microbiology to explain the symbiosis phenomenon.
According to Bottazzi with his colleagues (1971) found that the presence of formic acid which was considered as a stimulatory in milk can improve the ratio of rods to cocci at concentrations between 30 and 50 γ/ ml.

So conclude the above findings, the release of stimulatory factors by yogurt starter cultures occur during the incubation period, and the factors can accelerate the rate of producing lactic acid effectively.
In most fermenting processing, temperature should not be ignored.

Suitable temperature can keep the ratio of ferment in a high level.
Most microbes have different suitable growth temperature.

In yogurt industry, the S. thermophilus and L. bulgaricus as the important microbe also have different suitable temperature.
In 1977 Tamime, A.Y found the rate of acid development of S. thermophilus and L. bulgaricus increase with increase in incubation temperature up to the maxima of 40° and 45° respectively.

And he recommended that, in order to maintain and / or achieve a ratio of 1:1 between S. thermophilus and L. bulgaricus, the organisms should be propagated to together at 42° using a 2% inoculation rate.
(Tamime,A.Y,1977)

Pack in retail container, incubate, and cool
The followed step is package the yogurt and keeps the product's quality without spoilage.

The equipment of manufactory should be hygiene to prevent foreign object attach the products.
The kind of yogurt should decide the package methods.

To keep the flavor of yogurt, the way of store, transport also should be considered.
To sum up, the delicious and nutrient yogurt has to be produced during long producing process.

With the development of food industry, people become more and more understand the theory of milk ferment and more and more use those theories to add the flavor and change the texture of yogurt.
In the future, people will using the knowledge to produce more delicious and more nutrient yogurt.

Objectives:
Different packaging films have different functions due to the properties of packaging materials are varied.

In the practical, we intended to research the differences between the films provided by using simple tests and observing the changes of the samples wrapped by different packaging materials during 7 days.
Introduction: Anon (1988) stated "the packaging as the enclosure of products, items or packages in a wrapped pouch, bag, box, cup, tray, can, tube, bottle or other container form to perform one or more of the following functions: containment, protection and/ or preservation, communication and utility or performance".

Packaging provides a barrier between the food and the environment.
And it control light transmission, the transfer of heat, moisture and gases, and movement of microorganisms or insects (Fellows, 1988).

Light transmission is employed in packages that are intended to display the contents and provide a platform for labeling.
But the light transmission is restricted when foods are susceptible to deterioration by light, such as the oxidation of lipids, loss of color and denaturation of protein.

For example low-density polyethylene (LDPE) transmits both visible and ultraviolet light, polyvinylidene chloride transmit visible light but absorb UV light.
Packaging should also have certain level of heat resistance, especially when packaging hot food then package shrinking may occur.

The permeability of the packaging material to gases and moisture and the packaging procedures employed can influence the type of microorganisms that grow within the package.
For fresh food, such as fruit, vegetable and meat, which still respire after harvesting, the water vapor may condense inside the packages if the existed water can not be vapor out of the package that may lead spoilage occur.

At the same time, the gases produced by content such as the carbon dioxide, ethylene, have double-side influences.
On one hand, the high density of carbon dioxide in the package that can limit the microorganisms growth to achieve the extend shelf-life, on the other hand, the gases may deteriorate or over harvested the food.

By researching, the water existing in the package may occur interaction with the certain film and generate hazard compounds when the environment of outside have changed, such the high temperature, high moisture.
So the consideration of what kind of materials can run the job for individual food should be prior to the packaging.

Another essential property of film is mechanical strength which is the ability of packages to protect foods from mechanical damage had been measured in this practical.
The factor of material mechanical strength is influenced by the temperature of material and the length of time that the force is applied (Briston, 1974).

The materials of package are varied.
In this practical, we chose paper, regenerated cellulose 350 MS, regenerated cellulose 340 D.M.S, polyethylene, polypropylene, aluminium foil and Cryovac S film whose permeability of water vapor, violate compound and the tensile strength had been tested.

Paper almost used for food packaging is made from wood.
Some papers are made from repulped waster paper which increases the incident of contamination of microbes and the paper may give off odors which are likely to cause tainting of the contents (Brennan, 2006).

In food manufactory, the coated papers are more commonly used.
For example, aluminium coated paper enhance the barrier property of water vapor and gases.

Regenerated cellulose (cellophane) differs from the polymer films in that it is made from wood pulp.
It can provides general protect against dust and dirt, some mechanical protection and is greaseproof.

When the environment is dry it is a good barrier to gases, but becomes highly permeable when wet.
The codes are used to reflect the property of regenerated cellulose films: D: coated on one side only; M: moistureproof; S: heat-sealable; DMS: nitrocellulose-coated on one side only (Brennan, 2006).

Polyethylene (PE) commonly called polythene which can be divided into LDPE, MDPE and HDPE with different density.
LDPE have most widely used in the food packaging due to its strength, low permeability to water vapor and it forms a very strong heat seal.

But it is not good barrier to gases oils and volatiles (Brennan, 2006).
Polypropylene is a clear glossy film with good optical properties and a high tensile strength and puncture resistance.

It has moderate permeability to moisture, gases and odors, which is not affected by the changes in humidity ((Fellows, 1988).
Aluminium foil is in a good appearance, dead-folding and the ability to reflect radiant energy and an excellent barrier to moisture and gases.

Cryovac S film: Cryovac feature a full range of barrier properties and toughness to meet each product requirements, with added value features such as easy-opening on transversal or end seals, plain, pigmented or printed, taped, non-tapped or roll stock, others provide controlled permeability rates.
Value added features, such as easy-opening and re-closing systems, are also available (Sealed Air, 2007).

Method:
Rapid tests for the identification of film packaging materials

Water drop testWater tap testBreath testBiting testBurn testMelt testShrink testHalogen testColor testStretch test - Details refer to Food Processing Practical handbook, p29-31, 2006,, Food Science department.
Tensile Strength & Extensibility

This test should be carried out using the Stevens machine.
The method is most suited to paper and film materials that do not stretch much under stress.

The sample is 100 mm length and 25 mm width and 50 micro meters thick.
The sample will be extended for a maximum of 100 mm at a rate of 1 mms -1.

A recorder will record the data and draw a graph with load against time.
Water vapour permeability

Weigh the dishes which contained dry calcium chloride that can absorb moisture though the packaging materials which cover on the top.
After hours, the weights of dishes were varied due to the water vapour permeability different with different packaging materials.

Packaging and storage of fresh fruit
Using Re-cellulose 350 MS, and the same material with different size holes to packaged graph to observe the changes during 7 days.

Packaging and storage of fresh meat
Using 4 kinds of packaging way involved polypropylene, Re-cellulose 350 MS, Re-cellulose DMS and paper to packaged fresh meat and observe the changes during 7 days.

Packaging and storage of potatoes
Using polyethylene and the same material with different size holes to package potato to observe the changes during 7 days.

Packaging and storage of cheese
Using 3 kinds of packaging way involved polyethylene, Re-cellulose 350 MS, foil and Cryovac S film to package cheese and observe the changes during 7 days.

Permeability to odours and flavours
Using 6 kinds of packaging way involved polyethylene, Re-cellulose 350 MS, foil, polypropylene and original packaging to seal chocolate and seal them in a can with mint oil and observe the flavour changes after 7 days.

Result:
Tensile Strength & Extensibility

Water vapour permeability
Packaging and storage of fresh fruit

Note that no alteration was observed on the packages or the fruit inside the first two times (i.e. Wednesday and Friday), but on the last day we observed a slight amount of moisture inside the 350 MS Heat Seal, and an even less amount of moisture inside the 350 MS 8x5 mm.
Packaging and storage of fresh meat

Packaging and storage of potatoes
Both potatoes began sprinting but the package without hole has more sprout than that with hole.

Packaging and storage of cheese
3 means vacuum packaged in PE + heat seal ; then another vacuum packaged with reg.

cellulose 350 MS
Observations on cheese

Sample 1: reg.
cellulose 350 MS + heat sealed

Sweating started to appear on day 1 after packaged and continued to occur as the time pass by.Oiling could be noticed first on the 2 nd day after packaged.The cheese started to dry on the 4 th day.With the loss of vacuum was clearly seen on the 3 rd day.No sign of mould growth.First sign of drying out on the 4 th day.
Sample 2: reg.

cellulose 350 MS + vacuum packed + heat sealed
Sweating, oiling, loss of vacuum could be noticed on the 1 st day after packed and increased as the time pass by.Mould growth started to be seen on the 4 th day but with the same amount on the 7 th day after packed.First sign of drying out on the 4 th day.

Sample 3: vacuum packaged in PE + heat sealed ; then another vacuum packaged with reg.
cellulose 350 MS over the already packed sample.

Oiling started on the 1 st day and continued but very smallest oiling.The loss of vacuum could be observed on the 1 st day with relatively small compared to other vacuum packs.
No further loss of vacuum was noticed.No drying out, no sweating, no mould growth.

Sample 4: Cryovac S film + heat sealed
Sweating started on the 1 st.Oiling started on the 2 nd day and continued as well as sweating.No mould growth.

Sample 5: vacuum packaged in Cryovac S film
Sweating, oiling and loss of vacuum were observed in the 1 st day and continued to occur.Marked loss of vacuum compared to others (sample 2,3).No mould growth.

Sample 6: wrap in aluminium foil
Oiling and sweating started on the 1 st day and continued through out.First sign of mould growth on the 4 th day and lots of mould on the 7 th day.

Sample 7: unpacked
Sweating started on the 1 st day.On the 2 nd day: oiling and drying out were first observed.Sweating, oiling and drying out all continued through out the observation.Marked drying out compared to others.No sign of mould growth.

Hardness at the end of observation (7 days afterwards): 7 > 2 > 1 > 3 > 4, 5, 6 Oiling at the end: 4, 5, and 6 > 1, 2 > 3, and 7.
Best quality: sample 3 Worst quality: 6 because of lots of moulds

Permeability to odours and flavours
Discussion:

Tensile Strength & Extensibility
Compared the results of three films: polypropylene, polyethylene and Reg Cellulose 350 M.S.

Obviously, polypropylene has a high tensile strength and puncture resistance.
Its load was 186.47 Newtons which was the highest in this test; the data was similar with the reference value the tensile strength 145-200 Newtons (Fellow, 1988).

In test, the polyethylene sample did not tear and the stretch extensibility was more than 100 %.Due to the highly linear arrangement of molecules and the distribution of molecular contributed the flexible, tough tensile strength and puncture resistance of property of polyethylene.
Reg cellulose was frail during the tensile strength when the load was 85.2 Newtons, and the stretch extensibility was 8 % which was the lowest in the test.

The various properties decide the functions such as polypropylene is thermoplastic and therefore stretches, and has low friction, which minimizes static buildup and makes it suitable for high-speed filling equipment.
Polyethylene is suitable for shrink-wrapping.

Water vapour permeability
Using the data collected related to the water vapour permeability of 4 kinds of packaging material to draw a curd.

From the trend of curd, it is significantly weight increase of Reg cellulose plain that means the reg cellulose plain had high water permeability that correspond to the reference value that is 400-275 ml m -2 per 24h in 23° 85% RH.
The weight of dish and content which was covered with reg cellulose increased 12.25 %.

Other dishes had not dramatic increase and maintain stable during 7 days.
The moisture vapor transmission rate is the main factors that control the shelf life of dehydrated foods.

So the reg-cellulose is not suitable for foods that require a complete moisture or gas barrier.
Packaging and storage of fresh fruit

Observing the weight changes of graph packaged by 5 kinds packaging methods, we could find a vivid phenomenon that the graphs packaged in 250 Polyethylene just lose weight about 0.37 %, in another word, the change was minute and the polyethylene is good moisture barrier.
There was 2.7 % weight lost from graphs packaged with 350 M.S cellulose without holes, and the data could be explained that some leaks may exist on the seal and caused the moisture vapor exit from package.

But when turn to the package made from same material 350 M.S cellulose with hole, the percentage of lost weight were 4.07 % and 3.9% in the packages with 2 mm × 5 mm size hole and 8mm ×5mm size hole respectively.
And the proportion were similar about 4 % weight lost that means the moisture transmission rate can be upgrade significantly when provided holes on the package that is crucial for the contents are sensitive and easy-spoil in humidity environment such as graph, tomato, which will be spoiled on the surface growing microorganisms in a wet surrounding.

We also compared the weight change of graph in the same package material, 350 M.S cellulose, with same size hole.
But the gage was different.

One was sealed normally and another was sealed severe.
The weight change were 4.07% and 5.35% respectively, in other word, the effective of proof moisture vapour with different seal method was minute in this practical.

Packaging and storage of fresh meat
In this practical, we adept fresh meat as the sample to determine the changes involved the discoloration, lose of weight, condensation and spoilage during 7 days with different package films.

Normally, the compositions of meat are water, fat, protein and several trace minerals.
And the proportion of water is 75%.

During 7 days, the paper covering meat lost weight about 44.18%, and meat became dry and contracted.
The color of meat changed from cherry red to dark red and although there was not significant green patch found, we could not evaluate the meat packaged in paper that was not spoiled due to the meat was exposed in air where contain lots of microorganisms can lead the meat contaminated.

Base on the result, the paper just can package ready-to-eat foods that do not required storage for long period.
Polypropylene was good moisture barrier, so even during the 7 days, the weight had no significant change that means there was no moisture or litter moisture go through the polypropylene into environment, at the same time, polypropylene was low oxygen permeability, so the intensity of oxygen in the package was limited that avoided the oxidation interaction inside the package and contributed the quality better than other samples.

But we could find lots of water condensed from meat could not be emitted from package that may lead the meat total became green in the second Monday morning.
So the polypropylene is not suitable for foods contained lot of water that may lead alternation.

By compared the percentages of weight lost of meats packaged by 340 D.M.S reg-cellulose and 350 M.S reg-cellulose were 10.05% and 6.21% respectively.
The 350 M.S.'s property is moistureproof that had been proved by the data of this test.

But the meat's quality can not maintain satisfactory after packaging 3 days, some green patches had been found caused by oxygen that can transmission though cellulose into package.
The 340 DMS packaged meat had been determined spoil in the 2 nd day.

There was a green patch about 1 cm diameter in the center of meat.
So cellulose is used for foods that do not require a complete moisture or gas barrier.

Packaging and storage of potatoes
Using polyethylene and polyethylene with holes to package potatoes and observed the changes in 3 different environments.

No matter in fridge at 4° or in shelf at room temperature or in dark box at room temperature, polyethylene without holes had less water lost, in another words, polyethylene is a good water vapour moisture barrier.
The holes on the surface of package accelerated the moisture evaporation that leaded more than 67% weight lose compared with polyethylene without holes.

Compared the weight lost in fridge about 4 ° with the weight lost in room temperature about 25°, the previous had less sprouting than the later, the one reason may be the low temperature decreased the biochemical reaction in the potatoes and prolonged the shelf life and limited respiration rate.
The potatoes stored in warm temperature sprouted quickly than in the low temperature and light also contributed the color turn to green.

Permeability to odours and flavours
Compared 6 different packaging material to range the permeability to odours and flavours, we could found, if set no packaged chocolate was the standard of strongest mint flavours, polyethylene was greater than cellulose and polypropylene and foil.

In principle, polyethylene should have lower odours permeability than cellulose, I thought the reason contributed the result was the seal of polyethylene was not very hermetic, so the mint odors leaked inside and affected the plain chocolate flavours.
The original packaging was good odor barrier which was associated with double membranes including foil and cellulose so less mint odor in the chocolate after 7 days.

Packaging and storage of cheese
In this test, cheese had been package with 5 different methods, and we determined the changes about the sweat, oiling, mould growth and moisture during 7 days.

The result showed us that in each package, the cheese all sweated but the changes occurred in different days with different material that means all material have certain level gas permeability, and the vacuum packaged in PE and heat sealed, then another vacuum packaged with reg.
cellulose 350 MS over the already packed sample had the less vacuum lose compared with others.

Foil packaged cheese grew lots of mould may caused by the leaks between the foil.
Cryovac S film packaged cheese oiling at the 2 nd days but Cryovac S film limited the mould growth on the surface of cheese.

How much weight should I expect to gain?
During the period, even before conception, weigh is an important factor should be considered.

How much weight you should gain depends on your current weight.
If your Body Mass Index (BMI) which can be calculated in the follow formula: FORMULA If your BMI is more than 120, I suggest you gain 7.5 Kg during the pregnancy, if your BMI is between 80-120, I think 9 Kg is a desirable weight you should gain.

And if your BMI is less 80, you need about 13.5 Kg during the pregnancy.
More or less weight you gain is harmful for you and your baby.

Due to the reason is that during the pregnancy, extra energy is needed which is recommended 150 Kcal per day in the 1 st trimester and 350 Kcal per day during the 2 nd and 3 rd trimester for your body metabolism and the foetus growth.
More weight gain will increase risk of gestational diabetes, pre-eclampsia, large babies, high blood pressure and long term obesity.

If the weight gain is lower than 65 Kcal that may lead the low weight baby which associated with higher incidence in adult life.
Such as high cholesterol levels, type 2 diabetes.

Should I aim to eat twice as much as normal?
No. Although during the pregnancy, mothers need more energy and nutrients to supply individual metabolism and for the foetus that is widespread layperson's idea that the mother should "eat for two", actually it means mother should eat for herself and the foetus and it does not indicate mother intake double meal, too much energy intake may increase the incidence of risk of gestational diabetes, pre-eclampsia, large babies, high blood pressure and long term obesity.

Current estimated average requirements (EARs) for energy intake in pregnancy is pulsing 200 Kcal/ day on the value of normal energy intake.
So what you can do just increase energy intake or reduce physical activity during the last trimester.

Do I need extra iron?
Generally you need extra iron intake, but before I give the advice I have to know you have iron overload family history?

If the answer is yes, I suggest you take less Fe supplement than normal.
Because if you can iron overload, the excess Fe may increase the risk of series diseases, such as liver cirrhosis, hepatocellular cancer, pancreas diabetes.

If you have not iron overload family history, the supplement of iron during pregnancy is needed.
The effect of pregnancy on iron metabolisms which are associated with plasma volume, red cell mass is important to women reproductive years.

The net additional iron requirements during pregnancy are estimated to be 1040 mg.
Availability from different food sources is a key issue.

Cereal and meat is recommended for iron supplement.
Do I need to take any vitamin supplement?

Yes, Vit D, Vit C, Vit A, folate and thiamin and riboflavin are recommended during the pregnancy.
Vit D has the property that can be synthesized via sunlight, but the pregnancy women may reduce the activity outdoor.

Improved via A status can improved fetal growth and reduced xerophthalimia.
The major role of folate is one carbon unit metabolism, a key process in the synthesis of the purine and pyrimidine bases of the nucleic acids.

What should I avoid eating?
Yes, you should avoid eating raw fish, liver, soft eggs and unpasteurized cheeses during pregnancy which may contain the parasite and bacteria.

Fish containing accumulated levels of mercury in their fatty tissues, such as shark, swordfish, king mackerel, and tilefish, when a pregnant woman consumes large amounts of mercury, her baby may suffer brain damage resulting in developmental delays (for example, delays in learning to walk or talk).
Remember, everything before or during pregnancy should be done in moderation.

One way to determine the wholesomeness of the population is the dietary survey which reflects their energy and nutrients intake.
The recent survey on the students at (86 students aged 18-38 years old: 23 males and 63 females) during the first week of the autumn term of the 2006/2007 academic year, using the Food Frequency Questionnaire (FFQ) method, revealed the following data.

Despite their normal total energy intake, the saturated fatty acid intake of both men and women (15% and 12% of the total energy, respectively) is higher than the guided value 10% whereas the polyunsaturated fatty acid intake was more or less reached.
Females rather than males seemed to have eaten enough carbohydrate (~48% of total energy intake) while protein intake in most of the students was more than enough to keep them from deficiency.

All minerals intake statuses were fine in males except sodium and selenium.
Excessive sodium intake was nearly double the amount they should have eaten (~1,300 mg more) while there were some that did not get enough selenium.

The problem seemed to lie in female intake since only sodium, calcium, phosphorus, zinc and iodine were adequately eaten.
None of the female subjects reached the guidelines for potassium and most of them did not get enough magnesium and copper from their diet.

Some might get only half the amount of selenium they should be receiving while the iron intake too was not adequately eaten (10.2 compare to 14.8 mg/d).
Vitamin A deficiency is unlikely to occur since the high carotene intake could provide a substantial amount of vitamin A by converting to retinol in the bodies, compensating the low retinol intake (483 and 263 mg/d for male and female, respectively).

Little risk of vitamin E deficiency is expected since the intake amount exceeded the amount that would prevent such condition.
Thiamine, riboflavin, niacin, vitamin B6 and B12, vitamin C and folate were consumed in the amount more than enough to prevent from any risk of deficiency.

For pantothenic acid and biotin, the intake of both nutrients lied within the safe ranges of 3-7 and 10-200 μg/d, respectively.
Alcohol consumption in both men (24.1 g/d) and women (15.7 g/d) were higher than the average UK intake (22 and 9 g/d, respectively).

But the actual consumption of university's students is likely to be higher.
From table 1, phosphorus is the maximum minerals eaten by both men and women.

The high amount of intake probably due to the fact that phosphorus exists in virtually all foods especially in dairy products, cereals and vegetables, and meats (20-30%, 25-35%, 25-35% of daily phosphorus intake, respectively).
High phosphate food, i.e. processed foods and the carbonated soft drinks consumed by teenagers could significantly increase their phosphorus intake.

About 60-70% of dietary phosphorus intake is efficiently absorbed at the small intestine.
Approximately 85% of phosphorus in our bodies exists in the bone where it forms a complex with calcium, giving bone rigidity and strength.

The remaining is in the soft tissues and blood as phospholipids, phosphoproteins, nucleic acid and as part of the energy-rich compounds associated in our metabolism.
Deficiency is rare since phosphorus occurs in most food and body can increase the absorption and decrease the excretion rate accordingly.

But excessive level of phosphorus in plasma could contributes to the reduction in bone mass by increasing the parathyroid hormone level which in turn promotes the resorption of calcium from the bone.
However, this is likely to occur only when calcium intakes are low which apparently not the case in this survey.

After all, most of the students had enough nutrients to keep their body healthy and away from diseases.
No nutrients appeared to be over consumption to cause the adverse effect.

Even though the results were not 100% accurate and tended to be overestimated as they rely on the individuals' report of their food but FFQ is considered an appropriate method for dietary assessment for such a large number of respondents and in a short period of time.
How much weight should I expect to gain?

The average weight gain in British women is 12-13 kg.
Of which, around 7.2 kg results from the maternal physiological adaptations (e.g. increase in uterus weight, fat, breast) and the rest is from the products of conception (e.g. on average 3.3 kg foetus, amniotic fluid).

But the weight gained is also varying with the pre-pregnant weight, i.e. if under weight: 12.5-18 kg, if over weight: 7-11.5 kg and if obese: at least 6 kg weight gain.
Your weight gain is also associated with your baby birth weight.

Gaining more than 6.5 kg could reduce the risk of having a Low Birth Weight baby (weight<2.5 kg).
But gaining too much weight may result in many conditions afterwards such as prolonged labour, birth injury, pregnancy-induced hypertension or obesity in the mother.

Should I aim to eat twice as much as normal?
No, you should not.

Although there may be an increase in the amount of diet taken but the word 'eating for two' is exaggerated.
Pregnant women can eat as normal until entering week 29 th of pregnancy onward that the energy needed will be an extra 200 kcal/day.

However, this extra increment is mainly for non-protein diet (e.g. carbohydrate); because the normal protein intake in western women is exceeds the requirement for pregnancy.
Moreover; there are physiological adaptations and behavioural changes that will recompense the increased requirement of energy needed.

Do I need extra Iron?
If your iron status is normal prior to conception, the answer is no, you do not.

Although, during conception, there is an increasing in the amount of your plasma by 1.5 L, thus diluting your haemoglobin concentration (which could leads to iron-deficiency anaemia), and leads to an additional 900 mg (2-4 mg/day) iron, but this increment could be met by the maternal adaptations i.e. utilization of maternal iron stores, increase in the efficiency of small bowel iron absorption.
The cessation of menstruation also contributes to iron conservation as well.

In addition, there are evidences that iron supplementation could alters the shape of the red blood cells so lower their oxygen binding capacity but rise the blood viscosity eventually reduce the rate of placental perfusion, thus affect the nutrients sent to the baby.
Do I need to take any vitamin supplements?

Yes, you do.
Although many vitamins are suggested extra increment, most of them can be met by normal intake and maternal adaptations except the three, which are vitamin D (10 µg /day), vitamin C (extra 10 mg/day) and folic acid or folate (extra 100 µg/day).The appropriate amount of Vitamin D is crucial for Ca balance and absorption in bone.

Normally, people do get enough vitamin D by body synthesis from 7-dehydrocholesterol in their skin when exposed to sun light but the reduction in pregnant women activities may decreases the chances of sun exposure and leads to the necessity of supplementation.
Though vitamin C deficient is scarce in pregnancy, supplemented is preferred as the foetus uses vitamin C from maternal store, mainly during the final stages of gestation.

Many births are affected by neural tube defects (NTDs), a group of conditions that brain, spinal cord, skull and vertebral column fail to develop properly during the foetal embryonic period.
This risk can be reduced by supplementation of folate.

Some research shows 72% protective effect of folate supplementation against NTDs.
The extra in take of folate also assists in foetal new tissue formation as well.

What should I avoid eating?
Alcohol, retinol containing food (e.g. liver, pâté), soft cheese and non-pasteurized dairy products.

Alcohol consumption could leads to Foetal Alcohol Syndrome (FAS) which causes babies to become small, facial abnormalities, mentally retarded, immunodeficient and possibly slow growth after birth.
Large amount of retinol intakes causes the malformations of the embryo or foetus and eventually birth defects.

Pregnant women are vulnerable to Listeriosis than normal people.
This disease is caused by Listeria monocytogenes, which is detected in soft cheese and unpasteurized milk products.

The complications include blood infection, meningitis in mother and many detrimental effects to the baby (e.g. miscarriage, premature birth, blood infection, meningitis and death).
Even though fish is very nutritious but the particular group of fish should be avoided which are marlin, swordfish, shark, tuna (including canned) due to the methyl mercury contamination, which can be absorbed across the placenta to the foetus, resulting in many severe symptoms e.g. sensory problems and mental retardation.

When speaking of spore many will think of the fungal spores which cause bread to become fluffy.
But another kind of spore that is quite more interesting is the endospores.

They are produced inside particular kinds of bacteria.
This spore is different from the fungal spore in that it is not a means of reproduction.

Unlike fungi, bacteria will only produce only single endospore and die, leaving the endospore waiting for another chance to grow.
The generation of endospore and there components are quite interesting and significantly complicates the food industry.

The formation of endospore is called the sporulation, after such period the spore will remain dormant until further activated, causing germination and outgrowth.
Sporulation occurs when environmental conditions are unfavourable for the growth of the vegetative cells e.g. the nutrients deprivation or the extreme environment.

The process starts when the stationary phase of the microbial growth is reached and takes about 8 hours to complete (Dahl, 2000).
It begins with the inhibition of cell division by external signals followed by signal transduction within the cell and then the asymmetrical partitioning of the septum.

The 7 indistinct steps of sporulation based on the cytological changes detailed by Setlow and Johnson (2001) and Gould (2000) and Dahl (2000) could be summarized as follow.
First, the appearance of asymmetrical septation in the mother cell gives rise to 2 unequal-sized cells (the smaller, called forespore, will later become endospore), then the engulfment of the larger compartment around the smaller one results in the 2 membraned-forespore which has the unique back-to-font middle membrane and is believed to be responsible for the control of molecules into the protoplasm of the endospore.

Dehydration starts, lessening the pH by 1 and the dormancy begins.
The generation of the peptidoglycan layer called cortex between inner and outer membrane along with the synthesis of Spore-specific Acid Soluble Proteins (SASPs) and accumulation of Ca 2+ and Dipicolinic acid (DPA) of more or less 10% dry weight basis of forespore in the core occur simultaneously with the dehydration process.

After the cortex is synthesized, the deposition of spore coat protein on the surface of the outer forespore (back-to-front membrane) begins.
The transformation of the forespore into an endospore is believed to be induced by the specific transcription of the gene in the forespore which occur by the communication between mother cell and forespore via signal transfer across the septum.

This process results in the production of autolysin, releasing the forespore out while mother cell lyses and dies.
The resulting endospores are varying in shape and position relative to mother cell e.g. lateral, central or terminal.

The endospore consists of a number of layers which can be outlined as: the core - membrane - cortex -- reversed polarity membrane (back-to-front) - coat - exosporium.
The innermost part, the core or protoplast, this lowest water content layer contains DNA, ribosome, enzymes, calcium dipicolinate (DPA-Ca) and SASPs' pool which binds to and stabilizes the DNA.

The next layer is the inner membrane which inhibits the passing through of substance MW> 150.
Cortex, as stated, is a peptidoglycan layer but different from vegetative cells in that it contains only 65% muramic acid, lacking peptide residues and has lower degree of cross-linking.

It also involves in the dehydration process of the core and contributes to spore's resistance as well.
The back-to-front membrane being slightly different in the protein composition from the inner membrane will protect the influx of the small molecule into the deeper part of the spore.

The disulfide bridges-containing coat serves as a protective barrier from lytic enzyme, pH extremes and oxidizing agents (Garbutt, 1997) but not to heat and irradiation and finally the thin covering layer known as exosporium.
Apart from the differences in their physical appearance from their vegetative cell counterparts, the biochemical discrepancy are, for instances, the SASPs are found only in the endospore whereas the enzymes for amino acid or nucleotide synthesis are absent, high amounts of 3-phosphoglyceric acid in endospore and scarcely found the high energy compound i.e. deoxynucleotide triphosphate and the different protein founded at the DNA.

After they became endospore (released from the mother cell), there is hardly any metabolism but it is believed that there are 2 enzyme-substrate pairs responsible for the first few reaction during germination, ready and waiting for the right condition to start their reaction (Gould, 2000).
This means that they must have the ability to detect suitable environmental conditions to start their growth once again.

Such processes are the activation and germination (~25 minutes after activation).
Within a few minutes, the returning of nutrients such as L-alanine, glucose or mineral ion such as K+ or the sublethal heating i.e. the heating that cause no death to the microorganism or the chemicals such as long chain alkyl monoamine, lyzozyme or even pressure lower than 400 MPa (Patterson, 2000) could initiate the activation process.

Mechanism for nutrients activation is believed to be that they change the series of enzyme action by binding to the endospore receptors, causing peptidoglycan cleavage so water moves into the core, swelling it.
In addition, the excretion of stabilizing factors e.g. Ca-DPA and hydrolysis of SASPs also occurs.

Eventually, endospore starts to metabolize and lost their resistance properties.
The final stage before transform to fully vegetative cell is the outgrowth which takes around 90 minutes to complete (Setlow and Johnson, 2001).

Unlike germination, this stage needs exogenous nutrients for their increasing synthesis and growth.
By the end of the outgrowth, the endospore will have reclaimed the ability to synthesize amino acids and nucleotides.

However, the endospore partial lysis and the shedding of their coating during this stage lead to the susceptibility of the bacteria to food preservatives (Gould, 2000).
Such example is the use of bacteriocin nisin in cheese or canned food which inhibit the microorganism immediately after they have germinated.

The significance of the endospores in the food industry lies in their extreme resistance to adverse conditions.
Generally speaking, their resistances are mainly due to the dehydrated state of the core (15% water compared to 75% in vegetative cell); meaning less water is available in reaction.

At low water content, the protein denaturation is less likely to occur and the depurination of DNA is prevented by the stabilizing action of SASPs to DNA with the ratio of 1 SASPs to 5 base pairs (Gould, 2000), and to a lesser extent, the impermeability of their various membranes.
Freezing has no effect on them since the DNA damage is prevented by SASPs.

They can withstand a hydrostatic pressure up to 1200 MPa (Adams and Moss, 2000).
γ-ray resistance is second only to the virus but the precise mechanism is not known, possibly from the different DNA environment compared to vegetative cell.

Their UV-ray resistant at 254 nm, a maximum lethality, is 5-7 times greater than their vegetative cell due to the DNA stabilizing effect of SASPs.
Exposed to various chemical species e.g. oxidizing agents, phenol, pH extreme has little effect on them because the inhibitory effect of their coat on the influx of the molecule into the core.

In addition, the low water content in the core help in reducing damage from H 2O 2 by refrain the production of •OH from Fenton chemistry (Setlow and Johnson, 2001).
Their extreme heat resistances pose major problems in the food industry since heating is the widely used approach to control microorganisms.

The combination of the intrinsic heat resistance (inherited from their vegetative cell) and the additional resistance from the mechanism in the spores (add ~ 40 -50 °C more) contribute to their extreme resilient to thermal processing (Gould, 2000).
Again, the low water content in the protoplast is likely to be the main cause for the resistance.

Moreover, conditions during heating i.e. low a w, low Equilibrium Relative Humidity (ERH) and high Ca 2+ all contribute to the higher resistance.
The higher sporulation temperature also gives rise to higher endospore heat resistance (Leuschner, 2003).

Many survived after being heated at 100 °C for several minutes.
Unfortunately, most sporeformer is capable of producing toxin and cause food poisoning which can range from mild e.g. Bacillus subtilis, to severe e.g. Bacillus cereus, Clostridium perfringen to life threatening e.g. Clostridium botulinum (Gould, 2000) and many lead to the spoilage of the food.

For example, the flat sour food spoilage of low acid (pH>4.6) canned food e.g. canned peas, beans products by Bacillus stearothermophilus, which produce the most heat tolerant endospore.
The contamination of endospores can occur in the raw materials or ingredients used.

During heating, the endospore can be activated and multiply at high temperature region e.g. holding tank blanchers or warm filter bowls (Kotzekidou, 2000) whereas the vegetative cells might form endospores that could germinate later during the improper transportation and storage.
These complication leads to the necessity for controlling the endospore population in raw material and ingredients as well as the rapid cooling after heating process and low temperature (<43 °C) during transportation and storage of the products to minimize the germination of endospores thus lower the rate of food spoilage.

Although many sporeformers can cause food poisoning in human, the 3 notorious species are Bacillus cereus, Clostridium perfringens and Clostridium botulinum (Setlow and Johnson, 2001).
In B. cereus, their endospores can survive spray-drying process so they are found widely in raw milk and dry foodstuffs (Batt, 2000) such as starches, spices and also in infant formulas (due to dried-milk powder) and can cause food poisoning.

To prevent the germination of endospores, the cooked food must be rapidly cooled down to 7°C or maintain at >60 °C.
The most pathogenic heat resistance spore of C. botulinum makes it the target for heating processes in canned food.

Their spores germinate in anaerobic condition so the canning methods have to ensure the adequate inactivation of this microbe to prevent toxin production by the germinating spores.
The mesophilic C. perfringens presents a problem in food service establishments, e.g. hospitals, prisons (Penfield and Campbell, 1990) because the generation of toxins during sporulation due to an unfavorable conditions for their growth in our intestines.

To prevent the intoxication the mass production of cooked food has to be rapidly cooled down and thoroughly heated.
To reduce the incidents of food spoilage and poisoning, it is important to monitor the raw material to limit endospore load, adequate heat processing and rapid cooling of the processed food to adequately inactivate the endospores.

Always make sure that your food is thoroughly cooked before serving and reheated every time before consumption.
Objectives: To develop an understanding of the principles underlying the freezing processes.

To be able to estimate the freezing time for a particular type of freezer.
Introduction

Refrigeration can be described as the term covering 2 areas of processes, chilling and freezing in which both two processes involve the lowering and maintenance of the temperature below an ambient temperature.
Chilling involves no phase changes of the water and thus only sensible heat is removed in the process.

In freezing, this process includes the transformation of the water in their liquid phase into solid, so incorporate both sensible heat and latent heat removal.
For this reason freezing always comes along with more energy expenditure and time to complete the operation (Pardo and Niranjan, 2006).

Two most common ways of attaining the low temperature are the mechanical refrigeration system and the use of cryogenic fluids (Pardo and Niranjan, 2006).
The mechanical refrigeration cycle comprises of 5 components i.e. the evaporator (low-temperature heat exchanger), compressor, condenser (high-temperature heat exchanger), the expansion valve and the refrigerant.

Heat from the food will be transferred to the evaporator by the temperature gradient resulting in the evaporation of the refrigerant.
Then the compressor will caused the gaseous refrigerant to change into a superheated gas.

After that, the passing of this high energy refrigerant through the condenser will lead to the condensation of the refrigerant back into the liquid phase.
Next, the liquefied refrigerant will enter the expansion valve, which is a high/low pressure separator, leading to the pressure drop.

Consequently, a mixture of liquid and gas refrigerant is formed.
Finally, this mixture will participate in the evaporator and the cycle continues.

Pardo and Niranjan (2006) catagorised refrigeration into 4 systems.
First, the Plate Contact system in which the food is placed in direct contact with a cold surface e.g. a plate freezer where a refrigerant is circulating within the plate providing the temperature difference so the heat is transfer from the food to the colder surfaces.

Second, the Gas Contact refrigerator, the cooling of the food is achieved by the circulation or the flow of cold gas (usually air) through the food and absorbing heat from it.
Example of such system is air blast refrigerator.

This procedure can also be incorporated with vacuum cooling where reduction in pressure will cause the water to vaporize, carrying with it the latent heat thus leading to the lowering in temperature.
Third, Immersion or Liquid Contact refrigeration where an food is made into contact with the cold liquid e.g. water, brines or ethylene glycol by spraying or immersion.

By this means, the food (often unpacked) can rapidly attain the temperature of the cold liquid.
Fourth, Cryogenic freezing; this involves the direct contact of the food with the low boiling point liquid, for instance; liquid nitrogen (-196 °C) or liquid carbon dioxide (-78.5°C).

Low temperature is produced by the phase change (boiling) of the cryogenic liquid at the atmospheric pressure, so this process is a total loss system since the refrigerant is discharged into the atmosphere (Boast, 1985).
Consequently, it is the most expensive system of all.

The first three system mentioned above can be used either to chill or freeze the food while the last method, cryogenic system, can only be used in freezing process due to the difficulty in controlling the final temperature of the food.
Freezing process can be divided into 4 steps (Boegh-Soerensen and Jul, 1985); (i) Prefreezing stage where the temperature of the food is reduced to its freezing point but ice crystal has yet to form.

(ii) Supercooling stage, this is characterized by the further drop of the temperature below its freezing point without any ice formation.
(iii) Freezing stage, starts when the temperature increases back to its freezing point and water-ice transformation takes place.

This stage exhibit constant temperature until all water is crystallised i.e. completely change into ice.
(iv) Subfreezing stage, the further decreases in temperature of the food to the end temperature or the intended storage temperature.

This process occurs relatively fast due to the lower specific heat of ice compared to that of water and that no latent heat is being removed.
Most foods have to pass through a preliminary process prior to being refrigerated.

Two methods used are blanching and sulphiting.
Blanching can be done by either steam or hot water.

The purposes of this method are to heat inactivate enzymes e.g. polyphenol oxidase which would otherwise cause product deterioration during storage i.e. enzymatic browning reaction.
Moreover, the reduction of microbial contamination and chemical contamination e.g. pesticide residue have also been reported.

Sulphiting is the application of sulphur dioxide (SO2) or inorganic sulphites (SO3 2-) in order to reduce both enzymic and nonenzymic browning reaction, microbial growth and as an reducing agent or antioxidant leading to a better characteristics of food (Grandison, 2006).
Method

1. Blanching and sulphiting.Cut vegetables (sprouts, mushroom, potato, carrot) into smaller pieces in equal size.Blanching condition was 90°C.The imitated hard water was prepared using calcium chloride solution.Divide similar amount and types of cut vegetables into 7 set of experiment i.e.control: no further treatment blanched in soft water for 2 minutesblanched in soft water for 5 minutesblanched in imitated hard water for 2 minutesblanched in imitated hard water for 5 minutesimmersion in 0.1 % sodium metabisulphite for 5 minutesimmersion in 0.5 % sodium metabisulphite for 5 minutesby put into 7 different aluminium trays and label the tray.Performed the experiment according to each condition.After completed each set of condition, compared processed vegetables in each set of experiment with the control ones in terms of the inactivation of enzymes i.e. catalase (by added a few drops of hydrogen peroxide) and polyphenol oxidase (by added a few drops of guaiacol).
Look for the carbon dioxide formation and brown colour development, respectively.All sets of experiment were transferred to plate freezer.2.

Blast freezingPut each of the following : apple, tomato and potato into an aluminium tray and put into a blast freezer at -18°C for 60 minutesMeasured the temperature at the center.3.
Vacuum chillingCut 2 tomatoes into halves i.e. 4 pieces of half tomatoes and put in an aluminium tray.Measured the over all weight and temperature at the center.After the vacuum chilling was run for 10 minutes, the weight and the temperature at the center were recorded.4.

Freezing using liquid nitrogenUsing potato, tomato and apple as a whole fruit.
Measuring the center temperature of the fruits.Immersed in liquid nitrogen for one minute.Temperature at the center was measured and compared with that before processed.5.

Plate freezer: continued from blanching and sulphiting.All 7 aluminium trays from blanching and sulphiting were transferred to the plate freezer (-30 °C) for 30 minutes.6.
Extra: to illustrate that the food is a poor conduction of heat.Choose two different size apples i.e. one smaller and one bigger.

Both apples were blanched at 90 °C for 5 minutesTemperatures at the center were compared.7.
Make an estimation of the freezing time for tomatoes having diameter of 4 cm by liquid nitrogen and air blast freezer by Plank's equation.

Results
No further experiment was performed.

When cut, it was found that the there was a development of layers within both fruits.
7. Utilizing the Plank's equation, we can estimate the freezing time as follows; FORMULA For tomatoes diameter of 4 cm By liquid nitrogen where FORMULA by substitute the above values in the equation, the estimated freezing time for tomatoes having diameter of 4 cm using liquid nitrogen would be between 13.50 to 62.35 seconds depending on the extent of heat film coefficient (h).

By air blast where FORMULA by substitute the above value in the same equation, the estimated freezing time for tomatoes size ~4 cm using air blast freezer would be between 23 to 67 minutes depending on the extent of the heat film coefficient (h).
Discussion

From table 1, we could see the effects of blanching on the enzymes activity.
Normally catalase cause no deterioration to the food during storage (Fellows, 2000) but it is used as a marker because their relatively resistant to heat i.e. the absence of catalase could indicate the likelihood of other enzymes' absence as well.

Polyphenol oxidase cause browning of the food stuffs during storage of the freezing or dehydration products by transforming the polyphenol compounds existing abundant in the fruits into brown-coloured substances.
The presence of catalase can be detected by air bubble formation following the addition of hydrogen peroxide since hydrogen peroxide is broken into water and oxygen molecule.

Detection of polyphenol oxidase is demonstrated by the brown colour formation after the addition of the substrate for the reaction i.e. guaiacol.
In both soft water and imitated hard water, the longer the blanching time, the larger the extent of enzymes inactivation.

This could probably explained by the nature of the enzymes; that is the longer exposure to heat, the more inactivation they would get.
Calcium chloride added to the blanching medium is the reason for better texture retention of the fruits and vegetables after blanching due to the development of calcium pectate within the plant tissue (Grandison, 2006).

As blanching applies relatively mild heat treatment compared to other heat treatment methods (e.g. pasteurization), most of the spores would still survive the process.
Sulphiting controls browning reaction by inhibition of POP so lowering the amount participated in enzymic browning.

In addition, it binds with reducing sugar (formyl group containing compound) preventing them from binding with amino group in nonenzymic browning (Maillard reaction).
Plate freezer is a freezing method where temperature gradient takes the heat from the food to the cooler metallic plate which made into direct contact with the food.

Thus, in the plate freezer, the main heat transfer mechanism would be the conduction process from metallic surface to the food; hence heat removal could be enhanced by the closer contact between the metallic slab and food.
In cold air freezing, i.e. air blast, the air is circulated in the chamber containing food and as doing so it absorbs the heat from the food.

Both convection and conduction processes occur in cold air freezer i.e. convection between the food and the cold air and conduction within the food pieces.
The blast-freezer gave quite similar results in all three fruits.

The highest temperature in tomatoes after processed (0 °C) could be attributed to its water content being the highest among the three fruits used.
No layer development was seen in all fruits.

In liquid nitrogen, it was clear that 1 minute immersion was not enough to uniformly freeze them as the temperature between the surface and thermal center of each fruit was significantly different i.e. more than 10 °C.
Consequently, using liquid nitrogen for large food will result in the prolonged contact time which means the more lost of the expensive liquid nitrogen.

These results illustrated the draw back of freezing large food by liquid nitrogen.
However, cryogenic freezing can be coupled with air contact freezing for a better result e.g. first producing a 'cold hard crust' material by the cryogenic freezing then it is transferred to the cold chamber to finish off the solidification process (Pardo and Niranjan, 2006).

Because vacuum chilling lower the temperature of the food by losing some of its energy in the form of latent heat during evaporation, the final weight of the product were less than that of the fresh produce.
From the experiment, it was found that a number of general factors affecting the cooling of the food i.e. the temperature difference between the refrigerant and the food, the size of the food pieces and the nature of the food.

For temperature gradient; the higher, the better since more rapid lose of heat to the refrigerant could be achieved as can be seen from the lower temperature at the surface of potato from liquid nitrogen (-14 °C) and blast freezing ( -1 °C).
Because foods act as poor heat conduction medium as illustrated in the last part of the experiment, the smaller in size leads to faster freezing time.

This is clearly demonstrated in the freezing by liquid nitrogen.
Water content is one example of the nature of the food which could affect the rate of cooling of food i.e. the more water exist, the longer it would take to freeze them all.

In air blast system the higher velocity of the circulating air decreases the thickness of boundary film surrounding the food (Fellows, 2000) thus increases the rate of heat transfer.
The longer contact time between food and the freezing agent results in lower final temperature of the food.

Freezing time and freezing rate.
Freezing time can be defined in several ways follow Boegh-Soerensen and Jul (1985); First definition: it is 'the time required to lower the temperature of the product from its initial temperature to a given temperature (normally -10 °C) at the center'.

Second definition is 'the time from surface reaching 0 °C to the center reaching -10 °C'.
However, it is not very useful to compare freezing time between the products vastly different in size e.g. the large pieces of beef and peas.

For this reason, the freezing rate comes in.
It is defined as the average velocity of ice front advancing from the surface to the center of the food i.e. the ratio between the minimum distances from the surface to the thermal center (cm) and the freezing time (hour) using the second definition.

An advantage of commercial freezing over home freezing is the capability of regeneration.
The heat or energy in refrigerant acquired from the food can be recycled e.g. use to generate heat for the rest of the factory/building.

Moreover, the freezing chambers are normally a large with the sole purpose of freezing while home freezing is usually done by a small frozen chamber mostly with vertical loading/access facilities (Boast, 1985).
As a result, the opening of the door in home freezing will cause the loss of cold air out and is replaced by warm air, leading to the formation of frost which hinders the heat transfer mechanism on the freezing surfaces.

The more important is that in commercial freezing, the foods put inside the freezer each time are of the same condition e.g. same size, temperature whereas in the home freezing people put in different things in different time.
This could impair the already frozen food due to the rising of temperature.

In another word, more fluctuation in temperature is experienced by foods in the home freezing mode.
Objective

To develop an understanding of the principles underlying cheese making processes.
Introduction

Cheese is the generic name for a group of fermented milk-based food product.
Various definitions are given to them, for example, the FAO/WHO defines cheese as 'the fresh or matured solid or semi-solid product obtained by coagulating milk, skimmed milk, partly skimmed milk, cream, whey cream, or butter milk, or any combination of these materials, through the action of rennet or other suitable coagulation agents, and by partially draining the whey resulting from such coagulation' (Walstra et al., 1999).

It is essentially a concentrated product of fat and protein in the milk varying between 6 to 12 fold depending on the variety, thus considered highly nutritious Production steps for the majority of cheese varieties can be divided into two steps i.e. the manufacturing which is carried out within the first 24 hours and the ripening period; although some manufacturing steps such as salting and dehydration may continue over a long period.
Soft cheese is one type of cheese characterized by high (>40%) moisture content (Fox, 1993).

The basic steps in soft cheese manufacturing are coagulation (clotting), dehydration (removal of the whey), shaping (put into mould), further draining of whey and finishing.
Soft cheese is different from normal cheese production in that it can be consumed fresh without ripening period.

The important step in producing cheese before start clotting milk is the selection of the milk.
Pasteurised milk is common due to the safety aspect of the finished cheese as some pathogens can survive the condition of production.

After the milk has been pasteurised, comes the coagulation step.
However, since the pasteurization killed most of the microflora in the milk, a starter culture of selected bacteria, e.g. consisting of Lactococcus lactis subsp lactis and/or Lactococcus lactis subsp cremoris and Leuconostoc mesenteroides, are often added prior to the addition of enzyme to attain desired acid production and flavour characteristics.

Fox (1993) stated that acid production can influence:
1. Coagulant activity during coagulation.

2. Denaturation and retention of the coagulant in the curd during manufacture and thus the level of residual coagulant in the curd.
3. Curd strength which affects 'cheese yield'.

4. Gel syneresis.
5. The extent of dissolution of colloidal calcium phosphate (CCP) which in turn modifies the susceptibility of casein to proteolysis during manufacture and ultimately affects cheeses' rheological properties.

6. Acidification controls the growth of unwanted bacteria including some pathogenic ones and some starter bacteria also produce probiotics thus inhibiting the growth of non-starter microorganisms.
Clotting of the milk for production of soft cheese is often achieved by a combined acid and rennet coagulation (since starter culture is added prior to the addition of enzymes) (Walstra et al., 1999) A gel is formed due to casein particles aggregating into a network entrapping fat inside, separating the milk into curd and whey serum.

In enzymatic clotting, chymosin acts on the kappa-casein molecule at a specific site, i.e. between amino acid Phe 105 and Met 106, resulting in the splitting of kappa-casein.
The casein micelle will then be split into 2 parts: the released hydrophilic caseinomacropeptide into whey serum and the paracasein (containing the other part of kappa-casein called para-kappa casein which remains attach to the micelles).

Because the disappearance of the hairy structure of kappa-casein which is the principal factor stabilizing the casein micelles, the casein micelles thus could not disperse in milk.
Instead, they attach to each other and eventually coagulate.

In addition, the decreased electronic charge on the casein molecules, brought about by the reduction of pH by acid producing bacteria, cause micelles to aggregate.
At pH near isoelectric pH of the casein i.e. ~4.6, the repulsion between casein micelles is reduced allowing close contact between micelles and eventually coagulation occurs.

The coagulation of the casein starts when about 70% of the hairy region has been split.
By van der Waals attraction, the attraction of casein occurs.

However, this force alone is insufficient thus the necessary of Ca 2+ ions which reduce the electrostatic repulsion by neutralizing the negative charge on the micelles and forming bridges i.e. both negative-negative and negative-positive salt linkages between charged sites on the paracasein micelles.
After the addition of enzyme and starter culture, the milk is then incubated at 33 to 35 °C allowing chymosin to work and bacteria to grow.

The gel is formed.
The removal of whey which is resulted from syneresis of the gel or the curd is enhanced by cutting the coagulum.

The resulting curd makes up about 10 to 30% of the original volume of milk (Walstra et al., 1999).
All the steps promoting gel syneresis are characterized as a dehydration step.

Stirring and salting after formation of the curd improves whey lose from the curd by means of osmotic pressure differences.
Finally, shaping and pressing of the curd further improve whey removal.

In some traditional soft cheese production, large lumps are cut from the coagulum, put into molds, where syneresis occurs resulting in high water content cheese.
During these stages, a variety of cheese based on the moisture content can be made as a result of the different degree of dehydration.

Because salting is an essentail step of cheese manufacturing, most cheese contains added salt of 1% - 4%.
Addition of salt into the cheese has various advantages which are

To control the microbial growth and their activity;Control of various enzymes activities in cheese;Syneresis of the curd resulting in whey expulsion and hence reduction in moisture which eventually influence microbial growth and enzymic activity;Physical changes in cheese protein, affecting texture, protein solubility and conformation and also flavour.(Adapted from Fox, 1993)
The syneresis of the curd is one critical phenomenon in determining cheese's final result (Walstra et al., 1999).

It is defined as the process of gel contraction or shrinkage after it is being formed leading to the flow of the whey through the gel networks.
The process is not a persisting action of rennet since no additional caseinomacropeptide is removed from the micelles shortly after the gel has formed.

For high-moisture cheese, the syneresis should be slowed down or stop after a certain time though this is far less important in low-moisture cheese.
Walstraal. (1999) summarized the factors affecting syneresis as follows:

Firmness of the gel at cutting i.e. the need to be not too weak at cutting otherwise it tends to synerese slightly.Surface area of the curd i.e. the gel is often cut into cubes.Pressure by stirring.
This causes curd grains to collide and compress one another.

Stirring also prevent sedimentation of the curd which would otherwise leads to loss of surface area and eventually slow down syneresis.Acidity: the more acid production, the faster syneresis occurs.Temperature: Increased temperature leads to expulsion of moisture from paracasein gel whereas and vice versa.Composition of the milk.
For example, the fat content in milk hinders the flow of whey out of the casein gel.

Thus the higher the fat content, the less the curd can shrink.
Other factors include pH, Ca 2+ activity, protein content and concentration of CCP after syneresis.

Methods :
Refer to the methods describing in the practical handout.

Results
Discussion

Analysis of milk and whey composition revealed the differences between them.
From table 1, the fat content in whey is reduced because the majority of fat molecules are entrapped in the casein gel during milk clotting step which is attributed to the coagulation of casein micelles.

Similar reason applied to protein content i.e. in whey the protein content is lower compared to that in milk because most protein in milk (~80%) is casein which is coagulated after the addition of enzyme.
The protein content in whey, 0.93%, is probably ~0.6% whey protein plus the rest ~0.3% from casein and other small peptides.

On the other hand, lactose is slightly higher in whey.
This is because other major components in milk i.e. fat and protein were removed from whey serum thus lactose is more concentrated (higher proportion), though the amount should be similar in both milk and whey.

TA is the measuring of buffer capacity in the solution.
In whey, as >70% of protein is removed so the buffer capacity of the solution decreased (since charges on protein molecules contribute to the buffering capacity).

Consequently, the TA is slightly lower in whey.
In acid coagulation, Table 2, the higher acid addition led to higher degree of aggregation of the micelles.

The mechanism is that casein becoming insoluble at pH around its isoelectric pH.
This facilitates the aggregation of protein due to less repulsion between individual molecules.

As pH approaching an isoelectric point (~4.6), higher degree of clotting was observed.
Moreover, at lower pH, the Calcium ion activity also increases hence stimulate the clotting process.

However, if pH is further reduced below 4.6, the texture of gel is lost i.e. more viscous and harder curdling.
Temperature of 35 °C, slightly higher than normal incubating temperature, was used in rennet coagulation in order to speed up the reaction.

From Table 3, it is clear that CaCl 2 addition promoted gel formation best.
The reason is probably due to the improved cross-linking of calcium ions between casein micelles.

Gel formation in 'stirred' and 'refrigerated' are both very less since the stirring action disrupted the gel structure and low temperature inhibit the activity of chymosin (a minimum temperature of 20°C is required fro rennet coagulation (Fox, 1993)).
But this effect is reversible i.e. as the temperature in the 'refrigerated' condition was increased to 35°C, gel formation eventually occurred (table 4).

Boiled milk prevents coagulation process since heat denatures the complex protein structure, exposing thiol group of kappa-casein molecules.
As a result, β-lactoglobulin, a peptide in whey serum, will bind to the exposed thiol group.

The steric hindrance conferred by this attachment impairs the ability of chymosin to attack at the specific site.
Thus coagulation could not take place.

The effect of oxalate in milk clotting resulted from its affinity to calcium ions.
Oxalate strips out free calcium ions, preventing them from participating in coagulation process.

From the experiment, it is clear that the important factors contributing to coagulation of milk are the temperature as to allow enzyme to work, the presence of calcium ion which participate in gel forming and the presence of other agents capable of reduces the calcium ion concentration in the milk.
Question 1

In today's fast developing word of information, exchanging of data through computer networks is one of the most important topics.
As cable structures are inconvenient and cause much trouble, more and more people have started using wireless connections.

This modern solution is very comfortable and much easier to install in domestic, industrial, or business environment.
These effective networks have one big advantage; you can connect to them from any place where you can catch the signal.

But if you can do this, why somebody else cannot?
And what if this person has bad intentions, what if this person should not access your network, what if you have some data you would not like to show to anybody else, if the data is confidential?

This is the point where the biggest advantage of wireless networks; which is accessibility; becomes its biggest disadvantage; which is insecurity.
The most efficient way to solve this problem is cryptography.

The idea of creating and managing digital identities; which appeared in 1990's due to development and delivery of e-commerce initiatives, and rapid commercial development of Internet; was key to find the ways to make them secure.
One of them is a concept of public key infrastructure (PKI).

The idea is easy to understand, and can be compared with using passport.
Travelling from abroad requires a passport (private key), and documented paper transactions must be made.

That, however, needs to accept the document to authenticate the identity of the bearer or signer because it was attested by a trusted third party.
To obtain a passport one must provide several forms of identification (public key) to satisfy passport agency requirement.

While certifying the signature on the document, notary must see identification.
Within the context of a public key infrastructure (PKI), this same level of trust is needed.

The digital certificates created by the certification authority permit trusted electronic relationships inside and outside the network.
At the heart of PKI authentication is the concept of public key cryptography (PKC).

DEFINITION: Public-key is commonly used to identify a cryptographic method that uses an asymmetric-key pair: a public-key and a private-key.
Public-key encryption uses that key pair both for encryption and decryption.

The public-key is made public and is distributed widely and freely so that everybody can use it to encrypt a message.
The private-key is never distributed and must be kept secret so that only an authorised person can decode the message.

As the definition says in public key cryptography (PKC), each user is issued a key pair: • The public key can be accessed by anyone.
• The private key is known only to the user and is never revealed or transmitted.

The keys are mathematically related in such a way that it is virtually impossible to guess one key from the other.
We achieve this by using one-way mathematic functions.

What one key of the pair encrypts, only the other one can decrypt.
It is essential that while public key can be widely distributed, private key must be strictly protected.

There are two main reasons for private key protection: • Authenticity: When B receives message from A, B can be certain it came from A because it can be decrypted only by using A's public key, and that means it was encrypted using A private key.
B can verify identity of the sender as only A has access to A's private key.

• Confidentiality: A can send a secret message to B using B's public key (widely available) because only B holds B's private key to decrypt it.
Characteristic of public key cryptography is used to implement encryption and digital signature.

PKC-based digital signatures are the most secure ones.
Digital signature is a mechanism by which a message is authenticated.

It works like a signature on a paper document.
Supposing that A wants to digitally sign a message to B, A uses a one-way hash function of the document and then encrypts the hash using A's private-key, the encrypted hash is appended to the original document.

Then A sends the message along with A's public-key.
Only this public-key can decrypt that message.

B strips off the encrypted hash and uses the A's public key to decrypt it.
B also encrypts the hash from the received message, than compares it to the hash message he obtained.

If values are equal, Digital Signature Verification is produced, what means that there is no doubt that it is A's private key that encrypted this message.
A sender signs a message with his private key in a way that guarantees not only that the message came from this sender, but also that it has not been modified.

Even the slightest change in the document will negate the signature, as the crypto functions bind mathematically with a hash of the document.
And if the message needs to be kept private, then additional encryption is added.

Public-key technology is widely used in many branches these days.
Its effectiveness and reliability in both encryption and digital signature made it a useful tool in wireless networks.

Algorithms used in PKC, such as RSA, made illegal decoding of a message even more complicated.
Question 2

The concept of digital signature appeared few years before reasonable realization of it was available.
The first practical method which fulfilled developers' expectations about digital signature was the RSA signature scheme, which main innovation was introduction of RSA algorithm.

The RSA algorithm published in April 1977 was named after Ronald Rivest, Adi Shamir and Leonard Adelman who discovered and patented it.
Since then, the algorithm has been used in many Internet-based applications.

RSA type of encryption is employed in web browsing programs like Netscape Navigator and Microsoft Explorer, where it use used in implementations of the Secure Sockets Layer (SSL) protocol.
The algorithm is also widely used in our every day's life, which we are not even aware of.

Who thinks about RSA when one uses debit or credit card?
But as a meter of fact RSA is the algorithm which makes these transactions possible, and secure.

The companies like Mastercard and VISA employs RSA in the Secure Electronic Transactions (SET) protocol.
The algorithm is used in public key cryptography which was described in previous question.

But how does it work?
This cryptosystem is based on assumption: "It is not so difficult to find two large prime numbers, but it is very difficult to factor a large composite into its prime factorization form.

" This is how RSA algorithm employs one way functions.
Such function is simple to do in one direction but is nearly impossible (or very hard) to inverse.

According to RSA to decode a message one must find all prime factors of given number and use following mathematical algorithm to find a result:
M - message.C - encoded message.

n - very large number which consists at least 2 prime numbers p and q.
According to RSA formula: FORMULA This operation is very hard to reverse even if we now C, e and n.

That is why we change our exponent into d: FORMULA We know that: FORMULA,where p and q are prime numbers The encryption and decryption exponents, (d) and (e), are related to each other in the following way: FORMULA So simply to find decryption key d, one must know p and q used to calculate n.
And when that is done further operations are comparably easy.

Our task was to decrypt the message which was encoded with following 617 digit N number: FORMULA The encrypted message C is: FORMULA FORMULA At the beginning the task seemed easy.
But simple program written in Maple showed that number N is composed from more than two prime factors: I was quite surprised, because I thought that N is composed of two prime numbers.

But after consultations I knew that is not true.
When N has more factors, previous formulas are slightly changed: FORMULA FORMULA This could simplify my calculations.

But still the number that is left after dividing N by prime factors was 607 digits: FORMULA This number is not prime so I kept searching.
I gave up with my Maple program when after a week time of constant calculating my computer reached a value of 10000079.

And I believe that this search was useless and would not give any results.
Searching for factors with usage of this method is not effective and it would probably take few millions years for the fastest computer to solve it.

The next idea which I though about was extracting square roots from the divided number, and searching for the factors around them.
The value that I got was 2.697392774 * 10 33 and I did not manage to find any factors around this root.

This could be more effective if I were sure that this number has only two factors left, but it could as well have more of them.
I also tried to search using some random values but without any effect.

This is the Maple program which I wrote.
It was very helpful but still my computer was too slow to find any result: FORMULA

Heuristic evaluation
Heuristic evaluation is a usability engineering method for finding the usability problems in a user interface design so that they can be attended to as part of an iterative design process.

Heuristic evaluation involves having a group of evaluators examine the interface and judge its compliance with recognized usability principles (the "heuristics") (Nielsen 1994).
The problem with using heuristics is that they can be interpreted subjectively.

That makes it extremely significant to be conducted on a group of people, as a single participant will not be able to find all usability problems in examined interface.
In a set of expert evaluators, each of individuals concentrates on different issues, using their own experiences and knowledge.

Figure 1 shows an example from a case study of heuristic evaluation where 19 evaluators were supposed to find 16 usability problems in a voice response system allowing customers access to their bank accounts (Nielsen 1992).
Each row represents one of the 19 evaluators and each column represents one of the 16 usability problems.

Each square shows whether the evaluator represented by the row found the usability problem represented by the column: The black square- the problem was found The white square- the evaluator did not find the problem.
The rows have been sorted in such a way that the most successful evaluators are at the bottom and the least successful are at the top.

The columns have been sorted in such a way that the usability problems that are the easiest to find are to the right and the usability problems that are the most difficult to find are to the left.
The figure clearly shows that some usability problems are so easy to find that they are found by almost everybody, but there are also some problems that are found by very few evaluators.

Furthermore, it is impossible to identify the best evaluator and rely exclusively on that person's findings as: it is not necessarily true that the same person will be the best evaluator every time.
some of the hardest-to-find usability problems were found by evaluators who do not otherwise find many easy-to-find usability problems.

That is why, it is necessary to involve multiple evaluators in any heuristic evaluation.
General recommendation is to use three to five evaluators since using larger number does not improve result of the tests in comparison to the extra costs which would be required.

In order to ensure independent and unbiased results heuristic evaluation is performed by having each individual evaluator inspecting the interface alone.
Only after whole group of experts has been examined, are the evaluators allowed to communicate and have their findings amassed.

The results can be traced as written reports (after the test) or verbalized remarks (during the examination).
In my test I record the comments during the test, in my opinion that should make my work more effective.

Typically, a heuristic evaluation session for an individual evaluator lasts one or two hours.
Longer evaluation sessions might be boring for the tester, and therefore the test will not be very effective.

If the evaluation subject is a large or very complicated interface with a substantial number of dialogue elements it would be better to split the test into several smaller sessions, each concentrating on a part of the interface.
During the evaluation session, the evaluator goes through the interface several times and inspects the various dialogue elements and compares them with the list of heuristics supplied by examiner.

These heuristics are general rules that seem to describe general properties of usable interfaces.
In addition the list of general heuristics can be extended by developed category-specific heuristics that apply to a specific class of products, or by the evaluator himself.

One way of building a supplementary list of category-specific heuristics is to perform competitive analysis and user testing of existing products in the given category and try to abstract principles to explain the usability problems that are found (Dykstra 1993).
It is highly recommended that evaluators go through the interface at least twice, however standard says, that they decide on their own how they want to proceed with evaluating the interface.

The first pass should give them general scope of the system and the way it works.
The second (next) pass allows the evaluator to concentrate on specific interface elements and flaws while knowing how they fit into the entire system.

(Nielsen 1994).
If the system is intended as intuitive use interface for the general population or if the evaluators are field experts, it should be possible to let the evaluators use the system without further assistance.

If the system is sophisticated and the evaluators are fairly inexperienced with the area the system works in, it will be necessary to help the evaluators e.g. by supplying them with typical usage scenario.
Such scenario will list the different steps for a user to take to perform a sample set of realistic tasks.

One has to remember that it is not enough for evaluators to simply say that they do not like something.
They must explain why they do not like it with reference to usability principles.

Every tester provides subjective result of using the heuristic evaluation method, which is a list of usability problems in the user interface with references to heuristics that were dishonoured by the design.
The evaluators should be as specific as possible and list each usability problem separately.

There are two main reasons to note each problem separately: even if it was fully replaced with a new design, there is a risk of repeating some problematic aspect of a dialogue element, unless one is aware of all its problems.
it may not be possible to fix all usability problems in an interface element or to replace it with a new design, but it could still be possible to fix some of the problems if they are all known.

Heuristic evaluation does not offer a systematic way to fix the usability problems or to assess the quality of any redesigns.
However, because of the way that heuristic evaluation results are composed, it will often be fairly easy to modify design or fix many usability problems according to the guidelines provided by them.

Heuristic evaluation is explicitly intended as a "discount usability engineering" method.
Independent research (Jeffriesal.

1991) has indeed confirmed that heuristic evaluation is a very efficient usability engineering method.
As a discount usability engineering method, heuristic evaluation is not guaranteed to provide "perfect" results or to find every last usability problem in interface.

Heuristics Collection
There are a few different collections of heuristics that can be used.

During my research I decided to use twelve principles for good human-centred interactive systems designproposed by Benyon, Turner & Turner in 'Designing Interactive Systems' and also recommended during a lecture.
Learnability

Visibility
Things should be visible so that user can see what functions are available and what the system is currently doing.

It is easier to recognize things than to have to recall them.
Making things 'visible' through the use of sounds

Consistency
Consistency in the use of design features Consistency with similar systems and standard ways of working.

Consistency between the system and external things which the system is related to.
Consistent behaviours and consistent use of colours, names, layout etc professionally and culturally.

Familiarity
Take under account all cultural and professional aspects to choose the right symbols (language).

Try explaining any unique techniques which have been used
Affordance

Design things so it is clear what they are for, so that the user will know at the spot how to use them Affordances are culturally determined.
Ease of Use

Navigation
Provide support to enable people to move comfortably in the system

Control
Make it clear who or what is in control and allow people to take control.

Make clear the relationship between what the system does and what happens in the world outside the system.
Feedback

Quickly send back information from the system to users so that they know what effect their actions have had.
Constant and consistent feedback will enhance the feeling of control.

Robustness
Recovery

Enable recovery from actions, particularly mistakes and errors, quickly and effectively.
Constraints

Provide constraints so that users cannot do things that are inappropriate.
Secure the system using confirmation of dangerous operations.

This should prevent making serious errors.
Accommodation

Flexibility
Allow multiple ways of doing things to enable users with different levels of experience to use the system.

Provide people with the opportunity to change the way things look or behave (e.g. language) so that they can personalize the system.
Style

Designs should be stylish and attractive
Conviviality

Interactive systems should be polite, friendly, and generally pleasant.
Conviviality also suggests joining in and using interactive technologies to connect and support people.

(Every P., 2006)
Process of usability testing

I have carried out a usability test of the website http: // URL test has been carried out on the public version of the website.
Test settings

The heuristic evaluation was conducted on a PC running Windows XP with usage of Mozilla Firefox 1.5.0.8 and Internet Explorer 6.
Test Participant Profiles

The tests were carried out with 3 test participants who were somewhat experienced Internet users.
Results

Home Page
Severity: HighHeuristic: Accommodation: StyleDescription: The design of the first page is clear and simple.

It is written with usage of easy scripts and does not contain sophisticated flash animations.
This solution enables the quick loading and browsing of the website but makes it very unstylish.

Composition of colours, fonts and site deployment give even worse and less attractive overview on page design.
Solution: Redesign the website to make it more attractive and user satisfying.

Rationale: The design of this website looks extremely unattractive.
Try to encourage users to browse the website by making it more stylish

Mystery Letters
Severity: MediumHeuristics: Learnability: FamiliarityEase of Use: NavigationDescription: The letters on the main site are links, which cannot be recognized.

We cannot also say what kind of services they apply to.
Finally the links lead nowhere.

Additionally the letters are seen differently in MS Internet Explorer and in Mozilla Firefox.Solution: Describe usage of the letters, make them look like links (list, buttons) and link them with appropriate http addresses.
Alternatively delete the letters from the main site.Rationale: The letters, in the way that they are implemented now, are completely useless.

Nobody knows what services they are linked to, and what is more, they are not linked at all.
Mozilla Firefox MS Internet Explorer

Left menu: FORMULA sign
Severity: MediumHeuristics: Learnability: FamiliarityDescription: Menu on the left contains FORMULA signs which suggest that after clicking on it nothing on this site will change except from a list which should appear, in the same menu, under selected item.

Unfortunately menu in most cases contains just a link to another page without any list.Solution: Delete the FORMULA signs in cases they anyway do not work.Rationale: On websites the FORMULA sign means that the list of sub subjects will appear.
Left menu: Consistency

Severity: MediumHeuristics: Learnability: ConsistencyDescription: When clicking on the FORMULA sign which works as dropdown list the sub subjects appears menu changes its colours from yellow to white.Solution: Instead of changing a colour try changing a font i.e. to italic.Rationale: Menu looks in a very different way after the sub subjects list is shown.
Search this site

Severity: HighHeuristics: Learnability: AffordanceEase of Use: NavigationDescription: Text written in top right corner of the screen "Search this site" suggest that it is a link, but after clicking on it one can only write in the same field and acknowledge with "Go!" button.
What makes it even worse clicking on the text field does not erase "Search this site" text from the field; it does not even highlight it so that it would be easier to delete it.

User has to delete it letter by letter.Solution: Change the text field into link, or erase the text from the field and change the "Go!" button into "Search" button Rationale: "Search this site" text field does not look as text field and does not work correctly.
This action is extremely important for this service as number of sub sites is very large.

Welcome to Derby
Severity: LowHeuristics: Learnability: ConsistencyDescription: On the main page the top image is a link to the current page (we suspect that its role is the same as "home" button).

When we start surfing the service image still stays on top, but link disappears.Solution: Let this link be available on all sub sites, it will also help in navigation.Rationale: It is useful to have a link to Home page always in the same place, especially when the picture is anyway displayed on all sub sites.
Text version

Severity: MediumHeuristics: Learnability: VisibilityDescription: Two of the three test participants had problems finding the text version of the page.
They did not notice the link at the top of the page.Solution: Give the choice between text and graphical version before displaying the Home page.Rationale: It is good to have flexible page but interested user must be able to easily find text version if he wishes to use it.

Page Linking
Severity: HighHeuristics: Ease of Use: NavigationDescription: I.e. after clicking on "Business" link on the left, and than "Parking Permits", there is no way to go back except from clicking the browser back button.

We can go to main page (home button) but we cannot come back to previous page because category already changed from "Business" into "Transport and Streets> Motor Vehicles, Roads and Parking".
Solution: Add a "Previous Page" link or a page map which will help user to use the service.Rationale: This page is extremely complex and we can get very deeply into it.

Unfortunately it does not allow going to previous page.
Therefore the user after choosing the wrong option from different (than current) category must use the back button in the browser.

Survey
Severity: LowHeuristics: Robustness: ConstraintsDescription: On the right side of main page there is a survey "What Do You Think?".

You can vote as many times as you wish and in this way make this survey unreliable.
Solution: Add a script which will disable multiple voting (i.e. based on IP filtering)Rationale: If we want to obtain any reliable data we cannot allow the same user to vote many times.

Contact us
Severity: MediumHeuristics: Robustness: ConstraintsDescription: When going into a business section there is a possibility to send some questions or comments concerning this section.

In order to do that user has to fill in some information about him.
Unfortunately system allows user to perform all kinds of inappropriate actions as writing telephone number with letters or giving email address without any "." or "@".

Example given below was submitted successfully.Solution: System must check the content of all fields with regard to expected content (e.g. only numbers should be allowed in phone number).Rationale: How much sense does it make to ask i.e. for phone number if user can type in anything he wants?
If we expect people to contact us it is better to do that anonymously or if their personal data is needed we should check its reliability before collecting it

Summary
After conducting usability test of Derby City Council Website (http: // URL ) 3 participants managed to find 10 different usability problems.

Regrettably 3 of them have been recognised as the ones of the highest severity and therefore need urgent attention.
Inspected website is extremely complex and consists of numerous sub pages; this fact will undoubtedly make the changes extremely difficult to perform.

It is worth noticing that the most severe problems were found in style and navigation and the best solution to eliminate them would be to redesign the whole system.
Process of usability testing

I have carried out a usability test of a way-finding system for the Armstrong Siddeley building at Coventry University.
The test has been carried out on the initial interface of the website which is still under construction.

Test settings
The heuristic evaluation was conducted on a PC running Windows XP with usage of Mozilla Firefox 2.0.0.1.

Test Participant Profiles
It is not expected that students on MIS courses will use this system, as they will already know the location of these rooms.

The system description says that it will be used by:
• First time visitors to the school (such as prospective students and their families)• Visiting academics arriving for meetings with staff.• Students from other schools attempting to locate tutorial rooms.• Friends or relatives of students, submitting coursework.

That is why I carried out my usability test with 3 test participants who were somewhat experienced Internet users but did not have any experience with MIS building.
Results

Interface design
Severity: HighHeuristic: Accommodation: StyleLearnability: VisibilityDescription: The design of the first page is simple but not very clear.

It is written with usage of easy scripts and does not contain much graphics nor sophisticated flash animations.
This solution enables the quick loading and browsing of the website but makes it very unstylish.

Unfortunately also the composition of data gathered on this page makes it very hard to find appropriate information.
The composition is inconsistent and contains of plain text (different fonts and sizes), tables and picture.

The title of home page : "Faculty of Engineering and computing ROOM / STAFF LOCATOR " does not even fit the blue space provided for it.
It all looks like a big untidiness and mixtures of styles.

What is more there is a lot of unused white space on both sides of the screen.Solution: Redesign the website to make it more attractive and user satisfying.
Try to make few sub pages instead of using one page that contain all information, e.g. separate sub page for each floor or institute.Rationale: The design of this website is unattractive and therefore use of it will not be satisfying for the potential user.

Try to encourage users to browse the website by making it more stylish.
Staff list

Severity: MediumHeuristics: Ease of Use: NavigationDescription: Staff list is a table with names and details of staff members working in Armstrong Siddeley building.
The names in the left column are not links except from "S.Amin" which should be a link to his lecturer page.

Unfortunately this link does not work.Solution: Delete the link from "S.Amin" as system users are not supposed to browse Internet pages.
Possibly add the links to the other names and change the linking address for "S.Amin" but I do not think that is useful in our system.

I would suggest adding a link which will directly take user to the Building map and show him/her a way to selected staff member.Rationale: The home pages are not helpful in finding a way to lecturers' rooms.
Title

Severity: LowHeuristics: Learnability: ConsistencyDescription: On the home page there is a page title at the top unfortunately when user clicks on the link to ground floor map this title disappearsSolution: Add page title to all sub pages.Rationale: This page changes to completely new when user clicks on the link.
Beside the user who will approach the system when one of the floor maps is already opened does not know which floor's map it is.

Finding a room
Severity: MediumHeuristics: Accommodation: ConvivialityDescription: The system is extremely unusable.

Right now the user has to find the name on the list first and make a note with appropriate room number.
Than the user has to figure out from the room number the appropriate floor.

Finally user has to find appropriate link and find the room on the floor map.
This can be a cause of mistakes, as the system bases on user memory and skills.

It does not do anything to help the user.Solution: Change the system of finding room of staff member.
Let system do the hard work of matching the name with room number and make it display appropriate map with highlighted proper room and directions.Rationale: System should not rely on the user, it should work the other way round.

We cannot make user to be used by system.
We cannot demand that the user will have a pen, paper, and the most important time to manually find the room in the system.

Menu
Severity: HighHeuristics: Ease of Use: NavigationDescription: After clicking on the link to ground floor map there is no way to go back, to change floor or whatsoever.

The only way to go back is to click on browser "Back" button.Solution: Add a side menu with Home page link, floor links and search link.Rationale: This is a very simple page and it does not need a lot of effort to navigate it properly.
To achieve that system will need a search engine, as it operates on data base and side menu which will allow user to change currently displayed content.

Task analysis
To identify the way-finding tasks that are useful for the "MIS Staff/Room Localizator" I did a research among my family and friends and asked them what information they will find useful in searching the way to a room or staff member in a building.

From all the answers I managed to create the list of 5 tasks which appeared most often.
General search

This task should take any input, no matter if it is name, surname, position, module name, module code etc.
It can be a whole phrase, word or just a few letters.

This task returns found rooms or staff members with all the details: telephone, room number, modules taking place/taught.
It would be highly recommendable that the system displays also the photo of searched person what would make finding the right person much easier.

It is the most important search engine which should help in most cases.
Advanced search

This is more sophisticated search engine.
It enables to search by:

Staff member nameStaff member surnameStaff positionModule NameModule CodeRoom number
Besides it has also the feature to choose which department or institute the person is working at.

In this task we can also decide how the results should be displayed e.g. with or without the map.
The search can be conducted with one input e.g. by writing only "technician" in Staff position we should get the names and details of all technicians.

It can also be used to more complicated search e.g. Staff position "technician" and Staff member name "John" Should return details with all technicians whose name is John.
As previously, when searching for staff we should get all relevant information including the photo.

Maps
In case anybody would like to know what the plan of chosen floor is, he can choose this option.

It will show the map of the floor including names of the rooms and also details about modules and staff members who work inside.
Staff list

This feature shows the list of every staff member working in MIS building.
It also displays the following information:

Name - name of staff memberPosition - the position of staff memberRoom - room number that he/she occupiesTelephone - the telephone number useful in case university worker is not insideModules taught - the modules taught by staff member
Coursework and Assignment Submission

This feature gives some useful information of when and where to submit the courseworks and assignments.
It will provide appropriate directions.

This will be extremely important for friends or relatives submitting assignments on behalf of students.
Prototype Design

Overview
The prototype of "MIS Staff/Room Localizator" is based on the set of heuristics by Benyon, Turner and Turner described in "Designing Interactive Systems".

Its architecture has been designed in MS Word, so it is hardly possible to browse it in Internet Explorer or Firefox.
That is why I decided not to attach it on CD drive.

The prototype is usability and functionality driven, though it has not been designed professionally.
I have projected the left hand side menu which is visible all the time and helps in page navigation.

In the menu design I used Gestalt laws to group the links.
Home page

On the home page I resigned from placing the photo of MIS building (I did not find this picture attractive and as a user is in the building he already knows how it looks like).
All the information has been divided into sub pages and deleted from the main page.

Instead the general search engine has been added.
MAPS

Maps of each floor can be displayed.
In the final system it should be possible to click on the room and get information (which will be displayed below) whose room it is and what modules take place in it.

This is a map of ground floor.
Advaneced SEARCH

This should help in more detailed search.
Staff List

This is a backup in case the user is not sure about the name spelling etc.
Coursework and Assignment Submission

Information what the rules are and where to submit coursework and assignment.
Help

In case of any problems with using the system user should get some instructions.
Example result for phrase "Eve"

Part 1: Communication and synchronisation mechanisms and formal method definition.
Semaphores, monitors and message passing

Semaphores
A semaphore (invented by Edsger Dijkstra) is a protected variable and represents the classic method for restricting access to shared resources (e.g. memory) in a multiprogramming environment.

Semaphore is a non-negative integer with value initialized to the number of corresponding shared resources it is implemented to control (if there is only one resource, it is a binary semaphore with value 0 or 1).
The current value is the number of units of the resource which are free.

Semaphore "is accessed by means of two special operations, P and V.
If s is a semaphore, V(s) increments the value of s, and P(s) delays its cellar unit s is positive and then decrements s.

A V is used to signal the occurrence of an event, and a P is used to delay until the event has occurred." (Olsson, R, 2004) Notice that incrementing the variable s must not be interrupted, and the P operation must not be interrupted after s is found to be nonzero.
This can be done by special instruction or by ignoring interrupts in order to prevent other processes from becoming active.

That is why P and V operations must be atomic, which means that no process may ever be pre-empted in the middle of one of those operations to run another operation on the same semaphore.
The canonical names P and V come from the initials of Dutch words.

V stands for verhoog, what can be translated into increase.
Several explanations have been given for P (including passeer meaning pass, probeer meaning try, and pakken meaning grab), but in fact Dijkstra wrote that he intended P to stand for the made-up portmanteau word prolaag, short for probeer te verlagen, meaning try-and-decrease.

This confusion stems from the unfortunate characteristic of the Dutch language that the words for increase and decrease both begin with the letter V, and the words spelled out in full would be impossibly confusing for non - Dutch-speakers.
In software engineering practice the P and V operations are called wait and signal, or take and release, or pend and post.

To avoid busy-waiting, a semaphore may have an associated queue of processes.
If a process performs a P operation on a semaphore which has the value zero, the process is added to the semaphore's queue.

When another process increments the semaphore by performing a V operation, and there are processes on the queue, one of them is removed from the queue and resumes execution.
Different kinds of Semaphores:

Counting Semaphores - can take integer values.
"An instance of a counting semaphore is a single count.

For example if the count is five, then that semaphore has five instances.
Similarly if the count is zero, semaphore has no instances." (Lamie, E. 2004) The P operation takes one instance from the counting semaphore by decreasing its count.

If the count is equal to zero instance must wait in a queue.
Similarly, the V operation places the instance in the counting semaphore by incrementing its count.

Binary semaphore - the simplest kind of semaphore used to control access to a single resource.
It is always initialized with the value 1.

It "only takes values 0 and 1, like locks.
(...) Just like locks binary semaphores must be acquired and released, and only one process can hold a given semaphore at a time." (Kent A.al.

1990)When the resource is in use, the accessing thread calls P to decrease this value to 0 and lock access to this resource.
Only the current owner can unlock the semaphore so when resource is ready to be freed it is restored to 1 with the V operation.

Blocking semaphore - a binary semaphore that is initialized to zero.
This has the effect that any thread that does a P operation will be blocked until another thread does a V.

This kind of construction is very useful when the order of execution among threads needs to be controlled.
(Wikipedia, 2007)

Monitors
"Many modern software systems consist of collections of cooperating tasks.

In such systems, mechanisms for arranging exclusive access to resources and for synchronizing and communication among tasks are needed.
Many such mechanisms have been proposed, including semaphores and various forms of message passing.

One of the most natural, elegant, and efficient mechanisms for synchronisation and communication, especially for shared-memory systems, is the monitor." (Belzer J.al.
1987) A monitor can be used for synchronizing two or more computer tasks that use a shared resource.

Not only it ensures task an exclusive access to resources, but also to synchronize and communicate with other tasks.
A monitor consists of:

Entry routines Mutual exclusionDate items- the variables associated with the resourceMonitor invariant - defines the assumptions needed to avoid race conditions
Entry routines and Data Items

A contains a set of data items and a set of procedures, called entry routines that operate on the data items.
The monitor data items can represent any resource that is shared by multiple tasks.

A resource can represent a shared hardware component (e.g. hard drive) or a software component (e.g. file).
Generally monitor data can be manipulated only by the set of operations defined by its entry routines.

(Belzer J.al.
1987)

Mutual exclusion
Mutual exclusion is enforced among tasks using a monitor - only one task at a time can execute (called 'active task') a monitor entry routine.

Mutual exclusion is enforced by locking the monitor when execution of an entry routine begins and unlocking it when the active task gives up control of the monitor.
If another task invokes an entry routine while the monitor is locked, it is blocked until the monitor becomes unlocked.

Monitor invariant
The monitor invariant in this case simply says that the balance must reflect all past operations before another operation can begin.

It is usually not stated in the code but may be mentioned in comments.
There are however programming languages like Eiffel, which can check invariants.

(Wikipedia, 2007)
Condition variables

To avoid entering a busy waiting state, processes must be able to signal each other about events of interest.
Monitors provide this capability through condition variables.

When a monitor function requires a particular condition to be true before it can proceed, it waits on an associated condition variable.
By waiting, it gives up the lock and is removed from the set of running entry routines.

Any process that subsequently causes the condition to be true may then use the condition variable to notify a process waiting for the condition.
A process that has been notified regains the lock and can proceed.

Message passing
In computer science, message passing is a form of communication used in concurrent programming, parallel programming, object-oriented programming, and inter-process communication.

"The basic operations in message passing languages are "send a message" and "receive a message." Since a message must be sent before it can be received, message passing imposes an causal order on the actions of the program" (John H., 1999).
Destination of a send operation and the soured of receive, seen as a pair, is called communication channel.

Forms of messages include function invocation, signals, and data packets.
There are few different models of message passing.

As fundamental the message passing model is defined as:
set of processes having only local memory processes communicate by sending and receiving messages the transfer of data between processes requires cooperative operations to be performed by each process (a send operation must have a matching receive

Other models include:
data parallelism - data partitioning determines parallelism shared memory - multiple processes sharing common memory space remote memory operation - set of processes in which a process can access the memory of another process without its participation threads - a single process having multiple (concurrent) execution paths combined models composed of two or more of the above

(Maui High Performance Computing Centre, 1996) Message passing uses two communication mechanisms: Asynchronous message passing mechanism buffers the communication between sender and receiver.
That allows the sender to continue execution after sending a message.

This is analogous to mailing a letter - once a letter is in postbox, the sender can continue with other tasks.
Synchronous message passing - after sending the message the sender is blocked until its massage is received.

This is analogous to making a phone call, where the caller must wait until someone picks up the phone before talking.
(Reppy J., 1999)

Formal Methods
Formal methods are mathematically-based techniques for the specification, development and verification of software systems.

They are based on mathematical theories, such as: algebra, temporal logic, finite-state machines, functional programming or Petri nets.
The use of formal methods for software design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analyses can contribute to the reliability and forcefulness of a design.

The high cost of using formal methods implicates that they are usually only used in the development of systems, where safety or security is crucial.
Formal methods are rapidly becoming a promising and automated technique used at an early stage during the process of software development.

The reason for that is that most of the cost of software development branch from design (or requirements) defects.
It is obvious that defects detected in design cost hundred times less then the ones detected in testing or maintenance phase.

Formal methods enable us to identify those defects at the early stage of software life cycle.
As a result we can reduce cost of debugging, maintenance, and re-development.

Moreover formal methods give us greater confidence that safety-critical systems meet the desired properties in order to avoid disastrous consequences.
Many techniques with various degrees of rigor have been used in formal methods for specification, modelling, and verification.

(Juan, E. et al., 2002) Formal verification methods can ensure that a high-level system design really meets rigorously specified correctness requirements, thereby increasing the possibility that faulty designs can be discovered at the early phases of system development.
The other advantages of using formal methods are: In the process of formalizing informal requirements, ambiguities, omissions and contradictions will often be discovered.

The formal model may lead to hierarchical semi-automated (or even automated) system development methods.
The formal model can be verified for correctness by mathematical methods (rather than by intractable case by case testing).

A formally verified subsystem can be incorporated into a larger system with greater confidence that it behaves as specified.
Different designs can be evaluated and compared.

(Ostroff J. 1992)
Part 2: Real-time application

1. MULTI-STOREY PARKING SYSTEM
The application which I decided to describe is a system which works in a multi-storey car park.

System is provided for car park to monitor and control cars entering, leaving and staying inside the facility.
For the purpose of this coursework I made an assumption that car park has 200 parking places and 3 floors, with entrance gate on each of them and exit with pay centre situated on the ground floor.

Each space has its own sensor, which can tell if it is occupied or not.
Thanks to the sensors system can say how many places are still available and where exactly they are located in the building.

This information is provided to driver who should find a parking space without senseless driving around the floors.
Car will not be let in if there is no space to park.

My system is also capable of calculating amount of money to be paid when car is leaving the parking.
Payment is dependent on the time spend in the facility.

While designing the real-time applications we must ensure that it meets not only functional but also timelines requirements.
The Multi-Storey Parking System is a soft real-time system.

"A soft real-time system is one in which performance is degraded but not destroyed by failure to meet response-time constraints." (Laplante P., 2004).
For my system it simply means that if driver will have to wait e.g. 5 seconds before he will obtain detailed directions to find his parking space, no serious damage is made.

Processes Description
Multi-Storey Parking System contains following processes:

Display Panel - Gate 1 2 3:Informs driver how many (if any) free spaces are left.
It communicates with update process to update the number of free spaces.

It displays the way to the parking place.Update:This process communicates directly with the Central Panel to obtain number of free spaces available at the moment.
Afterwards it sends the information to the Display Panels on Gate 1, 2 and 3.Ticket Machine - 1 2 3:Ticket machines, assigned to each Gate, after being asked for a ticket by Display Panel check if any place is available, and book it in Central Panel (it means that the number of free spaces is decreased).

If the place is booked successfully the information including directions is sent to Display Panel and ticket is given.
If there is no space left information: 'Sorry we do not have any free places left.

Please wait.' appears on the Display Panel.Park:Car parks on a free space.
This activates the sensor of this place.Leave:Car leaves the parking place.

This deactivates the sensor of this place.Sensor - 1 to 200:Each parking place has its own sensor which is activated every time a car parks on this place and deactivated every time a car leaves the place.
Sensor process sends information about a parking car to the Central Panel so that the current localization of free spaces can be updated.

The number of free spaces is not updated by this process as the car leaving the place does not have to leave the parking (it can park on some other place).Pay Centre:When driver gives the ticket to Pay centre its details are send to Central Panel in order to acquire data of how much time this car spent inside the car park and how much should be paid.
When money is paid Pay Centre updates availability of free spaces in Central Panel (now we can be sure that the car has left).CENTRAL PANEL:This process has access to availability details.

It gives information how many places are available, and where they are located.
In order to be constantly updated It communicates with Ticket Machine, Update, Sensor and Pay Centre.

Processes I/O
Scenario I (One car at one gate)

Gates:Gate 1 - no carGate 2 - no carGate 3 - 1 carDisplay panel on gate 3 checks how many free parking places are leftInformation is updated from central panel :Availability = 5Driver asks for a ticketTicket machine checks again with the central panel and books a placeResponse to ticket machine on gate 3: 5 places available; book 1 and show the way, decrease free places number to 4Central panel sends localization of the place to the ticket machineInformation is updated on each gate from central panel :Availability = 4Ticket machine gives ticket and shows the wayCar parked in destined placeSensor activatedSensor send localization details to main serverMain server checks if the car is parked in correct place.
It is.Car is parkedCar leavesSensor is released and central panel is updatedPay centre takes a ticket and sends ticket details to central panelCentral panel answers with period of time that the car spend in the parking and amount of money to be paidAvailability information in central panel is updated when money is paidInformation is updated on each gate from central panel :Availability = 5Car goes toward the exit

Scenario II (One car at each gate)
Gates:Gate 1 - 1 carGate 2 - 1 carGate 3 - 1 carDisplay panel on each gate check how many free parking places are leftInformation is updated on each gate from central panel :Availability = 2Drivers asks for ticketsGate 1 asks for a ticketGate 2 asks for a ticketGate 3 asks for a ticketTicket machines check again with the central panel and book a placeResponse to ticket machine on gate 1: 2 places available; book 1 and show the way, decrease free places number to 1.

Send localization of the place to the ticket machineInformation is updated on each gate from central panel :Availability = 1Response to ticket machine on gate 2: 1 place available; book 1 and show the way, decrease free places number to 0.
Send localization of the place to the ticket machineInformation is updated on each gate from central panel :Availability = 0Response to ticket machine on gate 3: 0 places available.Ticket machines:On gate 1 and 2 give tickets and show the wayOn gate 3 display 'Sorry we do not have any free places left.

Please wait.'Car from gate 1 and 2 parked in destined placeSensors activatedSensors send localization details to main server:Sensor responsible for the car from gate 1Sensor responsible for the car from gate 2Main server checks if the cars are parked in correct places.Car 1 is.Car 2 is.Cars from gate 1 and 2 are parkedTicket machine on gate 3 checks availability with the central panel: Availability =01 of parked cars leaves parking placeSensor is released and central panel is updatedPay centre takes a ticket and sends ticket details to central panelCentral panel answers with period of time that the car spend in the parking and amount of money to be paidAvailability information in central panel is updated when money is paidInformation is updated on each gate from central panel :Availability = 1 Car goes toward the exitTicket machine on gate 3 checks availability with the central panel: Availability =1Driver asks for a ticketTicket machine on gate 3 checks again with the central panel and books a placeResponse to ticket machine on gate 3: 1 place available; book 1 and show the way, decrease free places number to 0Central panel sends localization of the place to the ticket machineInformation is updated on each gate from central panel :Availability = 0Ticket machine gives ticket and shows the wayAnother car leaves his parking placeSensor is released and central panel is updatedPay centre takes a ticket and sends ticket details to central panelCentral panel answers with period of time that the car spend in the parking and amount of money to be paidAvailability information in central panel is updated when money is paidInformation is updated on each gate from central panel :Availability = 1 Car goes toward the exitCar form gate 3 parked in released place (incorrect)Sensor activatedSensor send localization details to main serverMain server checks if the car is parked in correct place.
It is not.Central panel is updated with new car localization.Car is parked

2. Data Flow Diagram
Part 3: Definition of concurrent process and synchronisation mechanisms.

1. Concurrent Processes
Multi-storey parking system processes' concurrency is dependent on the number of cars which are currently inside the facility.

When car park is empty the only processes that work are the Display panels on all three gates, which wait for a car to come (they work all the time).
When first car arrives one of the gates exchanges the data with Update process and concurrently with ticket machine.

That means that Central Panel starts working parallel.
Than car parks, that activates the Sensor which updates Central Panel.

When car leaves Sensor is deactivated, that updates Central Panel.
Ticket is putted into Pay centre process, which again exchange data with Central Panel.

Finally car goes toward Exit.
As it is easy to notice when there is only one car in car park it does not happen to often that other processes than Gate processes run at the same time.

Situation changes dramatically when parking is full.
When there are more cars in the car park where, some of them are parked, some want to park and some are leaving the building, all of the processes work concurrently.

From all this processes that take place when parking is busy I believe that process Park, Leave and Display Panels on gates are the processes which require the most attention when concurrency is taken into account.
I decided to use semaphores as inter-process communication and synchronisation mechanisms.

Process Park uses semaphores to lock the place where the car is parked.
Process Leave unlocks semaphores when car leaves the parking place.

Display Panels on Gates are also based on semaphores as only one car can go threw the gate at one time.
2. SR program

The program below simulates working of Multi-storey Parking System.
With processes:

Parking - simulates the process of parking a car.Leaving - process of leaving the car park.Clock - counts the time.Gate 1, Gate 2, Gate 3 - they set the probability of appearing of the car at each gate.
Source code

FORMULA
Output

Though the program above is wrote for 200 parking places this output is from the program in which the number of places was decreased to 5 what illustrates better working of the system.
FORMULA

Firewalls and VPNWhat is a Firewall?
What can a Firewall protect and cannot protect?

Discuss the different types of Firewalls.
In your discussion give the details of the encryption systems used in each of the Firewall.

What is a Firewall?
A firewall is an information technology security device which is configured to permit, deny or proxy data connections.

Firewalls can either be hardware or software based.
A basic task of firewall is to control traffic between computer networks with different zones of trust.

Typical example is the Internet which is a zone with no trust and an internal LAN which should be a zone with high trust.
Firewall solves the security problem of connecting these two networks by interposing a specially configured gateway machine between them.

The primary benefits of using a firewall are:
Protection from vulnerable servicesControlled access to site systemsConcentrated securityEnhanced privacyLogging and statistics on network use and exploitationPolicy enforcement(Vacca, J, 2005)

Types of Firewalls
There are 3 base types of firewalls which can be distinguished by communication types: Is it between single node and the network, or between two or more networks?

Personal firewalls:software application which filters all incoming or outgoing traffic form a single computer.Network firewalls:usually running on a dedicated network device (router etc) or computer positioned on the boundary of two or more networks.
This type of firewall working as a gateway filters all incoming and outgoing traffic of the connected networks.

Is it intercepted at the network layer, or at the application layer?
Network layer firewalls:based on source and destination address and on number of ports and additionally, what higher-level network protocols the packet contains.

Network layer firewalls tend to operate very fast and transparently to users.
Also called "circuit-level" firewalls.

Application level firewalls:use various forms of proxy servers to proxy traffic instead of routing it.
As it works on the application layer, it may inspect the contents of the traffic, blocking what the firewall administrator views as inappropriate content, such as certain websites, viruses, and so forth.

An application layer firewall does not route traffic on the network layer.
All traffic stops at the firewall which may initiate its own connections if the traffic satisfies the rules.

Hybrids:most firewalls fall into the "hybrid'' category, which do network filtering as well as some amount of application inspection.
The amount changes depending on the vendor, product, protocol and version, so some level of digging and/or testing is often necessary.

Is it being tracked at the firewall or not?
Statefull firewalls:are able to hold in memory significant attributes of each connection, from start to finish.

It keeps track of the state of network connections (such as TCP streams) travelling across it.
The firewall is programmed to distinguish legitimate packets for different types of connections.

Only packets matching a known connection state will be allowed by the firewall, others will be rejected.
Stateless firewalls:treat each network frame or packet in isolation.

Stateless firewall does not know if any given packet is part of an existing connection, is trying to establish a new connection, or is just a rogue packet.
What can a Firewall protect and cannot protect?

What can Firewall protect us from?
Properly configured firewall can consist of many protective mechanisms, such as:

packet filtering - checking the sources of incoming data packets and accepting only this arriving from selected domains or IP addresses.
user identification procedures e.g. applying passwords or digital certificates.completing programs which operate with some protocols (e.g. FTP or Telnet) with security mechanismsenabling access only to services which admin consider to be secure.ensuring security by checking contents of network communication: WWW, FTP, SMTP, Telnet and Rloginrealizing SSN (Secure Server Network);translation and hiding private IP addresses of protected network.

That enables identification, authorisation and hiding a topology of a network.cooperating with break-in detection system, which in case of break-in alarms to close the channel of attack.
Cooperating with antivirus programs, which will scan all incoming data in search for viruses and other suspicious software.

encryption of data (DES, RC-4, FWZ-1, MD5, SHA-1, IPSec)enabling secure access and managing of SNMP devices in LAN, WAN or Internet scanning websites for Java applets, ActiveX procedures, and other potentially dangerous additions to HTML and possibly blocking them.detection and blocking of most dangerous break-in techniques, such as: spoofing, SYN Flood, LAND, Ping of Death, WinNuke;providing secure e-mail server, which controls in details all SMTP packages.
invigilation of LAN users - thanks to tracking system we can check e.g. which server was used at what time and for how long by each user, what files were used by which users, connection usage.

Monitoring of network enables admin to block in real time dangerous sessions.
What cannot Firewall protect us from?

It is important to realize that firewall is a tool for enforcing a security policy.
If all access between trusted and untrustworthy networks is not mediated by the firewall, or the firewall is enforcing an ineffective policy, the firewall is not going to provide any protection for a network.

However, even a properly designed network with a properly configured firewall cannot prevent following dangers: Malicious use of authorised services: e.g. a firewall cannot prevent someone from using an authenticated Telnet session to compromise your internal machines or from tunnelling an unauthorised protocol through another protocol.
Users not going through the firewall: A firewall can only restrict connections that go through it.

It cannot protect you from people who can go around the firewall, e.g. through a dial up server behind a firewall.
It also cannot prevent an internal intruder from hacking an internal system.

Social engineering: if intruders can somehow obtain password they are not authorized to have or otherwise compromise authentication mechanisms through social engineering mechanisms, the firewall won't stop them.
For example, a hacker could call users pretending to be a system administrator and ask them for passwords to "fix some problems".

Flaws in the host operating system: A firewall is only as secure as the operating system that a firewall is installed on.
There are many flaws present in operating systems that firewall cannot protect against.

In all types of firewall systems, the security of the protected network depends on the security of firewall device.
Someone who breaks into our gateway and can reconfigure its settings is likely to gain access to other devices on the LAN.

To prevent that, firewalls are specially configured and stripped down.
Typically they run a "hardened" version of the Unix or NT operating system.

(Welch-Abernathy D,2002)
Encryption used in Firewalls

Most firewalls support one or more levels of encryption.
Many firewalls that support encryption will protect outgoing data by automatically encrypting it before sending to the Internet.

Likewise, encryption-enabled firewalls will receive encrypted data from Internet and decrypt the data before it reaches the LAN.
Using firewall encryption one can connect geographically-dispersed networks through the Internet, as well as support remote user access through the Internet, without worrying about someone casually intercepting and reading your data.

That makes an idea of virtual private network.
These are some encryption techniques used in Firewalls:

DES
Data Encryption Standard (DES) found in 1970's by IBM.

It is the block cipher - an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bit string of the same length.
In the case of DES, the block size is 64 bits.

DES also uses a symmetric key to customize the transformation, so that decryption can only be performed by those who know the particular key used to encrypt.
The key has 64 bits, however only 56 of these are actually used by the algorithm.

Eight bits are used exclusively for parity checking.
As a result the effective key length is 56 bits.

The key is randomly chosen from 72 000 000 000 000 000 numbers and is changed for every massage.
The encryption system of DES uses the Feistel function.

The Feistel structure ensures that decryption and encryption are very similar processes - the only difference is that the subkeys are applied in the reverse order when decrypting.
The rest of the algorithm is identical.

This greatly simplifies implementation, particularly in hardware, as there is no need for separate encryption and decryption algorithms.
RC-4

RC4 was designed by Ron Rivest in 1987.
Although it is officially termed "Rivest Cipher 4", the RC abbreviation alternatively means "Ron's Code".

RC4, as Vernam cipher, generates a pseudorandom stream of bits which, is combined with the plaintext using XOR.
Decryption is performed the same way.

To generate key, the cipher makes use of a secret internal state which consists of two parts:
A permutation of all 256 possible bytesTwo 8-bit index-pointers

The permutation is initialised with a variable length key, using the key-scheduling algorithm (KSA).
Once this has been completed, the stream of bits is generated using the pseudo-random generation algorithm (PRGA).

FWZ-1
FWZ-1 designed by Checkpoint has not been opened to public analysis.

This is relatively new technology which uses a 40 bit symmetric key.
A message is encrypted with a secret key derived in a secure manner from the correspondents' Diffie-Hellman keys.

The Diffie-Hellman keys are authenticated by a Certificate Authority.
The TCP/IP packet headers are not encrypted, to ensure that the protocol software will correctly handle and deliver the packets.

The clear text TCP/IP header is combined with the session key to encrypt the data portion of each packet, so that no two packets are encrypted with the same key.
A cryptographic checksum is embedded in each packet to ensure its data integrity.

(checkpoint.com)
MD5

Message-Digest algorithm 5 (MD5) was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4.
It is a widely used cryptographic hash function with a 128-bit hash value.

An MD5 hash is typically a 32-character hexadecimal number.
Recently, a number of projects have created MD5 "look up tables" which are easily accessible online, and can be used to reverse many MD5 strings into their original meanings.

MD5 processes a variable-length message into a fixed-length output of 128 bits.
The input message is broken up into chunks of 512-bit blocks.

The message is padded so that its length is divisible by 512.
The padding works as follows: first a single bit (1) is appended to the end of the message.

This is followed by as many zeros as are required to bring the length of the message up to 64 bits less than a multiple of 512.
The remaining bits are filled up with a 64-bit integer representing the length of the original message.

The main MD5 algorithm operates on a 128-bit state, divided into four 32-bit words, represented A, B, C and D.
These are initialized to certain fixed constants.

The main algorithm then operates on each 512-bit message block in turn, each block modifying the state.
The processing of a message block consists of four similar rounds.

Each round is composed of 16 similar operations based on a non-linear function F, modular addition, and left rotation.
There are four possible functions F (different one is used in each round): FORMULA FORMULA FORMULA FORMULA

SHA-1
The original specification of Secure Hash Algorithm 1 (SHA-1) was published in 1993 and finally approved in 1995 by US government standards agency NIST (National Institute of Standards and Technology).

SHA-1 produces a 160-bit digest from a message with a maximum length of ((2 64 - 1) bits), and is based on principles similar to those used by Professor Ronald Rivest in the design of the MD4 and MD5 message digest algorithms.
IPSec

IPSec (Internet Protocol Security) is a suite of protocols for securing Internet Protocol (IP) communications by encrypting and authenticating each IP packet in a data stream.
IPSec also includes protocols for cryptographic key establishment.

There are two modes of IPsec operation: transport mode and tunnel mode.
In transport mode only the message of the IP packet is encrypted.

It is fully-routable since the IP header is sent as plain text, however, it can not cross NAT interfaces, as this will invalidate its hash value.
Transport mode is used for host-to-host communications.

In tunnel mode, the entire IP packet is encrypted.
It must then be encapsulated into a new IP packet to enable routing.

Tunnel mode is used for network-to-network communications or host-to-network and host-to-host communications over the Internet.
IPsec is implemented by a set of cryptographic protocols for securing packet flows and Internet key exchange.

Of the former, there are two: Authentication Header provides authentication, message and IP header integrity - and with some cryptography algorithm also non-repudiation - but it does not offer confidentiality.
Encapsulating Security Payload provides data confidentiality, message integrity, and with some cryptography algorithm also authentication.

The key exchange protocols is defined by IPsec, include the IKE (Internet Key Exchange) protocol and its successor, IKEv2.
IPsec protocols operate at the network layer of the OSI model.

This makes IPsec more flexible, as it can be used for protecting both TCP and UDP-based protocols, but increases its complexity and processing overhead, as it cannot rely on TCP (layer 4 OSI model) to manage reliability and fragmentation.
(Wikipedia)

Data and Network ProtocolsHTTP is the "stateless" protocol used by Internet browsers for most communications on the Internet.
Using the seven OSI layers as reference model describe in details how data (TCP segments) travels from a home user's browser to a website during a typical ecommerce transaction.

You are required to elaborate on any encapsulations and de-capsulations that occur during this process.
OSI reference model

In the 1980s, the International Standards Organization (ISO) began to develop its Open Systems Interconnection (OSI) reference model.
In this model, a networking system is divided into seven layers.

Within each layer, one or more entities implement its functionality.
Each layer uses different protocols, operates on different data units and has altered functions.

Each entity interacts directly only with the layer immediately beneath it, and provides facilities for use by the layer above it.
Protocols enable an entity in one host to interact with a corresponding entity at the same layer in a remote host.

Let's have a quick view on each Layer:
Layer 7: Application Layer

The Application layer provides a means for the user to access information on the network through an application.
Data processed at this level is send or received by the application used by user.

Layer 6: Presentation Layer
The Presentation layer transforms data to provide a standard interface for the Application layer.

Appearance of data is standardised, data is compressed, encrypted and some similar operations of the presentation are done.
Layer 5: Session Layer

The Session layer establishes, manages and terminates the connections between the local and remote application.
The OSI model made this layer responsible for "graceful close" of sessions, and also for massage sequencing and recovery.

Layer 4: Transport Layer
The transport layer controls the reliability of a given link through flow control, segmentation/desegmentation, and error control.

Some protocols are statefull and connection oriented.
This means that the transport layer can keep track of the packets and retransmit those that fail.

Prioritisation of services is also done here.
Layer 3: Network Layer

The Network layer provides the functional and procedural means of transferring variable length data sequences from a source to a destination while maintaining the quality of service requested by the Transport layer.
This is the layer where data is fragmented into standardised packets.

The Network layer performs network routing functions, and might also perform segmentation/desegmentation, and report delivery errors.
The best known example of a layer 3 protocol is the Internet Protocol (IP).

Layer 2: Data Link Layer
The Data Link layer provides the functional and procedural means to transfer data between network units and to detect and possibly correct errors that may occur in the Physical layer.

The standardised packets of the network layer are separated into MAC frames.
Layer 1: Physical Layer

The Physical layer defines all the electrical and physical specifications for devices.
This includes the layout of pins, voltages, and cable specifications.

The major functions and services performed by the physical layer are:
Establishment and termination of a connection to a communications medium.Participation in the process whereby the communication resources are effectively shared among multiple users.

For example, contention resolution and flow control.Conversion between the MAC frames obtained form Data Link layer and bit signals transmitted over a communications channel.
These are signals operating over the physical cabling (high and low voltage, light tension) or over a radio link (Radio waves).

Data flow from a home user's browser to a website during a typical ecommerce transaction.
Application Layer

The user launches his web browser, which registers a port with operating system.
When he types in the address of the internet shop and confirms it by pressing "enter" a 3-way handshake starts:

User sends a SYN to the server.In response, the server replies with a SYN-ACK.Finally the user sends an ACK back to the server.
At this point, both the client and server have received an acknowledgement of the connection and connection is established.

User browser shows a main page of e-shop (index.html).
User chooses the products to buy by browsing the page and adding them to the basket.

Finally he decides to make a payment by debit card.
There is nothing at the HTTP protocol to guarantee any data confidentiality, authenticity or integrity.

As soon as user types his debit card details and presses confirms them with "enter" key the data is send transport control protocol (TCP).
Application layer attaches an application header (AH).

Transport Layer
TCP as "connection" protocol establishes connection between e-shop's web server and user's browser to form "a live tunnel".

The debit card details are broken into manageable sizes called TCP segments.
Size of the segment is determined by how much the receiver (web server) can and want to accept.

Data is exchanged after TCP segment is confirmed.
When the protocol is agreed, the first TCP segment is encapsulated with TCP header and passed down to network layer.

Network Layer
Encapsulated TCP segments are encapsulated with IP header, and become IP datagrams.

The IP header includes source and destination IP addresses to help with routing the datagrams to intended destination (e-shop web server).
Once the encapsulation is complete the IP datagrams are passed down to data link layer.

Data Link Layer
When IP datagrams reach the data link layer they are encapsulated by the NIC with the header and become Ethernet frames.

The Ethernet Frames have to be passed down to Physical Layer.
Physical Layer

At this final stage Ethernet frames are divided into bits and send to a wire for feather routing to the web server, where all the data processing is reversed starting from Physical layer and ending at Application layer.
(Ahmed I., 2006, chap.

2)
WiFi NetworksWEP uses 40 bits encryption or 104 bits encryption.

How many key combinations are available in 40 and 104 bits?
Give a detail of WEP encryption and decryption process showing relevant diagrams where possible.

What is WEP?
Wired Equivalent Privacy - WEP is part of the IEEE 802.11 standard.

WEP uses the stream cipher RC4 for confidentiality and the CRC-32 checksum for integrity.
It can be typically configured in 4 possible modes:

No encryption mode 40 (64) bit encryption (2 40 = 1099511627776 key combinations (KC))104 (128) bit encryption (2 104 = 20282409603651670423947251286016 KC)232 (256) bit encryption (2 232= 6,901746346790563787434755862277 * 10 69 KC)
By default most of wireless devices have WEP turned off.

Most public wireless LAN access points (i.e., airports, hotels, etc) do not enable WEP.
If the Access Point does not enable WEP, the wireless clients can not use the WEP encryption.

In some base stations, it is optional whether the encryption is enforced.
The WEP encrypted may be turned on, but if it is not enforced, a client without encryption with the proper SSID can still access that base station.

Standard 64-bit WEP uses a 40 bit key (or 4 40bit keys if appropriate software), combined with a 24-bit initialization vector (IV, which is created automatically by the wireless network hardware) and the RC4 traffic key.
Most 802.11 hardware now supports a larger 104-bit key; this also has a 24-bit initialization vector and so it is also sometimes marketed as a 128-bit system.

A 128-bit WEP key is almost always entered by users as a string of 26 Hexadecimal characters (0-9 and A-F).
Each character represents 4 bits of the key.

4 × 26 = 104 bits.
Adding the 24-bit IV gives "128-bit WEP key".

Additionally as interesting fact a 256-bit WEP system is available from some vendors, and as with the above-mentioned system, 24 bits of that is for the I.V., leaving 232 actual bits for protection.
This is typically entered as 58 Hexadecimal characters.

(58 × 4 = 232 bits) + 24 I.V.
bits = 256 bits of WEP protection.

(Klaus C. W. )
Encryption

Stage 1
Wireless client generates message to be sentWEP produce 32 bits integrity check value (ICV) and appends to message The ICV is a check sum that the receiving station eventually recalculates and compares to the one sent by the sending station to determine whether the transmitted data underwent any form of tampering while communication.

Stage 2
One of the 40 bits encryption keys is selectedRandom-generated 24-bit initialization vector (IV) range: 0 -16 777 216 The IV lengthens the life of the secret key because the station can change the IV for each frame transmission.The IV and the 40 bits key fed into RC4 algorithm

Stage 3
Massage is fragmented into MPDUs (MAC Protocol Data Unit)The CRC appended to MPDUMPDU with ICV and 64 bits Cipherstream go into operation of XOR (exclusive OR) WEP adds the IV Header in the clear (unencrypted) within the first few bytes of the frame bodyIV Header Contains IV, Key ID and PadBefore sending Frame Header and Frame check sequence (FSC) are added

Decryption
Stage 1

Recipient stripes away MAC header 64 bits IV and key ID are extracted from IV HeaderThe key ID helps identify which of the four 40 bits key used
Stage 2

The concatenated IV (from IV header) and encrypted MPDU are fed into RC4 AlgorithmThe concatenated IV and IV on MPDU will cancel each otherThe end result is a Plaintext with appended ICV
Stage 3

ICV is stripped from massageNew ICV is calculated for a received massageIf new ICV (ICV') is equal to old ICV massage have been send correctly (compared in CRC)Decryption is successfulIf new ICV (ICV') is NOT equal to old ICV massage have been send incorrectly (compared in CRC)Massage must be send again
Internet and Ecommerce ArchitectureCross site Scripting, session hijacking and URL obfuscation are some of the tricks used by criminals to steal information from users over the Internet.

Discuss these forms of attacks in details.
Cross Site Scripting

Today websites are more complex than ever, containing a lot of dynamic content making the experience for the user more enjoyable.
Dynamic content is achieved through the use of web applications which can deliver different output to a user depending on their settings and needs.

Dynamic websites suffer from a threat that static websites don't, called "Cross Site Scripting" (XSS).
Most usage of XSS is benign to the local machine.

Its environment of execution is restricted to certain operations known as a sandbox, so a remote user cannot force your machine through cross site scripting to reformat its hard drive just by visiting malicious web page(Andrews M.
and Whittaker J. A.,2006).

Often attackers will inject JavaScript, VBScript, ActiveX, HTML, or Flash into a vulnerable application to cheat a user in order to gather data from them.
Everything from account hijacking, changing of user settings, cookie theft/poisoning, or false advertising is possible.

More and more uses are being attacked with XSS every day.
There are three types of XSS vulnerability: This form of XSS vulnerability has been referred to as Local cross-site scripting.

With Local cross-site scripting vulnerabilities, the problem exists within a page's client-side script itself.
For instance, if a piece of JavaScript accesses a URL request parameter and uses this information to write some HTML to its own page, and this information is not encoded using HTML entities, an XSS hole will likely be present, since this written data will be re-interpreted by browsers as HTML which could include additional client-side script.

Because of the way Internet Explorer treats client-side script in objects located in the "local zone" (safe zone), an XSS hole of this kind in a local page can result in remote execution vulnerabilities.
For example, if an attacker hosts a malicious website, which contains a link to a vulnerable page on a client's local system, a script could be injected and would run with privileges of that user's browser on their system.

This kind of cross-site scripting hole is also referred to as a non-persistent or reflected vulnerability, and is the most common type.
These holes turn up when data provided by a web client is used immediately by server-side scripts to generate a page of results for that user.

If invalid user supplied data is included in the resulting page without HTML encoding, this will allow client-side code to be injected into the dynamic page.
A classic example of this is in site search engines: if one searches for a string which includes some HTML special characters, often the search string will be redisplayed on the result page to indicate what was searched for.

If all occurrences of the search terms are not HTML entity encoded, an XSS hole will result.
This does not appear to be a serious problem since users can only inject code into their own pages.

However, with a small amount of social engineering, an attacker could convince a user to follow a malicious URL which injects code into the results page, giving the attacker full access to that page's content.
Due to the general requirement of the use of some social engineering in this, many programmers have ignored these holes as not terribly important.

This type of XSS vulnerability is also referred to as a stored or persistent or second-order vulnerability and it allows the most powerful kinds of attacks.
It exists when data provided to a web application by a user is first stored persistently on the server (in a database, file system, or other location), and later displayed to users in a web page without being encoded.

A classic example of this is with online message boards, where users are allowed to post HTML formatted messages for other users to read.
These vulnerabilities are usually more significant than other types because attacker can inject the script just once.

This could potentially hit a large number of other users with little need for social engineering or the web application could even be infected by a cross-site scripting virus.
The methods of injection can vary a great deal, and an attacker may not need to use the web application itself to exploit such a hole.

Any data received by the web application (via email, system logs, etc) that can be controlled by an attacker must be encoded prior to re-display in a dynamic page, else an XSS vulnerability of this type could result.
Session hijacking

The term Session Hijacking refers to the exploitation of a valid Session key to gain unauthorised access to information or services in a computer system.
Session keys are normally randomised and encrypted to prevent session hijacking.

Session hijacking is an advanced technique which exploits vulnerabilities in the TCP protocol.
Transport Layer establishes a session between network hosts.

Session hijacking calls for the intruder to spy on a TCP session.
The intruder can use this technique to slip commands into the security context of the original session.

One common use of session hijacking is to get the system to reveal a change of password.
Of course the hacker does not manually compromise spoofed TCP segments on the fly.

Session hijacking requires special tools (e.g. Juggernaut).
(Casad J., 2003) For the attack to succeed, the victim must use telnet, rlogin, ftp, or any other non-encrypted TCP/IP utility.

Use of SecurID card, or other token based secondary authentication is useless as protection against hijacking, as the attacker can simply wait until the user authenticates and then hijack the session.
This type of attack is very common as using a session management to solve a problem of storing state in web application is very common.

And when you do it incorrectly it is open to attack.
Session management works by each user having a unique identifier that travels with him during his use of a service.

The point is to overtake the identifier by breaking the session management, which can be done in 3 different ways:
Modifying data randomly, hoping to become another userFiguring out the sequence of unique identifiers that the site uses"Fixing" the session identifier of another user

Session identifiers are presented to the server as hidden fields, appended to URLs, or stored in cookies.
Storing the session identifiers as cookies and than passing it to the server as each page is loaded is the most common.

(Andrews M.
and Whittaker J. A., 2006)

URL obfuscation
Link based tricks are commonly played upon as they are relatively easy to execute.

It has been noticed that these kinds of phishing attacks were extensively executed when this Internet threat was in its infancy stage.
Below is a list of some of such tricks with appropriate examples explaining how an attacker can use such links to trick the end user: Using Strings - uses a credible sounding text string within the URL Example: URL This will point towards a web server hosting a fake login screen for your EBay account.

Using @ sign - this kind of syntax is normally used for websites that require some authentication.
However hackers make use of this syntax to trick victims to visit a fake login page.

This works on a simple concept where in the content on the left side of @ sign is ignored and the domain name or IP address on the right side of the @ sign is treated as the legitimate domain.Example: URL Status Bar Tricks- The URL is so long that it can not be completely displayed in the status bar - Often combined with the @ so that the fraudulent URL is at the end and not displayed thus the victim takes it as a legitimate host and gives away his confidential info!
Example: URL :UserSession=2f6q9uuu88312264trzzz55884495&usersoption=SecurityUpdate&StateLevel=GetFrom@61.252.126.191/verified_by_visa.html Similar Name Tricks- This kind of tricks uses a credible sounding, but fraudulent, domain name.

These kind of tricks have been often used by attackers gaining a psychological advantage over the victimExample : URL, URL URL Encoding Tricks: This kind of tricks is used to encode the URL or portions of the URL to disguise its true value using hex, or octal encoding.
Often combined with the @ which can also be disguised as wellExample: URL %33, which translates into 220.68.214.213 HTML Image Mapping Tricks: The URL is actually a part of an image, which uses map coordinates to define the click area and the real URL, with the Fake URL from the <A> tag being displayed.

Here is a small code that can easily help you achieve the same-: FORMULA As soon as the uninformed victim clicks any where on the image he is taken to the web server hosting a fake login page ( URL ).
URL as button Trick: the displayed URL is contained in the text description of a Form Button.

The Button itself is formatted to match the email background so that only the Button text shows.
Since it s a Form statement the Fake URL does not display in the status bar of the email client.

However when one brings the mouse over the button the attacker uses the mouse over HTML tag to forge the link displayed in the status bar.
URL Redirection Tricks: uses the redirection capability of a known provider to send the user to the Phishing site.

- Redirection is used by many larger sites like Yahoo, MSN, & Citibank.
- Example: URL :// URL Double Redirect Tricks - Combines the simple redirect method with a URL Masking service such as cjb.net or tinyurl.com.The Masking service assigns the user an alias for their URL.

Example: URL ://jne9rrfj4.CjB.neT/?uudzQYRgY1GNEnFirst sends to: URL and thenRedirected to: URL (cjb.net) Redirected to: Intended site through cjb.net redirection service The actual URL is stored at cjb.net and is accessed through the cjb.net alias.
(ContentVerification.com)

CryptographyThe security of the RSA crypto system is based on the difficulty associated in finding the prime factors of a very large integer.
Given the following real life situation:

E = Encryption Public Key (20000000089) N = A very large integer (34618195959169) M = Plaintext: "The security of an encryption system is as strong because most people rather eat liver than do mathematics!" C = Ciphertext D = Decryption Private Key (4771730348713) Write C++ or Java programme to implement the RSA encryption and decryption using the above given parameters.
You must demonstrate that your programme works!

Hint: You may use a text-numeric converter of your choice to convert the plaintext into numbers.
Programme Script

FORMULA
Programme output

Before Encoding the message is "The security of an encryption system is as strong because most people rather eat liver than do mathematics!" In number representation : FORMULA After Encryption FORMULA After Decryption "The security of an encryption system is as strong because most people rather eat liver than do mathematics!"
Abstract

Robot football provides a fertile environment for development of artificial intelligence techniques.
One of most interesting but in the same way the most demanding technique is designing strategy for football robot team.

In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multi-agent collaboration, strategy acquisition and real-time reasoning.
Match is a task for a team of multiple fast-moving robots under a dynamic environment.

Aim
Aim of this project is to investigate strategy for team of football robots.

And potentially investigate Bluetooth communication between robots.
Background

The idea of creating robots is as old, or even older than all computer science.
Artificial people, iron people, androids, etc are as frequent in ancient myths ( eg. myth of Pygmalion ) and fairytales (eg. "The Wonderful Wizard Of Oz", L Frank Bum) as their next of kin - robots are now.

But what does a word robot mean?
It comes from the Czech word robota, industrial labour.

The word has first appeared in Karel Čapek's science fiction play R.U.R.
(Rossum's Universal Robots) in 1921, and has probably been invented by author's brother, painter Josef Čapek.

According to Internet encyclopaedia Wikipedia: "The word robot is used to refer to a wide range of machines, the common feature of which is that they are all capable of movement and can be used to perform physical tasks.
Robots take on many different forms, ranging from humanoid, which mimic the human form and way of moving, to industrial, whose appearance is dictated by the function they are to perform.

Robots can be grouped generally as mobile robots (e.g. autonomous vehicles), manipulator robots (e.g. industrial robots) and Self reconfigurable robots, which can conform themselves to the task at hand.
Robots may be controlled directly by a human, such as remotely-controlled bomb-disposal robots, robotic arms, or shuttles, or may act according to their own decision making ability, provided by artificial intelligence.

However, the majority of robots fall in-between these extremes, being controlled by pre-programmed computers.
Such robots may include feedback loops such that they can interact with their environment, but do not display actual intelligence..

" The last paragraph treats on the same kind of robot which I will investigate in my project.
The Miniature Intelligent Autonomous ro BOT - Blue Tooth (MIABOT BT) is a fully autonomous miniature mobile robot.

It uses bi-directional Bluetooth communications, which provides a robust frequency hopping wireless communications protocol at 2.4GHz.
By usage of this feature, MIABOT communicates with central computer which makes decisions based on the video analysis.

To make it even more complicated my project will be not about one but about 3 robots playing in one team against other 3 robots.
The idea is pretty easy: create a team of robots capable to play football.

It was initiated by Jong-Hwan Kim, KAIST, Korea in 1995 from International Organizing Committee, and previously was named: Micro-Robot World Cup Soccer Tournament.
It finally changed name in 1996 to Federation of International Robot-soccer Association (FIRA).

Since then the concept evaluated and 6 tournaments has been played all over the world.
Depending on robot types tournaments have been classified into 7 categories:

Humanoid Robots (HUROSOT)Khepera Robots (KHEPERASOT & S-KHEPERASOT)Micro Robots (MIROSOT)ROBOSOTNano Robots (NAROSOT)QUADROSOTSimulation of robots strategy (SIMUROSOT)
The MIABOT BT, which I will be developing, belongs to MIROSOT, what means it is nit more than 7.5cm x 7.5cm x 7.5cm.

Some academic research about this robot soccer prototype is made under supervision of Dr. Olivier C.L.
Haas from Control Theory and Application Centre (CTAC).

My project is only a small part of whole research but it is also one of most significant.
My first aim will be to declare how a single robot should approach the ball and how it should behave in reaction with environment.

My final target will be to develop strategy for entire three-robot football team.
Most of my work should include programming in C++.

Without the written software robots are not be able to cooperate or even move.
The aim is to create a tightly-collaborative and successful team of three separate robots.

And finally it will make a truly autonomous system of 3 MIABOT's which should be able to play robot football without any human supervision.
Critical review of relevant literature

Before coming into details of my project I would like to introduce the reader to the robot and the system itself.
System is relatively easy to understand:

Images from Firewire Camera are analysed by Computer.PC finds tracks and predicts paths of specified objects (ball, players) based on images.After analysing the data orders are send by Bluetooth to single robots.Robots start moving, and the same algorithms repeats.
MIABOT BT

That means that robots they own do not make any decisions, and all they have to do is to move in directions provided by the central computer.
But is it necessary for the computer to make all decisions?

Maybe the robots can do part of the work on they own, maybe they should use some of their features to improve the system.
Let us have a look at robot's architecture due to MIABOT user manual:

Processor
"Brain" of MIABOT is Atmel ATMega8 microprocessor.

"The ATmega8 is a low-power CMOS 8-bit microcontroller based on AVR RISC architecture.
By executing powerful instructions in a single clock cycle, the ATmega8 achieves throughputs approaching 1 MIPS per MHz, allowing the system designer to optimize power consumption versus processing speed.

"(Atmel Corporation, 2006).
This microchip is very efficient and at the same time relatively easy to control.

Additionally it contains 32 general purpose registers and 3 flexible timers.
All this features suggest that this unit is capable to do much more than following the orders.

Motion
MIABOT has 2 wheels both of which have their own engines and control systems and can move independently.

Thanks to that robot can change directions of movement.
Engines power enables robot to move with maximal speed of 1m/s.

(Merlin Systems Corp.
Ltd)

Communication Bluetooth
"Mobile robots need to communicate with their base station, with one another, and with humans.

Most robot electronic communication systems employed in the past use radio frequency, transmitters and recivers.
Currently, many mobile robots working both indoor and outdoor environments use wireless communication Wi-Fi (Bekey, 2005).

MIABOT BT's are supplied ready to support Bluetooth communications at 19200 bps.
Accordingly to Bluetooth specification single PC can communicate via Bluetooth with 7 robots at the same time.

But why use Bluetooth?
"From the user's point of view, there are three important features to Bluetooth: It's wireless.

When you travel, you don't have to worry about keeping track of a briefcase full of cables to attach all of your components, and you can design your office without wondering where all the wires will go.
It's inexpensive.

You don't have to think about it.
Bluetooth doesn't require you to do anything special to make it work.

The devices find one another and strike up a conversation without any user input at all.
"(Franklin, 2004) Thanks to these features central computer can be easily connected to each agent using separate, independent parallel connection.

That is why Windows default tool - HyperTerminal can be used to send commands to agents.
Development and Programming

To program the robot I decided to use C++.
The C High-level Language has become increasingly popular for programming microcontrollers.

The advantages of using C compared to assembler are numerous:
Reduced development timeEasier maintenance and portabilityEasier to reuse code

But nothing comes for free, C has larger code size and as a result of that often reduced speed.
To reduce these penalties the AVR architecture is tuned to efficiently decode and execute instructions that are typically generated by C compilers.

The result of the cooperation between the compiler development team and the AVR development team is a microcontroller for which highly efficient, high performance code is generated.
(Atmel Corporation, 2003).

Single robot motion
According to MIABOT manual this robot can decode few commands as:

StopTestFlashMove and turn in various directions with set speed and distanceSet power-level for all movement or turn operationsControl wheel speed and power
Making MIABOT autonomous requires ability to plan motions to be developed automatically, by composition of known commands.

Fortunately I can assume that my robot will move in laboratory, where nothing unexpected should happen.
Even though, "it would certainly be useful to incorporate automatic motion planning tools in off-line robot programming systems.

This would allow the user to specify tasks more declaratively, by stating what he/she wants done rather than how to do it.
As robots become more dexterous, the need for motion planning tools will become more critical.

" (Latombe, 1991)
Multiple robot path planning

Even though robot will be autonomous it does not mean that whole team will be.
To achieve autonomous team, the system has to be able to plan paths of all units.

There are a few different approaches to multiple robot path planning: Path planning is either centralised - with a universal path-planner making decisions, or distributed - with individual agents planning and adjusting their paths (Fujimara, 1991) Path planning is either centralised (as above), distributed (as above), or hybrid - combination of online, offline centralized or decentralized (Arai et al, 1992) Path planning is either centralised - takes into account all robots, or decoupled - planning the paths for each robot independently (Latombe, 1991) Finally there has to be a set of rules, and concepts, how the paths should be found - strategy.
Strategy

"The word derives from the Greek stratēgos, which referred to a 'military commander' during the age of Athenian Democracy.
Strategy is an umbrella plan encompassing a number of smaller plans for some objective."(Wikipedia Free Online Encyclopaedia) Strategy of the team depends on the position of every robot and the ball.

According to the position of robots and the ball in the game field, computer can decide which strategy can be used.
To choose the strategy, system should perform the following steps to accomplish the algorithm:

Check attacking direction Check position of the ball and predict its motionCheck position of all robots with prediction of their movementsCheck primary position of team membersDecide which strategy should be used (defensive, offensive)Check if robot will not break the rules of the gameSend appropriate orders to each robot player
Other variables such as size of robot, ball or game field should also be considered.(Bingrong et al)

Objectives
Objectives of this project are to:

Gather and analyse available literature and previous researches concerning the topicImplement simple commends and programmes on robots in laboratoryInvestigate how a single robot should attack the ballFigure the way for the robots to cooperateModel the best strategy in simulation modeImplement the strategy in physical system Investigate the efficiency of default microcontroller programme and my programmeEstimate if Bluetooth communication between team members will improve efficiencyImplement the final strategy on robot football team
Research Methods

Phase 1: Analysis of available data
First I will review relevant literature concerning following topics to find as much useful information as possible:

Robot motionRobot cooperationAVR microprocessors architecture and programmingMIABOT BTTeam games strategyC/C++ programme efficiencyBluetooth connections
After that I should be ready to gain more specified knowledge from Dr Olivier Haas.

This phase should allow me to gather essential data concerning the project.
Phase 2: Software studies

In this phase I should start to program the physical robot.
I will use the robot laboratory tasks to learn everything that is necessary about single robot's motion.

Starting from really basic commends I will finish at the stage when robot can autonomously approach the ball.
The whole phase should be done under the supervision of Dr Haas.

Phase 3: Strategy investigation
First of all, because single MIABOT is already able to move, I will try to make 3 robots move simultaneously but independently.

Than I shall reach the toughest part of the project: strategy.
To figure which strategy is the best I will have to try a few different variants, test them on simulation programme.

Finally I shall implement it on the physical system to figure if it is working correctly.
Phase 4: Analysis of Bluetooth and programmes' code efficiency

At this stage of research I will try to optimise my programme.
This includes the programme I wrote as well as the one that was already implemented in microcontroller.

Afterwards, I will implement the Bluetooth communication between robot team members, which shall allow robots to exchange information without help of central computer.
Finally I will check if this solution will improve efficiency of the system, which will help me to choose final programme.

Phase 5: Final tests and Reports
My final phase will include implementation and testing of the improved programme on physical system.

At this stage I will concentrate at writing my project report.
Project Plan

ABSTRACT
This report presents the result of usability test of URL which was conducted in December 2006 and the beginning of January 2007.

The study investigates how usable the web service is with the strong focus on customer satisfaction.
The intention is to provide the Carphone Warehouse with information how to improve their web site to make it more usable.

The test has been conducted using Heuristic Evaluation Method on the public version of the web site.
APPROACH

PURPOSE
The key purpose of the test was to assess the usability of the current web site of Carphone Warehouse.

METHOD
The test has been carried out with 4 participants who are all experts.

Their profile will appear in next section.
The method used for the test is Heuristic Evaluation.

The experts based on twelve principles of good interface design by Benyon, Turner & Turner.
PARTICIPANTS

The test was carried out by participants who fulfilled the following requirements: Age between 18-50 (the target group of the Carphone Warehouse) Very experienced Internet users Mobile services users To avoid having bias I have chosen the equal number of men and women to conduct the test.
Participants' profile

DISCUSSION OF THE METHOD
Heuristic evaluation is the most frequently used method of usability testing.

It was invented by usability consultant Jakob Nielsen.
Its aim is to specify the usability issues of the user interface.

It has been chosen for conducting the test as it is suitable for all stages of development process, including product review after market release (the case of our test).
In this method a group of experts is supposed to examine the interface according to a standard set of heuristics - evaluation criteria used in usability testing.

The experts should have a wide knowledge of human factors and human-computer interface.
However it occurs that also those less experiences may report valid usability issues.

Ideally the evaluator should also have (besides usability testing experience) understanding of subject matter.
A group of evaluators note down all elements which do not comply with the established usability principles and next put them into the order of descending importance.

It is desirable that the experts go through the interface at least twice: first time - to get familiar with the system and second time - to focus on specific interface elements.
This is quick and relatively cheap inspection method.

It provides valid and useful feedback for designers how to improve the user interface and what needs improving.
Plan

First the criteria for the experts should be established.
The criteria should take use scenarios into account.

Running
To make heuristic evaluation successful we must ensure that: Experts have enough time to become familiar with all essential information concerning the user context and scenarios.

All experts agree on evaluation criteria Experts do not communicate with each other during the test, each of them should conduct the test independently.
The experts write down their observations during surfing the web.

They are supposed to inspect various elements of the interface and compare them with the list of heuristics.
Clearly they are allowed to examine the interface with regard to any additional usability rules.

Having completed the tasks they are supposed to prioritize the usability issues.
At this stage they can consult with each other.

It has been proved that even a very experienced expert can fail to notice some of the usability issues therefore to achieve better results the test should be conducted by several evaluators.
One would think that the more evaluators we hire the better results we get.

Unfortunately this is not necessarily true as the benefit to cost ratio starts to decrease rapidly with too big amount of evaluators.
According to Nielsen the optimal number of evaluators is 3 to 5.

A typical expert session lasts 1 to 2 hours however in case of large and very complicated interfaces more time might be needed.
It is then recommended to have more than one session each concentrating on a specific part of the user interface.

Having conducted the heuristic evaluation we should get a list of usability problems with reference which heuristic has been violated.
The evaluators are supposed not only to point the problem but also explain why it is a problem (according to usability principles).

It is essential to be as specific as possible.
Basing on heuristic evaluation output it will be easy to redesign the system to make it more usable.

Heuristic principles
There are numerous different sets of heuristic designed by different usability experts:

Ben SchneidermanNielsen and MolichMok and ClementJenifer FlemingBenyon, Turner and Turner
This usability test will base on the set of heuristics by Benyon, Turner & Turner (from "Designing Interactive Systems"): Visibility - it is essential that functions of the interface are easy to notice by the user.

The system should also inform the user about its current status.
Consistency - it means that the interface should be the same throughout (use of colours, fonts, layouts).

It should also adhere to general standards of similar systems.
Familiarity - in the system user's language should be applied.

We must ensure that all words, phrases and concepts are familiar to the user.
Affordance - it is essential that the users know at the first glance what the product is designed and how to use it e.g. every child knows that buttons afford pressing and knobs afford turning.

Some of the affordances are determined culturally.
Navigation - it is crucial for a system to be simple to navigate.

The user should easily find adequate menus and links placed in some logical order.
Control - it is important that the user know what control s/he can take, that s/he recognise the control menus and that s/he can see the results of the taken action.

Feedback - the user who receives feedback from a system can assess the results of the taken actions and has a feeling of control.
Recovery - the designer should remember that the future users of the system are only humans and as such they make mistakes.

The system should enable quick and effective way to correct them e.g. in case of filling a long form we should enable the user to change the content of just one, incorrect field (e.g. spelling mistake) instead of filling the form from the beginning.
Constraints - the ideal system should not allow the user to perform inappropriate actions.

It should also prevent the user from making serious errors e.g. by asking for confirmation.
It is highly recommended that system asks for confirmation in case of payments and other money transfers.

Flexibility - flexible systems enable to achieve goals by different means as users from various backgrounds may perform tasks differently.
The user should be also given a chance to change the look of the system (e.g. various colour schemes in Microsoft Windows) Style - the design should be stylish and attractive for the user.

It is also essential that the dialogues do not contain irrelevant information.
Conviviality - the system should "behave friendly" and the use of it should give satisfaction.

Constant error messages cause frustration and make the user leave the system.
TEST RESULTS

The test pointed out several problems which have been classified into 3 groups in descending order of importance:
HighMediumLow

Major problems
Recovery and navigation

Severity HighHeuristics Recovery and navigationDescription After adding things to the basket and choosing checkout option it is difficult to find a way back: no back or home button.
To go back the user must use "back" button of the web browser.

The interface does not provide any way to delete items from the basket at this stage.Solution Placing an additional "back" button.Rationale Before making final decision (providing credit card details etc.) there should be a way to go back and change basket content.
Navigation

Severity HighHeuristics NavigationDescription Search for pay as you go offers has different options: you can choose keyword and/or make and/or network.
Underneath there is a nice option to sort by best selling, price, or alphabetically.

Unfortunately you can sort only all available phones; you cannot sort e.g. Nokia phones by price.
Solution It should be possible to sort the result of previous search.

Rationale It is very common that people would like to see e.g. sorted list of phones of one make.
Style

Severity HighHeuristics StyleDescription The style of the main page is not very attractive.
At the first glance the page is very difficult to navigate, the user is provided with too much information, there are too many offers etc.

Solution Redesign of the central partRationale The web site is difficult to read.
Consistency

Severity HighHeuristics ConsistencyDescription When we click on most of the options of the top menu we change the colours of the menus at the top, on the left and the colouring of the offers.
We also adjust the left menu to the option chosen.

However, clicking on "Accessories etc." changes only the colours of the top menu and does not influence the left menu.Solution Make the menu on the left and the offers look consistent to the colours of the top menu in all cases.
It is also essential to adjust left menu adequately to chosen option, so that the menu links concern the Accessories etc.

Rationale Idea of different colours and different left menus for different services makes the service easier to use and look attractive however the designer must remember about consistency.
Style

Severity HighHeuristics Style Description There is a huge unused white space on both sides of the web page.
When surfing the page on lower resolutions e.g. 600X800 the white stripes on both sides disappear.

Solution We need to adjust the page to higher resolution.Rationale Higher resolutions are nowadays far more frequently used and the page looks much better if it fills the whole space given.
Constraint

Severity HighHeuristics ConstraintDescription After ordering an item we can choose the delivery date.
Unfortunately the only possible option is the next day.

Solution The user should be given more dates to choose from.Rationale The fact that we want to order an item does not mean that we are available to receive it the following day.
Medium problems

Navigation
Severity MediumHeuristics NavigationDescription When you click on Every phone & offer link (on pay monthly offers) on the left side menu many pages of offers are found.

The user has no idea how they are sorted: no logical or alphabetical order.
Solution Additional "sort by" optionRationale People are not going to check all subpages therefore they should at least know the sorting criteria are.

Affordance
Severity MediumHeuristics AffordanceDescription There is no link on "click here to upgrade" at the place where we expect it to be.

The test participants were unable to find the proper link.
Solution Place the right link.Rationale The text "click here" makes user click at this place.

Familiarity
Severity MediumHeuristics FamiliarityDescription Test participants did not understand that they had to click Continue Checkout in order to buy the products they had selected.

Solution Participants recommended Place order option instead of Continue CheckoutRationale It is easier to notice familiar phrases.
Constraint

Severity MediumHeuristics ConstraintDescription It is possible to place the "out of stock" items in the basket.
User who orders a product without noticing that it is "out of stock" is not notified about this fact directly.

While checking what "out of stock" means we are told the following " (..)When we have more in stock we will process your order." However while processing the order we can choose only the next day as the date of delivery.
It is unclear and confusing.

Solution There should be a clear information what actions will be taken when more stock will arrive (will they send us an email, call us etc.?)Rationale User can expect the product on the date chosen even though it is out of stock.
Navigation

Severity MediumHeuristics NavigationDescription When we go to Accessories > Chargers & Batteries there is a dropdown menu to select manufacturer.
Sony Ericsson option is displayed twice and we assume both options are the same.

However it occurs that the first one enables to choose models of Ericsson and Sony Ericsson whereas the second one enables to choose models of Sony brand.
Solution There should be three separate options: Ericsson, Sony, Sony Ericsson.

Rationale Two options of the same name cause much confusion and people think that both point to the same place.
Constraint

Severity MediumHeuristics ConstraintsDescription After adding a mobile into a basket, a Lifeline insurance is also added automatically without asking user for confirmationSolution The system should ask for confirmation when a service is not freeRationale People will be surprised that they have to pay for something they haven't ordered and they will not recommend the service to their friends.
Minor problems

Conviviality
Severity LowHeuristics ConvivialityDescription The search facility occurred to be intolerant to any spelling mistakes e.g. the search for "ericson" instead of "ericsson" gave no results.

Solution More tolerant search facilityRationale It is very common to make a spelling mistake while typing.
Search should find some spelling mistakes and suggest the correct spelling (like e.g. Google)

Constraint
Severity LowHeuristics ConstraintDescription While subscribing we are asked to fill the form.

In one of the gaps we need to provide the date of birth.
It occurs that we can subscribe somebody who has not been born yet.

Solution The future dates should not be available.
Rationale Providing fake data is not suitable and makes statistics unreliable.

Recovery
Severity LowHeuristics RecoveryDescription It is impossible to empty the bin at once.

We need to remove items separately.
Solution The "empty the bin" option should be available.Rationale This option is very useful.

Familiarity
Severity LowHeuristics FamiliarityDescription While processing the order we need to provide the title.

The options provided are untypical and except from "Miss" etc they contain e.g. Lord, Chief.
It causes some confusion.

Solution Provide only standard option available typically on all forms.
Rationale People are used to provide standard titles.

Consistency
Severity LowHeuristics ConsistencyDescription Clicking on Every phone & offer link gives different possibilities for pay monthly and pay as you go phones.

Solution Add the sort by menu for pay monthly offers.
Rationale It will be much more convenient.

Visibility
Severity LowHeuristics VisibilityDescription On some offers information "limited stock" appears.

Having clicked on the link a new window appears (Figure..).
The window is non-resizable and is not appropriate for the displayed text (we need to use not only the horizontal but also vertical scroll bar).

Solution The text should precisely fit the window so that is more visible.
Rationale It is easier for the user to see the whole text always if it is possible.

In this case the amount of text is small enough to be shown at once.
Navigation

Severity LowHeuristics NavigationDescription On the map we can see 10 cities.
Clicking on the map provides us a list of shops (probably the nearest to the place clicked).

When we try to click on Birmingham we get the list of shops situated not in this city.
It is very difficult to click on the city that we want to.

Solution There should be clear links for major cities.Rationale It is impossible to click precisely on the spot.
It is much easier to type the searched city (we can do it on the right side of the map)

SUMMARY
Test output summary

Having conducted the test the participants have found a few usability problems.
Even though they concern mostly aesthetic issues some of them need immediate attention as they have been classified to major severity group.

Most problems are easy to solve by the web designer - in some cases there is a need only to place additional link.
Some are probably just an oversight (e.g. lack of link on "click here" text).

In the future the designer should pay more attention to the text on the images.
The service is very complex and is changing quite frequently therefore we assume that at least some of the problems will be resolved in the nearest future.

Usefulness of the method
Heuristic evaluation is a low-cost and low-resource technique which gives broad results.

Even though only a few experts are involved it allows finding many usability problems.
Most test participants will probably find the same problems, but there will be also some issues identified only by one of them (what justifies the plural number of testers).

The experts not only point out the problem but also describe why it is a problem and they suggest the way to solve it.
It is highly advisable to test the system at early stages as it is easier and cheaper to solve problems.

However we must remember that even after market release some issues can be solved at low cost (e.g. adding a button, making the link work, changing the colour of menu etc.).
Purpose

The purpose of this report is to evaluate the usability of the current way-finding system for Armstrong Siddeley building and based on this evaluation prepare the prototype of the redesign.
Overview

The way-finding systems are designed in order to give people clear and appropriate directions in places they are unfamiliar with.
The most important attribute of such system is its usability and satisfaction it gives to the user.

It is crucial that the way can be found as quickly as possible.
In case of our prototype the target time for locating the person / room does not exceed 2 minutes.

The idea of introducing the way-finding system for Armstrong Siddeley building was to help the following people to find the way without help of reception (which might be closed):
First time visitors to the schoolVisiting academics arriving for meetings with staff Student from other schools attempting to locate tutorial roomsFiends/relatives of students

It appears that the current system is not usable and does not meet user requirements.
Usability problems of current solution

It appears that the current system is a very good example of a bad design therefore we are not going to concentrate on finding solutions to improve it but we will consider all the usability issues that were found to make redesign of the system more usable.
Visibility problem

Severity HighImpact User dissatisfaction
Probably the most important problem of current way-finding system concerns the visibility.

At the first glance user has no idea what are the features of the system.
What is more, the user might even not notice what the system is supposed to do ("ROOM / STAFF LOKATOR" in the left corner is not highlighted and is not eye catching at all).

This system will be installed on the computer by the entrance to the building.
As the first time visitor has no idea what this computer is for, there should be a short but easily noticeable note e.g. "LET ME HELP YOU FIND YOUR WAY" on the main page.

Navigation problem
Severity HighImpact Difficulty of navigation

One of the main problems of this system concerns navigation.
In a well designed system the user is able to find adequate menus and links.

In case of current way-finding system there are no menus and everything is put in one place, without any logical order.
User gets confused and does not try to use the system.

S/he goes to ask the reception instead.
In the prototype we will focus on grouping links and putting them in some logical order.

Style problem
Severity HighImpact User dissatisfaction

The current way-finding system is found unstylish because of too much information on the main page.
This information should be decomposed into sub pages.

The instruction how to find a person is written at the beginning of the page while it should be presented e.g. as a list of steps to perform.
The user will not remember the whole instruction at once; therefore he should be advised to take one step after another.

Familiarity problem 1
Severity HighImpact User confusion

Another very serious problem in current system concerns the way the map of the ground floor is presented (the rest floors would be probably designed in the same way).
When we click on "Ground floor" link the map of selected floor is presented.

Unfortunately there is no legend (the information "Teaching rooms are highlighted in blue on the map for each floor" was given on previous - main page) what can make user confused why some rooms are highlighted in blue.
A smaller problem with the map is the lack of "Back" button - navigation problem.

Familiarity problem 2
Severity MediumImpact User confusion

One more problem I found important in the current way-finding system concerns the way the e-mail addresses are presented e.g. r.bali.
Only part of the email with the person name is shown whereas the server domain is not visible.

It might cause problems for potential students and their relatives as they would not know that the domain is coventry.ac.uk.
The best and easiest way to fix it is just add the domain name to the email address e.g. r.bali@coventry.ac.uk.

Task analysis
The way-finding system is supposed to assist the user in locating appropriate rooms or staff members.

I have asked potential users what tasks they would expect the system to perform.
Basing on the most frequent answers I decided to create five main tasks (each of them will have a separate menu).

Most of the tasks will contain different options to make searching more successful and flexible.
For any search request the system should respond not only with the room/floor number or name of staff member but also should give the directions how to get there (display a map of the floor with desired room).

Task 1 - Search for Person
This feature will enable the user to find the right person.

It will contain several options: Search by name - the search facility should allow for minor spelling mistakes due to high number of international students and lecturers.
Search by subject - users do not always know what the name of the lecturer they are looking for is but they might know what subject he teaches.

What is more the potential student will not look for particular lecturer because of his name but he might want to speak to a person who teaches some particular subject e.g. to discuss the module content and objectives.
Search by role - again potential students or their parent might want to speak with e.g. Head of Department not because of his name but with regard to his role.

Once the user finds the staff member he is looking for, he should be provided with information regarding not only room number and its location but also with working hours and the schedule of the person.
Task 2 - Search for Room

This feature will enable the user to locate the right room.
Similarly to looking for person, it will also contain a few options:

Search by room numberSearch by room name (e.g. reception or laboratory)Search by module - this option might be useful if we are trying to locate where a lecture is taking place
The system should display a map of the floor with the searched room.

Task 3 - How to submit a coursework?
This feature will be used by student's friends or relatives who come to submit the student's coursework.

The system will show the way to Academic Office (a map) and will also give information about the latest time allowed for submission.
Task 4 - Maps

This feature of the system will show the map of selected floor.
It will be useful for potential students who are not searching for a particular room but only want to get familiar with the building.

The legend for all maps will be also provided.
Task 5 - Staff List

The system will also enable to list names of all staff members.
This approach might be useful in case the user does not remember the exact name but would recall it when he sees it written.

This feature is very easy and low cost to implement and very handy for the users at the same time.
The prototype that I build will comprise all above named features and can be further developed in order to increase user satisfaction.

In a final system all the maps should be interactive - when the user moves the mouse over a room, information concerning this room should be displayed e.g. whose office it is etc.
Prototype

In the process of developing the prototype I based on task analysis performed in previous section as the most important attribute of the system is meeting user requirements.
I have also considered and tried to avoid all usability problems described in section 3.

The system will be compliant with usability heuristics by Benyon, Turner & Turner.
There is menu on the left hand side displaying the system features.

This solution makes the system easy to navigate.
When a user clicks on any of the links, the relevant page will be displayed in the main window.

"Home" button enables coming back to the main page.
There is no need to place any "back" buttons as the system is very simple and does not have nested sub pages - to go back user always can click on either appropriate link or "Home" button.

The main page of designed system is shown at Figure 1: On the main page I have placed the "search" option.
It is a general search which enables to search for any phrase within the way-finding system.

"Search for person" page is shown at Figure 2: "Search for room" page is shown at Figure 3: "How to submit coursework?
" page is shown at Figure 4: "Maps" page is shown at Figure 5: To choose the floor the user needs to click on the floor link.

The map of the chosen floor will then be displayed (see Figure 6).
The legend and floor links will still be available (so that the user does not have to memorize what the colour means etc.).

In the future the user should also have option to print the map with the legend but this option will not be available at the first release as there is no printing device.
"Staff List" page is shown at Figure 7: Figure 8 shows an example search result.

We searched for a person, by name:"Hodder".
The system displays information concerning the searched person including his role, room number, phone number, email address and working hours.

System also shows a map of the relevant floor with the route marked on it.
It also gives the directions in text form (useful in case of any higher floors as it is impossible to show on the map e.g. "take a lift to 3 rd floor").

Summary
The proposed prototype seems to meet all user requirements.

It is simple and therefore easy to use.
It does not contain any irrelevant information which would make it less visible.

All main features of the system have their links on the left hand side menu which is available all the time.
Proposed solution is compliant to the set of heuristics.

Part 1: Communication and synchronisation mechanisms and formal method definition
Semaphores, monitors and message passing

Semaphores
Semaphore is an abstract data type invented by a Dutch computer scientist E.W.

Dijkstra in middle 1960's.
It was first introduced and successfully implemented to protect critical sections.

Semaphore is a shared, nonnegative integer variable which can be manipulated exclusively by only two operations:
P(s) for wait (Dutch word Passeren)V(s) for signal (Dutch word Vrygeven)

Both operations are indivisible - it means they are executed as atomic actions.
The third operation on semaphore is its initialisation "init".

There are two approaches in using the semaphores for synchronisation:
Binary semaphores - can have only two values: 0 and 1Counting semaphores - can have any nonnegative values.

How the semaphore works
The current value of the semaphore represents the number of free resource units that can be used at this time.

The binary semaphores are used in cases, where there is only one unit which can be either available or not available.
For plural number of units the counting semaphores are implemented.

If the semaphore value is positive it means that there are available resources and that they can be used.
Each resource unit occupation results in decrementing the semaphore value.

If the semaphore value is 0 it means that all resources are currently being used and the process must wait until the resources are available again.
Operations on semaphores

In order to decrease the value of the semaphore we use P(s) operation.
This operation decrements the value of s as long as s>0: FORMULA FORMULA the process is put to wait until s is positive so that the process may proceed again In order to increase the value of the semaphore we use V(s) operation.

If s was equal to 0 and there are any processes waiting, one process will be then awaken and able to proceed.
In other case s is atomically increased by one.

The "init" operation is performed only once for a single semaphore before any semaphore requests are made e.g. FORMULA
Examples of usage

The most common semaphore is a binary semaphore used for Mutual Exclusion.
It controls the access to a single resource used by two or more different processes.

Its aim is to assure that only one process at a time can access the resource.
This kind of semaphore is typically initialized with value 1.

It is decreased to 0 each time a process accesses the resource and then increased to one each time the resource is freed.
We can also use semaphores when we want to make sure that one section of the code is run before another section executed by completely different process.

Disadvantages
Semaphores are relatively difficult in implementation.

It is very easy to make mistakes such as forgetting the signal operation V(s), especially when several semaphores are used.
Due to lack of relationship between the operation name and mnemonic used for its description it is also very easy to mix up wait(s) - P(s) and signal(s) - V(s).

Monitors
Monitor is a structure invented by a Danish computer scientist Per Brinch Hansen in 1972.

Monitors are implemented to assure synchronization of different processes that use some shared resource.
They were introduced as a more structured approach than semaphores.

They can be found in many concurrent languages e.g. Java.
The monitors became very popular as they provide a very simple and safe way in which the programmer can obtain mutual exclusion (Axford 1987).

Monitor structure
The monitor consists of:

The shared variables associated with the resourceA set of procedures which can operate on the shared variables
FORMULA

How the monitor works
The key point of monitor approach to synchronization is that only monitor's procedures can access the shared variables.

Only one process can run monitor's procedure at a time.
It guarantees the mutual exclusion.

If one process is running the monitor's procedure, no other process can run any procedure of this monitor until the first one finishes and leaves the monitor.
It makes the use of shared variables easier for programmers - they do not need to assure the exclusive use of shared resources, it is enough to place them inside the monitor which will automatically take care for that.

In case when a process wants to run monitor's procedure which is currently used by another process, its request is put on queue.
Condition variables

Monitors also have condition variables, on which a process can wait if conditions are not right for it to continue executing in the monitor.
Some other process can then get in the monitor and perhaps change the state of the monitor.

If conditions are now right, that process can signal a waiting process, moving the latter to the ready queue to get back into the monitor when it becomes free (Hartley 1997).
Operations on condition variables

We can perform the following operations on condition variables:
wait(c) - it results in adding the calling process to the queue.

It releases the monitor what means that other processes can now access it.
signal(c) - it removes the head process from the queue.

In case the queue is empty it does not cause any result.
The awaken process resumes execution of instruction following after wait(c).

signal_all(c) - (sometimes referred to as broadcast signal) it removes all processes from the queue.
It has no effect if the queue is empty.empty(c) - it is a logical function.

It returns true in case there are no processes in the queue.
Even though wait(c) and signal(c) look similar to P(s) and V(s) there are crucial differences between them (Andrews 1991):

signal(c) gives no results in case the queue is empty (whereas the semaphore would be increased)wait(c) always delays the process until signal(c) is executed (whereas P(s) delays the process only if s=0)
Message passing

So far I have described solutions which can be used only in a shared memory environment.
Nowadays, when network architectures are becoming more and more popular, a new solution needed to be introduced.

This is where the message passing evolved.
Sometimes it might be also convenient that processes executing on the shared memory architecture use the message passing as communication and synchronisation mechanism instead of using the shared variables.

It happens e.g. when processes are executing on behalf of different users (Andrews 1991:339).
Message passing operations

The primitives operations for message passing mechanism are:
send (destination, message)receive (source, message)

How the message passing works
In case of message passing mechanism processes share channels.

To initiate communication one process sends a message to a channel; another process acquires the message by receiving it from the channel (Andrews 1991:339).
Message passing models

There are three message passing models:
synchronousasynchronousbuffered

In asynchronous message passing, channels are unbounded queues of messages.
Execution of each send statement attaches a message to a queue.

As the channels are supposed to be unbounded the sender is not delayed.
To receive the message the process executes receive statement.

If the channel is empty the receiver is delayed.
Using the asynchronous message passing model has three major disadvantages:

We can get the situation in which the receiver is far behind the sender.
The sending process does not known about any failures that can occur during transmission.There is a need to buffer the messages.

It is obvious that buffers are not infinite therefore in case of sending too many messages the programme can either crash or send will be blocked.
In synchronous message passing we can avoid these problems.

In this model both send and receive statement are blocking primitives.
The communication is performed synchronously that means that the sender blocks until the receiver does a receive and the receiver blocks until the sender does a send.

In buffered message passing the channel has a fixed capacity.
In case the channel is full the sender is delayed.

If the channel is empty the receiver is delayed.
The significance of Formal Methods in designing safety critical Real Time & Concurrent software systems

While designing new software, the main goal for the software engineer is to ensure that his product is reliable and predictable.
Therefore he must make sure that all design flaws are eliminated.

Unfortunately the more complex the system is, the more difficult it is to ensure its correct performance.
There are three main approaches in handling the design flaws:

testingdesign diversityfault avoidance
Several researches have shown that the fault avoidance approach is the best one to use for high integrity systems.

Formal Methods are the most rigorous of all fault avoidance approaches and therefore they are most hopeful.
Using the computers for monitoring and controlling safety critical system is a convenient solution on one hand, but very dangerous on the other.

Making a computer system responsible for controlling planes, medical devices or nuclear power plant puts our life in risk.
Even a single bug can be considerably hazardous.

Therefore the Formal Methods - as most promising - should be used in all applications where even a single error can be critical to the system or worse - to our life.
The software engineers must never forget that their products control several systems that influence our life: trains, planes, medical equipment.

Formal Methods are mathematical techniques and tools used for:
specificationdesignverification

of software and hardware systems (Butler 2001).
The aim of formalization is to decrease the risk of severe specification and design errors.

Even though the use of Formal Methods is relatively difficult and expensive, it also gives numerous benefits.
The Formal Methods enable verifying system correctness by mathematical methods, which is a much more reliable approach than system testing.

Thanks to using Formal Methods
the defects can be found earlier and therefore its correction is easier and less expensivecertain properties of the system can be checked automaticallythe rework is decreased

Without use of Formal Methods the requirements are written in informal manner.
Such approach can be very risky as the specification is then:

too general - it does not precise how to satisfy the requirementsnot accurate enough - different people can interpret it differently
A way to avoid these problems is to employ Formal Methods.

Formal Methods are very specific; they make designers think about the problem more thoroughly as they are supposed to describe the problem in precise formal language.
Specification expressed in mathematical notations is a very accurate description of desirable behaviour.

It has been proved that formalisation improves system quality, consistency and integrity.
There are many different Formal Methods to choose from:

Abstract State MachinesB-MethodCommUnityLotosPetri NetsSDLTRIOVDMZ
Once it is decided to use Formal Methods for the project, we need to think which one to use.

Part 2: Real-time application
A detailed explanation of the application

GREENHOUSE SYSTEM
The real-time application that I am going to describe is the Greenhouse System.

As greenhouse is supposed to provide "weather conditions" suitable for cultivating plants there is a strong need to monitor and control these conditions.
Therefore the real-time system will be designed.

This system will be used to monitor and control the current conditions inside the greenhouse.
It analyses (Analyser) information provided by sensors (Sensor Monitor) and based on it decides what action should be taken (Decision System).

The actions are taken if data is incorrect (conditions are different from desired).
In case of major variances between current and desired conditions the failure will be detected (Check for Failure) and the alarm will be activated (Activate Alarm).

All sensors of each type provide information simultaneously and the system must respond within given time limits.
The Greenhouse System is an example of a soft real-time system.

It means that any time delay does not cause any critical error but only decreases the quality of the service.
It is not that critical for the greenhouse if e.g. the temperature is a little bit too low for while and that the heating will be turned on a minute later.

Therefore it is assumed that the sensors will be read every 5 minutes and that the system should analyse the data and take suitable actions within 3 minutes.
The following processes will be monitored and controlled in the Greenhouse System:

Temperature-sensor monitor - checks the temperatures and sends the data from all sensors to the temperature analyserHumidity-sensor monitor - checks the humidity and sends the data from all sensors to humidity analyserCO 2 - sensor monitor - checks the level of CO 2 and sends the data from all sensors to CO 2 analyserLight-exposure sensor monitor - checks the light exposure and sends the data from all sensors to light analyserTemperature Analyser - collects information from temperature monitor.
It compares the reading from each sensor with the expected value and sends the difference to Heating/Cooling Decision SystemHumidity Analyser - collects information from humidity monitor.

It compares the reading from each sensor with the expected value and sends the difference to Humidifying/Drying Decision SystemCO 2 Analyser - collects information from CO 2 monitor.
It compares the reading from each sensor with the expected value and sends the difference to Air Exchange Decision SystemLight Analyser - collects information from light monitor..

It compares the reading from each sensor with the expected value and sends the difference to Change Light Exposure Decision SystemTime Analyser checks the current time and compares it with time set by administrator for activating irrigation and sends the data Irrigation Decision SystemHeating/Cooling Decision System - receives data from Temperature Analyser and based on it decides whether to activate radiators or notHumidifying/Drying Decision System - receives data from Humidity Analyser and based on it decides whether to activate humidifier or notChange light exposure Decision System - receives data from Light Analyser and based on it decides whether to change the light exposure or notAir exchange Decision System - receives data from CO 2 Analyser and based on it decides whether to exchange air or notIrrigation Decision System - receives data from Time Analyser and based on it decides whether to activate irrigation or notCheck for failure - this process checks if there are any failures e.g. data collected from at least one of sensors placed all around the greenhouse is totally different from expected.
The highest acceptable variance between current and desired parameters is set by the user.

If the failure occurs the alarm is activated and the failure is reportedReport - all data collected from the sensors and analysed and are placed in the report automatically.
Also the user can have access to the data.

Activate alarm - this process informs the greenhouse owner/administrator that the failure has occurred by turning on the alarm.
The system contains the user console so that the system administrator can define:

desired temperatures Tempdesired humidity Humdesired level of CO 2 Levdesired light exposure Lighttimes when the irrigation system is activated Timethe highest acceptable variances between current and desired temperatures ΔTemphumidity ΔHumlevel of CO 2 ΔLevlight exposure ΔLight
If the variance is higher than acceptable the alarm will be activated Administrator can also request the report data.

The exemplary scenario 1 (a failure detected)
The desired data parameters are set with the user console e.g.:Temp Hum Light LevTime (when the irrigation is activated) ΔTempΔHumΔLightΔ CO 2 Monitors read the sensorsCurrent parameters from Monitors and the clock are sent to Analysers Analyse the dataDifference between expected and current temperature is bigger than acceptable Δtemp > ΔTemp - failure detectedActivate alarm and report data

The exemplary scenario 2 (no failure detected, but the temperature too low)
The desired data parameters are set with the user console:Temp Hum Light Lev Time (when the irrigation is activated) ΔTempΔHumΔLightΔLev Monitors read the sensorsCurrent parameters from Monitors and the clock are sent to Analysers Analyse the data by comparison: the temperatures: desired - Temp and current - tempthe humidity: desired - Hum and current - humthe light exposure: desired - Light and current - lightthe CO 2 level: desired - Lev and current - levcurrent time with the administrator settings Time for irrigation systemThe results are:Temp > temp Hum = humLight = lightLev = levTime ≠ time - no need to activate the irrigation systemSend the following data to Report, Decision System and Check for FailureΔtemp = Temp - temp = 2° CΔhum = 0 Δlight = 0Δlev = 0 Time ≠ timeCheck for failure:Δtemp < ΔTemp - no failureΔhum < ΔHum - no failureΔlight < ΔLight - no failureΔlev < ΔLev - no failureThe Decision System orders:to activate heating due to the temperature differencenot to activate humidifiersnot to activate air exchangenot to change the light exposurenot to activate irrigation systemMonitors read the sensors...

Data flow diagram, inputs and outputs
Figure 1 shows the general Data Flow Diagram of the whole system:

The administrator sets the desired parametersSensor Monitors and a clock provide the current parameters inside the greenhouse to Data AnalyserData Analyser compares the actual data with desired ones.The compared data are sent to Decision System, Check for Failure and are added to ReportDecision System is responsible for activating heating/cooling, humidifying/drying, air exchange, changing light exposure and activating irrigationThe analysed data are checked for major variances (Check for Failure) and in case they appear the alarm is activatedAll data from Check for Failure and Data Analyser are written in the report.
"Sensor monitor" is a generic name for the whole set of processes shown at Figure 2: "Data analyser" is a generic name for the whole set of processes shown at Figure 3: "Decision System" is a generic name for the whole set of processes shown at Figure4: In the following table I will show inputs and outputs of all processes:

Part 3: Definition of concurrent processes and synchronisation mechanism.
Concurrent processes definition.

Communication and synchronization mechanisms
In the Greenhouse System there are many concurrent processes.

In fact all of them (excluding Activate Alarm) are running simultaneously.
All the processes contained in Sensor Monitor are executed at the same time (e.g. every 5 minutes).

Than all processes gathered in Data Analyser analyse the data at the same time.
Another set of concurrent processes is grouped within Decision System.

In this part of the coursework I will write a programme using SR language to show how the processes communicate and synchronise their access to shared variables.
In order to provide mutual exclusion semaphores will be implemented.

I choose the following processes:
Analyse Current TemperatureChange Desired Conditions Reading Report

This process is similar to all the processes from its group and all of them would be implemented in the same way.
This process has access to the following shared data: desired temperature which is set in Change Desired Conditions process Report which can be read by the system administrator.

In this case temperature is the only desired parameter that is changed.
In the same way we can change all the desired parameters (each parameter must have its own semaphore).

Temperature is of course the shared data.
The administrator can read the report which is also a shared resource.

To implement these processes I need to use two semaphores:
One for desired temperature - two processes have access to this variable: Change Desired Condition and Analyse Current Temperature.

While the desired temperature is being changed it cannot be accessed for comparison purposes.
One for Report - two processes have access to this variable: Analyse Current Temperature and Reading Report.

While the data are added to the report it cannot be read by the administrator.
SR code

The programme in SR: FORMULA A part of output from the programme: FORMULA
INFORMATION SECURITY (Question 1)

AIM:
The aim of this section is to:

Define what is information securityDescribe in details what are the information security goalsDescribe in details all possible threats to information on this system assuming that a computer is used as an information system.
What information security is

Information security is protecting the data against unauthorized access or modification.
This term applies not only to the electronically stored data but also to all aspects of safeguarding information, in whatever form or media (Wikipedia 2006).

Computer and network security is nowadays the most important field of computer science.
The main objective for all computer scientists is to ensure their systems to be secure.

We do not want unauthorised people to access our private information such as e.g. bank statements.
Therefore there is a strong need to deal with information security issues.

Some people might think that to provide information security we need only to control the access to the data.
It occurs however that this concept is just one of information security goals.

The most important aims of information security are (Lecture notes 2006):
ConfidentialityAuthenticityIntegrityAvailabilityNon-repudiation

Information security is an essential issue especially where electronic transactions are concerned.
We want to assure that information that user supplies to the web server (e.g. username, password, financial information) cannot be read, modified or destroyed by any third party.

We want the similar protection for the data that flow back from the web server to the user.
Security services

To understand the relevance of the information security issues, first we need to understand the meanings of the terms related to the topic.
Confidentiality - is ensuring that only authorized people have access to the information.

To achieve confidentiality, cryptosystems need to be developed and deployed.
The cryptosystems use modern cryptographic techniques for higher security level.

Information is protected by transforming it into unreadable format.
This is the process of encryption.

To read the data we need to decrypt them.
Only authorized person has the key which is needed for this process.

We must remember that not all the data are confidential e.g. special offers of the companies are the data which should be available among as many people as possible.
Authenticity - is verifying the digital identity of the person, computer or computer program.

We need to know if somebody/something is not pretending to be someone else.
We must remember that authenticity says nothing about the right to access the data, it only checks the identity.

To achieve authenticity we use: digital signatures - works similar to written one.
It is attached to the message.

It guarantees that the individual is the one whom he claims to be.
digital certificates - is also attached to the message.

Its aim is to verify the identity of the sender.
The sender applies for a digital certificate from Certificate Authority CA.

Integrity - is assuring that the data is changed only in a specified, authorized manner.
Any unauthorized changes must be detectable to authorized users.

It is difficult to prevent the data change, it is much easier however to detect it.
Therefore there is a strong need to back-up data regularly and in case of error detection use the back-ups.

To detect the attack on the integrity we use: Cyclic Redundancy Check CRC - it is a type of hash function to produce the checksum.
The checksum is verified by the recipient to check the data integrity.

Message Authentication Code MAC - MAC value is used to protect both integrity and authenticity.
It is generated and verified using the same secret key.

Therefore it does not assure non-repudiation - anybody who can verify the MAC value can produce it for other message.
Availability - is ensuring that the data and services are accessible by authorized parties.

The server which is not accessible is just useless.
To achieve availability of our server we must assure that no Denial of Service DoS attacks will be performed on our system.

There is a number of software (e.g. firewalls) administrators can use to prevent those attacks.
Denial of Service attack is based on flooding the server with huge traffic.

There are many different ways to conduct DoS attack:
SYN floodsICMP floodsUDP floodsTeardrop attackPing of Death

Non-repudiation - is a way to guarantee that the sender of a message cannot later deny having sent the message and that the recipient cannot deny having received the message.
This issue is essential for contractual documents such as e.g. payments.

To achieve non-repudiation we use e.g.:
digital signatures (explained above)confirmation service - the message travel agent creates receipt confirming sending/receiving the message

After the short insight to the subject we will discuss deeply the importance of information security, different kinds of threats to the systems and ways to avoid them.
Why we should protect the data

We need to understand why data protection is so important.
Everybody has his own secrets and does not want to reveal them to the rest of the world, we tell the secrets only to our friends whom we trust.

The case of data protection is a similar one - there are some data which need to be confidential.
The issue of confidentiality is essential in many areas, e.g. in business - there is a strong need to protect strategic plans, research data, product information, personnel data etc.

We cannot allow other companies to access our data and copy our ideas as it could lead to bankruptcy of our venture.
Another important case, where confidentiality plays an important role is internet banking - allowing strangers to access our account or read our credit/debit card details can result in stealing money from our account.

Now that we have proved the importance of data protection against unauthorized access we should think who is responsible for determining which data need to be confidential, what the level of protection should be and who is authorized to access and change the data.
The right person for this task is just the owner of the information - he is the one to decide about the data confidentiality.

There is no single mechanism which one should use to assure security of the system and data.
Security attacks

Normal flow of digital information (without participation of any third party) leads from the source (e.g. file, memory) to the destination (such as another file or user).
See Figure 1.

Unfortunately this flow of information can be attacked in the following ways (Stallings 1998:7):
InterruptionInterceptionFabricationModification

We can divide the attacks into two categories:
Passive:interceptionactive:interruptionmodificationfabrication

Interception - it is a passive threat.
It occurs when unauthorized party gains access to the data (see Figure 2).

This is an attack on confidentiality of the data.
The data can be accessed in an unauthorized way by:

personprogramanother computer
This attack is a very difficult one to detect as the data is neither blocked or modified, therefore we try to prevent them rather than detect.

Interruption - this attack concerns availability of the data or system.
The data becomes either inaccessible or unusable (see Figure 3).

The whole system or the part of is lost either because of physical damage to the hardware or due to software manipulation on the data.
Denial of Service attack is an example of interruption.

Fabrication - is an attack on the authenticity of the data.
It means that some data has been counterfeited.

For graphical explanation see Figure 4.
Modification - is an attack on the integrity of the data.

An unauthorized party not only gets access to the data but also alters it.
WIFI NETWORKS (Question 2)

AIM:
The aim of this section is to:

Explain what WEP stands forShow which of IEEE standard use WEP as default encryption system Discuss three major fundamental security flaws in WEP.
Wireless technologies

There is no doubt that nowadays wireless technologies become more and more popular.
Wireless devices like laptops, PDA's and 3G mobiles are getting cheaper and cheaper therefore more frequently used.

The expansion of these products resulted in the development of wireless technologies:
WiMax3GBluetoothWiFi

Each technology uses different standards, different frequencies and has different bandwidth.
The implementation area is mostly dependant on the range which technology provides.

(Ahmed I., 2006)
What WiFi is

WiFi is the short for wireless fidelity and is the technology based on the IEEE 802.11 specification.
This technology is the basis for most of Wireless Local Area Networks - WLAN.

It is widely used to create both hotspots at public places and by home users to create domestic network.
WiFi standard allows connections in peer-to-peer mode which is widely used for gaming applications.

Advantages of WiFi
Allows to develop LAN without cabling what reduces the costs of deployment and further expansionUsed world-wide Lots of compatible equipment

Disadvantages of WiFi
Can be interrupted by other devices Pretty high power consumptionWEP - rudimentary encryption standard for 802.11a, b is relatively easy to breakLimited range

What WEP is
WEP is the short for Wired Equivalent Privacy (Webopedia 2006).

This is a protocol for WLANs which has been defined in IEEE 802.11 standard.
Its aim is to provide the same level of security as wired LANs.

In case of wireless networks is not that easy as the information is broadcasted with radio waves and therefore can be easily received not only by intended recipient but is also vulnerable for eavesdropping.
WEP is a rudimentary encryption system for the following IEEE standards:

802.11a802.11b
WEP is a symmetric encryption system.

Symmetric means that the same secret key is used for both encryption and decryption of the data.
WEP is intended to provide:

confidentiality (by the use of RC4 - stream cipher)integrity (by the use of CRC checksum)authenticity (optional, Shared Key Authenticity).
Standard does not specify how to establish the secret key.

In practise all stations within the network and the access point has got the same key which is not the most secure solution.
For encryption WEP uses either

40 bit key or104 bit key or
and with the use of RC4 is supposed to assure confidentiality.

Unfortunately in most cases, wireless devices have WEP encryption turned off as default.
How WEP works

To discuss the security flaws of WEP system we first need to understand the way it was supposed to work.
The diagram explaining the encryption stage is shown at Figure 1 (Lecture notes 2006).

At the first stage of encryption process the Integrity Check Value is calculated and appended to the message.
For encrypting the WEP system uses 40 or 104 bits keys and 24 bits Initialization Vector IV.

Based on these two numbers the cipher stream is generated.
The next step is the XOR operation with the plaintext.

As a result we get the encrypted message.
Decryption process is easy provided that we have got the key (as intended recipients do).

The Figure 2 explains this graphically.
After the decryption process the new Integrity Check Value is calculated and compared with the transmitted one.

If they are equal the data integrity is achieved.
Security flaws of WEP

In the previous paragraph we explained how the WEP system was intended to work.
Now we will focus on the problems that were identified in WEP operation.

WEP system occurred to give very poor protection to the data transferred through the WLANs.
It fails to provide confidentiality, integrity and authenticity.

The main problems with WEP encryption system concern the following (Cam-Winget, Housley, Wagner, Walker 2003:35):
IVC (Integrity Value Check)IV (Initialization Vector) 40 or 104 bits key

The first major flaw of WEP system is the way of implementing the Integrity Value Check.
For this purpose CRC - Cyclic Redundancy Check has been used.

CRC is a function producing a checksum.
CRC32 is a linear code and therefore the attacker can negate bits in the encrypted text and modify appropriately the encrypted checksum at the same time.

It means that someone can change the data and we would not even detect the attack!
It appeared that the CRC is insecure and does not prevent from adversarial attacks.

For higher security we should use at least 64 bits checksum but still it only makes it a little bit more difficult to perform the attack without preventing it.
Initialization Vector IV is a block of bits that is required to produce cipher stream.

Its aim was to assure the uniqueness of the key.
Unfortunately in the WEP system IV is only 24 bits long which is definitely not long enough to fulfil these expectations.

24 bits give us only 16777216 (2 24) possible combinations which means that the same key stream will be used twice in a relatively short period of time (e.g. a few hours).
It makes the process of decoding the message much easier for the attacker.

Some badly designed systems use the sequential key generation.
What helps the intruders even more is the fact that IV is transmitted to the receiver in plaintext.

It has been also proved that some of the keys (around 2%) are weaker than the rest.
Last but not least - the key flaw.

In WEP system 40 bits encryption key is selected and together with the IV creates 64 bits cipher stream.
They are combined in insecure way and it enables cryptanalytic attacks.

The process of 40 bits key generation uses the pseudo random number generator PRNG.
The use of PRNG and ASCII mapping reduces the entropy and therefore instead of 2 40 we have got only 2 21 unique keys.

The WEP 40 bits encryption provides no security for the network as it can be cracked within just a few seconds.
The use of 104 bit encryption is also a very poor, basic protection.

An interesting fact is that even using Brute Force (which is trying to break into the system by checking all possible key combination) it would take only 200 days on average laptop to break the 40 bit WEP encryption.
Using 104 bit WEP encryption makes the system resistible to this attack as it would take 10 19 years to break it.

The algorithms that are used in WEP are not insecure themselves; they have just been badly deployed.
INTERNET AND ECOMMERCE ARCHITECTURE (Question 2)

AIM:
The aim of this section is to:

Explain the term "stateless"Discuss a mechanism or a protocol which is currently being deployed to overcome the difficulty connected with statelessness of HTTP.Explain in details the security issues posed by this mechanism or protocol.
HTTP

HTTP, the short for HypetText Transfer Protocol operates at application layer of TCP/IP model and at session layer of OSI model.
It is the protocol designed to use by www (World Wide Web).

HTTP became a standard defining the rules for formatting and transmitting data (e.g. text, graphic, sound or video) between servers and clients.
Basically when you type the URL address into your web browser you sent a request using HTTP.

In this case a browser is the HTTP client and a web page server is the HTTP server (see Figure 1).
HTTP enables files to contain references to other files.

By selecting it another HTTP request is sent.
(Schiflett, C., 2003) Figure 2 shows how a Web server can obtain content from another Web server.

It means that a Web server plays a role of the client as he is sending its own request.
HTTP is a simple protocol based on TCP.

It implements request - reply model.
Connection between the client and the server is short-lived and is closed after the document has been downloaded.

This protocol is stateless and sessionless.
"Statelessness" of HTTP

HTTP is called a stateless protocol therefore we need to understand what "stateless" means.
Stateless means that there are no records on previous actions.

All commands are executed separately as they are treated as unrelated to any past requests.
It means that each page refreshing generates new, completely independent request and there is no way to recognize the client.

Solution to this problem (cookies) will be discussed in next sections of this document.
The main advantage of this approach (statelessness) is the simplicity of designing the server - there is no need for dynamic allocation and nobody needs to worry about freeing it.

The disadvantage is the fact that all information included in the request needs to be interpreted by the server each time.
Most of modern applications are stateful.

It means that they remember what you were doing when you ran the application last time.
It is very helpful as they remember your personal settings, so you can customize it to your needs.

HTTP cookies
Cookie is a message sent by a server to the client.

This message is stored by the web browser in text file.
If the client wants to access the same web page in the future the corresponding cookie is sent to the web server.

Cookies were invented to overcome the statelessness of HTTP.
They help the server to identify the users and thanks to it customize the web pages for them.

We can group the cookies into two categories (Webopedia 2006): Persistent cookie - this is a type of cookie which has specified the expiry date which means that it can survive between sessions.
It is deleted automatically on the specified date.

Session cookie - this is a type of cookie which is deleted when user closes the web browser.
It does not contain expiry date.

Cookies can contain a variety of information about the user and his connection history.
Cookies are used for numerous purposes: Implementation of shopping basket - items are added / removed from the basket depending on user's actions.

Log in - to allow the user to perform operations the server needs to know if the user is authenticated.
The server gets this information from cookies.

Personalization - some services can be customized by the user.
Thanks to cookies, the settings will be remembered for future sessions.

Counters, polls - some web services count the number of guests or allow users to take part in various polls.
To achieve higher accuracy users should not be allowed to increase the counters by refreshing the page or voting more than one time in the same poll.

Thanks to cookies it is easy to prevent such tricks.
Monitor advertisements - cookies can tell the server which adverts have already been displayed on your screen.

Security and drawbacks of cookie solution
Cookie are only a piece of text therefore there are not a threat to your computer system.

They cannot be used as a virus, they do not delete any data and have no access to your hard drive.
Unfortunately it does not mean that use of cookies is 100% secure solution.

Cookies have a few drawbacks and security issues (Wikipedia 2006): Privacy concerns - even though cookies cannot read any information from your hard drive they can still be a threat to your privacy.
They can store your personal information (e.g. credit card details) that you have freely provided to a Web site.

Mistaken user identification - each user account has its own set of cookies.
However if there are multiple users working on the same user account, the web browser cannot differentiate them.

Cookie theft - generally cookies are sent between the server and the client.
Cookies theft means that unauthorized user overtakes the cookie.

There are two possible ways for stealing cookies: Packet sniffer - cookies sent on regular HTTP session can be sniffed with a use of special software (packet sniffer).
The theft takes place during the transmission process (see Figure 3) To overcome this problem by setting the secure flag on the cookie.

Then the cookie will be sent over a secure channel (e.g. SSL).
Cross-site scripting - is a different way of stealing cookie files.

It means that the browser sends the cookie to the wrong server (see Figure 4) If unauthorised party gets our cookie, s/he can log into web services using our identity.
Cookie poisoning - cookies are supposed to remain unchanged but it appears that attacker can modify the cookie values before sending them back to the server (see Figure 5).

It can be used for example to alter the total cost of the transaction in an online shop.
The solution to this attack is storing the session identifier in a cookie and the rest of information on the server.

Cross-site cooking - this is a type of attack where an attacker exploits browser's bugs.
It sets a cookie for a different site.

(Wikipedia 2006, Webopedia 2006)
Summary

Cookies are a simple piece of data used to overcome the statelessness of HTTP.
Even though they cannot do any harm to our system themselves they can be a subject to attack and can be used by unauthorised person maliciously.

It is worth knowing that most browsers allow the user to either enable or disable the cookies however we must remember that some services are not usable without cookies.
CRYPTOGRAHPY (Question 2)

AIM:
The aim of this section is to

Write C++ or Java programme to implement the RSA encryption and decryption using the given parameters.
E = Encryption Public Key (20000000089)N = A very large integer (34618195959169)M = Plaintext: "The security of an encryption system is as strong because most people rather eat liver than do mathematics!"C = CiphertextD = Decryption Private Key (4771730348713)Demonstrate that the programme works!

RSA is widely known and frequently used in public-key cryptography.
It was invented in 1997 by three mathematicians: L.R.Rivest, A.Shamir and L.M.Adleman - professors from Massachussets Institute of Technology.

Cryptography is strictly associated with encryption and decryption.
Encryption is the process of coding the plaintext into ciphertext, whereas decryption is the reverse process.

Both of them need a key.
There are two types of cryptographic systems: using either secret-key (symmetric) or public-key (asymmetric).

(Yan, S.Y., 2000) When designing the system using public-key cryptography we must make sure, that computing the private-key from the public-key is infeasible.
To achieve this we use sophisticated mathematics including one-way functions.

One-way function is a very easy one to compute, but extremely difficult to invert.
RSA algorithm, which uses this feature, is most frequently used in cryptography.

Security of RSA is based on difficulty in factorisation of large composite numbers.
RSA algorithm ( Yan, S. Y. 2000):

We choose two large prime numbers (p and q) such that FORMULA
We calculate n by multiplying p and q FORMULA We calculate the totient FORMULA We choose an integer e which is coprime to FORMULA v and FORMULA We calculate d such that FORMULA

Number e is called encryption exponent and together with n form the public key.
We use it to encrypt the message.

To send an encrypted message M we must first change it into a number m so that m<n.
To compute the ciphertext we use the following formula: FORMULA where: FORMULA Number d is called decryption exponent and together with n form the private key.

We mustn't reveal d to any unauthorised person.
We use private key to decrypt the message.

To decrypt the message we use the following formula: FORMULA While decrypting the message with private key is very easy, it is nearly impossible without it.
Even though both public and private key are mathematically correlated, it is very hard to compute one from another one.

This is why RSA is believed to be secure.
The following Java programme implements the RSA encryption and decryption.

FORMULA The following is the output from the programme to demonstrate how it works: Before encryption : "The security of an encryption system is as strong because most people rather eat liver than do mathematics!" Encrypted : FORMULA After Decryption: "The security of an encryption system is as strong because most people rather eat liver than do mathematics!"
FIREWALLS AND VPN (Question 1)

AIM:
The aim of this section is to:

Define what is a Firewall?
Explain what can a Firewall protect and can't protectDiscuss the different types of Firewalls.

Describe encryption systems used in Firewalls.
What the Firewall is?

Network security is nowadays one of the most emphasized topics within the computer science.
It is due to arising number of successful attacks on computer systems of different institution.

The best way to prevent break-in to the system is to prevent unauthorised users from access to it.
This is the place for a firewall.

Firewall is one of the ways to make our computer secure.
It can be either hardware or software protection to prevent unauthorised access both to and from a network.

Its aim is to decide which data should be permitted and which should be denied.
Each packet entering and leaving the network comes through the firewall, where is analysed and decision about the future of the packet is made.

How it works
Firewall is designed to monitor and filter communication between networks.

To stop attackers, the communication into our network should be limited.
The most common configuration is allowing the internal network users to access the web pages etc and denying outside users to access the internal computers.

Firewall controls the traffic between the computer and Internet basing on the defined rules (Pike, J., 2001).
Applications communicating with global network (Internet) usually use the specified ports.

The main two tasks that firewall performs are:
control of the outgoing trafficpermitting authorized programs to connect to Internetblocking the communication on the other portsdenying unauthorized applications access to Internetcontrol of the incoming trafficdeny applications and outside attackers to access our computer

The Figure 1 shows the idea of blocking and permitting certain traffic to protected network (in this case server).
The rules needed for proper functioning of our firewall are defined by the system administrator and can concern the following issues:

protocol e.g. TCP, UDP or ICMPaddress of the senderaddress of the receiversource portdestination porttime etc.
Firewalls are configured to monitor and control the traffic.

We must remember that not carefully defined rules can permit unwanted traffic to our computer station.
Firewall - what protection it gives

As previously stated the firewall controls the flow of traffic between internal network (which should be a trusted zone) and external, potentially dangerous networks.
Protection and features that a firewall can provide are as follow: Attacks against vulnerable services - installation of the firewall allows us to use vulnerable applications within the network provided that there is no inside attacker.

Control access into and from your site - with the use of the firewall it is very easy to monitor all traffic both entering and exiting network.
Administrator is responsible to allow certain traffic into internal network and to block unwanted one.

Administrator can also limit the traffic from internal network.
The limitations can concern either specific application or the user or both.

Every application wanting to get access to the Internet must first get permission, which is also the case for Trojan horses.
Any Trojan horse asking for permission is therefore detected and can be deleted with appropriate software.

Protection of network infrastructure - the firewall protects our system from port scanning, address scanning.
Potential intruders usually start the attack with surveillance of the network, therefore it is crucial to make it impossible for them.

Protection from specific attacks - some of the firewalls provide the antiflooding features to protect the station from flooding attacks e.g. SYN flooding or Ping of Death Activity trail and statistical data - it is essential to analyse and review network usage and attack attempts to verify network security.
Confidentiality, authenticity, integrity - using VPN technology and firewall at one time it is possible to encrypt the data so that no third party can read them.

Encrypting data will provide the data confidentiality, authenticity and integrity.
Firewalls can be used as gateway endpoint for VPN tunnels.

(Pike, J., 2001: 84)
What the firewall cannot do

Many people think that installation of the firewall will be a sufficient protection of their computer system.
Unfortunately it is not that easy and there are some threats which the firewall cannot protect us from.

Even properly designed network with well configured firewall cannot protect us from the following: Inside attacks - a firewall controls only the traffic passing through it therefore is not able to protect internal resources from inside users e.g. co-workers. Providing security to those resources is more complicated and demands other solutions e.g. one-time password authentication.
Data path that bypass the firewall - the most common example of it are dialup connections made from the internal network.

Any connection bypassing the firewall gives not only a way out for a user but also a way in for intruder.
Email - email attachments are the common way of spreading the viruses, worms and Trojan horses, therefore we should educate the users in this field, inform them about the potential risks associated with email attachments especially coming from unknown users.

The other solution is putting the Antivirus programmes into use and keeping them up to date.
Before opening any attachment we should scan in with appropriate software.

Social engineering - it is a technique of manipulating people (e.g. pretending to be system administrator) to get their logins and passwords.
Having a valid login and password intruder can access the system easily.

Operating system and other application flaws - we must remember that the firewall is not able to detect all the vulnerabilities of installed software.
It is crucial that the system administrator is aware of this problem and that s/he installs all the latest security patches.

Outgoing data - the firewall protects only inside resources of the network.
The leaving information is vulnerable to attacks such as e.g. sniffing.

In this case a good protection is data encryption.
Fabrication - firewalls do not protect us from entering fabricated web services with similar addresses.

Such www sites are designed to get users' logins, passwords or debit/credit card numbers and then misuse them.
(Pike, J., 2001: 86) We must be aware that most firewalls have many sophisticated features but to make them effective, we must first configure them properly.

Different types of firewalls
There are various ways to group the firewalls.

The first, most intuitive one distinguishes the following:
Hardware firewall Software firewall

Hardware firewall
work at a lower levelfilters IP packetsdecision is made by checking source port, destination port, IP address, destination service/protocol as well as source domain name, time to live (TTL) valuespreconfigured - do not have to spend time on it and can use it right out of a box

Software firewall
work at a higher levelthere is a need to configure it - define the rules for permitting certain applications and users and denying the rest

We can group the firewalls regarding to the amount of computers they protect: Personal firewall - they are used to protect a single computer e.g. at home (excluding the case in which other computers share Internet connection of the master one) Network firewalls - they are used to protect the network of computers e.g. company network.
Usually there is a separate device to perform this task.

We can also group firewalls according to the control mechanism they use: Packet filtering - the simplest mechanism.
It checks the content of individual packets.

They do not allow the traffic which did not match the rules.
Unfortunately the selection criteria are insufficient for some applications.

What is more it can be relatively easy tricked e.g. by: Replacing the source address with a permitted one.
In this case packet filtering mechanism will be helpless.

Splitting a connection request into a sequence of packets which separately look innocent to the firewall.
Firewalls using this mechanism are called Network Layer firewalls.

They work very fast and transparently for users but are not very safe.
Circuit filtering - this is the mechanism used by Circuit Level Gateways.

The filtering takes place at the session layer of the OSI model.
All the traffic is monitored by the firewall.

If an internal computer sends a request for a service (e.g. web) to the external network the traffic goes through a virtual circuit to the firewall where it is intercepted and recorded and then passed on.
The respond data from external network reaches the firewall and is compared with the request data to check if the addresses and ports match.

Based on this comparison the data is either blocked or sent to the client through the circuit.
The use of this type of the firewall hides the internal network from external world as all the information going out of the network appears to have originated from the gateway.

What is more the data that has not been requested will be never let into internal network.
Another advantage is the relatively low cost.

The main disadvantage is the need to combine this type of firewall with some other type of filtering.
If not, any type of data that has been requested will be allowed to internal network.

Application gateway - the process takes place at the application layer of OSI model.
They are commonly referred to as proxies.

All the traffic between internal and external network goes through the proxy so there is no direct connection.
For each network service (e.g. http, ftp, smtp, telnet) there is a separate proxy.

This type of firewall filters the actual content of data, not just its source and destination.
Thanks to it administrator can control not only access to web pages in general, but also can specify which pages can be viewed and which are restricted.

This type of firewall is considered to be very secure but has also some drawbacks - is slower than other types and requires more complicated configuration.
There are used to protect large business networks.

Another way to categorise the types of firewalls regards tracking: Stateful - this is a type of firewall which keeps records of the state of connections and based on this information it allows only the packets matching known connection state.
The rest of the packets will be rejected.

Stateful firewalls hold records containing attributes of the connection (e.g. IP addresses, ports etc).
This type of firewall depends on three-way handshake of the TCP protocol.

The connection needs to be established and after that incoming packets will be allowed only if they are part of this connection.
Stateless - this is a type of network which treats each packet separately therefore they do not distinguish if the packet is trying to set up a new connection or is a part of existing one.

Stateless firewalls offer less security than stateful ones.
(Wikipedia 2006)

Encryption systems used in firewalls
Although a firewall itself is supposed to control the traffic to provide protection to a single computer system or network, some firewalls also enable the encryption process for greater security of the outgoing data.

This feature is used to implement Virtual Private Networks - VPN.
VPN is a way to securely connect two networks over the Internet which is insecure media.

The following types of encryption are being used in firewalls: DES - (the short for Data Encryption Standard) is a widely used symmetric-key encryption algorithm.
DES is a block cipher which means that it operates on plaintext blocks of a given size (64-bits) and returns cipher text blocks of the same size.

The key in fact consists of 64 bits but 8 of them are used for parity check, therefore the effective key size is 56.
DES is based on Feistel scheme.

The structure contains initial and final permutation and 16 rounds.
RC-4 - is the most widely used stream cipher.

It is a symmetric-key encryption algorithm.
It is based on the use of a random permutation.

It generates the keystream which, in encryption process, is XORed with the plaintext.
It is currently used to secure a lot of the wireless protocols.

It is used in WEP and WPA, as well as SSL.
MD5 - (the short for Message Digest algorithm 5) is an algorithm used to create digital signatures.

MD5 is a one-way hash function.
MD5 process the message of any size into the output of 128 bits.

SHA-1 - (the short for Secure Hash Algorithm) SHA-1 is used to hash objects.
IPSec - (the short IP Security) is a set of protocols developed to support secure exchange of packets at the IP layer.

It has been invented due to lack of security capabilities of IP protocol.
It has been widely deployed to implement Virtual Private Networks (VPNs).

There are two modes of IPSec operation: Transport and Tunnel.
Transport mode encrypts only the data portion of each packet leaving the header untouched.

The more secure Tunnel mode encrypts both the header and the data portion.
Summary

It is very important to remember that a firewall does not provide much security itself.
The level of security it provides depends on configuration which is set by system administrator.

Introduction
Allowing for the rapid growth of the brain during the first months of life, the open sutures and thin bone of an infant cranium results in a malleable material also susceptible to influence through congenital condition, disease or intentional modification.

Should the skull of the baby be fused and immobile, the infant could not pass through the narrow birth canal of the mother.
Upon delivery, the skull is often elongated but returns to a natural profile within hours or days (McPherson 2004, URL ) although the cranial sutures remain open to a certain degree into adulthood.

For thousands of years, the temporary condition of the infant skull has been exploited for aesthetic, cultural or possibly religious purpose; adults through the use of deforming apparatus, such as bands and cradle-boards, artificially modified the cranial vaults of their children or those in their care (Blackwood & Danby 1955: 173).
Possible examples of this practice have now been identified to Shanidar Cave in Iraq, around 45,000 years BP (Trinkaus 1982 & 1983) and eyewitness accounts have been recorded since the time of the Hippocratic Corpus.

Hippocrates, following witness of a people with deformed heads living on the shores of the Euxine (Knox 1863: 271), in the areas of the Black Sea (Babcock Gove 1993) stated the unique heads of the "Macrocephali" were achieved by the application of bandages and devices to the crania of the neonates to elongate the profile (Chadwick & Mann 1978: 161).
The purpose in this case was apparently one of status identification as Hippocrates recognised the longest heads were those of the most noble individuals (ibid); a similar stimulus could be found in upper-class Chinese women who would have their feet bound to restrict the growth both by length and width (Ortner & Putschar 1981).

Other influence for the adoption of this practice may possibly include as an ethnic marker (Özbek 2001) or religious observance (Hrdlička 1919).
In other cultures, the modified skull has become one of aesthetic pleasure (Blackwood & Danby 1955; Ortner & Putschar 1981; Trinkaus 1983); there is record of the disappointment of a woman with mild deformity in New Britain, Papua New Guinea, that her mother had not pulled the bandage tighter, therefore increasing the modification (Blackwood & Danby 1955: 175).

When observed by an outsider, the affect is often displeasing and results in descriptions such as "a very ugly deformation of the head" (de la Vega 1966: 486).
The method of modification, the final shape of the head and the decision on how long the deforming apparatus remains in place may be dictated at various social levels; by the 'tribe', the family or by the opinion of the midwife (Hasluck 1947: 130).

For whatever reason cranial modification is undertaken, the practice is worldwide and spans prehistory through to historic periods.
The methodology involved in cranial modification in addition to ethnographic and archaeological examples will be provided within this paper.

Pathological conditions can result in various forms of modification from mild, benign deviation from the mean to severe deformation incompatible with life.
It has long been recognised that a child sleeping on a resistant surface may lead to occipital-parietal flattening (Hrdlička 1919; Ortner & Putschar 1981); possible examples have been recovered including an asymmetrical female with mastoid deformity from Paucarcancha, Peru, (MacCurdy 1923).

The Classical Greek author Pliny reported a "monstrous birth" was considered a portent (Rackham 1942: 529).
Physical characteristics were at the time thought to have occurred due to sights, sounds or thoughts at the time of conception (ibid).

Synostosis or premature fusion of sutures of the cranium will lead to various degrees of deformation dependant on the quantity and location of the sutures involved (Resnick 2002).
Some genetic conditions can result in cranial deformity and chromosome disorders incorporate many symptoms and/or deformities occasionally including modification of the skull, for example Trisomy 13 and Down's syndromes (ibid).

In addition to such conditions, there is evidence that diet can factor into cranial deformity; vitamin and/or mineral deficiency or alcohol abuse may result in conditions including microcephaly (Martin 2005; McPherson 2004, URL ).
The conditions and severity of pathological cranial deformity are vast, some of which will be addressed within this paper.

When examining human remains for pathological conditions, differential diagnosis should be considered.
To accept an initial interpretation may result in evidence being overlooked, misdiagnosis and neglect in considering an alternative condition.

This paper will describe the process undertaken during artificial head modification and pathological conditions resulting in a similar appearance; through the examination of ethnographic and archaeological evidence, similarities in cranial shape and size will be identified and possible differential diagnosis will be examined.
Considerations

Upon recovery of crania displaying values deviating from the average human skull, several points must be addressed.
Firstly, care should be taken to ascertain whether the deformity is intentional or unintentional; determination between unintentional cradle-boarding and head-binding can be difficult (Anton 1989: 254).

In addition, characteristic similarities suggest pathological conditions could be mistaken for intentional modification and such occurrences should be avoided.
Hominid species other than Homo sapiens resulted in misinterpretation during early work, for example Knox (1863) description of a skull recovered from Neandertal, Germany.

Prior to the classification of the Neandertal species, Knox was correct when he described the skull as round with prominent brow ridges and having "the resemblance to the chimpanzee, but not a gorilla" (Knox 1863: 272) and suggested the cranium was not deformed "by mechanical means or otherwise" (ibid); however, misinterpretation did occur upon the examination of the long bones; the robustness was attributed to retained foetal characteristics (ibid).
The recovery of a lone abnormal skull from a cemetery of average crania suggests a benign origin rather than intentional modification (Wells 1967: 6); conversely, recovery of both artificially modified skulls from the same cemetery as unmodified crania may suggest the presence of two distinct cultures within the region (Knox 1863: 272) or alternatively, a race that has desisted in using such methods.

Finally, differences between individual cases of artificial modification need to be carefully considered.
Defining factors include the duration and position of the deforming apparatus on the cranium (Blackwood & Danby 1955; Trinkaus 1983;), the tension and material density of the bands (Stewart 1941; Hasluck 1947), unintentional movement and the width of the band (Özbek 2001), and the healing response of the bones of the individual (Stewart 1941: 344).

These elements will not only affect the shape produced but the level of deformity visible in the adult (Trinkaus 1982).
The degree of resulting deformation has been graded by different authors using various terminology; in this paper, for the purpose of clarity, the grades of unmodified, modified and extremely modified have been adopted.

Artificial cranial modification
"History informs us that in very ancient times a belief prevailed that by mechanical means applied to the head of the infant, another form might be given to it than that intended by nature".

(Knox 1863: 271) The frontal flattening with excess curvature of the occipital bones of Neandertal remains recovered from the upper levels of Layer D of Shanidar Cave, Iraq, suggest artificial cranial modification has apparently been undertaken since at least 45,000 years BP (Trinkaus 1982; 1983).
Similar treatment has been identified throughout the world and in geographical regions with the adoption of artificial cranial deformation, the occurrence rate appears to be high; in analysis of 1515 skeletons from Mexico, Guatemala and Honduras, 88.65% displayed evidence of modification by the use of cradleboards (Tiesler 1999 URL.

mesoweb.com/features/tielser/media/headshaping.pdf).
The flat-headed race identified by Hippocrates extended from the ancient shore of the Euxine to central Europe (Knox 1863); this may suggest migration of modified individuals (Özbek 2001) and/or adoption of the fronto-occipital deformation practice by observers.

The motivation behind the practice varied and apparently included symbols of status, aesthetic pleasure (Blackwood & Danby 1955), religious symbolism (Hrdlička 1919) and possibly tribal identification (de la Vega 1966: 485).
An unintentional combination of multiple factors can be recognised; although attributed to aesthetics, the Greeks and Vlachs preferred round heads, while the Konia Turks would flatten the forehead and increase the height of their naturally large crania (Hasluck 1947: 130), coincidentally providing a cultural marker.

The process of modification begins soon after birth, possibly immediately following the postpartum bath, usually by a woman, most commonly the mother or another person present at the birth (ibid; Hasluck 1947).
Evidence suggests in some cultures the practice is a custom involving predominantly female children, possibly including the Chalcolithic people of Değirmentepe, Turkey, based on tentative sex determination of children (Özbek 2001), however, others have an even divide between the sexes, such as the prehistoric Peruvian Ancon (Cheverud et al 1992) and some Mayan civilisations (Tiesler 1999 URL ).

In groups where both sexes have undergone modification, variation can still be seen; examples recovered from New Britain suggested the females underwent head-binding for a longer period than the men due to the greater deformity seen in the specimens (Blackwood & Danby 1955: 187).
Head-binding

The act of binding was possibly significant in the early life of the child; anthropomorphic figurines musical instruments used during pre-Hispanic rites suggest the process of head-shaping was perhaps commemorated ceremonially (fig 1) (Tiesler 1999, URL shaping.pdf).
Symbolic meaning may explain the burial of an individual with in-situ deforming apparatus including a child from Moquegua, Peru (fig 2) (Ubelaker 2000) and the occasional adult (Aufderheide & Rodriguez-Martin 1998).

The process of head-binding appears similar worldwide; bands of various material encircled the head of the neonatal infant, evidenced by the remains of the Moquegua infant (fig 2), occasionally with the addition of small boards or hard pads (de la Vega 1966; Aufderheide & Rodriguez-Martin 1998).
The position of the bands did not dictate the final shape.

The tightness of the band, which appears to have been adjusted daily (ibid; Aufderheide & Rodriguez-Martin 1998), and the overall length of time the apparatus remained in place would affect the appearance into adulthood (Özbek 2001), however, minimal deformation would be visible after only one day (Blackwood & Danby 1955).
Some children would be head-bound until the desired shape was achieved, for a year or until they could walk (Trinkaus 1982), up to three years (Aufderheide & Rodriguez-Martin 1998), or until the child possessed the ability to remove the band (Blackwood & Danby 1955; Trinkaus 1982).

The people of New Britain would 'paint' the infant's head with a black mixture of charred magas wood and water prior to the application of the band; this was said to make the binding more effective (Blackwood & Danby 1955).
Following any initial preparation, all cultures apparently began the head-binding process with a band encircling the head, from the frontal bone at a level above the orbit, over the parietal region to the occipital bone (Özbek 2001).

The ears were carefully flattened against the head; the band would cover the entire ear of an infant or just the tips of an older child (Blackwood & Danby 1955).
The band itself was of various materials although can be identified as flexible due to a lack of flattening (Trinkaus 1982: 199; 1983:147); the cultures of New Britain would generally use a bark-cloth bandage, a strip of which was removed from the loin cloth of the men, however, one man wore a silk loin cloth, having obtained the fabric while working as a labourer in Rabaul; this too was utilised for the bandage of his child (Blackwood & Danby 1955).

A Chalcolithic infant from Değirmentepe, Turkey, of only 6-7 months of age was identified as having undergone modification with the use of one band (Özbek 2001) (fig 3).
Additional bands were often added to the first phase single band as the child grew or when the head reached the desired volume, often after the child reached 12 months old (ibid).

Individuals who have undergone multi-band modification can be identified through the bulge that developed between the bands (Özbek 2001) and included the Poroi Vlachs who occasionally secured a second band beneath the chin and over the crown to shorten the face (Hasluck 1947).
An example of the dual-banding form of modification, recovered from Değirmentepe (ibid), displays the intra-band bulge.

The infant, approximately 18 months old, was apparently head-bound with two bands passing over the frontal bone and lower portion of the parietals and occipital (fig 4A), resulting in a superior-posterior projection of the superior portion of the parietals and upper part of the occipital: the circular form of modification, also known as circumferential or Aymara type, (fig 4B) (ibid).
These two infants provide valuable evidence of the stages children underwent during the process of modification.

The preferred shape of the modified cranium would differ from region to region.
Within the culture of the Maya, the people of the various regions would shape the heads of their infants slightly differently; in the Lower Usumacinta region, the resulting profile was oblique; in the highlands, an erect shape (Tiesler 1999 URL ), again although possibly unintentional producing a cultural marker.

Within the Mayan city of Copán, Honduras, internal variation rather than indicating social status, identified the integration of individuals into society; Patios A, B and C of a housing complex revealed a predominance of mimetic and oblique shapes; conversely, Patio D, on the periphery of the site apparently inhabited by outsiders, showed a preference for the erect type (ibid).
Blackwood and Danby (1955) state when categorising artificial cranial modification, to name the deformed bone or to which pressure is applied is insufficient.

The overall result is dependant on the position of the apparatus above or below the greatest angle of the bone, together with individual growth patterns and diet (ibid).
Within this paper, in addition to the practice of cradle-boarding, three forms of binding will be discussed.

The apparatus and spatial arrangement used during each method of modification will be described and graphic examples provided where possible Fronto-occipital flattening (Cheverud et al 1992)/anteroposterior deformation (Anton 1989)/flat-head type (Hrdlička 1919), the most common form of modification (Ubelaker 2000), produced flattening of the frontal and occipital bones and bulging of the parietals, producing a tri-lobed (Anton 1989) appearance (fig 5) and a taller, wider face.
The overall rounded appearance of the various bones affected by this deformation suggests the apparatus employed were flexible bands or a specially designed headband (fig 6), restricting growth at the anterior-posterior area and also in the region of the sagittal suture.

The Peruvian Ancon, between AD60 - AD1450, employed the fronto-occipital flattening method of deformation on both men and women, although with a high degree of asymmetry (Cheverud et al 1992), as did the people of Chavina, Peru (fig 7) (Ortner & Putschar 1981).
Circumferentially deformed skulls, including the Aymara (Anton 1989) and circular (Özbek 2001) types were produced through the elongation of the frontal, temporal squamae and portions of the parietal and occipital bones without lateral bulging, with compensatory superior-posterior bulging of the upper occipital (Hrdlička 1919; MacCurdy 1923).

This form of modification, probably through the use of bands (Hrdlička 1919), produced a long, narrow superior appearance (fig 8).
An extreme example of this form of modification was that of a female, around 30 years of age, from Patallacta, Peru (fig 9) (MacCurdy 1923).

MacCurdy found a reduction in the dimensions of the foramen magnum in deformed crania, 3.3cm long x 2.7cm broad, to that of undeformed examples, 3.7cm long x 2.9cm broad (1923: 230); evidence suggests this form of modification restricts growth of the foramen magnum and could result in the constriction of the spinal cord as will be discussed later in this paper.
Cradle-boarding

Through localised pressure on the posterior cranial vault, the securing of an infant to a cradle- board (fig 10) produced a cranium with either unilateral or bilateral flattening to the occipital or lambdoid region (Kohn et al 1995) and have been used by the people of Albania to produce their characteristic flat heads (Hasluck 1947).
Growth restrictive pressure in the region between the frontal and occipital bones induced by the use of a cradle-board results in fronto-occipital reshaping and compensatory growth of the parietal in a mediolateral direction (Cheverud et al 1992).

The cradle-board, use of which was widespread throughout the American south-west (Kohn et al 1995), may also have produced compensatory posterior lateral growth (Cheverud et al 1992).
Care should be taken on examination of presumed artificially modified skulls as mild although frequent pressure on the occipital bone from a cradle-board could result in values equal to hyperbrachcephaly (Blackwood & Danby 1955).

Occipital or lambdoid deformation resembles that of accidental occurrence (Hrdlička 1919; Ubelaker 2000), such as infants sleeping with their heads on a resistant surface, for example the Navahos, or with the assistance of apparatus in ancient Peru, (Hrdlička 1919).
The appearance is that of a fronto-occipitally shortened but broadened skull with a high forehead (ibid) and vertically elongated occipital (Ubelaker 2000).

An extreme example such as that found in a south-west American Pueblo cemetery must display intentional modification opposed to unintentional deformation due to the severity of the deformation (fig 11).
Children have been reported remained strapped to their cradle-board until they were three years old (de la Vega 1966) and developmentally there seems to be little or no affect on the child through the use of a cradle-board; this may be connected to the belief of some North American Indians, including the Navahos, that cradle-boarding and the immobility induced (Hudson 1966), produced a strong child, a plausible explanation when considering the affects of isometric, resistance, exercise (Hudson 1966), alternatively that it encouraged straight growth (Kohn et al 1995).

The Songish North American Indians of Victoria, British Columbia, adopted the cradle-board and pads as the apparatus utilised during cranial modification (Cheverud et al 1992) and may have held similar beliefs.
European cradle-boards allowed movement (Hudson 1966) and therefore did not provide similar resistance; future research into skeleto-muscular development of children displaying evidence of cradle-boarding may identify a difference between the North American tribes and European examples.

Contraindications against the use of artificial cranial modification have been suggested to include the increased risk of cerebral disease and/or mental retardation (Blackwood & Danby 1955).
A trained nurse who had worked in the British Solomans prior to a transfer to the Arawe district, known for the practice of head binding, noticed an increased incidence of meningitis, cerebral malaria and severe headaches amongst the cranially modified individuals.

In contrast, a medical missionary of similar experience in the same regions had not noticed the correlation and did not attribute such disease to head-binding (ibid).
However, excessive tension of the deformer could cause necrosis of the bone and continued use beyond the synostosis of the sutures may result in constriction of the brain (Aufderheide & Rodriquez-Martin 1998), therefore, the opinion of the nurse should not be dismissed.

Despite evidence that the cranial volume can be reduced slightly by modification, 3.7% in the case of bilaterally modified Hopi crania when compared to non-modified (Kohn et al 1995), early work in the field identified artificial cranial modification and the resulting shape change of the brain did not apparently affect mental function (Knox 1863; Blackwood & Danby 1955).
Although the profile of the brain is modified, all areas are present and undamaged and therefore, normal activity remains unaltered, evidenced by the elaborate carvings and painted designs produced by the people of New Britain (Blackwood & Danby 1955).

Studies on the affect of artificial head modification on the shape of the bones of the base of the cranium or those of the face have produced inconsistent results (Cheverud et al 1992) and the sutures of the skull are apparently not affected anatomically but spatially (Blackwood & Danby 1955).
Extreme modification would be expected to affect the bones of the cranial base and face (Hrdlička 1919) and upon analysis of the Ancon series, data showed considerable shape change to the cranial base and lower facial bones, however, similar examination of Songish crania from British Columbia showed little significant change to the face (ibid).

This difference can be explained by the Songish adoption of the cradle-board opposed to the Ancon use of a headband.
There is however, evidence that modification can affect the size and location of foramen of the skull (Blackwood & Danby 1955); pressure on the spinal cord as a result of the relocation or distortion of the foramen magnum has been linked in pathological conditions such as achondroplasia, dwarfism, to apnea and sudden death (Resnick 2002: 4451).

Cradle-boarding in its nature appears to have little or no affect to the face.
Modification is apparently restricted to the occipital or lambdoidal regions with asymmetric size difference without affecting the cranial base or face (Kohn et al 1995).

Hippocrates considered over time the artificially altered cranial shape had become an inherited characteristic, in addition, with the increased contact between populations the practice was eventually rendered obsolete (Chadwick & Mann 1978).
Missionary influence further reduced the occurrence of modification.

Contact with outside influence caused fathers working away from their villages to reconsider their actions; on their return they would articulate their opinions to the women of the village, who having little outside contact would prefer to remain with the traditional methods.
The fathers considered the cranially modified children would possess reduced intelligence (Blackwood & Danby 1955); the opinion of decreased mental ability, argued by several authors (ibid; Knox 1863), has been discussed elsewhere in this paper.

Finally, British colonialists in the area of the Songish Indian Songhees Reservation may have discouraged the use of the artificial modification due to lower frequency in the area than similar ethnic groups (Cheverud et al 1992).
During more recent historic periods of cranial modification, although continuing during the mid-twentieth century, governments have not interjected, however, the practice reduced in frequency (Blackwood & Danby 1955).

Pathological and congenital conditions
Congenital conditions and neoplastic diseases are both well-known problems in modern medicine...

Based on skeletal material, both congenital and neoplastic diseases appear to be less frequent in archaeological populations (Anderson 2000).
Pathological and congenital conditions causing deformity have in the past included proposed causes such as God's wraith and imagination, however, hereditary or accidental illness and narrowness of the womb (Ambroise Paré, 16 th century French surgeon, cited by Anderson 2000) are not unacceptable suggestions; as previously discussed, constriction within the birth canal does cause temporary deformation.

Burial practices may suggest the relationship within society a sufferer of deformity may have experienced (Anderson 2000) as pathological conditions can produce a variety of deformities from the mild benign deviation from normal to extreme deformities incompatible with life.
Some of these deformities could be mistaken as various forms of artificial cranial modification therefore, some forms will be briefly discussed here and examples provided where possible.

Craniosynostosis, the premature fusion of one or more sutures of the cranium, results in deformation of the skull, and includes plagiocephaly, asymmetry caused by the unilateral premature occlusion of sutures (Resnick 2002).
Scaphocephaly, an abnormally elongated skull as a result of premature fusion of the saggital suture (fig 12) (Hrdlička 1919; Aufderheide & Rodriguez-Martin 1998; Children's Hospital and Health Center [sp], San Diego nd, URL 11005.cfm), has in the past been common among Afro-Caribbeans (Hrdlička 1919), without affect on the face or cranial base (Kohn et al 1995).

Also known as sagittal craniosynostosis, this condition may have limited the artificial cranial deformation of the Aymara type on a male recovered from Torontoy, Peru, (fig 13) (MacCurdy 1923); this may also explain the bulge anterior to the crown, however, dual-bands could equally be the origin.
In opposition to craniosynostosis, there are archaeological examples of adults with unfused sutures, including the frontal, parietal and occipital bones of an Akkas pygmy female from Central Africa (Flower 1889).

Congenital hydrocephaly, possibly induced by multiple vitamin deficiency (Bamforth & Baird 1989), can be difficult to differentiate from trauma, infection or tumour induced (Brothwell 1967); however, a true example was recovered from a Middle Saxon Christian cemetery at Naeingbury, Essex (Putnam 1978).
The partial adult male was recovered without the vertebrae, therefore, it was not possible to confirm if the individual suffered from spina bifida, a common association with hydrocephalus (ibid; Bamforth & Baird 1989).

Hydrocephalus can be a symptom of chromosome disorders such as Trisomy 13 (Knowles 1986), however, of the additional associated conditions including blindness, mental retardation and cleft palate (Bamforth & Baird 1989), only the latter would be visible in the archaeological record.
Anencephaly, the absence of the cranial vault, has a 75% mortality rate at birth, although some sufferers may survive for up to seven days (Aufderheide & Rodriguez-Martin 1998); this condition is commonly associated with vertebral abnormalities (Knowles 1986) including spina bifida (Aufderheide & Rodriguez-Martin 1998).

Ethnicity may be a key to increased occurrence of this abnormality in Eastern and African populations (ibid).
An example was described by an Albanian mother who recalled her many children all of which, died at or soon after birth, all except one (Hasluck 1947: 131).

With echoes of the Classical Greek opinion that the parents hold a bearing on the appearance of the child, she blamed their father's evil life for the fact they were all "born with their heads open" (ibid).
The one survivor was born at the end of the First World War, with the assistance of an American doctor with the Red Cross; the doctor was able to sew up the deformity (ibid), although no mention was made of the life span of the child; when examining medical examples one wonders how a child could survive such affliction, despite medical intervention (fig 14).

Although there are numerous conditions resulting in various forms and severity of cranial deformation, constraints restrict the detail in which they may be covered in this paper.
The attempt has been made to describe the forms of similar characteristic to artificial modification and those discussed in historic text.

Differential diagnosis
Post-depositional distortion, when occurring in children, may result in soil seeping through the open sutures into the cranial cavity causing bulging and therefore a hydrocephalic appearance; analysis of the entire cranium should clarify the situation (Wells 1967: 6) as hydrocephalus occurs in individual with open sutures.

An example of misdiagnosis was recovered from Windmill Hill, near Avebury in Wiltshire.
This unsexed child, affectionately known as Charlie, was considered to have suffered from hydrocephalus, however, this has been revised to that of post-depositional pressure causing distortion to the cranium (pers.

comm. Dr Ros Cleal, Alexander Keiller Museum, Avebury).
Post-depositional deformation can also mimic scaphocephaly, premature closure of cranial sutures producing a long and narrow skull; examination of the sutures and the fact such damage usually manifests as asymmetric (Wells 1967: 6) should therefore, be obvious as inadvertent damage; the small amount of subterranean pressure is unlikely to produce the more severe forms of modification (Trinkaus 1982).

The asymmetric appearance of post-depositional damaged crania (Wells 1967) could result in an incorrect diagnosis of plagiocephaly.
Conversely, premature multiple suture closure can result in a clover-leaf deformity (Aufderheide & Rodriguez-Martin 1998; Resnick 2002) and due to the similar mis-shapen appearance, possibly a misdiagnosis of fronto-occipital flattening.

Such deformity can result in hydrocephalus and retardation (Resnick 2002).
Microcephaly, defined as a cranium less than 42cm in circumference at full growth (McPherson 2004, URL ), has several causes; these include infection, severe malnutrition or drug use (ibid), atelencephaly, congenitally deformed bones of the skull (Siebert et al 1987) opposed to absence such in anencephaly, and cretinism, caused by a thyroid related hormone imbalance and iodine deficiency (Roberts & Manchester 2005).

The latter condition is triggered in-utero when the expectant mother fails to maintain normal thyroid stimulating hormone (TSH) levels (hypothyroidism), affecting the iodine uptake (Kumar & Clark 2002); as cranial growth is triggered by the development of the brain, retardation is not inevitable but common, especially in this form.
Microcephaly and artificially restricted growth of the cranium may cause confusion during examination.

An alternative form of modification will confront archaeologists of the future.
Children are now undergoing modification with the use of orthotic devices to correct certain deformities, including unintentional parietal flattening causing plagiocephaly due to their supine sleeping position.

Recent research and a belief babies should sleep on the backs to reduce the risk of sudden infant death syndrome or cot death, resulted in an increased incidence of flattening of the parietal bone (Resnick 2002: 4507).
Although this should be a temporary condition while the skull is still malleable the child will turn while sleeping, therefore reducing the pressure on one area of the skull, the device can be applied into the second year of the child's life, applying pressure to the area of abnormal growth while allowing normal development (Littlefield et al 2000, URL ).

Early cases have shown correction can take as little as 7.5 months (fig 15) (ibid), however, radiographic, imaging or post-mortem analysis could identify evidence of the medical modification induced on the cranium.
Several pathological conditions bear similar characteristics to artificial modification and differential diagnosis should, therefore, be carefully considered during skeletal analysis.

In many cases examination of the axial skeleton will clarify the situation through recognition of additional symptoms; in contrast, a lack of additional pathology may suggest the presence of artificially modified crania.
Conclusion

Artificial cranial modification has been practiced for thousands of years for a variety of reasons.
For a period of time, spanning a few weeks to several years, the head of the newborn infant would be bound with flexible bands alone, with the addition of small boards or pads or the child could be strapped to a cradle-board.

Dependant on the position and apparatus used, the resulting modification may be minimal and possibly invisible under the hair, as in the case lambdoid deformation of the male recovered from a Pueblo cemetery, to severe deformity including the fronto-occipital flattening such as practised by the Ancon.
Deliberate modification was one practiced by the community and continued for generations for the purpose of ethnic marking or aesthetic pleasure.

The same grades of modification can be seen in pathological conditions; under these circumstances, the individual may have been a social outcast, possibly reflected in their mode of burial.
During examination of unusual skeletal data differential diagnosis should be considered as artificial modification and pathological conditions could manifest with similar deformity.

Medical advancement has now created a modern form of modification for a therapeutic use; despite the medical benefits, this treatment could unfortunately cause some confusion to future archaeologists.
Introduction

The Avebury complex (figures 1 & 2), including the henge, avenues, Silbury Hill, Windmill Hill and numerous long and round barrows, has been of continued interest for hundreds of years since the prehistoric importance of the area was first recognised by the antiquarian John Aubrey during the 17 th century.
More significantly, during the 18 th century, William Stukeley created a record of the region including invaluable illustrations and descriptions of numerous monuments, many now partially destroyed, others completely lost.

During the 20 th and 21 st centuries, the area has undergone sporadic excavation and non-invasive research by many including Harold St George Gray 1908-1922, followed by more intensive work by Alexander Keiller, 1925-1939.
Following purchase of lands in and around Avebury, Keiller systematically excavated, recorded and in some instances, restored the monuments to such a high standard that following her own excavations, in 1965 Isobel Smith published Windmill Hill and Avebury: Excavations by Alexander Keiller 1925-1939.

At the time of publication, Smith's work, as well as publishing her own findings, summarised Keiller's excavations and interpreted the evidence in the convention of the day; although now out of print, this work is still much in demand and invaluable during re-evaluation of the evidence and the complex.
Since the publication of Windmill Hill and Avebury, further excavation has been conducted, advances have been made in scientific methods and alternative hypotheses have been reached regarding the location, design and the materials of the monuments.

Instead of an in depth description of the Avebury complex or the components therein, this paper will summarise some of the work completed since the publication by Isobel Smith in 1965; discuss some of the many advances made within the Avebury region in the last forty years, together with the implications this may have on the interpretation of the rest of the British Isles.
Avebury Henge and Internal Features

The earthwork of Avebury henge is 365 metres in diameter, divided into quadrants by four entrances, and encloses the remains of an outer stone circle, two inner stone circles, a cove and a multi-stone 'feature'.
In 1950, Stuart Piggott defined stone circles as "open sanctuaries" of the early Bronze Age, mainly of the British Isles (Piggott, 1950: 104).

Piggott writes of Stukeley's recordings of the complex and the fact that he either could not or would not stop the destruction during the 18 th century (Piggott, 1950: 112).
Therefore, the monument that Keiller excavated, recorded and restored in the 1920's and 30's was in a post-destructional state.

Three features recorded by Keiller in the north-east quadrant, internal to the outer circle, were presumed to be stone-holes of an earlier monument, underlying the bank and ditch.
Excavations undertaken by Piggott in 1960 opened a series of trenches in the area of these features in an attempt to prove the existence of a third inner circle, 105 metres in diameter (Piggott, 1964).

Following the excavation of 450 square metres, Piggott concluded that although the westernmost of the three, stone-hole A, had at one time contained a stone evidenced by the compaction of the packing-stones into the chalk bottom of the hole, the remaining two, stone-holes B and C, bore no similarity to the first in either dimension and basal content (Piggott, 1964: 203).
No explanation was offered for these features; therefore, one is left with Piggott's suggestion that stone-holes B and C are of medieval origin (1964: 28).

Unfortunately, during research for this paper, no further investigation regarding these features has been identified.
Smith (1965) suggested the size of Avebury henge indicated the population of the area and a pooling of manpower during the construction of the henge and avenues, possibly facilitated by the direction of a priestly hierarchy (1965: 253).

This writer finds Smith's ideas on the additional manpower required for the gathering of raw materials and the supply of food interesting.
She identifies the need of the populous to produce excess food and labour above the level of subsistence required to produce such monuments (Smith, 1965: 253).

Other than detailing the characteristics of the henge and the internal features, Smith does not offer a hypothesis regarding the henge.
During the years following the excavations by Keiller and Smith, due to preservation, little invasive research has been conducted.

On occasion, a watching brief or rescue excavation has been undertaken due to a threat to the site.
In 1982, building extensions caused such a threat and three trenches were opened (Harrington and Denham, 1986).

Site 232, adjacent to the workingmen's club, produced 1 st-2 nd century Roman evidence in the form of pottery and a dog burial together with medieval and post-medieval pottery.
Site 237, behind the gift shop, consisting several pits and a post-hole produced some medieval pottery, although, a majority of evidence dated to the post-medieval period, including earlier structural alterations.

Site 238, however, produced tenuous prehistoric evidence.
Two pits, behind a brick-built house, produced flint nodules, charcoal, chalk rubble, mudstone and animal bone fragments.

This trench also revealed post-medieval pits and a brick-lined well (ibid).
Despite a lack of pre-medieval evidence, this rescue excavation revealed further medieval and post-medieval activity in the southern half of the henge and shows the benefit of limited excavation on a protected site.

Since Smith's work, scientific procedures have assisted in the interpretation of the site.
In 1985, molluscan analysis identified the pre-bank environment of Avebury.

Snails recovered from the natural surface beneath the henge bank were grassland species, indicating either ungrazed or lightly grazed, dense, vegetative cover (Evans et al, 1985: 310).
Five samples of charcoal, animal bone and antler from the henge bank, old land surface and primary fills of the henge ditch were collected from evidence recovered during the excavation by Keiller and radiocarbon dates of 2504 ±84 BC were produced (Pitts and Whittle, 1992).

The combination of these two scientific techniques not only provides accurate dating for the construction of the henge but also the environment at the time.
Despite Iron Age intrusion into some samples, a reliable radiocarbon date was obtained from a pig bone from stone-hole 44 of the outer circle.

This date identifies the construction of the outer circle approximately 600 years after the henge, 1920±90 BC, making the outer circle late Neolithic/early Bronze Age in date (ibid).
In 2001, Aaron Watson, amongst others, approached the study of monuments from an aesthetic perspective; the visual, auditory, tactile features of Avebury henge were discussed.

Providing cognitive meaning, Watson suggested the location of sarsens in the landscape and their inclusion in the monuments was important, as was utilisation of axe polished stones (Watson, 2001).
The excavation of middle and lower chalk for use as packing with the stone-holes "invested it with a different kind of significance or value", as may the dark brown soil excavated from pits within the vicinity of the proposed Obelisk (2001: 301).

Having previously undertaken research on the acoustics of monuments, Watson understandably commented on the audiological significance of the henge.
He stated not only would the bank retain any noise within the henge but also absorb sound from outside (Watson, 2001), therefore, creating an isolated space.

Also, due to the broader stones of the inner circles compared to elsewhere in the monument, these areas were likely to create "intense acoustic spaces" (Watson, 2001: 308).
Watson suggested the axe polishing marks of the stones are more easily found through touch than by sight (ibid), leading neatly to the concentration of his work.

A majority of the approach by Watson focused on the visual effects of Avebury.
He indicated the ground level of the enclosed area of the henge is uneven and noticed the dimensions of the stones diminished beneath the visual level of the viewer.

Watson suggested the inner circles are so positioned as the only location with the henge where both Windmill Hill and Silbury Hill are visible simultaneously and that the internal subdivision of space may have physically or visually limited access (Watson, 2001).
Regarding the henge itself, the decrease in the height of the bank, Watson says, accentuates the size of the stones of the inner circles, while the symmetry between the bank and the distant hills provides a surrogate at points within the monument where the bank is hidden (Watson, 2001: 302).

A similar effect can be witnessed at the Stones of Stenness, Orkney, a large rock-cut henge with a single entrance, from which animal bone provided the radiocarbon date c3100-2775 BC (Burl, 2000: 211).
The henge, together with a second, the nearby Ring of Brodgar, is located in a natural bowl created by distant hills (Richards, 1996: 203); this local topography is reflected in the bank and ditch of the henges.

Watson confirmed the lack of astronomical alignment (ibid), however, caused this writer to consider the orientation of the four entrances.
In a prehistoric monumental world seemingly concentrated on astronomical alignment, for example Stonehenge, or an east-west orientation such as long barrows, why do so many henges, including Avebury, possess entrances off the cardinal points?

During brief research, it was found of eight henges examined, totalling 25 entrances, only four entrances, 16%, were aligned on cardinal points (appendix 1).
The remainder fell between the points with a majority on a south-east or north-west alignment, often occurring in the same monument; however, marginally over 50% occur in the northern range.

Harding depicts the north-east causeway of Stonehenge is centrally aligned with the northern lunar limit, however, is off-set to the midsummer sunrise and midwinter sunset (2003: 46).
The exceptionally low number of cardinal alignments suggests deliberate location of the entrances but to what end?

This brief research has left the writer intrigued and one hopes further research has unknowingly been undertaken or will be undertaken in the future.
Due to the 'ownership' of Avebury henge and other associated monuments by the National Trust, little excavation has been permitted within the complex.

This has not precluded non-invasive procedures, however, has resulted in scientific analysis of old samples and interpretation through observation within the landscape, resulting in calibrated radiocarbon dates, environmental reconstruction and aesthetic interpretation.
Beckhampton Avenue

Since the work of John Aubrey during the 17 th century and William Stukeley in the 18 th century, their work has often been re-evaluated.
Despite fairly accurate descriptions, mistakes have been recognised such as Aubrey's descriptions of 'The Devil's Coytes', now known as The Longstones, south of the henge opposed to their correct location to the west, as corrected by Rev.

Bryan King (1879: 378).
King posed the question of why so few, if any, of the stones survived of the "presumed Beckhampton Avenue" (1879: 381-382).

He summised that due to a lack of structures along the route of the West Kennet Avenue, the stones had not been broken and used as masonry opposed to the numerous structures to the west of Avebury, including cottages, walls and bridges, "more than sufficient, to absorb all the stones of the Beckhampton Avenue " (ibid).
Upon investigation, King found the broken stumps and chips of sarsen along a half-mile length of the suggested route of the Beckhampton Avenue (King, 1879), improved the validity of Aubrey's work.

Stukeley was also of the opinion that a second avenue ran westwards towards Beckhampton, which he named 'Bekamton Avenue' [sic], formed the tail of a 'serpent', linking some of the monuments of the Avebury region (Piggott, 1950: 126).
He believed a second stone circle would have stood at the terminal of the Beckhampton Avenue to balance the Sanctuary and sketched an estimation on an early plan of the complex in 1723, naming the proposed monument as the Temple of the Infernal Regions, before later scoring it out (Piggott, 1950).

Unfortunately, apparently due to land reclamation and the need of building materials, the Avenue had all but been destroyed by 1800 (Burl, 2002: 217).
Smith, however, included further details of Stukeley's observations in her work.

Contrary to the conclusion of Piggott that the proposed Beckhampton Avenue reached no further than the Longstones (Piggott, 1950: 114), Smith states of Stukeley's work, that the Avenue apparently continued south-westerly, terminating in a valley to the west of Beckhampton and equalled the length of the West Kennet Avenue (Smith, 1965: 216).
The fact that only ten percent of the stones present were standing is said to represent the remainder as natural recumbent sarsens, however, Smith does accept that some may have been "artificially placed" (1965: 217).

On examining King's work, discussed earlier, Smith concluded that there was strong evidence for the existence of a second avenue, however, the "full extent and precise relationship with the Longstones" was not fully explained (Smith, 1965: 217).
As early as 1969, the possible length of the Beckhampton Avenue had been extended following the discovery of a buried, partial sarsen stone west of Beckhampton roundabout (Vatcher, 1969:127).

Further work was undertaken in the late 1980's and 1990's in the form of geophysical survey and aerial photography.
Although geophysical survey in 1989 produced inconclusive results including possible pits and buried stones, aerial photographs in 1997 identified a flattened oval enclosure adjacent to the standing stones, which may have once been part of a cove (Gillings et al, 2000: 2), promoting further investigation and ultimately excavation in the vicinity of the Longstones (Burl, 2002: 219-220).

In 1999, following the discovery of medieval ridge and furrow, several trenches were dug on the circumference of the ditch, which enclosed an area of three and a half acres.
The excavations identified a ditch up to 2.10 metres wide, with varying depths reaching a maximum of 0.80 metres, suggesting a series of smaller ditches, separated by causeways, which were later removed (Gillings et al, 2000: 4).

In addition, six features were uncovered, forming two parallel lines running north-east to south-west, aligned on the Longstones.
Of the six features, three contained buried stones, one was empty, and the remaining two contained stone destruction debris in the form of charcoal and sarsen flakes (ibid).

The proposed avenue enters the enclosure through a wide entrance on the eastern side; the easternmost Longstone falling within the boundaries, the western stone beyond the boundary.
Both stones are on the alignment of the Avenue possibly representing stones of the Avenue (Gillings et al, 2000).

The backfill of the enclosure ditches consisted of chalk rubble and various soil types, together with limited finds including animal bone and pottery, one sherd of which was provisionally identified by Dr Ros Cleal of the Alexander Keiller Museum, Avebury, as early Neolithic (Gillings et al, 2000: 4).
Subsequently, animal bone was radiocarbon dated to the mid 3 rd millennium BC, a date supported by Grooved ware from the base of the ditch (Gillings et al, 2002: 255).

The rapid backfilling and silting suggests the closure of the monument is possibly contemporary with the construction of the Avenue, however, the lack of datable evidence from the stone sockets makes it difficult to determine whether the West Kennet and Beckhampton Avenues are contemporary, although a date of 2500-2300 BC is expected (Gillings et al, 2000).
Conversely, Gillings and his co-authors of 2000, questioned whether the Avenue continued beyond the Longstones to the south-west as suggested by Stukeley (2000: 7); no reference was made to the discovery of the buried stone uncovered during a watching brief of the late 1960's.

A later digging season of the site in 2000, produced possible evidence of temporary settlement.
Once the topsoil had been removed and the chalk weathered, in an area of 10 metres x 10 metres to the south-west of the westernmost Longstone, 200 stake-holes were revealed.

Although the distribution was non-random, no structural pattern was revealed, therefore may represent a palimpsest of dwellings and fences (Gillings et al, 2002: 255).
The location of third and fourth stones between the Longstones were identified confirming earlier theories of a cove-like structure similar to that of the Northern Inner Circle of Avebury henge, although rectilinear in design rather than a three-stone open structure (Gillings et al, 2002).

Backfilling and silting of stone sockets suggests the two outlying stones were removed and the cove was erected on a different alignment, possibly to commemorate the earlier enclosure (ibid).
Pit F.52 to the west of 'Adam', the westernmost of the Longstones, produced re-deposited chalk and soil, fragments of iron, including an Anglo-Saxon spearhead, pieces of shield-fitting and part of a knife blade, probably from a nearby disturbed 7 th century Saxon inhumation (Gillings et al, 2002: 254); this shows continued activity in the area, conversely of a period absent in the archaeological record of Avebury henge.

Since Smith, 200 metres of the Beckhampton Avenue have been excavated, although the full extent and any possible monument at the terminal, as sketched by Stukeley, have yet to be identified.
During the process of excavation further archaeological features and earlier monuments have been located and identified.

Despite structural hindrance, perhaps investigation on the areas to the north and east of Avebury henge will eventually locate further avenues of the complex and therefore identify the reason for four entrances to one henge.
Possibly similar work at other monuments will reveal similar structures, for example, local legend suggests the Stones of Stenness and Ring of Brodgar, Orkney, were linked by a stone avenue, however, no evidence has been found to prove this hypothesis (Richards, 1996: 199).

The Sanctuary
In 1723, Stukeley produced a plan of a double-ringed stone circle, approached by the 'Kennet Avenue', and named the monument the Temple of Eartha; however, in 1724 the site was renamed the Temple on Overton Hill (Piggott, 1950).

In a letter to his friend Roger Gale, Stukeley likened the Avebury complex to the form of a snake, the avenues forming the body and Overton Hill the head (ibid).
Although the sketches by Stukeley depicted an oval structure, following excavation in 1930 proving the monument to be circular, Piggott (1950) suggested this was to produce a snake with a naturalistic head.

In a later publication, briefly commenting on the Sanctuary as it became known, Piggott claims that the post-holes of the structure, whether it be a single or multiple phase construction, would represent a roofed timber-framed building.
Due to the location within a wider monumental landscape, together with the deposition, organised or otherwise, of pottery, flint implements and animal bone within the post- and stone-holes of all but the earliest phase, possibly representing ritual feasting, Piggott suggests the timber building was more than a secular construct (Piggott, 1962: 75).

Isobel Smith suggested the Sanctuary was of several phases spanning around 500 years, and was possibly completed around the same time as the southern end of West Kennet Avenue, around 1600 BC (1965: xxviii).
Although the Sanctuary lacks the bank and ditch profile of a causewayed enclosure, due to the internal, concentric pattern, Smith suggests the two are comparable (Smith, 1965: 249).

The first phase, Smith suggested, was that of a roofed timber hut slightly in excess of 4 metres in diameter, dated by pottery to 2000 BC.
The second phase, constructed due to adverse effects of the weather, was an eleven-metre diameter 'protective' timber structure.

A circular timber temple, almost 20 metres in diameter, was followed by a fourth and final phase of an outer stone circle, almost 40 metres in diameter, which incorporated tow stones, perpendicular to the rest and aligned with the West Kennet Avenue (Smith, 1965).
Of the numerous finds recovered from the different 'phases', none were recovered from the original structure, the evidence for which is known as the 'F-ring'.

Smith suggests this to mean all finds related to the later structures, although the G-ring, adjacent to and on the same circumference as the wall of the original hut was also barren of finds, suggesting the finds were deposited around the exterior of the structure but not within (Smith, 1965: 245).
Pollard suggested a marginally earlier date for the Sanctuary; based on relative dating of the pottery and comparison of the monument to similar timber constructs, a date of 2500 BC was suggested (Pollard, 1992: 213).

The phases of construction are tentatively reduced following the re-evaluation to only two phases; one of timber, one of stone.
The arrangement of the posts produced an internal cruciform pattern of equally spaced aisles, comparable to Mount Pleasant site IV (Pollard, 1992: 214-215).

At this time there was continued belief the structure had been roofed due to the oblique angle of the post-holes and molluscan evidence of freshwater species suggested reed thatching may have been in place (Pollard, 1992: 216).
Due to the internal arrangement, movement within the monument would no doubt have been restricted.

With difficulty, it may have been possible to negotiate a path between the posts; perhaps more likely, passage was restricted to the open aisles of the monument (Pollard, 1992: 222-223).
The internal organisation also revealed a spatial deposition of flint and arrowheads in the north-east quadrant; a similar pattern was recognised in the eastern half of Woodhenge (Pollard, 1992: 222).

Perhaps this is evidence of cardinal alignment, as discussed earlier, and further research on organised deposition within henges is required.
The Sanctuary, positioned 170 metres above sea-level, is located on a high point of the immediate area surrounding Avebury, however, rudimentary examination of an Ordnance Survey map shows that the monument is surrounded by higher peaks reaching a maximum elevation of 295 metres 3.75km to the south-east, near the reservoir on Milk Hill.

This examination reinforces Watson's hypothesis that opposed to the artificial bank and ditch of Avebury, the Sanctuary draws upon the natural topography of the surrounding landscape, as does Silbury Hill, the summit at 187 metres creating a restricted area, the partially surrounding hills enclosing the space (Watson, 2001: 307).
In contrast, few peaks surrounding Arbor Low, Derbyshire, equal or exceed the elevation of 370 metres.

Despite encircling hills, only three hills are of comparable height; a peak approximately one mile to the south equals the elevation; a mile to the south-east the hill known as Lean Low reaches 393 metres and a steep hill near the town of Taddington peaks at 414 metres.
The placing of a monument at a high elevation surrounded by superior distant hills is apparently not a countrywide phenomenon.

Although the Sanctuary has not undergone further excavation, the quality of early excavation and recording has allowed continued re-examination of the evidence.
Re-interpretation has reduced the monument from a multi-phase structure to that of only two phases; comparison of the structural layout has identified a strong resemblance to that of the slightly larger Mount Pleasant in ground plan; and the use of the natural topography surrounding the monument has been suggested as producing an enclosed area.

For a monument that has remained unexcavated for many years, it has proved invaluable in the hypothesis proposed during re-evaluation.
Neolithic settlement within the Avebury Complex

Despite regular discoveries of pottery, worked flint and animal bone, firm evidence of Neolithic settlement within the Avebury complex has yet to be discovered.
Neolithic pottery has been found on every excavated site in the Avebury area and suggests a large population in the immediate vicinity, concentrated in the lowland region due to a reduced frequency of lithic finds in the uplands (Smith, 1984: 106).

Various pits and holes have produced evidence, some of which will be discussed here.
In 1913, during the digging of a pipe trench, a Neolithic pit was discovered.

Following excavation the artefacts recovered from the pit were briefly discussed by Maud Cunnington in the same year, however, were then put into storage and were apparently not re-examined until 1955 by Nicholas Thomas.
Nineteen sherds of pottery, 41 flint implements and a sarsen hammer stone were discussed by Thomas (1955), who failed to discuss the relevance of charcoal and various animal bone briefly mentioned by Cunnington (1913: 14, cited by Thomas, 1955: 167).

Although quantities were not discussed, the presence of charcoal together with animal bone, some of which had been burnt, is evidence of a meal.
A small quantity may suggest a temporary settlement, a large quantity possibly a more permanent settlement or large feast.

Despite the lack of full evaluation of the evidence, Thomas concludes there was probably a Neolithic settlement on Waden Hill and recognition of a Neolithic settlement in the Avebury region is imminent (1955: 171).
During excavation by Keiller, a proposed Neolithic settlement was detected adjacent to the West Kennet Avenue.

Keiller exposed a series of pits and holes, which he later described as clay-lined hearths without prehistoric comparison (Keiller et al, no date/?unpublished: iv).
In 1965, Smith described the two pits and ten small holes with their contents.

The pits were 1.20-1.50 metres in diameter, 0.30-0.60 metres in depth, and contained pottery sherds, flint and sarsen nodules, arrowheads and animal bone.
The holes, however, were oval in plan, varied in diameter from 0.30-0.90 metres and were funnel shaped (Smith, 1965: 212).

All the pits and holes included charcoal and all but two contained flint nodules.
Both pits produced animal bones and teeth, however, only one hole, number 9, contained animal remains, in the form of an ox tooth.

The charcoal was of twigs and small branches of hazel, blackthorn and hawthorn (ibid), therefore, due to the relatively small nature of the timber, would unlikely be used for construction.
As the stone from the pits and holes showed no evidence of heating, Smith disputes Keiller's hearth hypothesis and suggests the charcoal was deposited from nearby hearths rather than in-situ burning (ibid).

Later analysis of three samples of antler and bone from the pits and holes produced a radiocarbon date of 2823±87 BC (Pitts and Whittle, 1992:205).
In the time between excavation and publication of Windmill Hill and Avebury, the possibility of organised deposition within areas, ditches or pits was not considered.

The brief analysis of the evidence undertaken during research for this paper revealed the arrangement of materials and a degree of formality within the pits and holes of the Neolithic settlement adjacent to West Kennet Avenue; future research is planned to determine whether such organisation exists.
Environmental research has produced evidence of prehistoric agriculture in the Avebury region.

South Street long barrow, long synonymous with early agriculture having produced ard marks from the buried land surface, has also provided evidence of the farming practices during prehistory.
Following the initial ground preparation, two phases of cultivation by hoes and spade were separated by fallow and pastoral periods (Smith, 1984: 109), suggesting knowledge of the benefit of allowing the soil to rest between phases of cultivation.

Evidence of manuring (ibid) may suggest the application of manure either from storage or directly during post-harvest grazing, improving the nutrient content of the soil (Holgate, 1988: 123), and the breakage of sarsen and burnt stubble evidences burning (Smith, 1984: 109), probably for post-harvest clearance.
Pollen recovered from a tree-hole at South Street showed the area was heavily wooded during the forth millennium BC, with some openings evidenced by molluscs (Smith, 1984: 113).

Bracken apparently colonised the area 2810±130 BC, prior to the construction of the long barrow (Smith, 1984: 117).
The erection of a sterile long barrow, containing no inhumations, on cultivated land may have meaning; perhaps reference to the fertility of the land.

Research in the areas surrounding other sterile long barrows may produce further evidence of agriculture.
Non-invasive research should continue within the Avebury region; due to the concentrated dispersal of pottery, flint and animal bone, the discovery of a local settlement, perhaps similar to that of Barnhouse, Orkney, is increasingly likely.

Areas of concentrated monumental construction, such as Thornborough, Yorkshire, a site consisting of three henges in close proximity, should also be examined; early farmers, and the probable manpower involved during construction, would have lived locally, possibly in small, supportive communities, hopefully leaving settlement and prehistoric agriculture evidence in the landscape.
Recently, Dr Ros Cleal of the Alexander Keiller Museum in Avebury examined a pottery sherd, unearthed by wildlife, close to a field boundary, to the east of Faulkner's Circle.

She saw a resemblance in the fabric to a fragment in the archive, which had been recovered in the same area during 1935 by a member of the public.
When the two sherds were compared, there was a striking similarity in the dark grey clay with flint inclusions, and double, linear impressed dot pattern, approximately 10mm below the rim; Dr Cleal considers the two to have originated from the same vessel (pers.

comm., 15/12/05).
This discovery has prompted speculation of settlement evidence on the eastern side of West Kennet Avenue.

Winterbourne Bassett Stone Circle
In the 18 th century, Stukeley sketched a stone circle at Winterbourne Bassett, north of Avebury (fig 3); unfortunately, the monument has been lost, possibly due to land clearance or use as masonry.

The location has been plotted on Ordnance Survey maps for many years at grid reference SU 0936 7552 (fig 4), although the position has been estimated with reference to historical material (fig 5).
Little reference was made to this monument by Smith (1965) other than to comment on a possible Neolithic "system of belief" and the proposed correlation between monoliths and fertility rites (1965: 250).

However, the inclusion of Winterbourne Bassett in the discussion was based on the work of A.C.
Smith in 1885 (ibid), who produced a plan of stones surrounding a central monolith; the accuracy has been questioned due to the absence of a central stone in Stukeley's sketch (David et al, 2003).

Recent geophysical survey, cast doubt onto the location of the stone circle.
Data recovered from the presumed position proved disappointing (fig 6, A); however, several anomalies were located in the field south of the Winterbourne Basset to Clyffe Pypard road, suggesting a revised co-ordinate for the monument at SU 0930 7535 (David et al, 2003: 203) (fig 6, B).

A comparison of the sketch produced by Stukeley was made to that of the supposed position of the structure.
As landmarks of the Avebury landscape, were in the background of the plan, including Silbury Hill and St James church, it could be estimated where Stukeley was positioned during production of the sketch; however, the topography did not match.

As the sketch depicted the double stone circle from a slightly elevated position, it was possible to estimate the location from which Stukeley viewed the monument; a position correlating with the positive results obtained from geophysical analysis.
Future research was proposed, although the size of the task was suggested as daunting (David et al, 2003).

A B In 2004, James Gunter, a post-graduate archaeology student, living in the Avebury area, conducted further geophysical analysis as part of a masters dissertation, and achieved positive results in the area suggested by David and his fellows (2003).
Through excavation, the area proposed as the location of the 'lost' monument, has now been confirmed as the actual position of the stone circle, as sketched by Stukeley in 1724 (James Gunter, pers.

comm., 15/12/05 & Ros Cleal, pers.
comm. 29/12/05).

Following the award of a distinction for the dissertation, Gunter hopes to publish his results.
Conclusion

Following the recognition of the prehistoric importance of Avebury by Stukeley, antiquarians and archaeologists have frequently recorded, excavated and interpreted the monuments of the complex.
Since the publication of Windmill Hill and Avebury: Excavations by Alexander Keiller 1925-1939 by Isobel Smith in 1965, scientific methods and alternative hypothesis have allowed re-evaluation of the Avebury complex and occasional excavation has provided new evidence and located 'new' monuments.

Pollen and mollusc analysis has reconstructed the environment prior to the construction of South Street long barrow and Avebury henge during the first half of the third millennium; procedures not conducted during the original analysis of the evidence.
New hypotheses have suggested the location of monuments was possibly influenced by the topography of the landscape, which may be mirrored in the monument itself; acoustic analysis has revealed affects within the Inner Circles and Avebury henge itself; and cognitive meaning behind the use of certain materials in the construction of the monuments has been suggested.

Through aerial photography the line of the Beckhampton Avenue has been identified in association with a previously unknown Neolithic enclosure and the 'lost' stone circle of Winterbourne Bassett has been relocated initially through geophysics and confirmed by excavation.
Although excavation within the Avebury complex is rarely permitted, the research that has been completed has been extremely successful.

In conjunction with Windmill Hill and Avebury, new evidence has allowed frequent invaluable reinterpretation of the region, providing new or alternative hypotheses that can be applied to comparable monuments within the rest of the British Isles.
Introduction

Over recent years the conception of the early Neolithic pattern of settlement has changed.
Traditionally, until the 1970's, a dispersed settlement pattern of rectilinear structures accommodating extended family groups were thought to have adopted stable mixed farming practices, domesticating animals and crops.

Archaeological investigation now shows the settlement of the British Isles was more diverse than previously thought, with a variety of domestic and task related buildings and farming techniques.
This essay will address many of the practices involved in the colonisation of an area including immigration, settlement and subsistence.

Immigration and acculturation or evolving cognition?
Immigrants have traditionally been cited as the origin of farming and their contact with the indigenous population would have led to the diffusion of new ideas.

Alternatively, the aboriginal population of the British Isles, having first observed the annual progress of the environment, may have chosen to embrace the natural resources.
Case suggested there was a tendency to explain the settlement of Britain and Ireland by large-scale immigration or invasion from western Europe (1969: 176).

Transhumance may have included 'communal' movement relocating the entire system, including people, stock and resources, to a fairly local site; or 'seasonal' expedition by a few individuals over greater distances (Case, 1969: 178).
It was suggested despite the establishment of a farming system using primitive transport and equipment (Case, 1969: 176), contact with the homeland would have continued for many years (Case, 1969: 180).

Following seasonal exploration of foreign lands, the potential immigrants would have transported their animals to Britain for summer grazing, however, over-wintered in their homeland (ibid).
This seasonal migration continued with the addition of crop cultivation (Case, 1969: 180) and eventually permanent settlement was established.

Why should the development of new techniques and technology require the introduction of a new culture?
Evolution of lifestyle from temporary camps to a sedentary but scattered community (Malone, 2001: 45) can be undermined by evidence of settlement in the Thames Basin by hunter-gatherer communities prior to the Neolithic period (Holgate, 1988: 150).

Transhumance, possibly seasonal migration, would enlighten the Mesolithic population to areas of the landscape with the greatest number of resources.
Following settlement in a fertile area of numerous resources, providing both food and raw materials, the passing seasons would have revealed the process of wild crop germination and maturation.

Advantage could be taken of nature eventually leading to the cultivation of crops, which in turn would require tools of specific purpose, compelling alternative production techniques and technology not previously carried in a mobile existence.
Could the early Britons have not brought themselves technologically into the Neolithic?

Whatever the origin of these changes, a low population density resulted in dispersed settlements with no pressure on land or resources (Schofield, 1987: 75).
The landscape was a mosaic of settlements, with areas of woodland clearance, regeneration and farming activity (Healy, 1988: 105), requiring sturdy structures for the inhabitants and their stock.

Structures
Early interpretation suggested the early Neolithic British Isles was settled by numerous dispersed small farms, consisting of one to three rectilinear houses, the material of which was dependent on local availability, inhabited by extended family groups (Bradley, lecture, 28/10/03).

The scarcity of structures in certain areas could be explained by erosion, such as the tidal destruction of a midden dating to the fourth century BC at Mannin, Co.
Galway (McCormick, 1995: 12), or "flimsy and temporary dwellings" (Thomas, 1996: 2) leaving little or no evidence in the archaeological record.

Malone, however, suggests this dearth may be due to a lack of recognition or failure to look for the evidence (Malone, 2001: 48); for example, a unique Neolithic enclosure perhaps exists at Garton Station, Yorkshire, as no trace of a burial mound was found as evident over surrounding Iron Age enclosures (Stead, 1987: 236).
As Malone states, the best Neolithic settlement evidence is located in "the least fertile, inhospitable areas" due to the possible destruction of sites through ploughing in the more agriculturally rich areas, including East Anglia and the chalklands of southern Britain (ibid).

In contrast, recent and continuing excavation, particularly of Ireland, suggests the Neolithic Britons lived in substantial houses coalescing into villages.
Archaeological excavation at Mullaghfarna, Co.

Sligo, revealed what was originally interpreted as a cluster of enclosures (Grogan, 1996: 54), is actually a Neolithic 'village' of approximately 120 houses (Bradley, lecture, 11/11/03).
Caution should be taken when examining the possible remains of a village: the rotation of a farmstead around a single point could potentially be misinterpreted as a larger settlement (Holgate, 1988: 114).

In contrast to Thomas' view, many of the recognised Neolithic structures of Scotland and Ireland are far from flimsy.
Some of these structures were well planned and the individual houses often included external areas, annexes, possibly for storage or sheltering of animals and internal storage areas.

Excavation of a Neolithic farmstead at Knap of Howar, Orkney (fig 1), has revealed two rectilinear structures, of double skinned dry stone walling with a midden core of domestic rubbish, apparently joined by a passageway (Ritchie, 1983: 42-43).
The larger of the two, House 1, interpreted as a dwelling, consisted of two rooms with a stone partition and partially paved floor; a probable secondary context of a massive trough quern suggested at least two phases of occupation (Ritchie, 1983: 42-43).

A central ash filled hollow represented a hearth (Ritchie, 1983: 48) and the location of House 2 to the north would provide some shelter from the elements.
House 2, incorporating three compartments, was interpreted as a workshop or storage structure due the inclusion of five recesses in the walls (Ritchie, 1983: 43) and the addition of shelves (Ritchie, 1983: 51).

Phases of occupation were recognised through a number of features including a doorway to an adjoining passage with House 1 was sealed in a secondary phase following a partial collapse (Ritchie, 1983: 51); and a surviving low stone bench may have replaced an earlier timber structure, evidenced by grooves in the clay surface (Ritchie, 1983: 50).
Animal bone collagen suggested a 500-year occupation during the third or fourth millennium BC (Ritchie, 1983: 57).

Abandonment did not occur in haste: no artefacts were recovered from the storage facilities, suggesting the evacuation was planned (Ritchie, 1983: 51).
The conclusion reached was the site probably represented a single, extended family farmstead (Ritchie, 1983: 58), supporting the early view of Neolithic settlement.

Waddell suggests the substantial timber buildings of the fourth millennium BC represent some degree of permanent settlement (Waddell, 1998: 30), however, construction was restricted to areas supporting substantial trees or importation of suitable material would be required.
The sub-rectangular house at Ballygalley, Co.

Antrim (fig 2) was presumably constructed of timber evidenced by an absence of structural remains, excluding the foundation trench (Waddell, 1998: 32), however, decomposition is not necessarily the case as the house may have been dismantled (ibid).
A slightly curved annex from the north-west corner, visible in figure 2, may have provided shelter for the cattle from a northerly wind.

Circular Neolithic structures are also known (Waddell, 1998: 30), for example, Lough Gur, Co.
Limerick, Site C, has produced three houses of circular plan (fig 3), probably dating to the third millennium BC (Waddell, 1998: 34).

Two of double-ring construction, possibly with an organic fill, contained a hearth and at least one pit for storage or rubbish (Waddell, 1998: 34), and may represent a secular building of the 'village'.
The third was possibly for storage suggested by the flimsy construction of a single ring of posts, possibly supporting walls of wattle and daub (Waddell, 1998: 34).

Within the same 'village', Site K consisted a low wall of double-skinned boulder construction with an earth and rubble core.
The enclosure contained a sub-rectangular house (fig 4) and external pit (Waddell, 1998: 34).

The size of the enclosure probably denotes a house of special status opposed to defence, however, the artefacts found within consisted of everyday objects including leaf-shaped flint arrowheads and plain round-bottomed bowl (Waddell, 1998: 34); nothing associated with status.
Comparison of the houses of Lough Gur identifies a difference in design, although, if they represent people of various status or structures of specific purpose remains uncertain.

The identification of hearths in two of the three smaller round houses does suggest they were inhabited, however, Site K apparently lacks a hearth (Waddell, 1998: 34).
In contrast, Site K produced albeit modest artefacts compared to a total absence in Site C (ibid).

If the houses of Site C were dwellings, the inhabitants either lacked possessions or made a planned abandonment reminiscent of Knap of Howar, or these were not dwellings but storage or processing buildings.
In permanently waterlogged conditions, organic material is well preserved, including structural remains.

In the Somerset Levels wooden trackways, including the Sweet Track, have been preserved in remarkable condition.
The raised path, of pole and plank construction, linked Westhay Island in the reed swamp to the Polden Hills to the south (Malone, 2001: 64) replaced an earlier version, the Post Track (Coles & Coles, 1986: 45).

The Post Track, situated slightly lower and to the right of the later structure (fig 5), possibly represents a former access during construction (Coles & Coles, 1986: 46) or a changing level in the water table.
Dendrochronological examination of an oak plank from the Sweet Track provided a felling date in the winter of 3807/3806BC (Hillam et al, 1990: 214) and studies of hazel used for repairs shows the track was used for at least ten years (Coles & Coles, 1986: 56).

The connection of the dry islands within the swamp to the mainland supports the view the inhabitants of the area must have settled on dry land (Coles & Coles, 1986: 63) and the pathways probably indicate access to resources, possibly reeds for thatching purposes.
Contrary to the early view of Neolithic houses, the plans and reconstructions of sturdy buildings detailed above show various designs.

Far from the dispersed but uniform rectilinear plan of family farmsteads, some structures are grouped into rudimentary villages, possibly displaying differences in status.
The sedentary Neolithic population of Britain and Ireland were capable of erecting substantial structures, but were they farming the surrounding land?

Economy
The Neolithic has traditionally been viewed as being based on a mixed farming economy (Thomas, 1999: 7), however, further archaeological investigation now suggests periods of successful farming followed by a collapse in the economy due to the unstable relationship between the population and the natural resources (Thomas, 1991: 8).

Changing farming methods may have been adopted in the attempt to overcome these imbalances as demonstrated by a changing pollen record between stages A and B of Ballynagilly, Co.
Tyrone (Pilcher et al, 1971: 560); evidence suggests forest was cleared in favour of cereal cultivation with a subsequent increase in plantain, probably a period of pastoral activity, before regeneration of woodland (ibid).

Molluscan evidence from Ascott-under-Wychwood, Oxon, may support this hypothesis as, although the long barrow was situated on the edge of arable land (Evans & O'Connor, 1999: 206), woodland regeneration occurred between two phases of occupation (Holgate, 1988: 111), possibly reflecting a collapse in the local economy and an abandonment of the land.
This instability may have influenced the location of settlements, the development of farming techniques and animal husbandry, although, the early Neolithic still appears dependant on wild resources to supplement any domestic production.

4a Agriculture
The sudden decline of the elm population in the early Neolithic, often attributed to woodland clearance by early farmers, can be undermined by the environmental records showing southern Britain was open prior to this period (Schofield, 1987: 274).

Small garden plots were probably adopted for the cultivation of cereal, where the addition of household rubbish and the application of manure, either from storage or directly during post-harvest grazing, would improve the nutrient content (Holgate, 1988: 123).
South Street long barrow, long synonymous with early agriculture, produced light plough marks from the buried land surface dating to the fourth millennium BC (Waddell, 1998: 29), although, associated charcoal deposits were radiocarbon dated to 2810 ± 130BC (Evans & Burleigh, 1969: 144).

The alkaline chalklands of Wessex, destructive to pollen and coleptera (beetles), are perfect for the preservation of mollusc shells, which provide information on the environment of prehistoric Britain.
The mollusc assemblage beneath South Street, predominantly Pupilla muscorum, identified an open landscape, with shade loving species suggesting either open woodland or contamination with late glacial shells (Evans, 1972: 257 & 260).

Therefore, man-made clearings within woodland or natural open areas were utilised for agriculture.
Plant impressions on pottery, produced on wet clay prior to firing, and carbonised cereal grains from Spong Hill, provided evidence of early British crop cultivation, and were identified as barley and various wheats including emmer (P. Murphy, in Healy, 1988: 103).

On the same site, fragments of saddle quern were identified (Healy, 1988: 39), representing the on-site processing of the crop.
4b Pastoralism

Case suggested during the early Neolithic, livestock were imported from western continental Europe (1969:180) and probably included cattle, pig, goat and possibly sheep (Case, 1969: 177).
Return journeys to the homeland to impregnate domesticated cows may have been required, as breeding with wild bulls would have imposed danger through oversized calves (Case, 1969: 177).

Once imported the livestock required containment, possibly a purpose of causewayed enclosures, where substantial banks and ditches or the addition of palisades would make effective kraals for containing cattle (Malone, 2001: 73).
The example of Hambledon Hill, Dorset, overlooks fertile grassland suitable for grazing (ibid) and environmental data from the ditches provided evidence of woodland clearance around 3660-3380BC (Malone, 2001: 34), possibly an indication of cattle grazing in the area.

Animal grazing in forests may have been introduced as a form of woodland management (Holgate, 1988: 123) and as hazel is unpalatable to cattle, an unequal ratio in favour of hazel over other plants shows grazing occurred (Pilcher et al, 1971: 561).
The extensive enclosed system of Céide Fields, Co.

Mayo, potentially provided animal enclosure due to the construction of long parallel stone walls, surviving to a height between 50-70cm, subdivided into large rectangular fields up to 7 hectares in size (Waddell, 1988: 36).
Pollen analysis suggests the fields probably date to a period of intensive pastoral activity between 3700-3200BC (Waddell, 1998:38).

Finally, the animal assemblage can identify the farming techniques.
Examination of an equal proportion of cattle and sheep bones from Knap of Howar shows the recently domesticated animals were slaughtered young, for their meat and hide (Ritchie, 1983: 56).

4c Wild resources
The evidence for the continued gathering of wild plant resources appears slight.

When examining a single site, the evidence can be meagre; a single hazelnut was recovered from the midden of Knap of Howar (Ritchie, 1983:57).
Examination of an area proves more informative; evidence retrieved from the Marlborough Downs identified acorns, blackberries, sloes, crab-apples, haws and hazelnuts from various sites (Holgate, 1988: 121), evidencing the wild resources of the region.

In contrast to the plant evidence, the faunal assemblage of Knap of Howar suggests infrequent hunting of wild animals, however, marine resources were of great importance (Ritchie, 1983: 56).
Numerous species of bird were identified; fish species identified line and deep-sea fishing from boats; and shellfish, although numerous, provided a negligible contribution to the economy (ibid), probably due to the low nutrient content.

4d Raw materials and domestic activities
As previously stated, a scarcity of timber in some areas of Britain, forces importation or adaptation in construction.

Any timber used in the earlier phases of the houses on Knap of Howar was either gathered as driftwood or imported from the Scottish mainland (Ritchie, 1983: 57).
In contrast, hazel samples recovered from the Sweet Track in Somerset revealed a seven year coppice cycle: definitive evidence of Neolithic woodland management (Coles & Coles, 1986: 56).

During analysis in 1988, pottery styles of the Thames Basin appeared regional, for example, the Abingdon style was recovered from the Upper Thames Valley opposed to a predominance of early Ebbsfleet in the Lower Thames basin.
A mixture of styles was recovered only from three causewayed enclosures including Windmill Hill, Wilts (Holgate, 1988: 125), suggesting an alternative purpose of causewayed enclosures as exchange centres.

A similarity of Thames Basin flint resources and production techniques utilised in the Mesolithic and Neolithic, show little or no disruption in settlement patterns and land use during the transition (Holgate, 1988: 132).
Case suggests during the Neolithic flint would probably have been gathered from surface exposure (1969: 182); in contrast, Holgate proposed a number of pits excavated at Liddington, on the Downs, may represent Neolithic flint mines (1988: 96).

No imported stone was recovered from Knap of Howar, only implements, including a stone axe, of a local fine-grained dolomite (Ritchie, 1983: 55).
Importation to Ballygalley, Co.

Antrim, of Arran pitchstone from Scotland, two stone axes from Great Langdale, Cumbria, and an axe of Cornish greenstone, suggested redistribution of imported goods (Waddell, 1988: 32).
Immigration, foreign travel or trade has been proposed due to the probable Alpine origin of an undamaged jadeite axe found adjacent to the Sweet Track (Coles & Coles, 1986: 59).

The condition and context has led to the interpretation of a votive offering (Coles & Coles, 1986: 59).
A restricted range of retouched forms of stone tools were recovered from Spong Hill (Healy, 1988: 46), supporting Schofield's 'home-range theory' of the Middle Avon Valley.

Schofield assumed the population, living in an area providing all the required raw materials, would travel only to complete their routine activities, producing a high-density of worked flint with a low proportion of retouched tools (1987: 277).
During the early Neolithic, farming practices became increasingly widespread.

Areas of woodland were cleared for the purpose of cultivation and animal pasture and enclosures were erected for the control of livestock.
As well as gathering their raw materials from the environment, the population of the British Isles continued to collect wild food resources to supplement an unstable subsistence pattern; this dependence on the environment may have led to defined territories.

Burial
A majority of early Neolithic burials have been recovered from long barrows or cairns and the examples found on the periphery of the Marlborough and Berkshire Downs (Holgate, 1988: 114) may have held some territorial purpose; a hypothesis possibly supported by regional variation identified in the burial monuments of Wiltshire where to the north earthen long barrows were constructed, opposed to the stone chambered tombs of the south (Thomas, 1999: 203).

The fourth millennium earthen long barrow of West Kennet in North Wiltshire contains five chambers, allowing segregated organisation of the bodies of the deceased by age and sex (Thomas, 1999: 204).
Various styles of pottery, predominantly Peterborough ware, show the burial tomb was in use over a long period (ibid) prior to closure by huge sarsen blocking stones.

The location and continued use of this tomb shows the reverence shown to the ancestors; an inconsistency in the number of bones shows the possible removal (Thomas, 1999: 207) and ritual exploitation.
Although there are no chambered tombs on Knap of Howar, the neighbouring island on Holm of Papa Westray, to the east, accommodates two, possibly three, tombs (Ritchie, 1983:59).

In contrast to other communities who are believed to have worshipped the ancestors, the inhabitants of Knap of Howar may have separated themselves from the dead; alternatively, there may be a correlation to the rising sun in the east, perhaps a belief in rebirth.
The erection of huge monuments for the inhumation of the dead denotes respect.

The generally highly visible location of the tombs evidences respect, possibly including ancestor worship or territorial rite, or both.
Conclusion

Due to the excavation of substantial houses and the introduction of new environment techniques, a re-interpretation of early Neolithic settlement patterns has been forced.
Rather than stable mixed farming, flexible farming practices were adopted in an unpredictable economic climate; no longer are the houses seen as flimsy rectilinear structures accommodating an extended family; instead the realisation the early Neolithic Britons constructed sturdy villages has been accepted; the reliance on local raw materials has been disproved by the identification of imported products, both from within the British Isles and from the continent; and the continued reliance on wild resources in addition to raw materials resulted in the introduction of territorial markers.

The early Neolithic pattern of settlement was far more complicated than previously thought.
INTRODUCTION

The evidence for prehistoric hunter-gatherers comprises the large floral and faunal assemblage, technology and architecture.
These remains enable archaeologists to reconstruct the subsistence of these ancient communities.

Through osteological refuse hunting methods and site function can be identified, and technology indicates the activities on site.
This evidence also assists in the recognition of simple hunter-gathering groups and the more complex societies.

However, the accuracy of these interpretations can be questioned and on occasion re-evaluated.
The following essay will approach the study of hunter-gatherer subsistence and, with the inclusion of case studies, evaluate the evidence involved during interpretation.

SETTLEMENTS
A simple hunter-gatherer community may be identified by the recognition of a small, highly mobile group with little material culture (Henry, 1985).

These groups were probably seasonally mobile, relocating to areas of predictable resource, possibly moving from the grasslands, rich in steppe fauna and wild cereals, into forested areas, for the harvesting of autumn maturing fruits and nuts, and onto the coastal fringes, to rely on the multi-seasonal supply of fish and other marine resources.
Although highly mobile, these groups would remain in one area for a period of time or an entire season.

The architectural remains are few but indicate lightweight habitats and shallow hearths, with little or no storage facilities.
The waterlogged site of Star Carr, in the Vale of Pickering, Yorkshire, has been and continues to be, one of the most re-evaluated sites of prehistoric Europe.

At the time of occupation, 9488 bp (Legge and Rowley-Conwy, 1988), Star Carr was a lakeside settlement.
A birch wood platform was constructed on a reed bed, flattened by probable human traffic, supporting an argument of occupation during the period of growth (ibid).

No traces of structures have been found at the site, although the excavator, Graham Clark, suggests only the lower levels of the platform, having remained permanently waterlogged, were preserved (ibid).
Structural evidence was discovered at the waterlogged Kebaran site of Ohalo II, Israel.

Semi-subterranean huts, one of three successive floor layers, were found constructed of plant material, still visible during excavation (Nadel, 1995).
The western most feature of the site was the lone grave of an adult male, orientated north-south, with stones positioning his head so he faced east, into the settlement.

There were no grave goods associated with the burial.
To the east, a small stone feature, measuring only 45 cm in diameter, had been constructed in the area between the grave and the dwellings; however, there is no evidence of association between the stone feature and the grave (ibid).

A series of hearths throughout the site varied in ash and charcoal, colour and size, and contained a wealth of flint and organic remains.
The changes may represent different activity areas, although not all the hearths were contemporary (ibid).

More permanent dwellings were uncovered at the site of Lepenski Vir in the Danube Valley, Yugoslavia.
These riverside huts, occupied by sedentary hunter-gatherers between 7750 and 6250 BP, were trapezoidal in shape, the wide end towards the river, and varied in size from 5 to 30 metres square (Mithen, 1994).

Limestone plaster floors were surrounded by postholes indicating a probable wooden structure and large sub-rectangular hearths were lined with limestone blocks (ibid).
Sandstone sculptures resembling half man/half fish images, found within the huts, may have had a ritualistic purpose, possibly connected with fishing practices.

The evidence for these settlements, amongst other remains, is the surviving architecture.
At the proposed temporary hunting camp of Star Carr (Legge and Rowley-Conwy, 1988), limited water logging has allowed only the lower levels of the platform to be preserved, rendering interpretation of the structures impossible.

A slight improvement of preservation at Ohalo II resulted in the organic remains of the huts to be preserved.
The location of this waterside site, suggests occupation may have been for longer periods through the year, with easy access to alternative resources, both aquatic and terrestrial.

Lepenski Vir appears a more permanent settlement, as great effort had been taken to make plaster floors with sunken hearths.
Again, this riverside position would provide access to a number of resources.

Through the time and effort invested in the construction of hunter-gatherer settlements, archaeologists are able to establish whether the site was permanently occupied or a transitory camp.
The surrounding resources probably dictated the location; careful planning would ease access to a number of environments, allowing a varied, multi-seasonal diet.

OSTEOLOGICAL ANALYSIS
The age and season of death of an animal can indicate the season of occupation, species specialisation and even hunting strategies.

Mammalian teeth display growth zones and wear patterns, which can indicate the age and season of death.
Through the examination of teeth eruption and wear, or through the examination of individual bones, the age and diversity of species present on site can indicate whether animals were hunted individually or in groups.

An attritional pattern, with an elevated number of individuals in the youngest and oldest classes, may reflect the relative ease in hunting the inexperienced or infirm (Pike-Tay and Bricker, 1993).
A catastrophic pattern, displaying the highest number of individuals in the youngest category reducing down to the oldest, may indicate a drive style hunting strategy or a natural death resulting from a catastrophic event (ibid).

A majority of the osteological evidence from Star Carr suggests a spring to mid-summer occupation of the site.
This conclusion was reached by the examination of roe deer and red deer tooth eruption, in comparison to modern animals, identifying the season or even month of death.

A juvenile red deer maxilla contained milk teeth with wear indicative of a five-month old specimen, a probable spring birth would suggest a summer kill (Legge and Rowley-Conwy, 1988).
However, one elk mandible resembled the stage reached by most animals in the October of their second year, although this would be dependent on the date of birth (ibid).

If the elk had been born in the spring, death during October would cast doubt on the season of occupation.
The presence of absence of antlers on the skull also identifies the season of death but does not necessarily identify the season of occupation.

The skulls of red deer with shed antlers indicate death occurred in spring, around April or May (Legge and Rowley-Conwy, 1988), possibly suggesting the period of occupation.
In conflict with this evidence was the discovery of red deer frontlets with the antlers still attached.

Assuming modern red deer display the same habits as those present during the Mesolithic, the animals were killed between mid-October and late April (Thomas, 1979).
This conflicting evidence may be explained by the importation of raw material, in the form of antler, for later processing for worked bone tools, and the frontlets are commonly interpreted as ritual objects.

There is a suggestion Star Carr was a hunting camp from which the meat was transported to the base camp (Legge and Rowley-Conwy, 1988).
This proposal could be acceptable, although the absence of pelvis and spine from the site would indicate butchery occurred elsewhere, possibly at the kill site (Darvill, 1987).

Through the study of the faunal assemblage, Star Carr is currently accepted as having been occupied during spring and early summer with a possible seasonal migration to a coastal site (Legge and Rowley-Conwy, 1988).
Storage of resources can distort the interpretation of seasonal occupation.

When hunters were on expedition, the likelihood is stored food would have been transported from the base camp or collected from stores in the landscape.
Depending on when the vegetal food was gathered or the animal killed, the food refuse may indicate a particular season, when the hunters occupied the site at a different period of the year (Champion et al, 1984).

Osteological evidence can reveal the season of occupation and the purpose of the site; however, re-evaluation can uncover conflicting evidence.
Changes in the habit of animals should also be considered.

Although modern red deer carry their antlers through the winter into the spring, ancient animals may have displayed different patterns.
A similar problem in changing habits may arise in the study of tooth eruption.

Altered nutrition may cause modern animals to erupt their teeth at an earlier age, therefore casting doubt on the ages of animals within the archaeological record (Rowley-Conwy, 1993).
Although studies of modern animals in comparison to medieval individuals suggest this is not the case, tooth eruption, as a seasonal indicator, may be problematic (ibid).

Species representation within the faunal assemblage may identify preference in the diet or the abundance of a particular animal.
At the Final Magdalenian site of Limeuil, south-west France, reindeer are the dominant species in the faunal assemblage, with horse as the second preference (Boyle, 1993).

Reindeer were an important part of the subsistence strategy and there is ample butchery evidence in the form of recognisable cut marks (ibid).
Reindeer remains, although the primary taxa, display less intensive processing than the secondary taxa, including less cut marks or fragmentation.

Of the reindeer remains, only 5.63% display evidence of processing in comparison to 34.71% of non-reindeer (ibid).
The greater frequency of butchery marks on the secondary species may indicate increased processing for greater yields, possibly to warrant the effort of hunting a less abundant animal.

The species representation in the archaeological record of Ohalo II indicates a diverse diet, and, when examined in conjunction with the presence of characteristic tools, suggests a Kebaran culture.
Although at 1500 square metres, Ohalo II is much larger than the average 200 square metres of most Kebaran sites (Henry, 1985), other characteristics reaffirm the identity.

Artefacts are few, restricted to beads of the Mediterranean shell dentalium, and flint and bone tools.
The diet, however, is varied as the faunal assemblage contained tortoise, birds, hare, fox, gazelle and deer (Kislev et al, 1992).

Thousands of fish bones were also recovered, although the pattern of deposition was uneven, common in places, scarce in others.
Fish, a multi-seasonal resource, is highly nutritious and obviously of great importance to the site economy (ibid).

Osteological analysis can identify many aspects of hunter-gather subsistence.
A dominant species in the diet may indicate preference, however, it may also suggest abundance.

The age profile of the faunal assemblage can signify the hunting method adopted by the population, whether they hunted individual animals or employed a drive strategy.
However, osteological analysis as a seasonal indicator is fraught with problems; the importation or storage of resources can distort the result.

POLLEN, MACROFOSSILS AND VEGETAL PROCESSING
As the climate warmed at the end of the Pleistocene, around 10,000 BP, the diversity of plants in the landscape increased and spread to areas previously too cold for colonisation.

The prehistoric hunter-gatherers were able to collect an increasing variety of grasses, fruits and nuts over a wider area, eventually developing storage techniques to preserve the excess for scarcer periods.
The observation of the growth patterns of wild grasses would later provide knowledge invaluable during early agriculture (Henry, 1985).

During the British Mesolithic, two phases of occupation are identified at Star Carr as indicated by the charcoal found during pollen analysis.
The first phase coincided with the construction of the platform, around 9488 BP, lasting 70 years and the second phase lasted 120 years but falls into a period of anomalies within the pollen record.

The occupants of Star Carr were extremely restricted in their vegetal diet.
The local woodland and waterside allowed the collection of bog bean, fat hen and nettle, all of which were represented in the pollen record of the site, but there was no evidence any were actually eaten.

Settlements in the Near East fared better in the plant community.
The warmer climate allowed the growth of various fruits and nuts not available in the cooler west.

In the spring, the forest surrounding the site of Ohalo II produced edible grasses, including wild varieties of barley and emmer wheat (Kislev et al, 1992).
In the autumn, the inhabitants were able to gather fruits and nuts including almond, olive, pistachio and grapes (ibid).

Evidence of all of the listed varieties was found on site, however, although they would have grown in abundance, no evidence was recovered to suggest the harvest was stored in vessels or pits (ibid) and there has been no discussion of plant processing technology.
The intensive collection, processing and storage of cereals, indicates the presence of a complex hunter-gatherer society.

The organisation of the harvest, followed by intensive processing, required man-management (Henry, 1985), probably requiring delegation by a person of status.
This exploitation of storable resources including wild cereals and nuts by the Natufians, may have led to a sedentary lifestyle (ibid).

The Natufians possessed processing equipment including querns, pestles and mortars and storage pits, and the abundance of cereals and other plants led to the expansion of the culture towards the highland Negev (ibid).
The warming climate allowed the spread of edible plant species, providing the hunter-gatherers with a predictable food source.

Not only did plants vary the diet and introduce vital nutrients, they provided a resource suitable for storage.
The value of plant evidence within the archaeological record is immeasurable.

Radiocarbon dating can provide a date for the site, and the diversity of plants will assist in the reconstruction of the environment and diet.
TECHNOLOGY

The technology available to the prehistoric hunter-gatherers was in the earlier periods limited, and is identified by, and in most cases restricted to, lithic assemeblages.
Mobile hunter-gatherers would carry only simple and portable technology and complex groups would store cumbersome objects at the base camp (Rowley-Conwy, 1983).

Although changing technology and the introduction of new tools can date a site, the possibility of trade must be considered.
Exploitation of a migratory species can result in a high kill rate, providing enough food for immediate consumption and also for storage, possibly required during sparse periods (ibid).

Rowley-Conwy (1983) suggests non-migratory species may be utilized during these sparse times to bulk out the stored resources.
During the Palaeolithic, a period known as the Early Gravettian, around 30,000 - 28,000 BP, is characterised by Gravette points and microgravettes, both of which have been recovered form the site providing the name for the period, La Gravette, France (Pike-Tay and Bricker, 1993).

There was a dominance of maintainable lithic tools and organic technology was rare (ibid), however, the environment may not have been favourable for the preservation of organic material.
Star Carr produced a lithic assemblage consisting of many thousands of microliths and flint tools, dominated by burins and scrapers (Legge and Rowley-Conwy, 1988).

A wealth of organic technology was recovered from the waterlogged conditions, including barbed antler points in such a number they may represent a cache for later use (ibid).
A change in the characteristics of the antler points may represent at least two phases of occupation (ibid), which, if radiocarbon dates tally, may coincide with the phases indicated by the charcoal layers.

In addition to rolls of birch bark, possibly used as containers, and heavy elk antler mattocks, a partially decayed wooden object was recovered.
The purpose of the artefact has not been identified, but suggestions include a digging tool or a paddle, although no evidence of a boat survives.

As a multi-seasonal food, fish have always been an important resource.
Shallow water fishing probably produced wrass, ling and saithe, the latter representing 95% of the fish bone evidence recovered from the shellfish middens of Oronsay, Argyll (Renfrew and Bahn, 1996).

Examination of fish otoliths can estimate the size and age of a fish at death, and growth rings can reveal the season.
Cod remains from Morton on Tay, Fifeshire, occupied between 7950 and 5950 BP, reveal the fish measured in excess of one metre in length and must have been caught in deep water, presumably from a boat.

The ErtebØlle site of Tybrind Vig, Denmark, is a Mesolithic settlement producing a wealth of fishing evidence.
Two canoes and decorated paddles were used to row out on to the water.

Large stones were used as ballast, a small fire, associated with eel fishing, burned (Mithen, 1994) and bone hooks and leisters, double fishing prongs, were used to catch small cod, spurdon, seals and porpoises (Whittle, 1996).
A Mesolithic fish spear was recovered from peat dredged off the Leman and Ower banks, 30km off shore in the North Sea.

The spear was associated with pollen dating to a reduction of pine in favour of trees that preferred warmer conditions, around 8950 BP (Flemming and Masters, 1983).
Fishing strategies, like hunting, can alter depending on the environment or the season.

The Finnish site of Askola produced an informative pattern of seasonal food.
Within the faunal assemblage, the remains of ringed seal were recovered and analysis disclosed the means by which they were caught.

From the autumn until the spring, the seals were hunted using harpoons or clubs, but as the year progressed into the summer months, the preferred method was harpoon or spear, and as autumn approached the seals were caught using nets (Matiskainen, 1990).
The reason for the changing strategy has not been explained but may result form changing weather patterns or animal behaviour.

The multi-seasonal availability of fish and other aquatic species provided much needed nutrition during sparse periods.
The introduction of boats and fishing technology enabled the fisher-hunter-gatherers to venture out onto deeper water and even on to the islands.

Organic preservation within the technological record allows radiocarbon dating, or pollen analysis can identify the period by the changing environment.
The appearance of new technology, or the improvement of existing tools, provides a dating tool for other sites and allows the archaeologist to reconstruct the activities on site.

CONCLUSION
The evidence for prehistoric hunter-gatherer subsistence is varied in type and reliability.

Osteological evidence may indicate seasonal occupation and species availability but changing habits of the animals must be considered and may result in re-evaluation.
The changing climate at the end of the Pleistocene encouraged colonisation by grasses, fruits and nuts into areas previously unsuitable for the different vegetation.

The annual collection of wild grasses by the hunter-gatherers exposed them to the cereal growth patterns, which would prove invaluable during early agriculture.
Technological breakthroughs introduced an increasing and diverse marine diet, possibly to supplement the sparser winter periods.

Organic evidence, dependent on preservation, can provide a radiocarbon date but the transportation or trade of resources can distort the evidence.
Although there is a wealth of evidence for the subsistence of prehistoric hunter-gatherers, with the development of new theories and scientific methods, re-evaluation is inevitable and necessary.

INTRODUCTION
This essay will approach the subject of the climate of the period ranging 200kyr - 30kyr BP, the methods in which the climate is reconstructed and the possible adaptation of Neanderthals as considered by archaeologists and palaeoanthropologists.

Although this adaptation would probably have included subsistence strategies, this paper will focus on the possibility of skeletal morphology.
Neanderthal anatomy and the reason or purpose for such unusual characteristics has been the subject of debate since the initial find in the Neander Valley in Germany in 1856.

Original propositions included the possibility of deformity but quickly altered to climatic adaptation through skeletal morphology.
Since the first discussions in the late 19 th century, many scholars have reconsidered the adaptation hypothesis; some of these proposals will be summarised and a personal interpretation proffered.

CLIMATIC RECONSTRUCTION
To gain an understanding of possible morphological changes in the anatomy of Neanderthals in response to the environment, one must first be aware of the changing climate over the period of their existence.

The climate of the Neanderthal age can be reconstructed through several methods; however, two remain more accurate and widely used.
Although oxygen isotopes 16,17 and 18 were originally regarded as a record of the water temperature due to the elevation of the lighter isotopes into the ice sheets, they are now generally agreed as indicating how much water is locked up in the glaciers (Aitken 1990).

By examining the isotopes, principally the heavier O18, with a high-precision mass spectrometer, it is possible to identify the concentration of this isotope in the ocean, reflecting the global climate at that time (ibid).
These isotopes can be found in both deep-sea cores and ice cores, both of which are highly studied and provide a continuous sequence of climate record, spanning hundreds of thousands of years.

Deep-sea cores are a form of absolute dating in that the substance itself can be dated rather than an associated object.
The core consists of terrigeneous sediments and organic matter, accumulated at a rate of around a few millimetres a century (Aitken 1990), therefore a core of ten metres in length, as is the norm, will cover a period of several thousand years.

The terrigeneous sediments, formed by the erosive action of tides and currents, possess a weak magnetic field, which can be analysed to identify magnetic pole reversals (ibid).
These reversals take place over a period of decades but occur only every few hundred thousand or million years, so can be used as a dating technique, although it must be used in conjunction with other methods (Lewin 1999).

Ice cores are drilled through the remaining glaciers of the Arctic or Antarctic though concentration is currently placed on the Greenland ice cap.
Information is provided on the annual climate displayed as layers built up each year with the addition of new ice.

These cores can be thousands of metres deep and the Vostok core from East Antarctica was the first to provide data spanning thousands of years; now a core from Summit in Greenland is used for the same purpose (GRIP 1993).
Through the combined oxygen isotope analysis of deep-sea cores and ice cores, a continuous sequence has been produced (fig 1).

Concentrating on Neanderthal occupation for the purpose of this essay, the Late Saale, described by Dansgaard (et al 1993), falls into the earliest period, oxygen isotope sequence 6 (OIS 6).
The environment was described as less cold and variable than the Late Weichsel, spanning OIS 3 and OIS 4, with a few spells of extreme warmth in the Early Saale (ibid), OIS 6 and OIS 7a, the period prior to the focus of this essay.

Oxygen isotope sequence 6, approximately 185kyr BP - 130kyr BP, was a very cold, open landscape.
Van Andel and Tzedakis describe the corridor between the Alpine and Scandinavian ice caps as incorporating elements of both tundra and steppe environments, including grasses and sedges amongst other vegetation (1996).

Around 132kyr BP the isotope-temperature relationship indicates a 10º maximum reduction in temperature.
This occurrence, described as Event 2, lasted approximately 8kyr, with the entire atmospheric shift completing within 30 years (GRIP 1993).

OIS 5e, the last interglacial, known as the Eemian, dates to 130kyr BP - 118kyr BP.
Lasting only 10-15 years, this sequence included five sub-stages of varying ice volume, resulting in relatively short periods of mid-glacial temperatures (Gamble 1999).

Solar radiation led to a temperature increase of 10-15º, resulting in temperatures slightly higher than today, allowing the spread of dense deciduous forest and a retreat of the ice sheets, roughly to their present position (ibid).
The following early glacial period, spanning 118kyr BP - 75kyr BP, fluctuates in climate with glacial and interstadial phases.

To this end, the sequence has been split into sub-divisions.
Two interstadials identified as peat horizons in a lake core from Ascherslebener See, Germany, known as the Königsaue profile, fall in the fifth sequence.

These interstadials are separated by fossil ice wedge casts identifying permafrost conditions found between horizons.
ISO 5d and 5b are identified as possessing large ice volumes with summers in northern Europe 5-10º cooler than today (Gamble 1999).

The horizons were separated by ISO 5c and 5a, longer interstadials of a forested, temperate climate.
Around 115kyr BP, the temperature decreased by a maximum of 14º as indicated by the isotope analysis; Event 1 as it is known lasted approximately 70 years.

The dust signal suggests a shift in atmospheric circulations completed in only 10 years (GRIP 1993).
Around 75kya BP, the Toba eruption in Indonesia may have resulted in a volcanic winter, possibly triggering the early onset of the next sequence, the Pleniglacial (Gamble 1999).

Eruptions as powerful as Toba can be identified in the ice cores due to the energy of the volcano expelling ash into the upper atmosphere, allowing the transportation of particles around the globe, eventually settling in the cooler climate of the ice caps.
The Pleniglacial, OIS 4, 75kyr BP - 60kyr BP, was a full glacial leading to the rapid expansion of glaciers (Gamble 1999).

The Königsaue profile shows increased ice wedge formation during this sequence without any significant interstadials.
Deep-sea cores show higher ice volumes resulting in lower sea levels, with no major continental glaciation (ibid).

The Monticchio core of southern Italy produced a variable record of tree, shrub and vine pollen to herb pollen.
This herb pollen identifies a colder open landscape with little tree cover in northern Europe (ibid).

This evidence is invaluable as the pollen records of this period are often blank due to the types of vegetation and the arctic-style peat resulting in poor preservation (van Andel & Tzedakis 1996).
The final period of the Neanderthal occupation, OIS3, known as the Interpleniglacial, dates to approximately 60kya BP - 28kya BP, the period of focus for this paper.

The Summit ice core from central Greenland identified five interstadials lasting 2-4kyrs each (Dansgaard et al 1993).
Comparison with the Monticchio pollen profile provided revised dates for the expulsion of icebergs into the North Atlantic between 64.2kyr and 14.5kyr BP.

This led to a significant effect on the climate of Europe corresponding to a high ratio of herb pollen, again indicating cold, open conditions, with extensive woodland around the source lake of the Monticchio core at 50-42.5kyr and 40.7-37.6kyr BP (Gamble 1999).
These results produced a picture of a relatively mild climate with reduced glaciers and recent work suggests an increase in temperature of 5-8º over a period of five years - this would have effected the vegetation within the landscape.

The oxygen isotope analysis of both the ice and deep-sea cores reconstructs a fluctuating climate with extremes of temperature.
The vegetation within the landscape would have altered depending on the ambient temperature and aridity.

To this end, the hominids occupying the region must have adapted to survive.
Whether this adaptation was physical or based on subsistence is a subject of much debate, however, this essay will focus on the skeletal evidence.

PAST HYPOTHESES OF SKELETAL ADAPTATION
The anatomical features of the Neanderthals have long been the subject of debate.

The possibility of skeletal morphology in response to an arctic environment has been proposed and continues to be a focus for much attention.
Many anthropologists have published papers and proposed arguments regarding skeletal adaptation and this section will chronologically summarise these arguments.

In 1927 the Bohemian physician and physical anthropologist Aleš Hrdlička was offered the Huxley medal, the highest British award for anthropology.
During his acceptance speech, Hrdlička proffered his opinion on the evolution of Neanderthals into human beings (Trinkaus & Shipman 1993).

Although the hypothesis of Hrdlička on Neanderthal evolution was at the time dismissed (ibid) and has now been disproved, he spoke of his results following examination of Neanderthal occupation and how the number of open-air sites had decreased in favour of cave positions.
This, he thought, was evidence of a worsening climate, a thesis supported by the Croatian palaeontologist Dragutin Gorganovic-Kramberger (ibid).

Hrdlička proposed the unusual Neanderthal features were also representative of the changing climate and the various specimens from around the world fell into alternative categories.
The original Neander Valley specimen from Germany, the fossils from Spy (Belgium), Le Moustier (France) and La Chapelle-aux-Saints (France) were described as a robust, primitive species, opposed to the more modern-looking examples from Krapina (Croatia), La Ferrassie (France) and La Quina (France).

He suggested this variable population was as a result of exposure to glacial conditions and subsistence in such environment would preserve only those suitably evolved to survive (ibid).
A new understanding of evolutionary processes prompted F. Clark Howell to begin work in 1951 in an attempt to explain Neanderthal morphology as a result of genetic isolation in addition to glacial adaptation (Trinkaus & Shipman 1993).

He identified the large-browed, stocky Neanderthal as synonymous with western Europe during the early part of the last glacial period.
Although there was some concurrence, there was also difference of opinion with division of sites to alternative types of Neanderthals as proposed by Hrdlička, in addition to fossils found in the intervening period.

Howell associated fossils including La Chapelle-aux-Saints, La Ferrassie, Le Moustier, Spy, Monte Circeo (Italy) and Feldhofer (the Neander fossil) with animals adapted to cold environments (ibid).
Those of Krapina, Saccopastore (Italy), Tabun (Israel) and Teshik-Tash (Uzbekistan) he concluded, dated to an earlier warmer interglacial period, preceding the last glaciation and suggested the shorter skulls would have possessed smaller brains and faces, and the limbs appeared less robust but also straighter than their later counterparts (ibid).

Howell emphasized the difficulty of survival in glacial areas without proper clothing and equipment and a scarcity of resources.
He speculated the isolation imposed by glacial barriers would have created evolutionary developments or restricted the gene pool (Trinkaus & Shipman 1993), resulting in a concentration of unusual characteristics.

Howell recognized an abrupt end to the trend that coincided with the end of the early glacial period (ibid), possibly following a reduction in the glacial barrier.
He observed the early Neanderthals of western Asia and western Europe possessed less pronounced features, and a warm period during the Würm glaciation would have removed the geographical barrier allowing more modern looking humans to spread from the east to the west.

Carleton Coon, an American physical anthropologist and expert on race and racial differences, compared man to non-human animals and produced evidence of man's adaptation to the surrounding environment (Trinkaus & Shipman 1993).
Coon had examined living races and the fossil record looking for such adaptation and had reverted to Bergmann and Allen's rule regarding heat conservation - a generally larger body mass with shorter limbs than similar species from warmer climes (ibid).

Eskimos, native to arctic conditions, for example, will have shorter necks and extremities and possess a heavier build; the Maasai, however, living in much warmer climes, are taller, of slighter build with long limbs (ibid).
Coon saw an adaptation to cold in the Neanderthal fossil record, a compact form similar to the Eskimos (ibid), a hypothesis supported by Stringer and Gamble in the same year.

He recognised the Neanderthal nose as a dominant but important feature possessing a large nasal aperture, a prominent bridge, producing a pronounced mid-face.
This has become known as the 'radiator theory' following the explanation of Coon for the need of "warming and moistening of inhaled air" (Trinkaus & Shipman 1993, 317).

He continued remarking on the proximity of the nasal passages to the blood vessels and the importance of maintaining the brains temperature in a society lacking equipment suitable for glacial conditions (Trinkaus & Shipman 1993).
Although this was seen as a step forward in the search for explanation of Neanderthal morphology, during the 1950's Sherwood L. Washburn, another American physical anthropologist, declared the radiator theory partially incorrect "but a credible and useful first attempt at understanding Neanderthal anatomy as a function of an environmental adaptation" (Trinkaus & Shipman 1993, 321).

An alternative hypothesis regarding the forward thrust of the Neanderthal face was proposed by C. Loring Brace IV, an iconoclastic physical anthropologist.
In contrast to the belief of others, Brace suggested the dimensions of Neanderthal faces and teeth were as a response to tasks performed using the mouth as an extra hand (Trinkaus & Shipman 1993).

The later reduction in the features was possibly as a result of the introduction of specialized tools - the decline in use led, over time, to less powerful facial muscles and lighter skeletal structure (ibid).
Approximately a decade later, Milford Wolpoff, a modern American physical anthropologist continued the belief of Brace in his study of prehistoric stone tools, in conjunction with an American archaeologist, David Brose.

Wolpoff believed the specialized tools Brace hypothesised, would date to the Middle Palaeolithic, as tools of this period were of a more specific function than previous periods (ibid).
This would date the tools somewhere between 250,000 - 35,000 years ago, coinciding with the age of the Neanderthals.

The 'current' view of Trinkaus and Shipman in their 1993 publication was Neanderthal characteristics were indeed evidence of a harsh environment: that their compact bodies, limbs and digits were adapted to conserve heat in near-arctic conditions; their large noses had evolved to prevent moisture evaporation in the cold, dry conditions but would also allow heat dispersal during exercise.
They identified the fact that although the Neanderthals possessed the ability to clothe themselves, find shelter and create fire, their subsistence strategies were unequal to that of later humans (Trinkaus & Shipman 1993).

DISCUSSION AND COMPARISON
During the process of research for this paper, it has been observed a majority of Neanderthal fossils derived from Europe and few from the Near, Middle and Far East.

A comparison was proposed of contemporary fossils from both the east and the west, in the attempt to prove or disprove the climatic adaptation hypothesis, however, difficulty arose in identifying two fossils of the same date.
The original fossil from the Neander Valley, dating to 40-50kyrs, was chosen as the basis of comparison, due to some 'classic' Neanderthal skeletal traits.

As Tattersall suggests faunal evidence proves the Neander specimen and others including Kebara are contemporaneous (1999), the Kebara 2 fossil, found on Mount Carmel, Israel, was introduced as the Eastern example.
The Neander Valley specimen (fig 2) includes bones representing almost the entire skeleton.

A long, low upper cranium displayed the prominent brow ridges of a 'classic' Neanderthal, however, the lower cranium and mandible were missing.
The robust skeleton possessed bowed long bones with large joint surfaces and well developed muscle attachments (Johanson & Edgar 1996), possibly relating to a strenuous lifestyle (Tattersall 1999).

The Kebara 2 fossil (fig 3), missing the cranium, right leg and lower left leg, was found in association with tools of the Levallois technique and burnt flint, allowing dating using thermoluminescence (Johanson & Edgar 1996).
There is contradiction between Johanson & Edgar (1996) as they report this individual was less robust than other examples from the Near and Middle East, opposed to Tattersall (1999), when he suggests Kebara 2 is the heaviest built Neanderthal fossil.

At 1.7m, Kebara 2 is certainly one of the tallest Neanderthal fossils, taller than the average European Neanderthal (Johanson & Edgar 1996); the 'classic' retromolar gap was, in the absence, evidence of a forward movement of the mid face; and the chin was minimal.
Unusually, the hyoid bone, used for muscle control required for speech, was recovered, leading to further discussion on the communication skill of Neanderthals.

Both fossils display a common skeletal trait of the 'classic' Neanderthal: the heavy bones.
At this point, partially due to the absence of many bones, the similarity diminishes.

The Kebara male was tall, the Neander specimen shorter; the Eastern fossil long bones were straighter than the bowed examples of the German individual, who also possessed larger joint surfaces; and observation of the long bones identifies the Neander example to be heavier, although both are more robust than those of an anatomically modern human.
Evidence suggests that both the fossils of Neander and Kebara were adapted to the cold.

The German fossil, in such close proximity to the glacier (fig 4), had developed a step further than the Israeli example, from the relatively warmer climate of the Middle East, reflected in his increased height.
Due to the absence of the mid-face of both fossils, this paper has not discussed the possibility of the 'radiator theory' as proposed by Coon, however, it is considered a plausible explanation and should not be discounted.

Gamble observed the fluctuations in climate during the last interglacial (OIS 5e) occurred within a single lifetime, suggesting hominid adaptation was less likely during the period 300kyr-60kyr BP (1999).
During this particular sequence the climatic oscillations were relatively short but when compared with considerably longer periods during other sequences, the possibility should be reconsidered.

CONCLUSION
The frequent oscillations in the oxygen isotope analysis of the Neanderthal period show a fluctuating climate of one extreme to another; temperatures fell dramatically over a fairly short period, and then rose to exceed the current climate.

Neanderthal communities were likely to have adapted their subsistence strategies to survive, however, examination of their fossil remains provide evidence that nature intervened.
The increased robusticity of the bones is evident in both the Eastern and Western fossils, although the taller stature of the Kebara remains, reflect a slightly warmer climate in the Middle East.

In conclusion, it is my belief that the Neanderthals did indeed adapt skeletally to survive a harsh climate, however, improved subsistence strategies must also have been implemented.
Unfortunately, neither secured their future.

INTRODUCTION
As Europe entered the Neolithic, a megalithic culture emerged.

From c.4800 BC to c.2300 BC calendar years (Renfrew, 1976) monuments incorporating large stones were constructed across the landscape from Southern Scandinavia in the north to the Mediterranean in the south (Chapman, 1981).
Many of these structures were utilized as burial mounds; however, this may not have been their sole or primary function.

Since a nineteenth century survey assisted in the recognition of similarities in form (ibid), the interpretation of these monuments has varied from the symbolic display of power to the ritualistic meaning.
This essay will detail the main approaches, their strengths and weaknesses, and will provide case studies from across Europe.

SHARED BELIEFS OR CULTURES
The study in search of shared beliefs or cultures within the megalithic population, examines evidence for similarities in the form of monuments and tracing the migration of people across the landscape.

There has been past discussion on whether local variants should be studied or only the original form, however, restricting the study may overlook vital factors.
The migration of people across the European landscape, possibly in search of land suitable for agriculture, led to the spread of ideas and beliefs, possibly including the introduction of megalithic monuments.

A subsistence pattern based on agriculture will rely on dedication to the land over the generations, involving the clearing of the landscape, preparation of the ground, sowing of crops followed by harvesting and storage, resulting in the continued success of the community.
This reliance on previous generations could be the foundation of ancestor worship.

The complicated treatment of the dead and the monuments constructed to them have been connected with ancestry (Whittle, 1996), however, a lack of visible monumentalism should not indicate the lack of ancestral ties as the community may have employed domestic shrines (Chapman, 1995).
Alasdair Whittle suggests the large passage graves of Co.

Meath, Ireland, may have been erected as shrines to the dead ancestors and "were the central point of a cult focused on the fusion of past and present" (Whittle, 1996: 248).
Apparently preceded and surrounded by approximately 17 smaller passage graves, Knowth, one of the three large passage graves, was built around 3000 BC (Whittle, 1996).

These three monuments are an element of a cluster of 40 structures, in an area of only 3.5 by one kilometre.
The earlier monuments of Knowth were raised around an open area, possibly already a sacred place, with their passage entrances facing the clearing.

This area appears to have been the location of early activity, having produced artefacts and the remains of structures.
The chambers of the tombs also contained artefacts together with cremated human bone.

The graves may represent the individual or a lineage; the later construction of the larger mound may signify the introduction of a form of control over labour and ritual (ibid).
Although studies may be focused on the model form of megalithic monuments, it should be realised there will be inevitable variation from one example to another.

When examining a possible territorial model, Renfrew suggested a particular set of Atlantic conditions could have prompted the construction of these monuments on the north-west coast of Europe (Renfrew, 1976).
This would account for the clusters of monuments along the coastal regions (ibid), however, local factors may dictate the form of the monument and as the culture diffused, local adaptations would occur.

Diffusionism of a belief will create distorted reflections of a single culture amongst the settlements of adoption.
The local communities may incorporate new ideas but dilution by pre-existing beliefs would likely have created an individual form of the original.

Although Glynn Daniels suggests this approach should be restricted to the study of the individual form, examination of the variants could reveal aspects of the local community.
Evolution is an important part of archaeology and affects the development of both the living and the structures they produce.

To ignore these variants could deprive archaeologists of data.
THE SEARCH FOR ORIGINS

In his work of 1976, Colin Renfrew suggested the European megalithic monuments were linked by a single process and could be traced back to a single area of origin.
There was a proposal that only early Minoan Crete, the early Bronze Age Cycladic Islands or Bronze Age Western Anatolia, could claim a strong argument as the origin of these structures (Renfrew, 1976).

However, the passage grave of Knowth has been radiocarbon dated to 3000 BC (Whittle, 1996) and Renfrew himself states the early Minoan round houses, of which the examples of Mescra were thought to be the inspiration for communal burial, cannot be dated prior to 2800 BC (Renfrew, 1976), disproving this hypothesis.
From this area of origin, it is believed possible to trace the diffusion of the original idea through the changing forms of the various megalithic monuments (Chapman, 1981).

The Bandkeramik long-houses are said to have influenced the form of the unchambered long-mounds, mimicking the shape, orientation and internal design.
These structures were adopted by the epi-Bandkeramik settlements as the central point of their dispersed communities, and later developed into chambered tombs (Bradley, 1998; Sherratt, 1995).

There is further suggestion the construction of these monuments is contemporary with the colonisation of prime agricultural land (Sherratt, 1995).
Liden examined the agricultural hypothesis by conducting a diet study of human remains from two Swedish passage graves, searching for evidence of a change in diet from a hunter-gatherer subsistence to one based on agriculture (Liden, 1995).

A chemical analysis of stable carbon and nitrogen isotopes revealed the dietary patterns of the populations of an inland site at Rössberga, Västergötland, and a coastal site, Resmo, Öland.
Over time, a lowering nitrogen value would indicate an increase in plant protein in the diet; however, this was not evident at the two sites under examination.

Results indicated during the Mesolithic, the Resmo diet was high in terrestrial protein, specifically sheep or goat and cattle, with a minor contribution, 20%, by marine animals, increasing to 80-90% during the Neolithic.
The population of Rössberga, however, relied entirely on a terrestrial based diet, with no marine input, suggesting a pastoral based economy.

Liden concluded the introduction of megaliths coincided with a change in diet towards agricultural and pastoralist products, although, he suggests, cereals may have been a luxury or ritual item in the form of bread or beverages (ibid).
The spread of megalithic monuments may be due to the migration of Neolithic farmers in north-west Europe.

There is a change in diet, albeit not the agricultural change expected by Liden, and there are a cluster of monuments along the Atlantic fringe and North Sea coast, reaching no further than 300km inland onto the European mainland (Renfrew, 1976).
However, the hypothesis of a single Mediterranean origin is questionable as some dates of Irish structures undermine Minoan sites as the basis of the megalithic monuments.

Renfrew does not discount the possibility of a single origin but he advises it should no longer be assumed (Renfrew, 1976).
SOCIAL INFERENCE

Through the examination of the European megaliths, social inference can be drawn.
The size of some of the monuments is testament not only to their individual importance but also the manpower involved in their construction.

The size of the monument could be interpreted as the representation of status competition or social relationships, and segregation within burial chambers can reveal the acceptance of children as part of the community.
It may be assumed a great deal of organisation would be required during the building process and labour may have been gathered from outside of the community.

Swedish megalithic monuments were unlikely constructed by 'band societies' due to the nature of this highly mobile society (Sjogren, 1986).
'Big Man' societies, however, through alliances and exchange policies would be able to summon adequate manpower required for such a project.

If the labour was not volunteered, 'Big Man' would possess sufficient authority to take by force what he needed, in the form of tributes or labour, possibly resulting in a slave society (ibid).
An increase in available labour would allow the construction of larger monuments, using larger stones, demonstrating the importance of the community in the form of status competition (ibid).

Sjogren analysed the correlation between chamber and roof slab size, requiring greater man-power to manoeuvre larger blocks of stone, and produced a possible interpretation of building principles as larger tombs possessed smaller, but more numerous, roof slabs (Sjogren, 1986).
He deduced, when examining larger chambers, roof slabs are not a good indicator of the labour force involved (ibid).

A high correlation was found between the mound and chamber size, however, and Sjogren concluded the labour force of the middle Neolithic societies of western Sweden were gathered through descent lines (ibid).
The greater necessity of labour during construction of these large monuments is generally accepted as having been pooled from the surrounding populations.

This labour force would have been required to retrieve materials from source as well as erect them.
During the construction of the passage grave of Knowth, a tremendous volume of earth was moved for the construction of the mound, and river stones were transported from the Boyne (Whittle, 1996).

Generally most megalithic blocks of stone appear natural in shape and more effort obviously went into transportation than the processing of the material, symbolizing the community effort involved in the construction (Sherratt, 1995); however, the later Neolithic monuments, such as the Ring of Brodgar and the Stones of Stenness on Orkney, were beyond the labour and skills involved in the construction of the earlier tombs (Renfrew, 1976).
Sjogren believes megalithic monuments should be viewed as representing a social or descent group (Sjogren, 1986) and although sedentism is a major factor for developing social complexity, Liden believes megaliths occurred in areas with a pre-existing complexity (Liden, 1995).

Neither considers, in great detail, the possibility of forced labour resulting in a monument to slavery.
It is commonly held that certain cultures, both prehistoric and modern, do not consider children to be part of the community until they have completed their rites of passage.

These beliefs can be continued after death as reflected in their exclusion from some forms of collective burial.
The passage grave of Rössberga contained human skeletal remains in both the passage and the chamber.

Preliminary osteological analysis of samples gathered from the chamber revealed 16 males, 17 females, only 5 sub-adults but no children.
In contrast, the roughly contemporary passage grave of Resmo, 300km to the south-east, produced the remains of approximately 30 individuals, including at least 5 children (Liden, 1995).

The segregation of children appears to be a belief held by the population of Rössberga that is not reflected throughout the region amongst contemporary sites.
Megalithic societies can be partly reconstructed during the examination of their monuments; manpower can be estimated and building practices proposed.

However, the possibility of slavery and segregation within the community are areas worthy of further study.
TERRITORIAL MARKERS

Colin Renfrew first produced the 'territorial model' in 1973 during examination of the distribution pattern of megalithic monuments on Arran, western Scotland, amongst other sites.
Thiessen polygons were used to define roughly equally spaced territories around a single tomb, however, Liden disregards division into territories as these areas are not always easy to identify or of equal size (Liden, 1995).

Examination in this manner can disclose the catchment area for individual territories but the purpose of the monuments remains unclear.
The construction of a megalithic monument as a territorial marker may have roots in different meanings.

There is a possibility those roots belong to ancestry and the tombs may be purposefully located adjacent to the settlement or at the centre of the territory so the ancestors are ever present in the lives of the descendants.
Alternatively, the monuments were possibly to identify the control over local resources.

Megalithic territories of a uniformly organized group commonly include several resources such as water, arable land, pasture, woodland and possibly marine resources (Sjogren, 1986) and a territorial marker might have established ownership rights to the region.
However, due to the location of some tombs on the shore of what was a probable lake during the Neolithic in Västergötland, Sweden, and the dearth of arable land in the surrounding environment, Sjogren rejects the interpretation of megaliths as territorial markers, suggesting this hypothesis is not applicable to west Sweden (ibid).

A possible strain on resources may have triggered a stronger emphasis on territoriality (Renfrew, 1976), resulting in the construction of markers possibly in the attempt to control access to the land.
There is a further possibility of monument use as territorial markers due to the distribution of the community.

A dispersed segmentary society could consider a megalith as the centre of their territory, or of their world (Renfrew, 1976).
It need not necessarily be for burial, it may be the location of feasting or gift exchange (ibid).

The location of Irish passage grave cemeteries across the centre of the country, studied as phenomenology, could possibly be a form of communication between the scattered population (Sherratt, 1995).
It should be considered that when examining megalithic monuments as territorial markers, many examples have either been destroyed or remain undiscovered.

A lack of intensive farming on the island of Arran has preserved a majority of the examples allowing intensive study, knowing the current distribution is representative of the Neolithic pattern (Chapman, 1995).
However, due to the drainage of the peat, it is only now that megalithic monuments and Bronze Age barrows are being revealed in the Fenland of East Anglia (ibid).

The discovery of more tombs may cause a re-consideration of the pattern of distribution and therefore the purpose of these monuments.
RITUAL ACTIVITIES

One of the most highly studied yet vague approaches to megalithic monuments are that of ritual activities.
The vast subject of ritual seems to cover all aspects of life, death, burial and post-burial activity.

Ritual not only involves the disposal of the deceased, it also incorporates ceremonial activities.
The process of inclusion or exclusion from ritual is commonly discussed.

The tightly spaced megaliths forming Stonehenge produced a ritual enclosure not easily visible from the outside.
The surrounding Class I henge, with a single entrance, may have formed a boundary, to keep the public at such a distance they could not hear internal activity, completely excluding a majority of the population from the ritual performed within.

In comparison, the henge of Avebury, Wiltshire, encloses an area of 365 square metres and incorporates four entrances.
The largest internal stone circle, one of three, was constructed around the periphery of the enclosure with wide spaces between the stones.

This open design probably included the public in the rituals conducted within, or at least permitted observation.
The clear difference between the two sites would suggest different ritual activities for separate divisions of the population; the closed site of Stonehenge was possibly utilised by a religious order, the open site of Avebury probably expected a large congregation of people, possibly for trade or ceremonial activity.

The use of megalithic monuments as places for disposal of the dead does not limit their function.
There was no abandonment of the passage grave of Knowth following the onset of decay during the 3 rd millennium BC, as the tradition was continued by the construction of large circular enclosures (Whittle, 1996).

Although the long barrows of Britain were not open for more than a couple of generations, after they were sealed they probably continued use as the location for ritual activity, either religious or feasting, at their impressive facades.
Skeletal analysis of the burials often shows an anomaly in the ratio between the minimum number of individuals and the total bones recovered.

It is not uncommon to find bones missing from the assemblage, although missing hand or foot bones could be explained by defleshing elsewhere after which the small bones were missed during collection, they may have completely decomposed.
However, this does not explain missing skulls or long bones.

The probability of their use as relics during ritual activity must be considered and there is also the possibility of exchange.
A common consideration in the setting of megalithic monuments is the possibility of astronomical alignments.

The construction of a monument with consideration to the heavenly bodies may suggest an annual activity (Sherratt, 1995).
The significance of the sun rising or setting behind a particular stone, or shining down the passage of a grave on a particular day, is mere speculation, but may be connected to rebirth.

Great emphasis is placed on art in the ritual activities of megalithic monuments.
During the Neolithic the importance moved from exterior decoration, such as that of the menhirs, prehistoric monoliths, which accompanied the long barrows of Brittany, to interior art within passage graves, only visible to those permitted within the tombs (Sherratt, 1995).

The presence of decoration on the backs of eleven of the 123 kerbstones surrounding the great mound at Knowth, however, remains unexplained (Whittle, 1996).
Whatever the meaning of this form of art, it would not have been visible, even to those involved in the ritual activities.

Megalithic monuments are often interpreted as sites of ritual activity.
Although some burial sites are only in use for a short period, their use may continue, involving repeated visits over many generations.

Ritual activity covers numerous aspects of life and death; however, much of it is supposition with no hard evidence, and therefore ritual cannot be proved or disproved.
CONCLUSION

The approaches involved in the interpretation of megalithic monuments are varied.
Through the search for shared beliefs or cultures, worship of the ancestors may be indicated by the location of tombs within an agricultural landscape; and similarities in the form of tombs may assist in the trace of migration across the landscape, however, distortions in the model may effect interpretation.

Tracing the form of the tomb across the European continent could also suggest the origin of the megalithic culture and similarities between Neolithic long houses and the long barrows are evident.
Monument size is often accepted as a representation of the manpower involved in the construction of the tomb and therefore an indication of the social complexity; however, the possibility of slave labour should be considered.

Although many accept the 'territorial model', believing the structures to be a marker in the landscape, the discovery of additional monuments may distort the pattern of distribution, resulting in re-evaluation.
The subject of 'ritual activities' covers numerous areas of life, death and burial and although attracts much interpretation, still results in supposition.

The presence of these monuments in the landscape has interested scholars since the nineteenth century and has resulted in many different approaches in their study.
Each approach has merits, however, a blinkered approach will only lead to restricted interpretation.

The variations in form should be considered to gain a greater understanding in the migration process across Europe and the adaptations made during diffusion of the megalithic culture.
Introduction

The purpose of this paper is to describe the process of molluscan analysis of samples recovered following coring and preparation of soil samples of Ufton Green, Berkshire.
After the production of a histogram incorporating all of the samples examined, the data will be interpreted to provide palaeoecological reconstruction of the period covered by the entire core.

Raw data and charts are included for reference purposes and ease of interpretation.
The Background of Molluscan Analysis

Molluscan analysis has been undertaken since the 18 th/19 th century.
Although early work concentrated on taxonomy, more recent analysis has utilised the data for palaeoenvironmental reconstruction due to the sensitivity of snails to environmental change and present habitat preference of various taxa, leading to an ability to identify local dominant vegetation and water volume and conditions (Lowe & Walker, 1997); this assumes the habitat of the molluscs has remained unchanged over time.

Due to the calcium carbonate of the shells, unlike pollen and Coleoptera (beetles), land snails are recovered from a wide range of oxidised conditions, specifically on chalkland, including pits, ditches and buried soils; in non-calcareous areas, the shells of molluscs are thinner and therefore are not well preserved (ibid).
Some specimens are large enough to be recognised in the field, although collection under laboratory conditions is preferable due to the possible bias towards larger species (Lowe & Walker, 1997).

Following soil sampling, in this case by core, the sample is air dried then immersed in water, possibly with the addition of a dispersive agent, usually hydrogen peroxide, to assist in the removal of organic matter.
The resulting froth is put through sieves of various diameter, 2mm, 1mm and 0.5mm, for ease of sorting (Lowe & Walker, 1997).

These procedures had occurred prior to the class analysis.
Ufton Green: The Sample Site

During field walking of an area in Ufton Nervet, 11km south-west of Reading, Berkshire, worked flint displaying characteristics of Mesolithic industries warranted further investigation of the site.
Excavation of the area at Ufton Green, near the River Kennet, produced tightly packed flint flakes and was interpreted tentatively as a knapping area; a lack of pottery, charcoal and pit features suggests this was a production site and not a settlement (Allen & Allen, 1994-7).

The floodplain was cored for environmental purposes and following standard preparation described above, master's students examined core 50, taken 100 metres to the south-west of the Mesolithic site.
Method of Analysis

With the use of a light microscope, the samples were separated into test tubes of different materials, either with the use of a moistened paint brush or by hand with the use of tweezers, dependant on the size of the object; as the sample examined by the writer weighed only 27 grammes, this took little time.
The material types were counted individually and noted for reference purposes.

Due to the morphological characteristics of the single spiral displayed in the shells of land and freshwater snails, it was possible to identify the gastropoda to species level; each species was allotted a test tube, although the bivalves were counted as a group and divided by two to account for possible duplication within the assemblage.
Within the class, fragments of shell lacking the apex were disregarded on this occasion due to the difficulty in identification, although examination under a high-powered microscope could reveal microsculpture and possibly identify the species (Lowe & Walker, 1997: 204).

The quantity of each species of mollusc was entered on a table and together with the non-molluscan assemblage, was produced as an Excel document of class data (appendix 1).
Following adaptation of the data within Notepad, the data was transformed, via the use of the Psimpoll program, into a histogram suitable for interpretation (fig 3).

Unfortunately, due to the volume of data, the histogram spreads over three pages.
Interpretation of Data from 179-181.5cm

Through microscopic examination and referral to reference data, it was possible to identify all molluscs present within the assemblage recovered from 179-181.5cm to species, with the exception of the bivalves, which have been considered at family level only.
Few had incurred heavy damage and of the gastropoda all retained the apex of the shell, the fundamental characteristic required during molluscan analysis.

Due to the extremely low number of molluscs recovered from the small organic sample examined by the writer, 27 grammes, the percentage of ecological groups and species, and the production of the pie charts is for illustrative purposes only and should be interpreted with caution.
A pie chart clearly displays the distribution of ecological preference among the species (fig 4).

The bivalves, at 47%, dominated the freshwater molluscs at a depth of 179-181.5cm.
Thirty-five percent of the molluscan assemblage was of moving water species and 18% were of the ditch group As the bivalves were not identified to species during the exercise, these were entered as a catholic group due to their various ecological habitats (appendix 2).

Within a freshwater mollusc assemblage, species distribution can reveal the nature of the water body.
Of particular interest within the assemblage of 179-181.5cm (fig 5), was Valvata macrostoma.

Unfortunately, despite best efforts, vegetation adhered to the example from the collection of this depth could not be removed for fear of damaging the shell, nonetheless, the sculpturing is visible in figure 6.
Rare even in the fossil record, this mollusc inhabits ditches with slow moving or stagnant water with diverse vegetation (Kerney, 1999: 28).

The species is on the decline and has not been reported in the Oxford region since1965 (ibid).
However, the similarity between this and other species, notably Valvata piscinalis, has resulted in some mis-identification (Kerney, 1999: 28); both possess deep sutures and regular ridges.

The latter species also dwells in slow moving water, although prefers a silt content to the water.
Valvata piscinalis is still common in England and Ireland, although lower in number in Wales and Scotland (Kerney, 1999: 29).

Digital photography clearly displaying the morphological characteristics of this species is difficult due to the depth of the shell; the apex may be clearly seen, however, the body whorl, at a different focal length, will be out of focus (fig 7).
The remainder of the assemblage consisted Bithinia leachii, a lowland taxa mainly restricted to the eastern half of England, Bithinia tentaculata, a common species favouring silty situations, and bivalves, all of which are potentially moving water species.

On this evidence, one could tentatively suggest that the environment at the time of deposition was that of slow moving water.
With the exception of Valvata piscinalis, all the identified species of gastropoda prefer diverse vegetation, suggesting dense foliage within the water body.

Non-molluscan data recovered from the sample consisted a relatively large quantity of floral evidence: 73 pieces of plant remains; 5 small seeds, including Chenopodiaceae (goosefoot) and immature Carex (sedge) - both potentially marginal plants, reaffirming the wet environment; and 13 pieces of macrocharcoal.
As natural burning is unlikely in the British Isles and even more improbable in a wet environment as suggested by the molluscs, this probably suggests local human acitivity.

The faunal assemblage was limited, nevertheless included a Coleoptera appendage fragment and 38 worm granules.
The only remaining objects of interest were 42 unworked flint fragments.

Despite such a small sample, a vast quantity of evidence was recovered.
As the origin of the core is in proximity to the River Kennet (figs 1 & 2), floral and molluscan evidence suggests diverse vegetation within a subsidiary channel or, due to the low elevation (50 metres), possibly a water meadow.

Interpretation of the entire sequence from Ufton Green Core 50
Throughout the sequence of core 50 (fig 8), freshwater species dominate principally of Valvata macrostoma, suggesting near constant water-logging or a water body.

The preference of moving water species (fig 9), dependant on the assumed habitat of the bivalves, and the underlying but near constant presence of Bithynia tentaculata with a preference to an oxygen-rich environment, suggests slow-moving water with diverse vegetation.
Some molluscs remained constant throughout the sequence.

The sample at 181.5-190cm, the earliest deposits, was one of the lowest in molluscan content.
Only 48 shells were recovered, 70% of these were freshwater species, dominated by Valvata piscinalis and Valvata macrostoma.

This, together with the shade loving species Zonitoides nitidus, which inhabits areas colonised by marginal plants (Kerney, 1999: 148), represents a water body, probably a ditch of flowing water.
Later in the sequence at 75-86cm, correlation can be noted between Zonitoides nitidus and Arianta arbustorum, a species of the catholic group inhabiting the tall, rank vegetation, possibly growing waterside.

The highest representation of Valvata macrostoma occurs in this sample and together with Bithynia tentaculata suggests a moving water filled ditch.
Conversely, a high proportion of Vallonia excentrica at this depth suggests areas away from the waterside wereopen country or possibly grazed.

With the exception of the sample 179-181.5cm, examined by the writer, all of the sample depths contained a proportion of open country species (fig 8); these marginally outnumbered by shade loving species for the majority of the earlier period but predominate in the latter half of the sequence, probably due to climate change and therefore vegetation.
The quantity of shade loving species, together with a near constant underlying presence of catholic species, suggests a presence of open woodland or dense herbs.

Having examined the land and freshwater molluscs separately, a decline in molluscs was noted at 181.5cm.
Following a predominance of freshwater species, the overall quantity of snails reduced sharply followed by a steady increase in quantity and ecological representation from 173cm; the sudden change in the assemblage suggests a short but dramatic change in the climate.

The non-mulluscan data of the entire Ufton Green core 50 sequence is diverse and varies in quantity throughout, with the exception of the samples at 173-177cm and 130-151cm where there is an absence.
The earliest sample, at 181.5-190cm, produced 486 worm granules, the highest quantity within the sequence and indicated a well-sorted strata.

Forty-one worked flint fragments were recovered from 151-173cm; evidence of human activity.
Further evidence of a human presence was found at 115-130 where 81 fragments of macrocharcoal were identified; as natural burning in the British Isles is unlikely, the presence of macrocharcoal indicates intentional burning in the local area - microscopic charcoal particles could be carried on the wind.

Similarities in the molluscan assemblage found during comparison of Ufton Green core 50 to deposits examined and dated from Holywell Combe, Kent, indicate the site of Ufton Green can be tentatively dated from the Mesolithic 8700BP until AD43.
Conclusion

Through examination of molluscan data, and comparison to a dated site, it has been possible to reconstruct the environment surrounding the origin of Ufton Green core 50, from the Mesolithic to the Roman occupation.
The increase in the mollusc assemblage over time suggests an improving climate following the end of the last glaciation.

The dominance of freshwater species identified throughout prehistory and beyond, indicates the site has remained waterlogged, perhaps a water meadow, possibly at times a stream flowed, perhaps feeding, or a subsidiary of, the River Kennet.
The plant life was diverse and included waterside, grassland and woodland or heavy herb growth.

Unfortunately, due to the area travelled by molluscs during their life-time, this reveals only the local environment; further study would reveal the character of the wider landscape.
INTRODUCTION

Language can "distinguish the 'human' from the animal" suggests Iain Davidson (1991: 39); if Neanderthals were non-verbal, does this place them in the animal bracket?
Chase and Dibble observe only indirect evidence has been preserved and therefore there is no definitive answer (1987), however, a development in the skeletal morphology might have been required to allow a greater range of sounds (Davidson, 1991).

Cerebrally speaking, Steve Mithen finds it difficult to believe the early human brain could be equivalent in size to, or in the case of Neanderthals in excess of, modern humans but produce a hominid incapable of speech (1996).
Is it possible to identify a spoken language in the Neanderthal fossil record and through their material remains?

This essay will approach the subject by examining articles detailing fossil evidence, neurological interpretation and the function of verbal communication, attempting to identify whether Neanderthals were capable of speech.
FOSSIL EVIDENCE

Examination of fossils has revealed as evidence for and against the possibility of language among Neanderthals and reinterpretation has identified some inaccuracies in earlier work.
The presence and absence of the hyoid bone, necessary for muscle control required during speech, has also caused much debate, together with the position of the larynx.

Here, the work of several individuals will be summarised, providing evidence for both sides of the argument.
In 1983, the discovery of the Kebara 2 fossil in Israel renewed discussion on the possibility of Neanderthal speech.

The skeleton was incomplete, however, the hyoid bone, located within the cartilage surrounding the larynx, was recovered (fig 1).
Although Johanson & Edgar (1996) describe this small bone, required for speech, as identical to modern examples, Trinkaus and Shipman depict the bone as a "slightly enlarged version of a human hyoid and nothing like an ape hyoid" (1994: 391).

The discovery of the Kebara fossil caused reassessment and proposed the morphologically modern bone was capable of a modern range of sounds (Johanson & Edgar, 1996), a hypothesis supported by Mithen (1996) when he submitted, assuming the presence of a cognitive ability, there is no reason a full range of sounds could not be produced.
However, Lieberman and his colleagues suggested the dimensions of the Kebara hyoid could not distinguish the fossil from that of a modern human or pig hyoid and even suggested that Neanderthals may have oinked instead of spoken (Trinkaus and Shipman, 1994).

However, the recovery of the hyoid bone and the comparison to that of a modern human does not assist in the interpretation of the remaining components of the vocal tract - what did the unpreserved soft tissue look like and did it allow speech?
The pharynx, a tube connecting the oral cavity to the throat, is also partly responsible for the ability to speak; without a long pharynx, the range of sounds is limited, as this soft tissue moderates the vibrations at the larynx (Tattersall, 1999).

Previous arguments have proposed the low position of the larynx in the human throat allowed a wide range of sounds; interpretation suggested the high position of the Neanderthal larynx produced a limited range, for example, Lieberman's reconstruction of the Neanderthal vocal tract of the Chapelle-aux-Saints fossil suggests an inability to produce the vowels [a], [i] and [u] (Tattersall, 1999).
There has been suggestion that the position of the Neanderthal larynx has less to do with the capability of speech and more to do with climatic adaptation; Tattersall (1999) considers the possibility the high position allowed the warming and humidification of cold dry air before reaching the lungs.

In their work of 1993, Stringer and Gamble also considered the position of the larynx, and how the larynx descends in an infant.
The newborn human larynx is positioned high in the throat, enabling the infant to suckle and breath simultaneously without choking.

As the child grows, as their language skills develop, the larynx descends to the adult position, enabling a wide range of sounds.
This descent coincides with a change in skull shape from the flat base of a newborn to the angled skull of an adult.

Some correlation has been found between the flattened shape of the infant skull and that of some Neanderthal fossils, possibly evidence of an underdeveloped vocal tract resulting in an inability to produce a modern range of sounds (Stringer & Gamble, 1993).
This proposal should be considered together with the possibility of misinterpretation.

On examination the skull of the Neanderthal from La Chapelle-aux-Saints had been pieced together and glued by Marcellin Boule.
Later cleaning and reconstruction produced a rounder shape to the skull - more human-like (Trinkaus & Shipman, 1994).

This misinterpretation of shape may have led to a reconstruction of the Neanderthal vocal tract, by Lieberman and Crelin, resulting in a hominid with the inability to talk, swallow or even open the mouth (ibid).
Pre-Neanderthal fossils, including examples from Steinheim and Petralona, show a modern morphology to the skull.

The smaller brains may have resulted in less sophisticated behaviour than the later Neanderthals, however, they may have been capable of speech.
The Neanderthals possessed flatter skull morphology and a larger brain, yet are suggested as being incapable of speech - Stringer and Gamble find this unlikely (1993).

Houghton (1993) provided an interesting critique of the work of Lieberman (1989) when discussing the possibility of Neanderthal speech.
Lieberman had assessed the dimensions of the hard palate and the epiglottis and suggested a position of the larynx low but within the neck (fig 2).

Lieberman claimed there were similarities between the supralaryngeal tract of Neanderthals, chimpanzees and newborn humans, resulting in a restricted speech capability.
Houghton disagreed with this hypothesis, saying the interpretation of the oral cavity was incorrect and the tongue would occupy 90% of the oral cavity; this, Houghton suggested, was too much and proposed the examination of several features of the Neanderthal skeleton, including the superior genial tubercles and the length of the palate, allowed successful reconstruction of the Neanderthal oral cavity and an appropriate sized tongue to be applied.

This reassessment allowed a suitable position, only obstructing the oropharynx, part of the pharynx between the soft palate and the hyoid bone, during the acts of swallowing and breathing (fig 3).
This reconstruction produced a vocal tract similar to that of modern Homo sapiens, although possibly with a facial extension in the anterior direction (the characteristic forward movement of the Neanderthal mid-face), resulting in a proposed capability of speech (Houghton, 1993).

Recent work has examined the correlation between the size of the hypoglossal canal and the capability of speech.
This canal contains the nerve supplying the tongue; it was proposed the varying canal size indicated the size of nerve and reflected the speech capability of modern humans in contrast to the linguistically impaired non-human primates (Kay et al, 1998).

The study concluded the canals of Neanderthal fossils were comparable to those of modern humans and therefore Neanderthals were physically capable of speech, as were other members of the genus Homo, by at least 400,000 years ago (ibid).
A re-evaluation of data and a later publication by others was prompted by the discovery of numerous non-human primates with a hypoglossal canal comparable to that of modern humans.

The conclusion was that there was no apparent correlation between the size of hypoglossal canal and the nerve it contained, and therefore does not reflect linguistic capability (Degusta et al, 1999).
NEUROLOGICAL INTERPRETATION

The presence of a vocal tract capable of speech is not evidence of language in itself; the brain is responsible for the creation and comprehension of language (Stringer & Gamble, 1993).
Some palaeoanthropologists and archaeologists, including Mellars (1996), suggest a neurological evolution may have resulted in the introduction of language, probably prior to the beginning of genus Homo (Johanson and Edgar, 1996); however, Johanson and Edgar (1996) are quick to counter this argument with the observation brain evolution continues, indicating a single mutation is unlikely to have caused the initiation of speech.

Neurological studies have been attempted over the years to explain the introduction of language; here, some early work and more recent data will be summarised.
Of the fossils currently known, preservation has not allowed the examination of a Neanderthal brain, although the study of endocasts of the inside of the cranium, revealing the peaks and troughs on the brain surface, has allowed the study of various early hominids.

Experts look for asymmetry in the brain hemispheres and structures linked to language competence, including Broca's area and Wernicke's area (Johanson & Edgar, 1996), searching for reasons behind linguistic communication problems.
Working during the nineteenth century, French anatomist Paul Broca conducted an autopsy on a mute, revealing a lesion or defect on a particular area of the brain.

Broca deduced this area was responsible for muscle control required during speech (Trinkaus & Shipman, 1994), however, recent data, including brain scans of humans, suggest several areas contribute to the control of speech and Broca's area may be responsible for motor function including limb control (Johanson & Edgar, 1996).
Examination of non-human primate brains has found the presence of Broca's area, although under-developed compared to humans, resulting in the interpretation of the pre-evolutionary Neanderthal brain as capable of only limited speech or possibly a complete absence (Trinkaus & Shipman, 1994); however, Tattersall (1999) believes the examination of the contours of the brain will not disclose linguistic ability.

More recent studies in the effects of Alzheimer's disease have reaffirmed the possibility of speech impairment if there is an under-development of the brain or if damage is present.
An underdevelopment of the left hemisphere of the brain may restrict the ability of language as evidenced in the study of aphasia, conducted by Kempler in 1993.

Lesions on this area were found to cause symptoms including problems in formulating and understanding spoken language; the same area of the brain is responsible for motor functions, possibly resulting in disrupted movement of the limbs, a symptom of apraxia (Kempler, 1993).
Damage to this hemisphere could cause speech and/or motor function impairment; this may suggest an underdevelopment would have the same effect; therefore, early hominids, including Neanderthals, prior to evolution, may have been unable to form a complex language due to cerebral impairment.

This possible impairment may also explain the limited range of lithic tools recovered from Neanderthal sites; their restricted motor functions may have prevented them from making technological advances.
Further neurological research may produce more evidence to prove or disprove the ability of Neanderthal speech.

THE FUNCTION OF VERBAL COMMUNICATION
It seems likely that some verbal communication would have been required to pass on knowledge of hunting strategies and lithic tool production; however, although Neanderthals may have been physically capable of speech, did they, and for what purpose?

Having approached the physical possibility of speech, here the purpose will be discussed.
Although speech as we know it may not have evolved, language of another form may have been present.

Non-human primate calls possess some meaning (Dibble, 1989) and the same could apply to a system of communication reliant on a series of grunts within an early hominid culture; the only requirement would be a "consistency with their way of life" (Tattersall, 1999: 173) and may even have provided a bond within the group (Johanson and Edgar, 1996).
A gestural language would be successful providing there is mutual understanding between at least two individuals (Davidson, 1991) as evidenced by sign language for the deaf.

However, a non-verbal system of communication could be open to misunderstanding; errors between the provider and receiver may result in misinterpretation and may restrict the number of possible gestures within the language, possibly resulting in the evolution of a spoken language (Nowak & Krakauer, 1999).
Several individuals have discussed the restricted geographical movement of early hominids over the years.

Feblot-Augustins traced movement between resources and found Eastern Europeans travelled further, repeatedly, than their French counterparts.
This movement would provide not only a wide range of resources, but also an increase in human contact (1993); this may have led to an increased use of language, whether gestural or vocal.

In contrast, Foley and Lee found little or no geographical human movement and observed the distribution of resources would shape the social unit (1996).
This would possibly lead a sedentary lifestyle, with no human contact outside of the group, resulting in a local dialect if any language was present.

Stringer & Gamble observed the lack of cross-oceanic movement by 'the Ancients', including the Neanderthals, led to a lack of colonization of the Asian interior; this simple society, they suggested, would also possibly have had no purpose for a complex language (1993); there has been suggestion the production of non-utilitarian objects and the boats required for the colonization of Australia, probably prior to the glacial maximum of around 20,000 years ago (Fagan, 1998), are the first evidence for the presence of language (Davidson, 1991).
The production of lithic tools is a specialised task and became increasingly complex through the Palaeolithic evidently requiring instruction (Dibble, 1989).

However, this does not necessarily identify a language based communication system; the simplistic tool types in the Lower and Middle Palaeolithic may reflect the lack of a complex language (ibid), although, this in itself does not determine a total lack of language.
Tattersall (1999) suggests the development of tool-making techniques may be contemporaneous with the evolution of language and regional dialects: one possibly prompting the other.

However, the possibility of observation and imitation must also be considered during the spread of lithic technology (Davidson, 1991), although Davidson is quick to counter this argument by suggesting the skill would be lost in a newly-colonized region in which the traditional materials could not be obtained (ibid).
Johanson and Edgar (1996) observed there appeared to be no technological advancement in the production of stone tools around the time of the proposed introduction of language; it was during the Upper Palaeolithic that there was a diversification of materials, design and function, possibly contemporary with the introduction of art (ibid); the pre-language cultures may not have required art for ritual activities or needed it for social identification (Stringer & Gamble, 1993).

The production of any form of lithic tool requires substantial cognitive ability, reliant on sufficient neurological evolution.
It has been suggested that tool-making and language involve similar cognitive processes (Chase & Dibble, 1987); again one triggering the other.

Continued neurological studies, related to the disorder apraxia, may reveal the beginnings of tool use and the communication systems for their continued production (Kempler, 1993); the techniques applied require skilled motor control and advanced planning (Davidson, 1991) and Mithen argues the Levallois technique may be too difficult to acquire through observation alone, without verbal instruction (1996).
As language is a form of 'displaced reference', where symbols, on this occasion words, are used to refer to objects in their absence (Davidson, 1991), it proves invaluable when planning for the future.

The evolution of language resulted in the ability to communicate "potentially life-saving information" about the environment in which early humans lived (Johanson & Edgar, 1996: 106).
Animal movement patterns, hunting strategies and gathering techniques could be relayed to assist survival in a hostile environment; Mithen suggests such tasks would be difficult to organize without the ability to verbally communicate (1996).

An evolution in communication systems would potentially lead to success as a culture (Davidson, 1991); if this is so, and the Neanderthals were capable of speech, why did they die out?
Lieberman argued the better adaptation of modern humans for verbal communication might have a link with the extinction of the Neanderthals (Trinkaus & Shipman, 1994).

CONCLUSION
Early arguments of whether Neanderthals could speak relied on the skull shape and reconstruction, although, loose interpretation, of the vocal tract.

The discovery of the hyoid bone of the Kebara 2 fossil was initially accepted as proof of vocal communication, however, there has since been evidence to suggest little difference between the early human fossil, the vocal tract and the oral cavity and that of members of the animal kingdom.
Some researchers have considered specific areas of the brain responsible for speech; others also assign motor function to these areas, possibly explaining the diversification of stone tools around the same time of the proposed spread of a complex language.

The spoken language may have developed as a more reliable form of communication to aid the transmittance of information, possibly to improve the way of life through hunting, gathering and tool-making techniques.
Following this research, it is difficult to believe survival skills could be passed on in any other way than verbally, however, the physical ability of speech in Neanderthals does not necessarily prove the cognitive ability.

Unfortunately, the question of whether Neanderthals were capable of speech remains unanswered.
Introduction

The 13 th century house of Hulton Abbey, north Staffordshire (fig 1), was that of a Cistercian monastery (Wise 1985).
Following frequent structural alteration, the abbey was abandoned as a religious house, a consequence of the dissolution in the 14 th century, and was "integrated into secular society" (Wise 1985: vii).

First excavated in 1884, the site has been investigated sporadically over the last 100 years and has produced a wealth of skeletal evidence (fig 2).
Following skeletal analysis of two osteological collections representative of two individuals recovered from Hulton Abbey (fig 2), this report will identify evidence of pathology including dental disease, trauma and joint disease.

In addition, where possible, age and sex determination and height estimation will be proposed.
HA G07: Partial maxilla and mandible

Preservation
Identified by the sharp edges of the break and therefore an absence of healing, postmortem damage has resulted in only partial preservation of the maxilla; the entire mandible is represented, although damaged.

Postmortem loss of the calvaria transverses the anterior surface at the level of the levator anguli oris attachment (McMinn & Hutchings 1985), beneath the zygomatic bone.
The anterior nasal spine displays postmortem breakage at the base of the nasal spine (fig 3).

A fracture to the palatine process of the maxilla aligns along the groove for greater palatine vessels (fig 4) (Brothwell 1981); further postmortem breakage is present at the transverse palatine suture and the tuberosity of maxilla.
The entire mandible is represented and measures a maximum of 83.29mm wide and 84.37mm deep.

A fracture runs from the left angle of mandible, transverses the mental fossa and terminates at the origin of the missing left mandibular canine, resulting in a broken edge measuring 59.86mm.
In addition, there is partial postmortem loss of the alveoli at the socket of the left mandibular first and second incisors and canine, and cortical bone loss distally from the first left mandibular molar to the region of the left second incisor; this has resulted in the remaining three teeth of the left mandible becoming separated from the alveoli.

Both the condyle processes of the mandible are fractured, resulting in a loss of 11.06mm on the medial aspect of the left and an almost total loss above the pterygoid fovea of the right.
Cracks on the line of the absent mandibular molars towards the ascending ramus probably originate from post-excavational drying of the bone.

Dental representation together with ante-/postmortem condition is displayed in figure 5.
The enamel of the first molars of both the left maxilla and left mandible was damaged postmortem.

An area measuring 3.35mm x 3.26mm has been chipped from the mesial bucchal cusp of the first molar of the left maxilla.
However, the mandibular first molar has undergone significant postmortem modification as the mesial buccal cusp has been chipped in addition to both the lingual cusps, with a maximum enamel removal of 3.35mm x 5.54mm mesio-distally.

Description of lesions
Significant lesions are present on both the maxilla and mandible.

A large occlusal dental caries of the first right molar of the maxilla reduced the occlusal surface of the enamel, resulting in the decay of the primary and secondary dentine down to the pulp cavity; the surface of the resulting void measures 7.68mm (bucco-lingual) x 8.82mm (mesio-distal).
The adjacent second molar has a smaller occlusal fissure caries measuring 4.39mm (bucco-lingual) x 8.83mm (mesio-distal) x approximately 2mm deep.

Interproximal caries is evident in several locations.
The first and second premolars and first molar of the left mandible show minimal interproximal neck caries; the second premolar and first molar on the mesial and distal surfaces, and the first premolar on the distal surface only.

The second incisor of the left maxilla displays interproximal neck caries distally, towards the canine.
Calculus is present throughout at various degrees of formation.

Using the recording method of Brothwell (1981), light calculus was found on the left buccal and lingual surfaces of the teeth of the mandible with a medium degree on the maxilla.
In contrast, a medium degree of calculus is evident on the right lingual surfaces of the mandibular teeth with considerable calculus on the buccal surfaces of both the mandible and maxilla (figs 6 & 7).

The medium-considerable degree of calculus on the teeth of the right maxilla and mandible is consistent with reduced use due to the presence of caries on the first and second molar of the left maxilla, therefore an avoidance of the area while chewing.
Dental attrition is extensive and varies in degree; different scales have been used during interpretation due to a molar wear pattern inconsistent with the scale favoured by Buikstra et al (1994).

The incisors, canines and premolars belong to grade 5/6 of the scale recommended by Buikstra et al (1994: 52); the molars, however, fall under the category of 4+/5 classification of Brothwell (1981: 72).
The unusual occlusal wear pattern of the right maxilliary incisor could suggest unusual activity, possibly industrial, requiring repetitive use of the teeth; however, the severe molar caries present in this specimen represents an individual in great discomfort who probably chewed their food with their incisors to avoid the pain induced on the posterior teeth.

Unfortunately, due to the loss of some of the mandibular incisors, it was not possible to take either interpretation any further.
The degree of dental disease has resulted in periodontal disease and the resorption of alveolar bone.

Although the resorption varies throughout, generally the degree falls within a slight-medium classification (Brothwell 1981).
The disease evidently began with the molars with antemortem loss of four of the six mandibular molars, resulting in remodelling of the tooth socket and profile of the bone (fig 6).

Alveoli remodelling of the first and second left mandibular incisor sockets provided further evidence of antemortem tooth loss.
Periodontal disease interpretation of the maxilla proved difficult due to the postmortem loss of the third molars; however, as the second left mandibular molar was lost antemortem, evidenced by alveolar remodelling, further disease was suggested.

Surprisingly, despite extensive caries, calculus and periodontal disease, apparently no abscess formed in the remaining alveoli.
No dental hypoplasia was recorded suggesting an absence of extended periods of illness during the developmental process of the teeth.

Age at death and sex determination
The unequal tooth wear on the first right mandibular molar together with the attrition of the five remaining molars suggests this individual fell in an age range of 33-45 at death (Brothwell 1981).

The mental eminence was examined for sex determination resulting in tentative suggestion of a female gender due to a minimal projection of 2 (Buikstra et al 1994); this interpretation is reinforced by a slight mandible with flat goinal regions and a fairly short, narrow ramus with a delicate coronoid process, all indicative of a female (Brothwell 1981) due to less defined muscle attachments than that of a male.
However, these findings are inconclusive due to the absence of diagnostic elements such as the frontal bone of the skull or the pelvis.

Following this interpretation, examination of figure 2 revealed this skeleton has been sexed as male.
Conclusion

Following a relatively healthy childhood during the development of the teeth, this individual died between 33-45 years of age suggested by dental attrition.
Considerable yet uneven attrition, including unusual patterning to the remaining incisors, suggest an activity requiring the use of the teeth or more likely, use of the incisors during mastication to avoid the painful molars to the rear.

Antemortem tooth loss resulted in alveoli re-absorption in the region of the molars and mandibular incisors.
Postmortem damage to the fragile facial bones has resulted in much reduced facial anatomy, however, caries of the right maxillary first molar, accounted for the considerable calculus on the teeth of the right maxilla and mandible; due to the pain involved the individual would have avoided use of the right side of the mouth, therefore the process of chewing would not remove food residues and plaque, eventually resulting in calculus.

Serious periodontal disease, as a consequence of considerable caries and calculus, was probably as a result of a diet rich in sugar, therefore, possibly indicating a high status.
HA 23: Right scapula and arm

Preservation
The right scapula (fig 8) possesses extensive postmortem damage.

The coracoid process is absent resulting in a maximum lateral width of 436mm, measured from the glenoid cavity border.
The acromion has been broken postmortem at 229mm, with an additional fragment of 545mm present but detached.

The subscapula fossa is also broken, 755mm from the glenoid cavity with a separate length of 485mm, resulting in a maximum width of 273mm.
There is a post-mortem fracture of the neck of the right humerus.

As will be discussed later in this paper, evidence suggests the individual was of an age that the epiphysis should have fused; however, post-depositional damage has resulted in the loss of the lesser tubercule and greater tubercule.
Postmortem breakage of the right radius resulted in the remaining bone measuring 2205mm.

The right ulna, measuring 2400mm, displays a postmortem distal transverse break at 463mm from the styloid process.
Additional bone has broken from the distal right ulna postmortem, as a majority of the coronoid process has been lost.

The bones of the wrist and hand are absent.
Description of lesions

The scapula displays no evidence of healed fracture or joint disease, although an absence of the left scapula within the assemblage does not allow comparison of dimension and therefore restricts interpretation of possible atrophy.
Lateral epicondylitis (tennis elbow) is suggested by a blastic lesion, additional bone growth, on the lateral epicondyle of the humerus (fig 9).

As the origin of the common extensor tendon (McMinn & Hutchings 1985), alone this could suggest excessive, repetitive use of the forearm, infection or impact to the joint (Martin 2005).
However, there is a second blastic lesion of the medial epicondyle (medial epicondylitis) and an osteophyte (bone spur) is present; this may be as a response to tendonosis, degeneration of the soft tissue, possibly through overuse or age related wear and an attempt by the body to redistribute weight on a damaged joint (ibid).

In addition, minimal eburnation on the capitulum of the humerus corresponds to similar wear of the right lateral margin of the radius (figs 9 & 10); a porous surface to the joint in the region of the trochlea is probably natural in origin.
The presence of osteophytes and eburnation of the distal humerus, is evidence of osteoarthritis (Roberts & Manchester 2005), albeit of an early stage due to minimal modification.

A healed simple fracture of the right radius 465mm from the distal postmortem break resulted in two separate fragments (Buikstra et al 1994); healing produced a blastic lesion and consequential thickening of the lamellar bone.
Further osteoblast activity between 1191-706mm from the styloid process of the right ulna, resulted in increased bone thickness to a maximum of 186mm x 156mm when measured 958mm from the styloid process; the presence of lamellar bone is evidence of a healed simple fracture.

The partially aligned, well-healed, oblique midshaft fracture of both the radius and ulna (fig 11) suggest a parry fracture (a defensive action) either against impact with, in this case, a blunt instrument due to a lack of blade injury to the bone, or accident.
There is no evidence of cloacae and therefore it appears the fractures healed cleanly without infection, although this cannot be excluded without x-ray as the exudate may not have collected to the point of eruption.

Age at death, sex and height determination
The poor preservation of the bone assemblage and quantity of data restricts age determination to that of epiphyseal union.

Union of the medial epicondyle of the humerus reaches completion by 17 years (Flecker 1942, cited in Mays 1998).
The Buikstra et al classification (1994) can tentatively age the individual to 18-20 years or over, due to the fusion of the distal humerus and proximal radius.

The post-mortem break of the acromion suggests the individual had passed the late pubescent onset of the union and can therefore once again be aged to beyond 22 years (Brothwell 1981); therefore, the individual is a sub-adult or adult.
Although osteoarthritis could develop in a young adult as a result of trauma or disease, the presence of osteoarthritic lesions suggests either repetitive manual labour of a young adult or a person of greater age; without further skeletal evidence, a more precise age is difficult to determine.

Due to a lack of diagnostic skeletal elements, sex determination is restricted to a tentative suggestion of the male gender due to the robustness of the bones and development of the muscle attachments (Mays 1998).
The acromion, the origin of the deltoid muscle, the tuberosity of the radius, the insertion of the bicep by way of a strong tendon, and the supinator crest of the ulna (McMinn & Hutchings 1985), are well developed indicating strong muscles of both the upper and lower arm.

Further methods, such as bone articulation or weight, could be adopted in the attempt to confirm the sex, however, this should be attempted during a population study rather than that of an individual (Brothwell 1981).
The height of the individual can be estimated based on the length of the long bones.

The almost complete humerus is the most reliable, providing a male height estimation of 160cm; however, the three damaged bones of the arm suggest a height range between 160cm-163cm (Trotter 1970).
Due to the proximal break to the humerus and distal damage to the radius and ulna, the three estimations have been combined to provide an average measurement and therefore a suggested minimum male height of 162cm.

Alternatively, if the remains are those of a female the average suggested height would be a minimum of 158cm.
Later examination of the location map of the graves of Holton Abbey (fig 2) revealed the individual to be male.

Conclusion
Post-mortem damage resulted in limited data regarding the scapula and distal radius and ulna.

The muscle attachments suggest this individual was male and the epiphyseal union suggests he was over 22 years of age.
Traumatic injury, overuse or age related wear to the elbow, resulted in lateral and medial epicondylitis.

The presence of osteophytes and eburnation to the distal humerus, is evidence of osteoarthritis, further suggesting overuse, an older individual, or both.
Examination of further joint surfaces or dentition is required for a definitive conclusion of age and joint disease.

Introduction
Areas of the prehistoric environment containing a concentration of monuments have often been described as 'ritual landscapes'.

Such areas include the Stonehenge and Avebury regions of Wessex, the Boyne valley in Co.
Meath, Ireland, and the Stenness-Brodgar-Maeshowe complex of Mainland, Orkney.

The term 'ritual landscape', however, implies the segregation of the secular areas of everyday life.
In contrast to this, excavations during the last thirty years have produced the remains of settlements and/or evidence of industrial activity in relatively close proximity to these monuments.

This essay will approach the subject of 'ritual landscapes' and the use of the term in the light of recent discoveries.
Landscape or complex?

The traditional view of a prehistoric monumental concentration has been described as a 'ritual landscape'; a cluster and possible dominance of non-domestic monuments with a segregation of the secular world (Malone, 2001: 165; Thomas, 1991: 26).
When examining plans and maps of antiquarians, including those of William Stukeley, the source of the misinterpretation of a landscape apparently devoted entirely to ritual and devoid of everyday life is understandable.

The presence of the magnificent monuments in and around Avebury attracted much attention during the 18 th century (fig 1), however, evidence of settlement was not so easily recognised.
Only since systematic excavation and possibly scientific analysis has been introduced have the settlements of the communities responsible for the construction of the monuments been located.

Possibly the term 'ritual complex' would be more applicable when describing these areas.
Clusters of monuments were erected in an occupied landscape, interspersed with settlements: ritual without the exclusion of domestic - these large monuments formed the focus of the larger populations of the later Neolithic (Malone, 2001: 165).

Thomas suggests environmental evidence could ascertain the location of monuments on marginal land; however, the likelihood is the inclusion of the monuments within the farmed landscape of the fertile chalk uplands of Wessex (Thomas, 1991: 26).
The area surrounding Avebury in Wiltshire still lacks evidence of major settlement during the later Neolithic, however, numerous sherds of late Neolithic pottery, including both fine and coarse Beaker ware, and Beaker type scrapers have been recovered from the ditches and pits of Windmill Hill (Keiller, 1965: 80).

In addition, Keiller recognised a series of pits and holes adjacent to West Kennet Avenue as an occupation site (fig 2), producing an abundance of late Neolithic flint tools and pottery sherds, (1965: 210).
The holes also produced evidence of feasting in the form of burnt ox and pig bone, red and roe deer antler, an ox tooth and charcoal (Keiller, 1965: 215).

Keiller initially interpreted the charcoal and associated flint and sarsen 'packing' as the remains of post-holes; analysis showed the charcoal represented only twigs and small branches of hazel, blackthorn and hawthorn (1965:212), which, due to the relatively small nature of the plants, would unlikely be used for construction.
Unfortunately, although numerous pits and holes were excavated, no visible trace of the settlement remains on the surface, as evidenced by aerial photography (fig 2).

The henge monuments and stone settings of the Ring of Brodgar, Stones of Stenness and the passage grave of Maeshowe, have long been recognised as ritual sites (fig 3).
The relatively recent discovery of the settlement Barnhouse at the centre of the monumental complex allowed comparison of contemporary structures, of both ritual and domestic origin, and the organisation of the landscape (Richards, 1992: 445).

The village, located on the shore of the loch of Harray, founded c3200BC, consisted of phases of construction as the stratigraphic sequence revealed new houses were constructed so the foundations overlaid those of the old (Richards, 1992: 446).
The exception to this was House 2, which remained in use throughout the occupation of the settlement before the eventual abandonment (ibid).

A central area of the occupation site produced evidence of flint knapping, pottery and bone tool production, indicating an industrial area; commonly recognised as a focus of gathering and working (Richards, 1992: 446-7).
Sherds of grooved ware recovered from the site, was found to be identical to a sherd recovered from the ditch surrounding the Stones of Stenness, 150 metres to the south-west (Richards, 1996: 445); this evidence indisputably links the domestic site of Barnhouse to the ritual site of Stones of Stenness.

A similar pattern of occupation/ritual land use can be seen in the Boyne valley of Co.
Meath, Ireland; the major monument of this complex, the passage grave Newgrange, was constructed c2500 bc (O'Kelly, M.J.,1983: 2).

The cairn, measuring eighty-two metres in diameter (ibid), possessed three adjacent satellite mounds and a possible stone circle of later construction, possibly the Bronze Age (O'Kelly, M.J., 1983:5-9).
As the cairn fell into a state of disrepair, a domestic settlement was established on an area of partially collapsed revetment (O'Kelly, M.J., 1983: 5).

The eastern area of excavation revealed a large hearth, although there was a total absence of charcoal and ash; this was interpreted as an external hearth and, following abandonment and lengthy periods of disuse, the wind dispersed the charred evidence leaving only heat-cracked stones and oxidised soil (O'Kelly, M.J., 1983: 15).
Stone and flint tools were recovered from this area together with various pot sherds (O'Kelly, M.J.,1983: 43-4), suggesting everyday activities, possibly a communal work area; adjacent pits and a trench produced various types of pottery including Beaker ware, undecorated bowl and coarse flat based domestic ware.

Charcoal from each has provided an average radiocarbon date of 2025 ± 34 years (O'Kelly, M. J. 1983: 15), however, no evidence remained of any structures.
There was, however, evidence of a building in the far western area, where two parallel lines of post-holes suggested the presence of a six-post structure, possibly enclosed by wattlework (O'Kelly, M.J., 1983: 35); a concentration of oxidised soil in the immediate area suggested this structure had burnt to the ground (ibid).

Excavation produced few artefacts: only an axe chip and three stone beads or pendants (O'Kelly, M.J., 1983: 44): the absence of domestic or industrial tools suggests a ritual purpose to the building.
The identification of both domestic and ritual areas adjacent to monuments implies the presence of a settlement; the lack of structural evidence could be interpreted as a lightweight design, leaving no archaeological trace; whatever the purpose of this site, it remained in use over several centuries.

Many Neolithic communities have adopted the use of both stone and timber during the construction of their ritual monuments.
Does local availability factor into the equation?

Is there a preference of one material over the other?
Does the incorporation of specific resources into the structures represent ritual meaning?

Ritual meaning of materials
The use of particular materials during the construction of specific monuments may hold meaning; alternatively, the communities responsible may have utilised locally available resources.

West Kennet, primarily known for the long barrow and Avenue, revealed evidence of two palisaded enclosures.
Both sites were of concentric ditch design, having previously contained a post built palisade, and produced two antler samples, providing the average radiocarbon date of 1765 ± 50 bc; the post-holes produced an abundance of charcoal, grooved ware and antler (Whittle, 1991: 257-9; Thomas, 1991: 218) and the size of the holes implies posts of a diameter of between thirty and fifty centimetres, indicating primary woodland (Whittle, 1991: 261).

These sites, possibly late Neolithic settlements (ibid), are contemporary with the stone phases of Avebury (Thomas, 1991: 217) and yet, in contrast, are constructed entirely of wood.
The local woodland may have also been the source of the mature oak posts used for the wooden phase of the Sanctuary on nearby Overton Hill (Burl, 2002: 123).

There has been suggestion of a correlation between material and the purpose for which it is used; for example, timber for house construction - for the living; stone for burial monuments - for the dead (Bradley, lecture, 2004, 10 Feb).
The Sanctuary casts shadow over this hypothesis due to the proposed development of the site from that of a timber mortuary house at the beginning of the later Neolithic (Burl, 2000: 311; Burl, 2002: 120; Malone, 1989: 84) or the freestanding post structure proposed by Maud Cunnington (Burl, 2002: 136).

In addition, Woodhenge in Wiltshire was constructed during the period 2480 - 2039 BC (Thomas, 1991: 181), contemporary with Avebury and West Kennet Avenue.
The henge, containing six concentric rings of posts, was of ceremonial purpose (Burl, 2002: 126) evidenced by the segregation of deposits: pig bones to the perimeter; cattle bones centrally (Burl, 2002:137) and the remains of a probable female child, apparently sacrificed and buried close to the centre of the monument (Burl, 2002: 223).

Again this structure, associated with death and burial, contemporary with substantial stone structures, was constructed entirely of wood.
Opposed to the hypothesis of material of choice, the material used may be due to local availability; studies show some correlation between stone circles and henges in an area abundant with the corresponding material (Bradley, 1998: 118).

In Shetland and Orkney, although trees were present during prehistory, they were few in number; therefore, evidently stone was favoured during construction of monuments.
The use of unusual or foreign materials in a settlement or construction of a monument may have some special significance; for example, the incorporation of local materials may mirror the surrounding landscape.

Knowth, one of three large passage graves of a group numbering around forty within three and a half square kilometres of the Boyne valley, was built around 3000 BC (Whittle, 1996: 245).
During the construction of Knowth and Newgrange, a tremendous volume of earth was moved for the construction of the mounds and river stones were transported from the Boyne (ibid).

During the third millennium BC, Knowth began to decay, although the area surrounding it remained the focus of attention as Beaker pottery has been found in association with a small timber structure of circular plan constructed at the passage grave to the east of the main mound (Whittle, 1996: 247).
In opposition to this, the importation of exotic stone to the region may have also transferred the essence of the foreign land to the monument; for example, rather than glacial action, the Bluestones of Stonehenge are now generally agreed to have been moved by human transportation (Childe, 1955: 281; Thomas, 1991: 177-8).

The greater necessity of labour during construction of these large monuments is generally accepted as having been pooled from the surrounding populations.
This labour force would have been required to retrieve materials from source as well as erect them.

Most megalithic blocks of stone appear natural in shape and more effort obviously went into transportation than the processing of the material, symbolizing the community effort involved in the construction (Sherratt, 1995: 246).
Why would the population of Neolithic Europe choose to adopt stone as their preferred material for construction instead of the lighter wood, a resource probably more abundant in the landscape?

Improvements in technology may explain the change: transport to move large and heavy objects over greater distances and the tools required for processing the material; however, the availability of new technology may not have altered the techniques involved in the processing itself and many ritual sites did not relocate over the generations.
Continued use of the landscape

By examining the landscape around major monuments of the later Neolithic, an impression of continued use of the same region emerges.
In the area surrounding Avebury, the earlier Neolithic sites of Windmill Hill and West Kennet long barrow were superseded by the henge, Silbury Hill, West Kennet Avenue and finally the stone circles (Malone, 1989); an unexplained bend in the essentially straight Avenue may be explained by an alignment with the entrance to an earlier timber phase (Ros Cleal, pers.comm.

1999).
A similar pattern can be seen by the development of Stonehenge again from a timber phase, through the early construction of a henge and the post-filled Aubrey holes, followed by the erection of the Bluestones in a semi-circle (probably an uncompleted circle).

This was followed by a period of remodelling of the site, involving the dismantling and re-erection of the Bluestones, the positioning of the sarsen trilithons and finally, enclosure by the lintelled sarson circle (Renfrew, 1973: 540; Burl, 2000: 349-352).
Both Avebury and Stonehenge, amongst other large stone circles, were constructed in such a position to be surrounded by early Neolithic long barrows; possibly due to a dearth of construction stone on Salisbury Plain (Burl, 2000: 352), the Stonehenge long barrows were of timber and earth construction, typical of the Wiltshire and Dorset cemeteries (Renfrew, 1973: 544).

The skills obtained during the construction of monuments may have been passed on to the descendants, as Burl believes carpenters erected the timber structure at Stonehenge (ibid); this inherited knowledge would also explain the use of carpentry techniques including mortise and tenon joints in the construction of the lintelled sarsen circle at Stonehenge.
Early Neolithic burial monuments appear to be the focus of continued activity as evidenced elsewhere in the British Isles.

The passage grave at Knowth in the Boyne Valley was apparently preceded and surrounded by approximately 17 smaller passage graves (Whittle, 1996: 245) and examination of the local Ordnance Survey map shows the hilltop stone circle of Arbor Low in Derbyshire is surrounded by later round barrows, predominantly located on adjacent peaks, although they appear to cluster to the south and west (fig 4).
Specific areas of the landscape remained the focus of ritual attention over thousands of years; as new forms of monument were developed and adopted, they were incorporated into the existing monumental landscape.

What factors initiated the development of new monuments and why were they sited in their specific locations?
Uniformity of design

The resemblance between the plans of certain monuments and their surrounding landscape has long been recognised.
The passage grave of Maeshowe (fig 3), located at a slightly higher elevation is most clearly visible from the settlement of Barnhouse to the north-west (Richards, 1996: 196).

Chronologically overlapping the occupation of Barnhouse (Richards, 1996: 201), the cairn is of cruciform plan and the central chamber incorporates four corner buttresses, creating recesses and reflecting the plan of House 2 of the settlement (Richards, 1996: 195-6).
The central buttresses may originate as a dismantled stone circle which has been recycled into a new monument; these standing stones are thought to be in excess to the requirements of the structure (Richards, 1996: 197), and are therefore presumably only for the purpose of representing House 2; this could be interpreted as a reflection of the domestic in the ritual site or vice versa.

South-west of Barnhouse are the Stones of Stenness, surrounded by a large rock-cut henge with a single entrance, from which animal bone provided the radiocarbon date c3100-2775 BC (Burl, 2000: 211); Richards suggests the arrangement of the internal space of the henge may reflect the world mirrored in the house and passage grave plans (1996: 199).
The lochs of Harray and Stenness surround the Stones of Stenness, together with the Ring of Brodgar, and both are located in a natural bowl created by distant hills (Richards, 1996: 203); this local topography is reflected in the bank and ditch of the henges.

Waterlogging has preserved organic remains within the ditch, and Richards suggests during long periods of the year, the ditches contained water (Richards, 1996: 324) and the enclosing ditch of Maeshowe may also have been water filled (Richards, 1996: 203); if the flooding was human induced rather than a natural collection in the rock, the process of crossing water may be connected with purification when entering a ritual site and/or a separation from the secular world (Richards, 1996: 204).
A mirroring of the local topography can also be seen at Avebury; the henge is surrounded by distant hills.

If the hypothesis of the construction of monuments to reflect the landscape is followed, does this explain the recumbent stone circle of Arbor Low?
When the local Ordnance Survey map is examined, one can see the henge in question is located 370 metres above sea level (fig 4).

A majority of the surrounding hills fail to reach or exceed this height with few exceptions.
Approximately one mile to the south, a tumulus crowns a peak reaching only 370 metres; the same distance to the south-east, the hill of Lean Low exceeds that of Arbor Low at 393 metres and is topped by a tumulus; and 3 miles to the north, close to the town of Taddington, a steep hill reaches 414 metres, however, there is no tumulus, possibly due to the incline.

With so few hills exceeding the height of that of Arbor Low, this may explain the supine posture of the stones.
The Neolithic adoption of farming may have prompted a respect for the land to such a degree the surrounding fertile landscape was reflected in the monumental architecture.

There is a feeling of inter-ceremonial visibility at Avebury; the mound of Silbury Hill, reaching a height of forty metres (Barrett, 1994: 29), is visible at certain locations within the henge (Barrett, 1994: 31), as it is from West Kennet long barrow and other monuments of the Avebury region.
This implies the possibility various ceremonies or rituals were undertaken throughout the landscape simultaneously and the monuments were located so visible to each other.

The location of so many monuments within a relatively small area and the construction of processional routes linking these monuments, suggests a ritual complex.
Local myth suggests the Stones of Stenness and Ring of Brodgar were linked by a stone avenue; there is no evidence to prove this hypothesis (Richards, 1996: 199) and without a lower level of the loch, the avenue would have to traverse the waters.

The avenues of Avebury, probably originating at the Sanctuary, pass several important monuments of the Neolithic these include, to the east, Silbury Hill, West Kennet long barrow and to the west the Beckhampton Avenue past the Cove and on to the long barrows of Beckhampton and South Street.
Malone describes the avenues as having increased the importance of the complex by expanding the ritual landscape (1989: 88).

These monuments have been positioned in such a way as to be revealed in sequence rather than simultaneously (Thomas, 1991: 217), however, as previously discussed, at certain locations along the route more than one ritual site can be seen from a single point.
A cursus and second avenue, albeit of bank and ditch construction, have been located at Stonehenge; antler from the primary silts of the ditch of the avenue have produced an average radiocarbon date of 1749 ± 69 BC - contemporary with an antler from the pit of stone 56 of Stonehenge, one of the inner trilithons, which produced a date of 1720 ± 150 BC (Barrett, 1994: 67).

From Stonehenge Bottom, the avenue is on a west-south-west alignment but turns to the south after half a kilometre.
After crossing King Barrow Ridge, north of a series of Bronze Age barrows, it turns again to the south-east, between two pre-existing round barrows, on approach to Stonehenge (Barrett, 1994: 47).

At this point, the stone circles provide excellent screening of the activities within the enclosure (Barrett, 1994: 43).
Once again, the avenue has provided a processional route, observing pre-existing monuments, before arriving at the centre of the contemporary ritual world.

The respect shown for pre-existing monuments and the formality of design possibly reflects ancestor worship.
Whatever the reason for reusing house and tomb design and the imitation of the surrounding landscape, the concentration of monuments does suggest a ritual complex.

Conclusion
The purpose of this essay was to ascertain the definition and contemporary use of the phrase 'ritual landscape'.

The origin of the phrase described a concentration and possible dominance of monuments in a landscape to the detriment of everyday life; however, recent excavations have located settlements and industrial areas within visual distance of these ritual sites.
The clustering of monuments and their connection by way of processional routes implies the use of the entire landscape rather than isolated sites; this may include showing respect to ancestral monuments or their incorporation into contemporary rituals.

Although the term 'ritual landscape' is now unpopular, and the term 'ritual complex' is little better, there appears difficulty when trying to describe an inhabited area with a concentration of monuments without implying a total absence of settlements; perhaps the phrase 'ritual settlement' is more applicable.
1. Abstract

A platinum film thermometer and a thermistor thermometer were calibrated over the range 0°C - 50°C so as to quantify their response characteristics and associated errors.
The sensitivity and resolution of each instrument was calculated and these were used to compare the instruments.

Recommendations are made as to the suitability of these instruments for meteorological applications.
2. Introduction

Thermometry involves measuring and quantifying the temperature of a medium.
Thermometers are based upon the principle of measuring a known physical quantity that varies with temperature.

Resistance thermometers measure the change in electrical resistance of an electrical conductor.
The relationship between resistance and temperature can be mathematically quantified.

Thus, if the resistance of a given material is known, the temperature can be calculated (DeFelice, 1998; Met Office, 1981; Strangeways, 2003).
Many metals exhibit such behaviour, for example nickel, iron and copper.

Platinum is commonly used in resistance thermometers owing to its ability to maintain its characteristics over a long period, resistance to corrosion and relative linearity in response.
These factors help minimise deterioration in the instruments' performance over time and allow for a simple calculation to determine temperature from the recorded resistance (Met Office, 1981; Strangeways, 2003).

The response characteristic of platinum resistance thermometers is accurately approximated by FORMULA where R is resistance (Ω) at temperature T (°C), R 0 is resistance (Ω) at standard temperature T 0 (taken as 0°C) and α and β are constants characteristic of the instrument (in °C -1).
Over the atmospheric range of temperature, the quadratic term in equation (1) is much smaller than the linear term.

Therefore, an accurate meteorological approximation to the characteristic is given by FORMULA where R is resistance (Ω) at temperature T (°C), R 0 is the resistance at 0°C and b= R 0α ( Ω°C -1) (DeFelice, 1998; Met Office, 1981; Strangeways, 2003).
Thermistors are an alternative type of resistance thermometer.

These are semiconductor resistors that display a larger change in resistance with temperature than metals.
This change in resistance may be either positive or negative.

Thermistor sensors can be engineered to be very small, thus lowering the thermal capacity and time-constant.
The characteristic of thermistors can be expressed as FORMULA where R is thermistor resistance (Ω) at temperature T (K), A is the characteristic resistance (Ω) and B is the characteristic temperature (K).

It should be noted that this is not a perfect fit and that the error may be up to 5% of the actual resistance.
It follows that this equation may be simplified to give the linear relationship FORMULA (DeFelice, 1998; Met Office, 1981; Strangeways, 2003).

This paper describes the calibration of a platinum film resistance thermometer and a thermistor thermometer over the range 0°C to 50°C (273.15 K to 323.15 K).
The values of all constants in equations 1 to 4 will be determined for the instruments used, and the calibration errors quantified.

The characteristics of the instruments will then be compared.
3. Experimental Method

The platinum film resistance thermometer (PFT) and thermistor thermometer were immersed in a thermostatically controlled water bath.
A precision platinum resistance thermometer was also immersed in the bath, and was used to record the 'actual' temperature of the water, required for calibrating the other two thermometers.

The PFT and thermistor thermometer were each connected to individual multi-meters, which displayed the resistance of the respective instruments.
The resolutions of the multi-meters differed, with the meter connected to the platinum film thermometer having a resolution of 0.001 Ω, and the meter connected to the thermistor thermometer having a resolution of 1 Ω.

The temperature of the water bath was raised from 0°C to 50°C in approximately 5°C intervals.
Resistance was recorded for the PFT and thermistor thermometer when the precision thermometer indicated that the water temperature was relatively stable (i.e. varying by only approximately 0.01°C s -1).

Resistance readings were taken simultaneously for both thermometers so as to avoid errors arising from differing lag characteristics
4. Results

The results arising from the experiment are shown in Appendix 1.
4.1 Basic Analysis

Resistance was plotted against temperature for both thermometers.
Figure 1 shows the resulting characteristics for the platinum film thermometer.

Error bars were not plotted as errors were in the order of 10 -3 Ω, and would thus not appear on the scale used in figure 1.
As described in section 1, a linear relationship exists.

Using ordinary least squares regression, the constants R 0, b and α (equation 2) were estimated as follows: FORMULA where the quoted errors represent 2 standard errors of the mean.
Figure 2 shows the response of the thermistor thermometer.

Again, error bars have not been plotted owing to their small order of magnitude (1 Ω) relative to the plotted scale.
Thus the values of b and ln (A) (equation 4) were calculated as: FORMULA Therefore: FORMULA (quoted errors represent 2 standard errors of the mean) These values were used in equation 3 to produce estimated values of R at 1°C intervals between 0°C and 50°C.

These estimates were then plotted alongside the observed data as shown in figure 3 The observed data in figure 3 shows a close correlation with the fitted data, being within the 5% error margin expected.
This is not surprising, since the fitted data was derived using the calibration data arising from the observations, but figure 3 at least confirms that the estimated values are within acceptable ranges.

4.2 Further Analysis
The resistance values for the platinum resistance thermometer determined by using equation 2 were subtracted from the observed resistance values recorded in Appendix 1.

The resulting residuals are plotted in figure 4.
The linear residuals show a marked variation, especially at either end of the experimental temperature range.

Therefore the response characteristic predicted by the linear relationship (equation 2) could be improved upon so as to increase the precision of the instrument.
This was investigated by fitting a quadratic equation (R = -5x10 -5T 2 + 0.3899T + 100.39) to the observed values.

The residuals generated by this method are also plotted on figure 4.
The quadratic residuals seem to reduce the variability of the predicted resistance relative to the observed resistance.

The quadratic readings are also generally consistent, in that they tend to slightly underestimate the actual data.
Whilst the residuals are relatively small in both the linear and quadratic cases, the improvement in accuracy gained by using the quadratic equation may prove important in precision thermometry within the temperature range 0°C - 50°C.

The sensitivity of the thermometers is given by the differentials of the relationships described in equations 1, 2 and 3 (the gradients of any plotted response characteristic).
In the case of the platinum film thermometer, this is relatively easy to calculate.

It is slightly more complicated for the thermistor thermometer.
The derivations are detailed below FORMULA Using these equations, the sensitivity of each instrument was calculated.

For the platinum film thermometer, the sensitivity was derived by both the linear and quadratic methods for comparison.
The resolution of each thermometer was then calculated, assuming that the resolution of the digital meters used in the experiment (see section 3) was the only limiting factor.

Table 1 summarises these results.
Both thermometers show very high resolutions.

The thermistor shows the best resolutions, especially at lower temperatures.
The resolution of the thermistor appears to approach that of the platinum film thermometer as temperature increases.

The use of the quadratic equation for the platinum film resistor has a minimal effect in improving the resolution when compared with the linear equation.
So as to assess the measurement accuracy of the thermometers, consider the uncertainty on the recorded value of resistance.

Assuming a systematic uncertainty associated with the resistance (dR) of ±0.1%, the corresponding uncertainty associated with the measured value of temperature (dT) can be derived using the resolution of the instrument (in °C Ω-1) and the definition of resolution (resolution = dT/dR).
The range of uncertainty on temperature was thus calculated for both thermometers at 20°C, and found to be: FORMULA Therefore the platinum film thermometer can be said to have the superior measurement accuracy at 20°C.

The calibration errors are also of smaller magnitudes for the platinum film thermometer (of orders of 10 -4 and 10 -2) than for the thermistor thermometer (orders of 10 1 and10 -2).
If measurement accuracy were the most important consideration, then the platinum film thermometer would appear to be the instrument of choice, as smaller calibration errors will equate to lower uncertainty values.

5. Discussion
The calibration of the platinum film thermometer and thermistor allowed the relationships between temperature and resistance to be quantified for each instrument.

These were: FORMULA Investigation of the linear relationship for the platinum film thermometer revealed that the use of a quadratic equation (R = -5x10 -5T 2 + 0.3899T + 100.39) better approximated the observed response of the instrument.
The improvement in the range of accuracy was shown to be in the order of 0.01°C through the use of the quadratic equation.

Whilst this is likely to be of minimal importance for most atmospheric applications, it may be of use in high-precision thermometry and therefore should be noted.
Both thermometers were shown to have a high sensitivity relative to the resolutions of the digital meters used to measure resistance.

Thus either instrument will respond to a small change in environmental temperature with a detectable change in the resistance of the sensor.
It should be noted that in the case of the thermistor, the sensitivity of the instrument is not constant, and actually deteriorates with increasing temperature as a result of the response characteristic.

However, the sensitivity is still sufficiently high at 30°C for the thermistor thermometer to be comparable with the platinum film thermometer.
The resolution of each thermometer was also calculated, assuming that instrument resolution was only limited by that of the multi-meter used in the calibration.

The thermistor displayed the highest resolution, although this also deteriorated at higher temperatures, tending towards the constant resolution of the platinum film thermometer.
A brief investigation of the measured temperature uncertainty related to a known resistance uncertainty was undertaken.

This revealed that the platinum film thermometer had a smaller temperature uncertainty than the thermistor.
The platinum film thermometer also had lower calibration errors than the thermistor.

Overall, it would therefore be more desirable to use the platinum film thermometer for meteorological measurements.
This instrument has a relatively accurate linear relationship between resistance and temperature, which can be improved upon by use of a quadratic relationship if necessary.

Other aspects of the platinum film thermometer are relatively constant, such as the sensitivity and resistance, and show little reliance on temperature (in the quadratic case), or none (in the linear case), unlike the thermistor, for which these characteristics decline with increasing temperature.
Also, smaller calibration errors associated with the platinum film thermometer mean that more accurate results are likely when compared with the thermistor.

It would be wise to repeat this calibration experiment over several runs so as to determine the typical responses of each instrument.
Also recommended for future investigation is an increase of the experimental temperature range so as to determine the operating ranges of each instrument and if either instrument demonstrates better accuracy than the other within certain temperature ranges.

1. Introduction
This practical aims to assess the best method for determining the saturation vapour pressure (SVP) of water over the temperature range 268.2K - 313.2K (-5°C - +40°C).

The formulae referred to throughout the practical are shown in Appendix 1.
All exercises were carried out using MATLAB 7.0.4.

2. Comparison of formulae
A MATLAB script was created to calculate the values of SVP derived from the theoretical and empirical formulae for a given temperature range, and compare these values to measured values of SVP.

The script is shown below.
The output was written to a text file, which is included in Appendix 2.

FORMULA The data generated by the aforementioned script (and shown in Appendix 2) was used to plot a graph in MATLAB.
This graph is shown in Figure 1.

The two data sets derived from the theoretical and empirical formulae generally show a good correspondence to the measured data.
However, there does seem to be some divergence from the measured values at higher temperatures (above ~ 300K).

To determine which of the formulae represented the best approximation to the measured values, the measured value of SVP was subtracted from the corresponding value given by the theoretical and empirical formulae.
These differences were then plotted using MATLAB (see figure 2).

In this graph, y = 0 may be regarded as representing the measured values of SVP, whilst the lines represent the difference between the measured values and those derived from the formulae.
The empirical formula for deriving saturation vapour pressure appears to correspond closest to the measured values over the temperature range from 268K to 313K.

The kink in the empirical SVP line at ~311K in figure 2 is the result of an error correction made to the measured SVP data in a previous exercise.
The presence of this feature suggests that the corrected value of measured SVP is not completely accurate, and may be slightly too low.

However, since this feature does not significantly distort the data, it may be considered a negligible error within the context of this assignment.
3. Accounting for variation of L with temperature

It is possible to derive another formula to calculate SVP (see Appendix 1).
As with the theoretical formula previously discussed, the new formula is derived from the Clausius-Clapeyron equation.

However, whereas the theoretical formula assumes that L (the latent heat of condensation of water) is constant, the new derivation accounts for the fact that L varies with temperature.
Therefore a relationship must be established between T and L. Figure 3 illustrates this relationship, assuming L is a linear function of temperature.

The data was taken from the Department of Meteorology data sheet.
A best fit line fitted to this data specifies the approximate relationship between temperature and the latent heat of condensation of water.

Of particular interest is the value of the gradient (-0.0024 J kg -1 K -1), which approximates the rate of change of L with temperature.
This value may be used in the new Clausius-Clapeyron equation for SVP.

The Clausius-Clapeyron SVP values were calculated using this value, and the difference from the measured values was determined by the same process used to generate figure 2.
These results were compared with those for the empirical formula, currently the best approximation available to the measured values.

This comparison is shown in figure 4.
As in figure 2, the line y = 0 represents the measured values.

Figure 4 indicates that accounting for the variation in L with temperature has little effect on improving the accuracy of the theoretical SVP derived from the Clausius-Clapeyron equation.
The empirical data remains the best approximation to the measured data available for the range of temperatures considered.

4. Conclusions
The empirical method for deriving SVP has been shown to correspond better to measured values of SVP than the formulae derived from the Clausius-Clapeyron equation across the entire temperature range.

This is not surprising since, as its name suggests, the empirical formula has been derived from empirical (observed) data.
The inaccuracies of the Clausius-Clapeyron based formulae lie in their mathematical handling of the latent heat of condensation of water (L).

The theoretical formula assumes L to be constant across the range of temperatures considered (dL/dT = 0).
As figure 3 demonstrates, this is not the case.

However, even when dL/dT ≠ 0 is accounted for by a refined Clausius-Clapeyron derivation of SVP, the approximation is still relatively poor (figure 4).
It is possible that this results from oversimplifying the relationship between L and T by assuming it to be linear.

Improved approximations may be achieved by better defining this relationship, at the cost of a more complex formula defining SVP for a given temperature.
There appears to be little practical use for such a formula, given the accuracy of the empirical version.

Introduction
This report describes the findings of a series of experiments using a simple climate model.

The model was provided in the form of an Excel spreadsheet and allows the user to specify radiative forcings and climate feedback parameters.
The model uses the specified parameters to calculate global mean temperature anomalies, and compares these values with observed anomalies for the period 1850 - 2006.

Any figures or tables referred to may be found in the Appendices.
Part A

The spreadsheet model was used to attempt to identify those radiative forcings, or combination of forcings responsible for the observed temperature changes since 1850, assuming a mid-range climate feedback parameter of 1.3 Wm -2K -1.
The model allows individual forcing mechanisms to be turned on or off, and calculates the cumulative forcing and the resulting surface temperature change.

The predicted surface temperature anomaly was qualitatively compared with the observed temperature anomaly from 1850-1879 by means of graphs, and qualitatively compared in terms of the χ2 error.
By only turning the well mixed greenhouse gases (GHG) forcing on, it can be seen that the model predicts a positive radiative forcing, and subsequent temperature rise (figure A1).

However, the predicted temperature rise only consistently matches the observed temperatures in the early part of the record (1850-1890).
This may be partly due to the large error in the early observations.

After about 1890, the model is systematically overestimating the observed temperatures.
Throughout the observational record, high frequency fluctuations are present which are not captured by the model.

This would suggest that the radiative forcing attributable to GHGs is being somewhat offset by other forcing mechanisms, some of which are working on shorter timescales to cause short-term variation in the historic record.
However, the general trend of rising temperatures in the GHG only model matches the overall trend in the observations.

The χ2 error of 501.72 should therefore be improved upon by the inclusion of other forcings.
The model shows a better fit with the observations when all anthropogenic forcings are turned on (figure A2), with the χ2 error reduced to 330.01.

This confirms the above suggestion that some forcings are apparently negative, thus offsetting the warming of the GHG only scenario.
It is interesting to note that the omission of the aerosol indirect effect improves the χ2 error, reducing the value to 217.71 (figure A3).

It would seem that the importance of the aerosol indirect effect was initially overestimated, hence the reduction in the error resulting from its omission.
This version of the model still does not seem to reflect short-term oscillations in the observation record, such as the cooling events of 1855-60, 1880-95, 1900-1910 and 1940-50 and subsequent recoveries.

It therefore seems likely that these events were the result of some natural process, or an anthropogenic mechanism not considered by the model.
The error should therefore be further minimised by incorporating such forcings.

The natural forcings considered in the model have a cooling effect (figure A4).
Examination of each of the natural mechanisms reveals that volcanic forcing is the dominant of the two natural mechanisms.

Cooling resulting from volcanic forcing is unique, in that the cooling effect is sudden, typically occurring over a period of 4-5 years whilst the recovery tends to be more gradual (~10 years).
Several large volcanic eruptions may be identified within the model, most notably those of Krakatoa (1883), Agung (1963), El Chichon (1982) and Mount Pinatubo (1991).

However, by only considering the natural forcings, the model considerably underestimates the observed temperature anomalies, predicting a generally stable global mean surface temperature, with frequent cooling events.
This is emphasised by the relatively high value of χ2 (2319.02).

This suggests that natural forcings alone are unlikely to be responsible for the observed warming trend, although natural forcings may contribute to the high frequency variations in the observed data.
When all forcings are switched on, the model shows a reasonable fit with the observations (figure A5).

For much of the period under consideration, the model predicts temperature anomalies within the range of instrumental error.
The most notable departures of the model from the observed data seem to be associated with the large volcanic events noted previously, for which the model consistently overestimates the associated cooling effect.

It is likely that it is this sensitivity of the model to volcanic events that has caused the relatively high value of χ2.
It is interesting to note that the model consistently underestimates the observed temperature anomaly since about 1960.

Overall, it would appear that the general warming trend since 1850 is driven by anthropogenic radiative forcings, since these produce the smallest errors when compared with observations.
Large volcanic eruptions tend to reduce the global temperatures, although in the latter half of the 20 th century such events seem to merely slow the rate of warming in the observed data rather than to induce a period of cooling.

The effects of such events are seemingly overestimated by the model.
Part B

This section will explore the effect of the uncertainty in the climate feedback parameter, Y.
The Intergovernmental Panel on Climate Change (IPCC) estimate the value of this parameter to lie within the range 0.8 - 3.3 Wm -2K -1.

To simplify the investigation, an arbitrary radiative forcing mechanism was considered.
This forcing was taken to be non-existent before 1900, and constant since 1900, simulating a step-change in radiative forcing, and consequently a step-change in the equilibrium temperature (the new "stable" temperature resulting from a given forcing).

The response of the model temperatures is compared to the change in the equilibrium temperature for differing values of Y.
At the lowest value of Y, the response of the model is slow, only reaching about a third of the equilibrium temperature within 5 years (figure B1).

The response curve then stabilises and only predicts a temperature rise of about two-thirds after 100 years.
At the upper end of the range of Y the equilibrium temperature is lower, and the model responds more rapidly, although the equilibrium temperature is still not reached (figure B2).

Therefore, the climate feedback parameter determines both the magnitude and speed of climate response.
A lower value results in a slower response, but a greater temperature change over time (figure B1).

Higher values result in quicker responses, but generally lesser temperature changes (figure B2).
The uncertainty in Y therefore has implications for assessing the observed temperature rises.

If the true value of Y lies at the upper end of the range of uncertainty, then it is likely that much of the warming due to anthropogenic forcing has already occurred owing to the rapid response of the climate system.
If this forcing can be reduced then global temperatures may stabilise relatively quickly.

However, if Y lies at the lower end of the range of uncertainty, then the climate system may not yet have fully responded to anthropogenic forcing and the trend of rising temperatures will continue.
In this scenario, warming is likely to continue even if anthropogenic forcing is reduced due to the slower response time of the climate system.

Reducing the uncertainty in Y is therefore an important area of research, as this will determine the response characteristics of the climate system to radiative forcing and reduce the uncertainty in predicted future trends.
Improved scientific confidence in predictions may better facilitate the formulation and implementation of appropriate mitigation strategies.

Part C
This section includes a hypothetical forcing arising from stratospheric water vapour changes.

This effect is assumed to be negligible prior to 1960, but increases linearly from 1960 (by 0.013 Wm -2 Yr -1), rising to a maximum of 0.6 Wm -2 in 2006.
The climate feedback parameter was set at the mid-range value assumed in Part A (1.3 Wm -2K -1).

As comparison of figures C1 and A5 shows, the inclusion of the proposed additional forcing improves the fit between model and observations, bringing the model temperature anomalies within the range of observational error by the end of the observed time period.
This improvement is quantified by the reduction in the χ2 error to 726.78 (compared with the all-forcing value of 1025.54 noted in figure A5).

This demonstrates that additional forcings may be shown to be physically consistent with observed data.
However, this should not be regarded as proof that a particular forcing mechanism exists in reality.

If such forcings are shown to be consistent with the observations, they should be further investigated so as to establish whether they are present within the climate system and behave as anticipated by the model.
The effects can then be incorporated into climate models, thus reducing the uncertainties inherent within models regarding relevant forcing mechanisms and therefore future climate predictions.

Part D
In this section, the values of the climate feedback parameter and the scaling of each forcing were altered so as to minimise the χ2 error.

This was attempted by adhering to the published uncertainty in scaling factors according to the IPCC Third Assessment Report (TAR), and by not adhering to the published uncertainties.
The methodology adopted builds upon the findings of Part A.

The fit of the model temperatures to the observations was improved by the appropriate adjustment of specific scaling factors and the climate feedback parameter.
For example, it was noted in Part A that the model demonstrated a particular sensitivity to volcanic forcing, thus the scaling of this mechanism was reduced so as to minimise the resulting χ2 error.

There was an element of trial and error involved in this methodology as it was necessary to 'home in' on the optimum scaling values for each forcing mechanism.
Table D1 shows the final values of the scaling factors and the resulting χ2 errors.

Using the published uncertainty estimates, an error of 125.69 was achieved.
As may be observed in figure D1, the model is still overly sensitive to the effects of volcanic eruptions, despite this scaling being set to its minimum estimated value.

Solar forcing was also set to a minimum, and the relatively small error obtained therefore suggests that anthropogenic forcing has a strong influence over the observed changes, thereby supporting the IPCC stance.
This result is not wholly unexpected, since it is the IPCC uncertainty estimates that were used to constrain the scaling factors.

The model performs relatively poorly in the early part of the 20 th Century, and this may be due to the underestimation of the scaling factor for any of the forcings, although such an error could not be identified.
If this realisation of the model were to be accepted as accurately representing the climate system, GHGs, tropospheric ozone and soot aerosols would be the most important forcings responsible for the observed temperature changes.

A modest improvement in the error value (93.94) was achieved by not constraining the scaling factors according to the TAR recommendations, resulting in a slightly smoother fit to the observed data (figure D2).
There are some notable contrasts between some of the scaling factors in these two realisations of the model (Table D1).

For example, volcanic forcing has been reduced considerably below its TAR-estimated minimum and organic carbon aerosols have no associated radiative forcing.
The dominant forcings now appear to be stratospheric and tropospheric ozone, biomass burning, indirect aerosols, solar variability and GHGs.

Whilst this remains consistent with the IPCC stance on dominant anthropogenic forcing, the situation has become more complex since there is a greater range of forcing mechanisms at work, in a more sensitive climate.
This makes the attribution of observed climate change a more difficult task, and some uncertainty therefore has to be accepted as a trade-off for quantifying the relative importance of the most likely radiative forcing mechanisms.

Conclusions
This report has demonstrated some of the difficulties inherent in accurately attributing observed global temperature trends to specific causes.

Whilst models based on scientific knowledge permit some investigation of the attribution issue, there are limitations relating to the uncertainty estimates of the controlling parameters as well as to the ability of models to accurately capture all atmospheric processes.
Any apparent agreement between modelled and observed data should always be investigated in order to understand any specific atmospheric phenomena causing the correlation, thus avoiding uncertainty arising from spurious conjecture based solely on model output.

1. Introduction
This report summarises the weather observed during the 2006 "Experiencing the Weather" field weekend.

The following datasets were gathered over the weekend and shall be referenced throughout the report:
Measurements taken by students using hand-held instruments;Data from an automated mast at Durlston Head;Data from an automated weather station at Leeson House;Radiosonde data from launches at Durlston Head;Synoptic charts and satellite imagery provided by Ross Reynolds.

The weekend was held near Swanage, Dorset in southern England (see Appendix 1 for map), between Friday 27/10/06 and Sunday 29/10/06.
All times given are in UTC, so as to correspond with synoptic charts and satellite imagery.

2. Surface Observations: Walk and Mast Data.
Five groups of students undertook walks within the study area, taking meteorological observations at regular intervals.

On the Saturday morning, a group led by Giles Harrison walked from Durlston Head to Leeson House, via Dancing Ledge (see map, Appendix 1).
The observed temperature and humidity data are plotted in figure 1.

The most striking feature of the temperature graph is the marked drop in the dry bulb temperature (-0.8°C), rise in the wet bulb temperature (+0.8°C), and the resulting increase in relative humidity (+9%) that occurs between the 0800 and 0830 readings.
The corresponding time series for the automatic weather station at Leeson House (fig 3.a) and the automated mast at Durlston Head (fig 3.b) do not show such a signal.

This suggests that the fall in air temperature and the rise in relative humidity observed was not the result of any sudden atmospheric phenomena as it was not captured by either of the automated masts.
The 0800 readings were taken at Leeson House, and the 0830 readings at Durlston Head.

It would seem that the different observation locations are responsible for the change displayed in the data.
This would therefore suggest that the coastal site (Durlston Head) was slightly cooler and more humid than the inland site (Leeson House).

Reconsidering figure 3, it would seem that Durlston Head experiences a smaller temperature variation throughout Saturday 28/10/06 than Leeson House.
These observations may indicate the thermal 'damping' effects of a large water body (in this case, the English Channel).

Water has a greater heat capacity than land, thus it gains and loses heat at a slower rate than the land.
Therefore, measurements made inland can be expected to show a greater diurnal temperature variation than measurements near large water bodies.

However, it should be noted that this signature within the data may result from the use of incomparable instruments at each site, or even the specific siting of each instrument.
It is interesting to note that this does seem to be a general effect, with temperature data gathered by groups on coastal walks (figure 1) showing less variation than the Leeson House measurements (figure 3a).

Another interesting feature reflected in some of the walk data is the drop in air temperature at about 1400 on Saturday 28/10/06 (figure 4).
It should be noted that the data in figure 4 was taken at the same location from 1330 onwards, so the effect of temperature varying with elevation or location can be discounted.

This drop in temperature, of approximately 1°C is also present in the Leeson House and Durlston Head mast data (figure 3).
It is possible that this is due to the passing of the trailing cold front belonging to the weak frontal system shown just to the south west of the UK in the 0600 analysis chart for 28/10/06 (Appendix 2b).

This would introduce slightly cooler air into the study area, as reflected by the temperature data.
This system also appears to slightly reduce the air pressure on Saturday 28/10/06 (figure 5).

The pressure appears to rise again as the high pressure systems reassert themselves early on Sunday 29/10/06 (Appendix 2c), just prior to the end of the pressure observations.
Figure 6 shows the data gathered by the group led by Ken Spiers.

This group took an inland route from the Isle of Purbeck Golf Club to Leeson House (Appendix 1).
Although the readings cover a shorter period of time than for other groups, there is an interesting phenomenon captured by this group's observations.

Of particular interest is the coincident drop in temperature and pressure and subsequent recovery that occurs in this data.
This signal is not found in any other dataset.

The route taken by this group was unique in that it crossed a large ridge (Appendix 1).
Closer inspection of the data reveals that the lowest pressure and temperature readings were made at the highest elevation (~200m above sea level), as would be expected.

The subsequent rise in temperature and pressure coincides with the group's descent from the ridge on the approach to Leeson House.
Although other groups experienced temperature and pressure fluctuations associated with changes in elevation, none were as marked by such a large range in altitude.

It is also interesting to note that the humidity is highest at the greatest elevation, likely due to the group being closer to the low cloud base at this point
3. Vertical Atmospheric Profiles: Radiosonde Ascents.

Each group launched a radiosonde from Durlston Head during the weekend.
An additional launch was undertaken by the Department of Meteorology staff on Friday 27/10/06.

It is immediately obvious from all the tephigrams (figures 7, 9, and 10) that the atmosphere was generally stable throughout the weekend, thus convective uplift would have been inhibited.
This is reflected in the observed cloud patterns, dominated by stratus on Saturday 28/10/06 and high-level clouds on Sunday 29/10/06, and with minimal occurrence of cumulus throughout the weekend.

The tephigram for Friday 27/10/06 (figure 7) shows a reasonably constant air temperature between 900 and 800 hPa (approx. 1-2km).
There is also a region of low relative humidity between 950-600 hPa (500m - 4km).

This would suggest that there is little low and mid-level cloud present during the radiosonde ascent.
Comparison with radiation data from the Durlston Head mast (figure 8) and satellite imagery (Appendix 3) confirm this suspicion.

The inversion at ~250 hPa (~10km) in the tephigram suggests that this is the level of the tropopause.
This feature is not visible on the other ascents carried out during the weekend (figures 9 & 10).

A lower tropopause on 27/10/06 would imply higher pressure on this day.
This is confirmed by the pressure data recorded by the Leeson House mast (figure 5).

It is therefore valid to suggest that the pressure drop during the evening of 27/10/06 is associated with the vertical expansion of the troposphere.
The frontal system that moves through the area on Saturday 28/10/06 is forcing warm air upwards, thus raising the height of the tropopause relative to the Earth's surface in this region.

The tephigrams for Saturday 28/10/06 (figure 9) show a generally moister profile.
The sky was obscured by low stratus clouds throughout the day, as observed by all students whilst on the walks.

Both tephigrams suggest a low lifting condensation level, and thus low cloud bases.
This is also consistent with observations, which generally estimated the cloud bases to be ~600m-700m.

The field calculations used an approximate lapse rate (6K/km) for ease of calculation, and hence slightly overestimated the cloud base heights.
The later tephigram also suggests that the cloud base rises slightly throughout the day, as noted by the field observations.

It is interesting to note that the observations of improved visibility seem to coincide with the lifting of the cloud base.
Since the cloud is not convective, as established by the stable atmospheric conditions and field observations, it must be formed by frontal mechanisms.

The synoptic chart for 28/10/06 at 0600 (Appendix 2b) shows a frontal system to the south west of the UK.
It is possible that the stratus observed throughout 28/10/06 was associated with this system.

The warm air flowing from the south west would have been forced over the surrounding cooler, denser air, thus providing the vertical lifting mechanism.
The observed stratus clouds may have been caused by the associated warm fronts, whose shallow gradients would have resulted in condensation over a wide area causing complete obscuring of the sky.

This scenario is consistent with the observations and the surface analyses.
It is interesting to note that no passing warm front (indicated by a rapid temperature rise) is detected by the automated masts (figure 3).

This would suggest that the warm fronts passed close to the study area, but did not actually pass across it.
However, a small drop in temperature is recorded between 1400 and 1500.

As previously mentioned, this suggests that the cold front of the system did pass over the study area.
Given the relatively small temperature drop, and lack of any associated rainfall, it would seem that this is a very weak front, with low temperature contrast between the two air masses.

The tephigram of 1502 suggests the presence of such a feature, as indicated by an inversion at ~950mb.
This may be the signature of a warm air mass being undercut by a cooler one, as occurs at fronts The tephigram for 29/10/06 (figure 10) shows the lower atmosphere to be less humid than the preceding day.

Some cloud is present, in the form of cumulus (probably formed by the second passing cold front - see chart) as well as high-level clouds (cirrus, cirrostratus - see Appendix 3).
The tephigram does not suggest the presence of such clouds.

This is most likely due to the vertical resolution with which it was plotted (50 hPa).
Any variation over smaller pressure scales will be overlooked at this resolution, and this effect will become more severe with increased height.

The high level clouds observed are typically associated with a distant warm front.
In this case, this would be the warm front to the south of Ireland at 0000 (Appendix 2c).

4. Conclusions
The weekend was dominated by a high pressure system, with the passing of a weak frontal system on Saturday (28/10/06).

The weather was relatively warm throughout, mainly due to a dominant south-westerly airflow drawing warm air from lower latitudes.
There was little cloud on Friday (27/10/06), but almost complete cloud cover on Saturday, associated with the weak frontal system moving over the English Channel.

The trailing cold front of this system appeared to pass over the study area between 1400 and 1500 on Saturday.
Sunday (29/10/06) was clearer than Saturday, and there was evidence of another frontal system approaching.

Interestingly, the pressure appeared to rise early on Sunday (figure 5), perhaps as the region of high pressure over France began to extend north-westwards (Appendix 2c).
Abstract

A computer model was used to characterise the evolution of urban and rural boundary layers.
Physical mechanisms are proposed for the observed boundary layer evolutions.

Vertical distribution of a pollutant was also calculated for each environment, and links with boundary layer evolution suggested.
The practical implications of the findings are considered.

1. Introduction
This practical shall investigate the diurnal evolution of the atmospheric boundary layer and the vertical distribution of a pollutant emitted at a constant rate from the surface within the boundary layer.

The boundary layer is simulated using a 1D computer model driven by surface sensible heat fluxes and geostrophic winds.
The model calculates vertical profiles of potential temperature (θ), pollutant mixing ratio and the east-west (u) and north-south (v) components of the horizontal wind speed.

These outputs are displayed on a series of graphs, some of which shall be used within this report.
It is also possible for the user to vary certain non-meteorological physical parameters such as roughness length, latitude, and pollutant lifetime.

Within this investigation, most of the parameters shall remain constant.
Appendix 1 shows those parameters used to define a rural environment.

Alterations to any of these parameters will be explicitly noted within the text of the report.
2. Results

2.1 Diurnal evolution of a rural boundary layer.
The model was run using the input parameters as listed in Appendix 1.

Figure 1 shows the diurnal evolution of the vertical potential temperature profile as predicted by the model.
Within the boundary layer, structure becomes apparent as shown in figure 1.

This is the result of surface emission of sensible heat creating unstable conditions conducive to convective uplift, and turbulent processes arising from vertical wind shear.
These two effects combine during daylight hours to promote the formation of large turbulent eddies, resulting in the formation of a well-mixed layer (1) above a convectively unstable surface layer (2).

The upper limit of the boundary layer is marked by a sharp inversion in θ (3), representing a region of stability inhibiting further ascent of air parcels.
Above the inversion is the free atmosphere (4), which is also convectively stable.

During daylight hours, progressively more energy enters the system as sensible heat, and so the boundary layer extends upward.
At night, the air above the surface is warmer than the surface, and so there is a sensible heat flux to the ground, resulting in radiative cooling of the air.

This forms a region of stability near the surface, known as the nocturnal boundary layer (NBL) which inhibits the mixing observed during daylight hours (5).
The well mixed-layer remains above the NBL as a residual layer (6), with a gradually weakening capping inversion.

Following sunrise, there is once again a sensible heat flux from the surface, and the NBL is eroded by the formation of a new surface layer (7), ultimately forming a boundary layer similar to that of the preceding day.
The height of the boundary layer therefore varies over a diurnal cycle.

This is illustrated in figure 2, and arises due the processes described above.
2.2.

Diurnal evolution of the urban boundary layer.
So as to simulate an urban environment, the following values were altered from the values shown in Appendix 1: FORMULA The model output is shown in figure 3 Figure 3 demonstrates that the urban boundary layer is typically deeper than rural boundary layers (figure 2) during the day and that the urban NBL is deeper than the rural NBL, with a weaker stable surface layer characterising the urban NBL.

This must be due to the effects of increased surface roughness and/or increased daytime and nightime sensible heat fluxes, as these were the only model parameters altered.
The model was re-run for each adjusted parameter so as to attempt to determine how each factor contributes to the evolution of the urban boundary layer.

Increasing surface roughness is analogous to introducing tall urban structures into the model.
This modification of the initial parameters deepens the daytime BL typically by approximately 200-300m relative to the rural case.

This may be physically explained by the wind shear being vertically displaced (see figure 4a), as surface roughness represents the height at which the horizontal wind speed is 0 ms -1 according to the logarithmic wind profile calculation.
This typically occurs at 10% of the height of the roughness elements (Arya, 1999; Holton, 2004).

This will result in the vertical displacement of turbulent eddies (see figure 4a).
Surface roughness also deepens the NBL, and results in a shallower θ profile.

This implies that nocturnal radiative cooling of the air is less intense when surface roughness is increased.
Physically, this may be understood as the surface having elements that penetrate further into the troposphere.

This causes cooling across a deeper slab of atmosphere than observed over a 'smoother' surface.
Since there is a greater volume of air being cooled than previously (but at the same rate), the resulting decrease in θ is less than that observed for the smoother' surface.

Figure 4b illustrates this concept schematically.
The sensible heat fluxes were then changed to the values shown on page 3 whilst holding the surface roughness constant (0.01m).

This simulates a lower proportion of surface area covered by vegetation (as would be expected in an urban area), resulting in less emitted energy being used to evaporate water, and thus a higher sensible heat flux.
Higher nightime minima correspond to the accumulation of heat by materials in urban environments, which have higher thermal capacities than the natural materials of rural environments (Hogan, 2007).

These factors resulted in a deepening of the daytime BL (approaching 2km), and a deepening of the NBL.
In this case the higher daytime surface flux creates a more unstable surface layer, promoting buoyancy and thus the production of larger turbulent eddies.

The higher value for nightime flux results in less vigorous cooling of the air, and therefore a shallower temperature gradient, implying a weaker region of stability and thus a deeper NBL.
The urban BL characteristics therefore represent the cumulative effects of increasing maximum and minimum heat fluxes and surface roughness.

The net effect is a deeper day and nightime BL in urban areas relative to the BL of rural areas.
2.3.

Pollutants in the Boundary Layer
Figure 5 shows the typical diurnal evolution of the pollutant mixing ratio within the rural boundary layer.

These results represent the effects of a long-lived pollutant, using the rural model parameters listed in Appendix 1.
Figure 5 shows that the pollutant is confined within the boundary layer throughout the day, demonstrating that there is negligible exchange between the boundary layer and the free atmosphere.

During the day, the pollutant is distributed evenly throughout the depth of the boundary layer (1), with a minor peak in concentrations near the surface (taken as the pollutant source in the model) (2).
As the boundary layer deepens throughout the day, the pollutant is dispersed across a greater volume by turbulent eddies, resulting in progressively lower concentrations - for example a typical well-mixed layer ratio of 1.2 g/kg at 1230, compared with a ratio of 1.05 g/kg at 1630 (figure 5).

This effect is most notable during the morning.
At night, the formation of the stable NBL confines the pollutant within a shallower portion of the atmosphere, resulting in progressively higher concentrations at lower levels as the pollutant accumulates, peaking before dawn (5).

The pollution model was re-run for the urban boundary layer, using the urban parameters from section 2.2, and the same pollutant lifetime as the rural case.
The resulting pollution mixing ratio profiles are shown in figure 6.

In the urban case, the pollution concentrations are generally lower due to the greater depth of the atmosphere through which the pollutant is mixed (see section 2.2).
During the nightime, the weaker NBL results in a less well-defined surface peak in concentrations than the rural case, with typical values of about half those of the rural boundary layer.

3. Conclusions
The model used within this study shows that the boundary layer of an urban area is generally deeper than that of a rural area in similar synoptic conditions.

This finding is consistent for both daytime and nocturnal boundary layers.
This effect is believed to arise as a result of the larger sensible heat fluxes and greater surface roughness that characterises urban areas.

The diurnal evolution of vertical pollution mixing profiles was also shown to differ between urban and rural environments.
Concentrations tend to be higher in the shallower rural boundary layer.

This effect is especially pronounced in the rural nocturnal boundary layer due to this being significantly shallower than the urban nocturnal boundary layer.
These findings suggest that the urban boundary layer is more efficient than the rural boundary layer at vertically dispersing pollutants, given similar synoptic conditions pollution emission rate (and lifetime) and latitude in both cases.

This will have health implications for those susceptible to the effects of pollutants, and suggests that urban residence may be more beneficial than rural residence if health were the only factor under consideration.
These findings may also be of interest to planners, suggesting that polluting industrial activities will have a lesser environmental impact in urban areas.

In order to evaluate these findings, it is recommended that pollution dispersion be examined in a 3-dimensional model under a range of synoptic conditions.
This would result in a more accurate representation of reality and would test the validity of these preliminary findings.

1. Introduction
1.1 What is this study about?

The main aim of this study is to describe the meanings and patterns of use of a nominal form of address, the vocative mate, which belongs to the group of general names known under the label of terms of friendship.
The starting points for the survey are the Oxford English Dictionary 2ed.

on CD-ROM version 3.0 (2002), A Dictionary of Epithets and Terms of Address by Leslie Dunkling (1990) and some of the most relevant references on systems of address, which permit a complete and detailed description of this vocative.
Moreover, data provided by the British National Corpus (BNC) are analyzed along the sociolinguistically relevant dimensions of gender, age, social class and level of formality, in order to present a picture of the uses and meanings of the term mate in today's British English.

The final aim of the study is to point out the areas of overlap with the previous literature and the evolution in the use of this vocative by English speakers.
1.2 Theories on address: a brief overview.

In every language and society, a reasonable question arises when people take part to communicative exchanges, namely whether and how participants will be addressed, named and described.
We can define the totality of available forms and strategies of address and their interrelations in one language as system of address (Braun, 1988).

The way in which the speaker of a certain language uses this repertory of variants can reflect her social and linguistic background, so that we can consider "personal address [...] a sociolinguistic subject par excellence" (Philipsen and Huspek 1985: 94).
Roger Brown and his colleagues are considered the initiators of modern sociolinguistic investigation on forms of address.

Throughout their papers (Brown and Gilman, 1960; Brown and Ford, 1961), they discuss the different use of pronominal forms along the dimensions of power and solidarity, pointing the attention to the interpersonal relationship amongst the participants of the interaction (mainly in terms of age, professional status and degree of intimacy).
Friederike Braun goes one step further in the investigation, emphasizing variation according to sociolinguistic factors like regional dialect, social class, education, age, gender, ideology, religion, etc.

All these features are the basis on which variants are created, both in the address repertoire and in the rules according to which forms are selected.
2. Forms of address and vocatives

Before introducing a more detailed classification of the forms of address, it is important to distinguish between bound forms (integrated parts of a sentence) and unbound forms (syntactically free forms; preceding, succeeding or being inserted into the sentence).
The most part of pronouns of address tend to appear as bound forms and carry a syntactic function of subject or object, while nouns of address occur as free forms.

To a certain extent, the term vocative can be considered a synonym of unbound form (including also unbound pronouns).
Vocatives share many properties with unbound items.

They are not integrated into the structure of the clause, which explains their variable word order (vocatives may precede, follow or interrupt a clause).
They are also independent from a prosodic point of view (they have a separate intonation profile).

Only their meanings and their functions within the social event of conversation are not totally independent.
According to a specific social component, there are rules that systematically govern the choices speakers make and the meanings receivers attribute to these same choices.

This social component consists of information about the dyad, namely speaker-addressee relationship, speaker evaluation of addressee (and situation), and speaker's social background as expressed in the use of a given form of address (Braun, 1988).
A substantial contribution to the studies of vocatives is given by Zwicky (1974).

In his paper the author point to the role of vocatives in getting the addressee's attention (call vocatives), and in maintaining or emphasizing the relationship between speaker and addressee (address vocatives).
He goes on underlining two important properties of vocatives: idiomaticity and sociolinguistic markedness.

In his opinion, vocatives convey much additional information, namely the general attitude of the speaker toward the addressee, the speaker's estimate of her status, the degree of intimacy between the participants, the level of politeness, the level of formality, the speaker's judgments about the addressee, the speaker's belonging to a particular subculture, social class, geographical dialect.
Since the context of use and the social characteristics of the participants to the conversation play such an important role in selecting the appropriate address form, it is not easy to define meanings and functions of a term of address in a definitive way.

The same problem exists also for a classification of vocatives.
A complete and detailed classification is proposed by Biber [et al.] (1999), who identify eight classes of terms of address in English.

The eight classes described by the authors can be grouped into four more general groups: proper names (first names, last names, nicknames, etc.), kinship terms, titles and occupational terms (honorifics, social titles, medical and academic professional titles, etc.), endearments and terms of friendship.
In this study I will point the attention to the last group.

The principal characteristic of these vocatives is that they cannot be defined only by formal or semantic features.
They are nouns and adjectives which convey general ideas of affection or friendliness and their literal meaning is only the starting point in the definition of their sense.

An exhaustive description of these forms can be proposed only relying on the context of use and on the function they serve.
In other words, their meaning is conventionalized to a certain extent, but it is the way they are used (and the information inferred by this use) which each time defines the exact meaning.

Another difference between this group of forms of address and the other (at least kinship terms and titles) is the great productivity of the former.
Endearments and terms of friendship can be considered an open class of items, in which linguistic creativity and individual imagination play an important role.

The group of endearments and friendship terms include words and expressions like angel, baby, boy, buddy, dear, doll, duckie, dude, feller, honey, kid, lad, love, lovely, mate, my darling, pal, sweetie, etc.
(Dunkling, 1990).

The main problem associated with the great heterogeneity of these vocatives is the halo of vagueness in meaning which surrounds them and the difficulty in defining whether a term is an endearment or a friendship term.
The gender of the speaker and of the addressee seems to be a determinant dimension.

Kramer (1975) affirms that women and men have different repertoires of address forms.
In fact, women tend to use terms of endearment, which are considered 'sweet' terms, to express fondness and affection, while men are more likely to address each other in a 'rough' manner, using friendship terms which convey a sense of camaraderie and male solidarity.

3. The vocative mate in the previous literature
In this section I will give a closer look to a form of address in particular: the vocative mate.

The main properties, functions and patterns of use of this vocative will be presented, relying on two dictionaries: the Oxford English Dictionary 2ed.
on CD-ROM version 3.0 and A Dictionary of Epithets and Terms of Address by L. Dunkling.

The entry for mate in the OED describes its referent as "a habitual companion, an associate, fellow, comrade".
The definition is completed with a statement which is very important for this study.

Here the OED says: "Used as a form of address by sailors, labourers, etc".
The main idea conveyed by the OED about this form of address is its reciprocal use among people from the working class.

From his part, Dunkling (1990) provides an analysis oriented more on usage and social meanings, pointing out information about uses, connotations and variations.
Biber [et al.] (1999) in the Longman Grammar of Spoken and Written English considers mate a marker of familiarity in British relationships between speaker and hearer.

Before proceeding with the corpus analysis, it is worth giving a brief summary of the main characteristics and patterns of use of the vocative mate found in the relevant literature: Mate is a term of friendship, in particular a camaraderie form, and it conveys the general meaning of 'companion', 'friend', 'comrade'; Mate is used mainly by men addressing other men amongst working-class speakers (sailors, labourers, colleagues, etc.) ; Mate has the function of maintaining and reinforcing the relationship between speaker and addressee; Mate appears frequently in informal contexts (service encounters, small talks, etc.); Some variants are possible, adding the suffix -y for diminutive forms ('matey') or the suffix -s for plural forms ('mates', 'mateys').
4. The vocative mate in the BNC

In the following section the vocative mate is analyzed more thoroughly through linguistic data provided by the British National Corpus through the Corpus Query System 'Sketch Engine'.
The BNC is a sample corpus of about 100 million words collected from both written (about 90%) and spoken texts (about 10%).

For the aims of this study, data were collected taking into account only occurrences from spoken texts to describe the spontaneous use of the term amongst English speakers in Great Britain.
A first selection within the Query Results was made in order to rule out homonyms.

All the occurrences in which the word mate is used as verb or as common noun have been removed whereas the cases in which mate is used as a vocative have been retained.
The total number of occurrences considered is 323, from 77 different texts.

Texts were recorded from 1990 to 1994.
The great deal of contextual information provided in the text headers allows us to consider characteristics of both speakers and texts in the discussion along the social dimensions of gender, age, social class and level of formality.

In what follows the main functions and patterns of use of mate will be pointed out.
4.1 Usage along social dimensions

4.1.1 Gender
The variable of gender provides striking results about the usage of the vocative mate.

Among a total of 124 different people, the number of female speakers is anything but irrelevant (31) if compared to male speakers (70).
The percentage is of about 26% for women and about 56% for men, but if we do not consider the group of 'sex unknown', they increase to 30% and 70% respectively.

These results are clearly in contrast with the descriptions in the literature, which considers mate a specifically male vocative.
Little can be said about differences in use between male speakers and female speakers, because the contextual information is often incomplete or unclear.

However, a general hypothesis can be outlined.
Women tend to use the vocative mate above all during conversations within the family, and sometimes in addressing members of the family (especially sons and daughters).

Uses amongst colleagues, friends and acquaintances are less frequent and seem to be in imitation of male behaviour.
This usage of mate expressing friendship and companionship is widespread amongst men.

4.1.2 Age
As far as the age of the speaker is concerned, it seems that the use of mate is not limited to a particular age group.

The data show that the vocative is distributed in all age groups, although a more frequent use is observed for men between 15-24 years old and 25-34 years old (about 23% and 19% respectively), and for women between 25-34 years old (about 26%) and 45-59 years old (about 22%).
The variation along the sex and age dimensions is summarized in Table I.

4.1.3 Social class
Another dimension which provides interesting results is that of speakers' social class.

All the authors who describe the uses of the vocative mate agree on the fact that it is a form principally employed by speakers from the working-class and lower middle-class groups.
After the analysis of the corpus, a different pattern may be outlined, although the sample observed is restricted to 41 speakers and 190 occurrences because of unclassified data.

While a prevalence of working-class speakers could still be true for the frequency of use (about 38% of the occurrences are produced by speakers from DE social class against a 13% produced by AB social class speakers), it is no longer so obvious when the number of speakers is taken into account.
The number of speakers, in fact, is more equally distributed among the different groups.

A more detailed description is presented in Table II below.
From a closer analysis of the data, an important bond between gender of the speaker and social class can be outlined.

Almost all the women who use mate as a form of address belong to the C2 and DE social class.
It means that the use of mate is still marked amongst women if considered along the social class dimension.

This is not the case with male speakers, who are distributed among the four social class groups.
A further observation needs to be made about the spreading use of mate amongst the students in secondary schools and universities.

The reciprocal usage of the vocative is intended as 'social cement' and plays an important role in the creation of closer in-group nets.
This phenomenon is surprising because it is extended to high social class students, in possible contrast with what was found by Ervin-Tripp: "In upper-class boarding schools, boys and some girls address each other by last name instead of first name" (1972: 224).

The formal use of the last name as form of address amongst upper-class students pointed out by Ervin-Tripp may have been replaced by the more informal use of the term of friendship mate.
[1]

4.1.4 Level of formality
According to the literature, mate is a vocative utilized mainly in informal settings, with a very informal connotation.

The BNC distinguishes two groups of spoken texts: 1- a demographic component of informal encounters recorded by a socially-stratified sample of respondents, selected by age group, sex, social class and geographic region.
2- a context-governed component of more formal encounters (meetings, debates, lectures, seminars, radio programmes and the like), categorized by topic and type of interaction (educational and informative, business, institutional, leisure).

The trend of informality claimed by the literature is supported by the kind of texts which make up the Query Results.
In a total of 77 texts, 52 of them are demographic texts (about 70%), while only 25 are context-governed texts (about 30%).

Furthermore, the majority of context-governed texts in which mate is used are business conversations (see Table III).
The spreading use of the vocative mate in these more formal contexts could be a signal of the increasing informality in social relationships.

4.2 Functions and patterns of use
Let's now analyse the occurrences in more details to point out the main functions and patterns of use of the vocative.

From a structural point of view the term tends to appear in final position, both in long and short sentences.
The concordances show that the vocative very often collocates with expressions of agreement such as alright, okay, right, yeah.

A few examples are found with expressions of disagreements such as no, not a lot, not really.
Moreover, mate collocates with informal greetings like cheerio, cheers, bye, hello, see you later, or thanks like thanks, thank you, and the colloquial ta.

Instances in which these expressions cumulate are very frequent and in many cases, the form of address can be surrounded by pauses and hesitators.
In addition, mate is quite common in utterances that underline the involvement of the speaker through expressions of surprise and excitement (interjections and words like cor, blimey).

[2] Another interesting co-occurrence is that of the vocative mate with expletives and taboo words, especially referring to sexuality.
These uses of swear words may be traced back to the high informality of the settings in which conversations take place, but above all they reveal the existence of very closed relationships between the participants (on the other hand, the usage of expletives as terms of address is common among very intimate friends - Braun (1988), Biber [et al.] (1999), Gramley and Pätzold (2004)).

[3] In all the examples discussed above, the use of the vocative mate implies a certain degree of friendliness, or at least an attitude which is not hostile to the addressee.
Only few utterances show an unfriendly connotation of mate, which is associated with offensive expressions like bollocks, fucking wanker, load of crap, up yours.

[4] Defining if a linguistic item has a positive or a negative connotation is not always an easy matter, especially when vocatives are concerned.
Words of this kind have an inherent pragmatic force and in most cases it is the way they are used which determines the precise meaning.

Partington (1998) tackles the problem of connotation analysing the semantic prosody of terms, assuming the spreading of connotational colouring beyond single word boundaries.
In other words, the favourable or unfavourable connotation of a term in an utterance is influenced by the positive or negative orientation of the words it collocates with.

As far as the vocative mate is concerned, corpus data show a statistical tendency toward a positive connotation, thanks to the contribution of favourable elements like greetings, expressions of agreement and gratitude, phrases of surprise and excitement.
Also the employment of certain swear words with ironic intent conveys friendly attitudes.

Only few concordances show a negative connotation of mate, due to the collocation with insults and offensive.
With reference to Zwicky (1978), who introduced a distinction between address vocatives and call vocatives, and according to the occurrences in the Query Results, the main function served by the vocative mate seems to be that of regulating social relations (address vocative).

It is employed both to maintain and reinforce the relationship between speaker and interlocutor (e.g. with greetings, expressions of agreement and gratitude, etc.) and, to a lesser extent, to create distance conveying unfriendliness and even aggressiveness.
On the contrary, examples in which mate can be considered a device for attracting the attention of the listener are very rare; in this case it often occurs together with proper names and interjections, which act like attention getters as well.

[5] Finally, with regard to formal variation, only few examples are recorded in the corpus: the diminutive form matey, the possessive form my mate, and the more intimate form old mate.
[6]

5. Conclusion
The more striking result of the study is the difference between the uses described in previous literature and the present uses of the vocative as recorded in the BNC.

Two main reasons may be suggested for the observed discrepancies.
The first refers to the work by Dunkling (1990), who examines the vocative in more details and gives information about meaning and patterns of use.

In this case the incongruity is probably due to the fact that the author describes the term of address drawing on English novels written in the 1950s, which are very distant from the effective use captured in the texts of the BNC, recorded in the 1990s.
The same is true for the essays published in the 1970s and in the 1980s, which may depict social habits (and linguistic behaviours) very different from today.

The second reason could be traced back to the Oxford English Dictionary providing a diachronic description rather than a detailed synchronic one.
The results point out that the usage of the vocative mate tends to be less socially marked than before, especially as far as the dimension of gender and status are concerned.

This does not mean that the term is used in the same way by men and women or by speakers from different social classes, but it is evident that a clear-cut distinction no longer exists.
Moreover, the vocative plays an important role in conversations amongst students, serving the function of 'social cement' and acting as an effective in-group marker.

Finally, it is important to notice the spreading use of mate amongst colleagues in business settings, while previously the vocative was reported to be limited to conversations within the lower working class.
This may also be considered a signal of the increasing informality in social relationships.

1. Introduction
1.1 Definition of 'literacy'

It is a difficult challenge to provide a unique and exhaustive definition of the term 'literacy' since many different studies reflecting a variety of theories and models have been conducted in the second part of the last century.
We can divide works into two traditions, the 'narrow' and the 'broad' (Williams, 2004).

The former one explores literacy from a psychological point of view, focusing on the individual and their development of initial reading and writing skills.
The latter, on the other hand, combines sociological and anthropological approaches, examining and evaluating literacy practices in social contexts.

From the 1980s onward, a gradual change in position has taken place, shifting from the traditional notion of basic reading and writing to context-specific communication and critical thinking skills.
More specifically, literacy is increasingly defined as the set of skills, knowledge and social practices that young people and adults use to perceive the world around them and to achieve personal and social goals.

1.2 What is this study about?
In the essay, I will discuss literacy practices of children attending the primary school in a small village in Italy in the second part of the 1980s (see appendix 1), describing different activities inside and outside the classroom according to both the narrow and the broad tradition.

The aim of the study is to describe the complex system of literacy practices in the primary school and to underline the importance of the family and the community for the development of children's literacy skills.
2. Literacy in the classroom

2.1 The National Curriculum
According to the National Curriculum the activities in primary schools are divided into two parts: in the first two years of education children concentrate on learning how to read, write and calculate, in order to gain sufficient confidence to master more complex texts and topics; in the last three years subjects like grammar, science, geography, history and geometry are introduced.

Furthermore, additional disciplines like 'physical education', 'education to sound and music' and 'education to images' were introduced in 1985 and became part of the curriculum as compulsory subjects in 1987, after being considered the Cinderellas of primary school for many years (Massenz, 2004).
Finally, religion is taught weekly in the 'religion hour' (appendix 2).

2.2 The role of the teacher
The teacher plays a fundamental role in the classroom; she is the mediator of literacy, the expert whom children rely upon.

Her duty is to create "a genuine literacy workshop" (Meek, 1990: 149), providing positive invitations, demonstrations, information and explanations that fit their understanding.
The transition from the nursery to primary school is not abrupt.

The teacher introduces children to literacy through recreational activities, including songs, rhymes and story telling.
The atmosphere of teaching is relaxed and informal, although the respective roles of teacher and learners are well defined and not negotiable.

Discipline is recognized as an important matter.
Teachers also play a key role in establishing profitable links between school and home, involving parents' close collaboration.

Parents are encouraged to become involved with their children's reading and writing, in the form of hearing them read, engaging in one-to-one interactions that elicit the discussion of the text (shared reading), providing writing opportunities related to the domestic life (short notes, shopping lists, compiling phone and address books entries etc.).
It is important to establish a continuity among literacy practices in the classroom and within the family, since home activities have been largely recognized as beneficial for children's literacy skills development (Hannon, 1995).

2.3 Learning to read
The first important step in early literacy is the acquisition of the alphabet principle (i.e. the sound-letter correspondence).

Italian is a rather easy language from this point of view, since it has a good sound-spelling correspondence (technically 'orthographically shallow').
The phonic approach to alphabet (Campbell, 1995) involves the learning of the names of the letters and their sounds, which are both taught in isolation and as a combination of vowel and consonant.

Thus, the notion of syllable is also introduced and children gradually learn to recognize words as strings of phonemes put one after the other.
This practice is highly structured and children learn through repetition of single words, whose syllables are beaten by claps of hands.

They also receive other inputs, like written words (sound-letter correspondence) and pictures (sound-meaning correspondence).
After a few days the phonics approach is substituted by another approach to reading which involves the actual reading of 'real books' (Campbell, 1995).

According to many experts in literacy analysis, this is a more effective strategy for the development of literacy skills in children and it does not prevent the teaching of phonics, which occurs incidentally and arises naturally out of the material being read (Schonell, 1961; Goodman, 1986).
Real books are predictable and meaningful, they are written with natural language.

Through them teachers can emphasize forward-moving narrative with logical connections and consequences.
They also present texts imparting knowledge about the author, explaining the reasons of the choice; they involve children in guessing activities about the content of the text and try to relate it to the readers' real life and experiences.

Gregory and Williams (2000) define this approach as 'modelling' strategies and underline how it enables children to interpret texts and the world around them, especially in communities that share the same cultural background.
Complementary to the 'modelling' approach are 'scaffolding' strategies, which aim "to help the child decode the text and are directly based on the text itself" (Gregory and Williams, 2000: 190).

They can be applied both in individual reading (teacher and child dyad) and in group reading, giving children "an excellent opportunity to scaffold each other's learning by providing answers, by correcting each other, allowing repetitions as well as echoing" (Gregory and Williams, 2000: 197).
2.4 Learning to write

In her work, Tolchinsky (1998) points out the regularities in children's writing development, describing four different stages: an undifferentiated and unconstrained series of signs ('scribbles'); a formally constrained writing, containing a limited number and a certain variety of letters; a syllabic period in which there is a term-to-term correspondence between syllables and written elements; and eventually an alphabetic stage, with a clear awareness of letters-phonemes correspondence.
The author also claims that "the acquisition of writing appears as a natural, endogenous driven construction [since] nobody taught these principles to the child" (Tolchinsky, 1998: 285).

As we have seen, in reading skills development children's first approach to written language occurs through phonics teaching, children being exposed to written words and to their pronunciations.
The identification of grapheme-phoneme relations seems to be the main strategy as far as writing is concerned, while global strategy, which involves the recognition of words as a whole without paying attention to letters, does not play any role in the acquisition of spelling (Goswami and Bryant, 1990).

An additional strategy is the teaching of spelling rules, especially for those words which are an exception in the regular orthography of the language (e.g. homophones, digraphs, graphic accents and apostrophes).
The main problem with these words is that learners cannot rely on their knowledge of the phonetics of the language.

The spelling in these cases is mainly based on grammatical concepts (e.g. word classes), which children of this age can only difficultly cope with.
The teacher usually tackles these issues through rhymes, short stories and songs, which convey the rules in a playful and relaxed atmosphere.

Another effective practice in this stage of acquisition of basic written competence is the dictation, since it is expressively focused on testing spelling abilities in a more formal way and it allows children to have a positive or negative feedback on their use of written language.
It is important to underline that reading and writing are integrated practices, which "develop together and support each other" (Goodman, 1986: 47).

In the first phases of autonomous writing, children rely on stories they have already heard or read in class and they try to relate them to their own personal experience.
Children's written language at this stage has the form of short sentences and the text often lacks in cohesion.

However, through regular reading and writing practices, children gradually improve their skills in terms of fluency, syntax and lexical complexity.
They acquire the notions of textual coherence and cohesion.

Moreover, they gradually build up background knowledge through the identification of different structures of texts according to different genres ('formal schemata').
2.5 The importance of additional disciplines

In this section I would like to draw the attention to the additional disciplines scheduled within the National Curriculum, namely 'physical education', 'education to sound and music' and 'education to images'.
Psychological and physical factors are very important for the learning process of children and 'physical education' can play an important role in the development of coordination, spatial and temporal organization skills, control of posture, good attention and concentration abilities, etc.

These are fundamental prerequisites for the normal acquisition of reading and writing skills (Massenz, 2004).
Moreover, the practice of gymnastic and other recreational activities in groups helps children to reinforce friendship relations amongst schoolmates and develop social qualities such as collaboration, cooperation, responsibility, self-control, self-discipline and leadership.

'Education to sound and music' is also very important in schools and it can be more than just a source of enjoyment and pleasure.
Many studies have focused on the relations between music and language development.

In particular, music can help to control the instruments of speech, making the voice more modulated, varied and expressive; it can be useful in identifying hearing problems and it enables to overcome them through special programmes working on pitch, duration, intensity, rhythm and timbre; it is effective in developing reading abilities, making fine aural discriminations between different sound, experiencing decoding and left to right orientation (Pugh and Pugh, 1998).
Douglas and Willatts (1994: 105) claim that "the abilities to process melodic as well as rhythmic aspects of music analytically may help to stimulate a similar response to language".

This can be noticed in the practice of learning nursery rhymes, finger plays, songs and poems, which leads naturally to the acquisition of phonological awareness which is an important part of children's subsequent reading development.
Furthermore, these practices help children in the recognition of onsets of words and rimes (opening and end unit respectively) and encourage them towards another advance in reading, being able to read new words by analogy (Goswami and Bryant, 1990).

During 'education to images' hours, children perceive the power of pictures and images in terms of expressiveness and communication.
They learn how to describe their feelings and experiences through different materials and techniques, being involved as active participants.

Moreover, the teacher promotes the discussion of pictures of different kinds (advertisements, photographs, paintings, etc.), stimulating children to evaluate meanings, communicative intentions and intents of persuasion.
Therefore, the approach to art and language of images since the first years of education seems to be an effective strategy both for the acquisition of communicative and expressive skills and for the development of analytical and critical abilities.

2.6 The 'religion hour'
The 'religion hour' is not limited to reading and commenting the Bible and the Gospel in class.

It is a literacy practice that can be analysed on three different levels.
First of all, the teacher reads the sacred texts and explains their content to the children.

It is the linguistic level, in which the stress is posed on the meaning of the message.
The second level of analysis is the critical level, in which the attention is put on the interpretation of the message and its implication for the lives of people.

In this part of the lesson the teacher encourages the interaction with the children, trying to relate spiritual issues to their experiences in everyday life.
She also introduces moral and ethical topics which are discussed comparing points of view of different religions and cultures in the world.

The religious practice in the classroom can be finally analysed on a social level.
Religion plays a fundamental role in the life of a small community.

It is a factor of cohesion, a point to which the individual members converge in a unique homogeneous group.
The teaching of religion is seen as an important matter for children, who are introduced to religion by members of the community, in religious classes and during the Sunday mass.

Therefore, the 'religion hour' acquires a significant value both from the educational and social point of view, connecting the school to the community.
3. Conclusion

As this brief analysis shows, literacy practices in the primary school are various and complex.
It is evident that literacy is far more than just learning how to read and write.

The different activities are expressly conceived to engage children in active reading and writing, encouraging discussions and sharing of ideas and experiences for the development of independent thoughts and judgements.
This is a critical approach to literacy, which aims at "a joint interactive construction of knowledge through critical enquiry" (Baker and Jones, 1998: 598).

Literacy is thus considered as a mean to develop consciousness and critical thinking habits; an instrument that helps children to learn how to explain and analyse the world and the society.
The teacher is not only the main mediator of literacy, but she also has an important role in integrating the school, the families and the community.

Teaching is not perceived as an independent and distinct activity, but it is embedded in a continuous dialogue and exchange of information among the school and the outside reality.
1. Introduction

Second Language Acquisition (SLA) can be defined as the process by which "people learn a language other than their mother tongue, inside or outside of a classroom" (Ellis, 1997: 3).
Describing and explaining how people acquire a second language is a fascinating challenge for researchers in many disciplines, not only from a theoretical point of view but also from a practical one.

For instance, a deep understanding of learning mechanisms can assist teachers in adopting the efficacious methodology, or it can help learners in developing effective strategies for learning a second language.
In approaching this question, I shall be focusing on the role of input, output and formal instruction according to the main theories on acquisition, relating them to my experience of learning Persian and pointing out the main reasons that led me to a failure in mastering the target language.

2. Different views of the role of input, output and formal instruction
2.1 The role of input and interaction

The role of input is of crucial importance in understanding the process of second language acquisition.
Many theories during the decades evaluated this variable and considered the efficacy of the information given to the learner.

Researchers' first attempt was to construct a model to explain systematically the nature of a child's first language acquisition, but sooner they extended their interests to adult second language learning assuming, sometimes quite carelessly, direct global analogies between first and second language acquisition (Brown, 1994).
The first approach I would like to introduce is known as behaviourism.

This theoretical position conceives language learning as a process of habit formation, which is environmentally determined and controlled from the outside through a stimulus-response conditioning.
Learners imitate models of language, producing responses which receive positive reinforcement if they are correct and negative reinforcement if they are incorrect.

The attention is put on observable data (i.e. the input received by the learner and the learner's own output), while no account is taken for what happens in the learner's mind.
In other words, for behaviourists "the environment is all, and the role of the organism is considered insignificant" (Johnson, 2001:43).

As a reaction to the behaviourist approach, which provided inadequate explanations to certain phenomena, an opposite theory emerged.
The mentalist (or rationalist) theory claims that human beings are equipped with a language acquisition device (LAD), an innate faculty which is responsible for the language acquisition process.

This genetically inherited faculty is regulated by pre-existing properties: the principles, which are 'wired in' to the brain and the parameters, which can be set in a way or another depending on the language.
Whereas for behaviourists, the environmental input is everything, in the mentalists' view linguistic competence cannot be acquired from social interaction or from imitation of the sentences heard (the so called 'poverty-of-the-stimulus argument').

External samples of language are needed only to show how principles are instantiated and to trigger particular parameter settings on which knowledge of language can be built (Johnson and Johnson, 1998).
Within the mentalist/rationalist framework of SLA studies, other theories developed during the 1980s, acknowledging the importance of both input and internal language processing.

Krashen's 'monitor model' is a complex theory based on the distinction between acquisition and learning, the former being responsible for a creative construction process, the latter being only a secondary process of monitoring the output (sometimes occurring after the performance as a self-correction).
The strongest part of Krashen's theory is the Input Hypothesis, which claims that humans acquire language by receiving comprehensible input.

In particular, learners move forward in the acquisition process by being exposed to comprehensible input which contains structures that are a bit beyond their current level of competence ("we move from i, our current level, to i +1, the next level along the natural order, by understanding input containing i +1" - Krashen, 1985: 2).
Comprehensible input is "the essential ingredient for second-language acquisition" (Krashen, 1985: 4), but it is not a sufficient condition.

Krashen's model acknowledges the existence of an 'affective filter' (the Affective Filter Hypothesis) which can act as a mental block preventing the input from reaching the LAD.
When the filter is 'down' (i.e. the acquirer is confident with the language, she considers herself member of the group, she is not anxious, etc) acquisition is successfully achieved.

Michael Long and other interactionist theorists still consider comprehensible input necessary for language acquisition, but they go one step further in their investigations, pointing the attention to conversational interaction and its role in second language acquisition (Long, 1996; Gass, 2003).
According to the Interaction Hypothesis, conversations between native speaker and non-native speaker or between non-native speakers with different level of proficiency involve a process called 'negotiation of meaning', which is an adjustment affecting the input (linguistic form, conversational structure, message content) in order to achieve an acceptable level of understanding.

Moreover, interactional adjustments imply an increment in frequency and salience of target forms, which are more likely to be noticed by the learner.
In general, "the increased comprehensibility that negotiation brings helps reveal the meaning of new forms and so makes the forms themselves acquirable" (Long, 1996: 452).

Another component needs to be taken into consideration during the acquisition process through interactional input.
In order to progress in their learning of the second language, learners need to switch their attentional focus from message to form, identifying mismatches between target language forms and learner-language forms.

Attention, awareness and noticing are thus fundamental mechanisms for extracting items from linguistic input and storing them in long-term memory, i.e. making progress in second language learning and acquisition.
2.2 The role of output

In the previous section of the essay, I partly considered output as learner-language forms produced during the interaction and I discussed its role together with input in the process of negotiation of meaning.
In this part of the discussion, I would like to focus more on how output can be effective in second language learning and acquisition.

Swain (1985, 1995) argues that output plays a crucial role in the development of a second language and that it is not just a means of practising already existing knowledge or a way of enriching input.
For this reason, she introduced the notion of comprehensible output as the need for a learner to be "pushed toward the delivery of a message that is not only conveyed, but that is conveyed precisely, coherently, and appropriately" (Swain, 1985:249).

She bases her 'output hypothesis' on the idea that understanding language and producing language are different skills.
Through language production learners are stimulated "to move from the semantic, open-ended, non-deterministic, strategic processing prevalent in comprehension to the complete grammatical processing needed for accurate production.

Output, thus, would seem to have a potentially significant role in the development of syntax and morphology" (Swain, 1995: 128).
Producing language is acknowledged to serve several functions in second language acquisition (Swain, 1995; Gass and Selinker, 2001).

As far as fluency is concerned, a sensible improvement is noticed through practicing, while concerning accuracy three functions of output have been suggested.
The first one is the 'noticing/triggering' function, which refers to the role of output in raising consciousness in learners about the gaps in their knowledge of the target language.

Moreover, noticing can also trigger cognitive processes that generate new linguistic knowledge or consolidate the existing one.
The second function is the 'hypothesis testing'.

The production of output is a way of testing hypotheses about the comprehensibility or the well-formedness of new structures.
The hypothesis testing 'strategy' is made evident by the fact that learners modify their output after receiving instances of negative feedback, such as clarification requests or confirmation checks.

Self-correction is another piece of evidence.
The third function of output is a metalinguistic one.

Learners reflect upon language making hypotheses on how something should be said or written.
Swain (1995) recognizes in this linguistic behaviour a particular kind of negotiation which she defines as 'negotiation of form', since the topic of the discussion is the language itself.

The output produced through collaborative dialogue supports learners in outperforming their competence and in developing their interlanguage (Donato (1994) talks of 'collective scaffolding').
Thus, reflecting on output will result in language learning.

2.3 The role of instruction
Another important aspect to consider in SLA studies is the role played by formal instruction in the learning process.

A first distinction between 'form-focused instruction' and 'meaning-focused instruction' needs to be made.
The former refers to "any planned or incidental instructional activity that is intended to induce language learners to pay attention to linguistic form" (Ellis, 2001: 1); the latter emphasizes the need to provide learners with real communicative experiences without any direct teaching of grammar (Ellis, 1997).

Here, I shall concentrate on form-focused instruction trying to outline a classification and to point out links between research and pedagogy.
Three different types of form-focused instruction have been proposed (Ellis, 2001; see also Long and Robinson, 1998): 1-Focus-on-FormS; 2- Planned Focus-on-Form; 3-Incidental Focus-on-Form.

In the first approach, the target language is treated as an 'object' and it is presented as a series of pre-selected grammatical items that the learners are required to focus on.
The process of acquisition is conceived as a gradual accumulation of rules until the whole structure of the language is completed.

The learning process can either have the form of explicit instruction, deductively or inductively addressed to learners, or it can follow an implicit pattern with a total absence of awareness of what is being learned.
This approach is evident in traditional grammar teaching based on a synthetic syllabus and it is mainly directed at language production, creating opportunities for learners to practice the target structure.

However, an alternative instructional method involves structured input, which is specially designed to draw learners' attention repeatedly on a preselected linguistic feature.
Planned Focus-on-Form instruction still relates to both input and production, but this time the primary focus is on meaning rather than form.

This kind of instruction makes use of enriched input, which like structured input consists of a large amount of exemplars of the target language.
Learners are required to respond to the content of the input through tasks which are communicative in nature, in order to induce noticing of the target form in the context of meaning-focused activities.

Furthermore, focused communicative tasks elicit the production of a specific target feature and facilitate its incidental acquisition.
The third type of form-focused instruction accounts for incidental focus-on-form, which can arise either because of a problem of communication or because of the occurrence of a form which is perceived as problematic.

In these cases, the attention of the learner (or the teacher) switches from the meaning-focused communicative activity to features of the linguistic code.
This process of noticing is triggered by the negative evidence (feedback) provided by teachers in response to learners' errors.

In particular, it is implicit negative feedback, in the form of recasts or request of clarification, which seems to enhance noticing and assist the acquisition process.
As it is clear from the brief description above, form-focused instruction plays an important role in second language learning and acquisition, especially from a pedagogic point of view.

Indeed, studies on this topic can be very useful for the developing of effective materials and supports which will assist teachers in educational activities.
3. Comments on a learning experience: the Persian language

In this last section of the essay, I shall be describing my learning experience of the Persian language, trying to relate it to the theoretical framework presented above.
The value of input, output and formal instruction will be evaluated and the main reasons of failure in mastering the target language will be outlined.

The learning experience has been developed on a period of ten weeks and it has been almost entirely based on self-study through a guided textbook and audio cassette.
Three additional one-hour sessions of classroom practicing with a native speaker have been scheduled.

3.1 The textbook
The first issue I would like to concentrate on is the textbook, describing the overall organization and assessing its effectiveness in meeting the learners' requirements.

The book is divided into sixteen units; each of them is focused on a different communicative task, which is brought in by an introductive dialogue available both as a text and on the tape.
Four different kinds of input underlying different aspects of the language are available in each chapter: dialogues, linguistic information (phonetics, morphosyntax, and pragmatics), lists of words (vocabulary), and exercises.

The dialogues, which appear at the beginning, in the middle and at the end of the unit are especially useful for the learner and serve different functions.
The first dialogue aims at introducing the topic and it is an example of positive evidence that can be valuable for the learner to identify certain structures of the target language.

The same is valid for the closing dialogue (Comprehension dialogue), whose degree of complexity is a bit beyond the learner's level of competence in order to facilitate the acquisition of certain forms (see Krashen's theory of comprehensible input).
Particularly interesting is the dialogue in the middle (Role play dialogue), in which the learner has to perform the turns of one of the party.

The exercise not only stimulate oral production (which enhances fluency (Swain, 1995)), but it also provides the learner with feedback.
In fact, the right utterance is given after the learner's attempt and it operates as positive evidence (if the learner gave the right answer) or as indirect negative evidence (if the learner was wrong).

This second form of feedback (recast) is believed to be effective in triggering noticing in the learner and in assisting the learning process (Long and Robinson, 1998).
Despite this apparent orientation of the textbook towards a meaning-focused approach, a more thorough analysis reveals its strong tendency to formal instruction.

A great part of each lesson is dedicated to linguistic information, focusing on various aspects of the grammar and of the pragmatic use of the target language through explicit grammatical rules.
Furthermore, the exercises (mainly matching of forms, recombination and 'fill in the gaps') are designed to present and practice single linguistic rules, reflecting the traditional approach of synthetic syllabi.

The application of Focus-on-FormS instruction has been heavily criticized, especially by rationalist/mentalist theorists of SLA, although a timid acknowledgement of its positive role under certain circumstances has been suggested - - for example, formal instruction can be the primary source of comprehensible input, especially for beginners who may find 'real world' input too complex to understand (Krashen, 1985); or the application of Focus-on-Forms instruction can enable improvement in oral production, providing opportunity for frequent oral practice (Sheen, 2005).
In my opinion, the exercises proposed to the learner are one of the weak points of the textbook.

They are primarily based upon grammar translation and heavily focused on morphosyntactic forms and paradigms, without any attempt to improve the spontaneous use of the language and the fluency.
As far as I am concerned, despite the frequent practice of grammatical rules, the learning process ended in a mere memorization of formulaic expressions and other chunks of language, as a result of the exposition to samples of natural language contained in the dialogues.

3.2 The interaction in the classroom
The most interesting and effective part of the learning experience has been the conversation in class with the Persian native speaker and other learners.

The interaction gave a valuable contribution to my learning process, mainly because it acted as a stimulus to the production of well-formed utterances in target language (comprehensible output).
Through the interaction, I could notice structures of the language different from the ones taught in the textbook, especially when comprehension problems emerged and the native speaker reformulated the utterances to enable us to understand (comprehensible input).

Furthermore, few cases of collaborative dialogue took place among the students in my group (collective scaffolding).
An additional advantage of real interaction is the possibility of feedback.

Undoubtedly, positive feedback (in response to utterances successfully produced) and negative feedback (both as explicit correction and as recasts or requests of clarification) played a central role in assisting acquisition.
The former contributed to transmit positive feelings about the language and the learning experience, enhancing my motivation; the latter led my attention to certain structures of the Persian language.

4. Conclusion
From the analysis of the experience of learning modern Persian, it emerges that interaction played a crucial role in the process.

Conversations among learners and native speakers gave valuable contribution, encouraging the production of comprehensible output and enhancing the noticing of structures of the target language through comprehensible input.
Furthermore, the presence of a native speaker was an important source of positive and negative feedback, which effectively assisted learning.

Nonetheless, interaction and practicing in the classroom were limited to three hours and the learning experience heavily relied on the textbook.
From its part, the text provided samples of natural language through dialogues recorded on audio cassettes, but the overall organization of exercises, based on formal instruction according to the traditional approaches of synthetic syllabi, prevented the learner from developing an autonomous mastering of the target language.

1.0 Introduction
Discourse analysis can be broadly defined as the approach to the study of language which aims at investigating how spoken and written texts are organized in order to communicate messages in the social and cultural contexts in which they are produced (McCarthy and Carter, 1994).

This interdisciplinary field of study enables analysts to develop models and methodologies to discuss issues of different nature, along the traditions of linguistics, sociology, philosophy, anthropology and others (Schiffrin, 1994).
In the essay, I shall concentrate on the analysis of a written text, the fairy tale 'The white snake' by Jacob and Wilhelm Grimm, focussing on the micro- and macro-structural patterns deployed to give cohesion and coherence to the text.

Moreover, I shall make some considerations about the relations between certain structural devices and the narrative genre of popular tales for children.
2.0 Cohesion

Cohesion has been defined as the links among the parts of a text which make the text itself united and meaningful.
Several types of cohesive devices ('ties' in Halliday and Hasan's (1976) terminology; 'links' in Hoey's (1991) more recent work) have been identified by scholars, who provided a complete taxonomy of the forms employed to hold texts together.

In the analysis of the text, I will use the classification introduced by Halliday and Hasan (1976) in their pioneering study on cohesion in English.
The authors identify two main categories of cohesive devices: the grammatical forms and the lexical forms.

The former is made up of four subgroups (reference, conjunction, substitution, ellipsis), while the latter is a very productive and rather untidy category that contains elements characterized by sense relations of synonymy, antonymy, hyponymy, meronymy, and others like collocation and repetition.
2.1 Grammatical cohesive devices

Intratextual reference, i.e. links between items contained in the same text, is generally a very productive category.
This is not surprising, since the employment of pronouns, demonstratives and other grammatical elements is an effective strategy to develop the text in an economic way, avoiding continuous repetitions of the same nominal expressions, which would affect the fluency and would be perceived as non-neutral (see for instance the illocutionary force expressed by insisting repetition in political speech).

Throughout the text many examples of grammatical reference can be found, especially in the use of pronouns (personal, possessive, reflexive, reciprocal, indefinite), possessive adjectives, demonstratives, definite articles, locative and temporal deixis.
Another frequent kind of cohesive ties is conjunction, which include all those expressions useful to make all the relations between ideas and concepts within a text very explicit.

Connectors are particularly effective to express the logic and temporal development of a story and they are thus numerous when the intended audience is made up of children.
In the tale being analyzed, a large amount of conjunctions can be identified: additive (especially 'and'), adversative ('but', 'however', 'nevertheless', etc.), causative (mainly 'so') and temporal ('now', 'then', 'at the same time', 'presently', etc.).

Substitution is a cohesive strategy through which an item (usually an indefinite pronoun, the auxiliary verb 'to do', comparative expressions like 'so', 'the same') replaces another one to avoid repetition.
It is a concept similar to that of reference, but there is a subtle difference.

In this case the two elements (the one that substitutes and the one being replaced) are not in a relation of identity, but in a relation of similarity (i.e. they share the same features but they do not refer to the same entity).
No examples of substitution have been found in the text.

The last subgroup of grammatical cohesive devices is ellipsis, which is characterized by partial repetition of a previous expression (or a parallel structure) with the omission of some elements.
Biberal. (1999: 1099) consider ellipsis "a pervasive feature of conversational dialogue" to avoid unnecessary repetition of what the previous speaker has said, which sometimes leads to the production of non-grammatical utterances (e.g. dropping of the subject and even of the main verb).

As far as the written text under analysis is concerned, a common example of ellipsis is the omission of the subject in coordinate verbs, whereas only one example of dropping of larger parts of a sentence has been recorded.
An explanation of the almost total absence of substitution and ellipsis could be again the audience whom the text is designed for.

In order to make the story comprehensible to children, the structure of the text is supposed to be clear and without omissions which could create misunderstanding and difficulties in comprehension.
2.2 Lexical cohesive devices

As it has been mentioned before, the category of lexical cohesive items is rather complex and untidy, since it embodies a large number of different meaning relations among words which sometimes are not clearly distinguishable.
In the text many examples involving several sense relations have been found: synonyms and near-synonyms, antonyms (both converse antonyms such as 'king'/'servant' and complementary antonyms like 'guilty/'innocence'), hyponyms and co-hyponyms, meronyms.

Some of the cohesive devices above and other strategies can be grouped under the notion of reiteration, which is defined as the total or partial repetition of the same form (proper repetition) or the same meaning (synonyms or near-synonyms, hyponyms).
However, these categories are not enough to explain the great variety of reiterated elements that characterize the text.

Some peculiar examples can be better described with the term reformulation, since they combine the simple repetition of words with a paraphrase at the level of the clause (see appendix 2).
Reformulations serve at least two functions in these sentences.

The first opposition seems to have a stylistic force, since it provides variation within the text making the story more interesting.
The other three examples on the contrary, combine the expressive function of stylistic variation with the functional intent of clarification.

Through the reformulations in (2) and (3), in fact, previously mentioned characters are identified avoiding confusion to the reader, while in (4) a complex statement is summarized in a simple and comprehensible sentence, which is fundamental for the understanding of the whole story.
Moreover, the insisting repetition of the number 'three' is not random and meaningless, but it can be traced back to the genre of fairy tales, with its frequent use of symbolic features.

The relation existing among the items grouped under the definition of reiteration can be considered a paradigmatic one, since the several forms substitute one another for stylistic or functional purposes.
Along with this 'vertical' perspective, Halliday and Hasan (1976) propose a complementary set of cohesive ties which stand on a 'horizontal' and syntagmatic relation.

The general term employed to define this kind of relation is collocation, that embodies the idea of co-occurrence of items in the same environment in order to establish meaningful links between the parts of a text.
Sense relations of antonymy, co-hyponymy and meronymy are included in this group and can be considered 'strong' collocations, since they are characterized by well-established semantic links between the words of the lexis.

On the other side, interesting examples of 'weak' collocations can also be found in the fairy tale.
With this expression I refer to co-occurrences of words which are not frequent in the common use of language, but that create special links when employed in certain contexts.

For instance, the two words 'Queen' and 'ring', which are usually loose collocates, cooccur three times in the text (§ 3 and 4) establishing a connection with cohesive effects.
In two cases the relation is one of possession ('the Queen lost her most beautiful ring'; 'the Queen's ring'), which helps to establish a collocation between the terms, while in the other case there is no evident connection ('a ring which lay under the Queen's window').

The 'ring' in this last sentence is understood as the 'Queen's ring' only by virtue of the collocation previously created.
To conclude this section I would like to propose another strategy of cohesion, based on the concept of connotation.

An accumulation of terms with positive or negative connotation, in combination with other devices like pronominal reference and lexical relations, can be an effective way to increase cohesion.
This strategy is exploited to enhance the linkage between paragraphs 3 and 4 in the text, through a cluster of negative words ('guilty', 'executed', 'in vain', 'no better answer', 'trouble', 'fear') which support the cohesive force expressed by personal pronouns and possessive adjectives.

3.0 Identity chains and similarity chains: cohesion and coherence
An alternative perspective to investigate lexical cohesion is embodied in the notions of identity chain and similarity chain (Halliday and Hasan, 1985), defined respectively as a series of expressions that refer to the same thing, event or abstract concept, and a chain of items that point to non-identical members of the same class of things, events or to different classes related to each other.

Particularly interesting is the whole range of expressions used to refer to the main character of the story, made up of synonyms, near-synonyms and general noun phrases which alternate according to the features of the context in which the character appears (see appendix 3).
Instances of similarity chains are the numerous expressions used to develop literary themes throughout the text: the theme of travel ('travelling', 'go about', 'ride on', 'wander', etc.); the theme of listening/hearing animal voices ('hear', 'whispering', 'chattering', 'listening', etc.); the theme of tasks to be performed ('attempt', 'hard task', 'perform', 'reward', 'succeed', etc.).

Identity and similarity chains are useful means not only for textual cohesion, but also to create coherence, a property of texts which is strongly related to the receiver's comprehension of the message.
The main conditions needed for a text to be perceived as coherent by the readers involve the recognition of the genre, the competence about its organization and functions, the shared knowledge of the content and the context in which it has developed.

According to Halliday and Hasan (1985) the semantic grouping in similarity chains is genre-specific.
This is evident in the themes presented above, which can be considered typical of fairy tales and popular stories.

Furthermore, elements such as fixed expressions ('A long time ago', 'they lived in undisturbed happiness to a great age'), proverbs and popular beliefs ('it is said that fishes are dumb', 'one good turn deserves another'), animals with human features, the repetition of symbolic numbers, the use of a certain tense (mainly past simple), help the reader to evaluate the text using the appropriate mental schema: a story written for children with educational purposes (it contains a moral), involving fantastical characters, imaginary places and undefined time in the past.
4.0 Clause and sentence relations: macro-structural patterns

In this section, I shall move from the micro level of grammatical and lexical cohesive devices to the macro level of clause and sentence relations in order to point out certain structural parallelisms that contribute to hold the text together.
The prospective adopted follows the one proposed by Hoey (2001), who provides a model to analyse texts based on the meaning relations between clauses and recursive structural patterns.

The story 'The white snake' can be divided into three main sub-stories which develop chronologically in three distinct settings: 1- the servant lives and works at the court in his place of birth (§ 1 to 5); 2- the main character leaves the court and sets out to travel around the world (§ 5 to 8); 3- the young man comes to a city and tries to conquer the heart of a princess (§ 9 to 15).
The three phases of the plot, although involving different characters and situations, are structured in a similar way.

Firstly, all of them have an introductory statement, which helps to collocate the situations in time and space (even if in an imaginary settings).
The introduction at the beginning is longer and more elaborated, since it presents the main character and explains a fundamental point for the comprehension of the whole story: how the man received the gift of understanding the language of animals.

Secondly, in the three sub-stories the 'recycling' of a pattern can be identified.
The sequence problem - solution - reward occurs many times throughout the text establishing interconnections between the different paragraphs and awakening the interest and attention of the reader.

The patterns are not distributed in an even and plain way, but create a climax in the story, which increases in terms of frequency and complexity towards the end of the tale (see appendix 4).
Finally, other interesting patterns develop more locally to serve both stylistic and cohesive functions through the repetition of the structure of the clauses.

Winter (1994) proposes two groups of clauses: sequenced clauses, which are logically and temporally ordered; matched clauses, which are characterized by repeated parts and obligatory changes (replacements).
An example of the first type is the sequence cause - result which appears several times in paragraph 2, while instances of matched clauses are used to express contrasts (e.g. the sequence hypothetical - real in paragraph 9) or to enhance the moral message contained in the story (e.g. the repetition of the statement in paragraphs 6, 7 and 8).

5.0 Conclusion
In the brief analysis presented above some of the properties of cohesion and coherence have been described in relation to a written fairy tale.

Several strategies are employed to hold the text together on both the micro- and the macro-structural levels.
Grammatical and lexical devices, such as pronominal reference, conjunction, ellipsis and sense relations among words establish effective links between the parts of the text, whereas, at the macro-level of clauses and sentences, it is the recursive pattern of sequences logically and temporally ordered that is responsible for the cohesion.

Furthermore, there seems to be an interrelation between the linguistic level (here intended as lexical and grammatical structures) and the genre of fairy tales which the text belongs to.
The several strategies exploited not only contribute to the cohesion of the text, but they also help to make it coherent, developing the story with attention to the genre, the audience (mainly children) and the moral message expressed.

The recognition of the genre by the reader and his or her competence about the organization and functions of the text are in fact necessary conditions for the creation of a coherent piece of narrative.
The social variables of class and gender are two concepts which are paramount in many of the concerns of today's society.

The link of language to class and gender is a common one.
Language is a central issue in our lives thus it is natural to link the way we speak to the society we live in.

The very things that determine our gender and class are created by the place we live in, the roles that we assume are linked to these social variables.
They are socially created.

I think that the issue of class and gender can be significantly linked together, particularly if you look at the role of prestige.
The way we speak is often linked to prestige through the sound of an accent and the use of standard English.

It is thought that those who speak with a higher use of standard English are likely to be of higher class.
Assumptions about the way we speak and gender are often made.

In this essay I am going to concentrate on the role of prestige and how this affects the way we speak.
I think that class and gender can be linked through prestige.

Following the work of Trudgill and Labov I can see how it can be supposed that women seek overt prestige which can be seen through their speech.
I intend to look at the issues of class and gender separately and then to try and support the idea that the two are indeed linked.

I want to look at to what extent it is true that women are more class conscious than men and to see whether this can be seen through language use, thus showing a link between gender and class.
I am first going to look at the issue of gender.

The gender debate, (why the differences in the way men and women speak occur) is an ongoing discussion.
There is no definite answer on this issue and the debate remains, as linguists are still unsure as to why women use certain forms and men use others.

Gender is a socially constructed term.
I am not looking at the biological differences when I use the term gender I am looking much deeper.

Gender differences include not just features of behaviour but extend to a whole way of regarding ourselves.
They include questions of gender role, (ways of behaving) and also of gender identity (relating to ourselves and others).

Pitch for example is a biological difference between males and females even though it is as not as pronounced as one might think.
However the connotations of pitch have become a social construct.

Different cultures associate pitch with gender according to their particular social norms.
When I look at gender I am looking at the sexual identity of a person in terms of the life they lead and what makes them female.

This is a difficult category as it is near impossible to define what a male or female is without talking about the physical differences.
This is when it becomes clear gender is a social construction.

The gender debate began through Otto Jesperson in the 1920's when his theories of gendered language became known.
His theories were unfounded yet became the basis for many stereotyped views on language.

Lakoff was one of the earliest linguists to deal with the issue of the differences between male and female language.
Again her theories were not supported by factual evidence.

This has meant some of her views have been criticised, however her work is still viewed with high regard and has been the basis for many other linguists.
These linguists have then been able to develop her points and back them up with evidence however many of the approaches still have their flaws.

Lakoff and Cameron look at the issue of power struggles between males and females.
In society, despite there being significant developments, women have been seen as inferior to men.

It has been suggested that males assert their domincnace through speech thus women resort to certain forms in order to regain some status.
This links to the idea of prestige and may suggest why females tend to use covert prestige forms.

I think it is clear that there is a link between gender and class when the issue of prestige is discussed.
The phrase Standard English has meant that society has created the view that any other variety is considered to be 'incorrect'.

Prescriptivists believe that there is a right or wrong way to speak thus any variation in the form of a dialect is wrong.
Linguists however take the stance that language has many varieties and can see that most varieties are reliable.

Trudgill believes that as long as a dialect has rules to govern it then it can been seen as being linguistically correct.
'It follows that value judgments concerning the 'correctness' and purity of linguistic varieties are social rather than linguistic' Trudgill pg.8.

The gender differences in language have often been associated with standard and non standard forms.
The differences between men and women's speech has been well documented over the years and much research has been done to assess what differences there are and why they occur.

To begin with the research that occurred was done primarily by men which meant that male linguistic behaviour was assumed to be the norm.
Any deviation from this was therefore considered to be a departure from the norm.

It has only been recently that close scrutiny of the previous findings had been done.
This was mainly due to the feminist movement whereby women began to question the stereotypes that had been made about them.

One area that had been questioned was whether women tended to speak more 'correctly' than males.
This is when the issue of class and gender can be fully linked.

Society has created a label whereby standard English has come to represent the 'correct' form.
It is suggested that those who speak other varieties are therefore inaccurate.

Linguistically this is not the case as we have seen through the discussion of accent and dialect.
We know that any variety can be linguistically correct if it is led by a set of rules.

The issue of whether females use more standard forms than males has been directly linked to the issue of prestige.
It has been suggested that women are more prone to hypercorrection.

Hypercorrection is when in formal settings people over produce socially favoured patterns of pronunciation.
For example received pronunciation forms.

Much research has been done in social settings to see if indeed women do use more standard forms than males.
Macauley (1977) looked at the contrast in speech between lower middle class males and lower middle class females in Glasgow.

The results suggested that women were 40% more likely to avoid using the vernacular form, the glottal stop than the men.
This links to the works of Trudgill and Labov in their variationist approach.

They noted that women across all classes preferred to use the standard form.
'women are more sensitive than men to prestige forms; lower middle class women showed the most extreme form of this behaviour; (Labov 1972:243).Women from upper classes used the same forms that women from lower classes would use.

This links to the idea of prestige.
It suggest that all women seek overt prestige, this could link to the fact that women may have felt they had no other way of acquiring status, their language use was the only way.

The work of Lesley Milroy (1980) looked at the issue of social networks in working class Belfast.
The men here were part of a closed network, as they all worked together.

They had more networks as they worked with their friends and had more social bonds.
This meant they were more prone to using the vernacular form.

Women however stayed at home and had fewer connections.
This meant their language use tended to be more standard as they weren't part of a close community.

However a separate study was done on an area called Clonard where unemployment was high.
Here the social networks were reversed.

This suggests that perhaps females do not necessarily seek to use overt prestige forms but of are affected by the level of employment.
A study by Nichols (1983) showed that age had a lot to do with the use of vernacular forms and it was not necessarily all women that seek overt prestige forms.

Here the older women and men both tended to stick to the Creole form of English.
The younger women rarely spoke to the older women in the same Creole form, instead they would use more standard forms.

This was linked to the fact that the younger women had spent more time in education where they had been surrounded by prestige forms.
The older women had spent more time at home and the men had had manual jobs.

This suggests that the prestige forms do occur more in female speech but age, occupation and the time spent in education are also important factors.
The research of Trudgill however is key when we consider whether the prestige forms used by males and females is conscious.

Trudgill found that women claimed to use more prestige forms than they did, also men claimed to use less prestige forms than they did.
This links to the idea that men seek masculinity.

The use of non-standard forms thus can be seen through society to represent masculinity.
I think what can be seen through the research that has been done is that there is a clear link between class (incorporating status and prestige) with gender.

I think it is apparent that there is no definite answer as to whether females are indeed more class conscious through their language use than men, but indeed there are some situations which suggest this.
I think that it is important to look at other social variables to be able to fully assess whether gender and class alone affect speech.

The work of Viv Edwards (1986) for example showed how education was an added factor towards sex differences.
Through the work of Nichols it can be seen that age is also a factor, the lifestyle and outside influences are also key in considering what affects the way we speak.

The topic of gender and language is an ongoing debate thus it is impossible to state solid facts on this issue.
In my opinion the issues of the gender and class have a definite link.

I think the effects they have on language are indeed clear but I think that to conclude firmly is an impossible task whilst the debates continue.
1.0 Brief background to study:

In order to determine how words are stored and retrieved from within the mind psycholinguists have undertaken many experiments in an attempt to establish how closely connected words are.
Aitchinson (2003) suggests that words are related to each other in the form of 'a multi-dimensional cobweb in which every item is attached to scores of others' (2003:84).

Early research concentrated on meaning networks and 'finding out the strength of a link between one particular word and another' (2003:84), suggesting that links between words were formed by 'habits'.
When certain words were frequently associated with each other they were thought to 'develop strong ties' (2003:85).

A way to test these theories was through 'word-association tests'.
In these experiments subjects are asked to respond with the first word that comes into their head when faced with certain stimuli.

Moss (1996) describes the motivation behind these studies as the belief that 'word associations provide a direct window onto the underlying structure of semantic memory' (1996:1).
Emphasis was placed not on the individual but in the general responses for large groups.

Results showed that different people generally gave rather similar results.
Aithcinson (2003) noted that 'the consistency of the results suggested to psychologists that they might therefore be able to draw up a reasonably reliable 'map' of the average person's 'word-web' (2003:85).

I will be conducting the same experiment to see which relationships are most frequent among my subjects.
I will then assess how reliable this experiment is in determining which words are connected in the lexicons of my subjects.

2.0 Description of project:
I chose a mixture of nouns, adjectives and verbs as my stimuli.

In each word class I chose a selection of frequencies, i.e. high and lower frequencies.
For example; 'hair' and 'love' had a similar high frequency, and 'sing' and 'bread' had a similar lower frequency.

This ensured that all subjects would have a similar familiarity with the selection of nouns, verbs and adjectives.
It also ensured that the experiment took into account that the words we encounter vary in frequency.

In order to assess if we map all words in a certain way it is important to look at whether specific relationships are more common in a variety of types of words.
For example: do people respond with coordinates for the majority of their answers despite the frequency of the word.

I will therefore be look at the relationships between the words compared to the stimulus.
I will also be looking to see if there are any significant differences between the younger and older participants.

I chose two 20 year old females, and two 50 years old adults, one male, one female.
3.0 Results

Results continued.....
A pie chart showing the most frequent relationships between the stimuli and the participants responses.

Responding with a matching word class (to the original stimuli)
3.1 Analysis and discussion of results:

The results showed that certain relationships were more common than others.
In particular was the selection of an antonym.

All subjects, despite their age, selected the same word for a variety of the stimuli.
For example, 'new-old', 'big-small'.

The subjects also tended to pick items if they were part of a pair, therefore coordinates and collocates were common.
Interestingly one of the words I chose caused problems for all of the subjects.

I chose the word 'chips'.
If one has selected the word 'fish' I would have expected one of the most common responses to be 'chips' (Moss 1996:49).

However when I presented the subjects with the second half of a pair it elicited a confused or different response.
The subjects delayed their answer on this stimulus or came up with semantically unrelated words.

One subject picked 'daddy', this I presume relates to an advertising campaign which involved the question 'daddy or chips'.
One of the subjects selected the word 'chops'.

This I can only presume relates to the rhythmic relationship between certain words, for example 'hip-hop' and 'flip-flop', it is also possible that a slip of the tongue meant the subject was intending to say 'chip-shop'.
As the word 'chips' was pluralized it may have caused problems among the subjects.

This could suggest that words with inflections may be treated differently within the lexicon.
However it could also be that because 'chips' is so often linked to 'fish' it is difficult when they are separated, as you may be expecting a word to precede it.

Aitchinson (2003) suggests that in language there are 'numerous 'freezes', (whereby) pairs of words which have been frozen into a fixed order'.
(2003:91).

Some of the words I chose were more flexible, 'chips' perhaps was more restrictive thus problematic to the participants.
Field (2003) suggests that 'adults tend to choose a word in the same word class as the stimulus'.

It was evident from my results that the majority of subjects responded to the stimuli with words from the same word class.
This was particularly evident when it came to adjectives.

91.7% of adjectives were responded to with another adjective.
87.5% of the nouns were responded to with another noun.

The statistic for verbs was slightly less, 67.5%.
This may be because people often associate an action with an object, for example: read and book.

However the majority of the time subjects did respond with a word in the same word class.
3.2 Age differences:

One of the aims of the experiment was to see if there were any differences between the older and younger participants, in terms of their responses.
However as my experiment was only on a small scale it makes it difficult to make generalizations about any significant age differences among the participants.

In general the responses were similar, however in one instance a response could be described as an indication of the age of the participant.
When presented with the word 'hair', a participant responded with 'loss'.

This would be more likely to be expressed by someone older; this could also be an indication of gender as hair loss is more common in males.
However it is important to note that the other responses to this word gave no indication of age, thus it cannot be assumed that these demographic features affect the association of words.

The main difference I noted between to two age groups was the time it took to respond to the stimuli.
The 2 older participants took slightly longer to give me their answers.

This is concurrent with the research of Cramer (1968) who found that older subjects had 'longer associative reaction times than younger adults'.
However I cannot describe my results as conclusive as I only had a comparison of 2 older adults to 2 younger adults there is not enough evidence to suggest that older adults take longer to respond.

4.0 Conclusions-
The word-association test is useful in that it shows that humans tend to generate similar results, Moss (1996) also states that the test is useful as 'a simple measure of relatedness between two words', however the results produced are not very conclusive about how words are stored within the mind as a whole.

Aitchinson (2003) suggests the main problem with word association tests is that 'they cannot tell us about the probable structure of the human word-web' (2003:85).
She identifies the fact that as participants are only required to give one response this cannot 'fully reflect the variety of semantic links that would presumably exist in their semantic memory' (cited in Moss 1996:2).

When participants are asked to give the first word that comes into their head there is a slight delay between what the person thinks and what they actually say.
Before the participants give their answer they are making a quick decision as to what word they wish to choose.

De Groot (1989) found that even when asking participants to respond quickly they take about one and half seconds to respond.
This shows there may be a decision making process occurring before responses are given.

Aitchinson also notes that the responses are also multifarious.
The top responses for most words are semantically similar but some connections are stronger than others.

Aitchinson gives the example of butter which is linked to bread, yellow, soft, cream, eggs, milk, cheese.
Bread is linked to butter as you eat the two together, whereas yellow and soft describe butter itself.

Cream, eggs, milk and cheese are other kinds of dairy food.
Therefore although the most people tend to respond in the same way the answers do not help us to determine how words are linked within the lexicon.

The test in itself is considered to be unnatural due to the fact that the words are presented on their own.
Normal speech would involve the stimuli being surrounded by other words therefore the process of retrieval would no doubt be different.

Equally the surrounding words can have an effect on the meaning, Aithcinson (2003) concludes that 'if a word's associations can be changed so easily by the context then it is possibly wrong to assume that we can ever lay down fixed and detailed pathways linking words in the mental lexicon' (2003:85).
This I suspect would be the case with one of the some of the stimuli that I chose, particularly 'chips'.

If this word occurred in a sentence, or in a different order, it may well have elicited more expected connotations.
It is evident through my results that there are definite commonalities among participants.

Moss argues that this is often because certain words are more commonly linked, for example 'cat and dog'.
Aitchinson also notes that 'two types of link seem to be particularly strong: connections between coordinates and collocation links' (2003:101).

This was true with my results although antonyms featured highly.
Although it is undeniable that there are patterns in the responses of participants it is important to note that these findings merely help to provide 'a general framework' (Aitchinson, 2003:101) of how words are linked within the lexicon.

Perhaps more useful is the technique of priming which looks at how closely words are associated within the lexicon.
Priming measures how quickly participants notice words which are/are not associated with the sentence.

Field (2003:17) uses the example 'We saw a camel at the zoo....
fosk - bank - lidge - hump'.

The participant has to press a button every time they see an actual word.
The reaction time to 'hump' will be quicker than 'bank' because it has already been triggered by the semantically related 'camel'.

This test is more useful than word association as it looks at how closely words are associated before activation occurs and also how long the activation lasts.
This gives a deeper insight than previous word-association experiments.

Saussure proposed that our perception and understanding of reality is constructed by the words and other signs that we use' (cited in Bignell, 2002: 6) In this semiotic analysis I will attempt to identify the predominant view of reality by looking at the symbolic and photographic codes used in a selection of beauty adverts.
By identifying the signs in the adverts it will be possible to identify the predominant ideologies behind the adverts.

This analysis will look mainly at two contrasting adverts from the current beauty industry taken from mid-price magazines.
Both the 'Dove' adverts and the 'Nina' perfume adverts have also been televised on primetime television.

The intended audiences are both female and of a similar economic status although the products vary.
This analysis will look at areas in which the versions of typical feminine beauty are challenged and where they conform.

In order to do this I will look at the following areas; symbolic codes such as- colour choices, appearance of human beings, background/setting, photographic codes- such as lighting/editing and linguistic codes- such as captions/brand names.
Perfume is a particularly difficult product to advertise, as Cook suggests, 'a smell has no denotation, no component which distinguishes it from another'(1992:107), it therefore relies heavily on the connotations associated with the product in order to sell.

Danesi suggests that 'advertisers rely on a handful of hedonistic themes-happiness, youth, success, status, luxury, fashion and beauty- to promote their products' (1999:182).
Advertisers therefore make specific and calculated decisions when selecting which symbolic codes are used in certain adverts.

This links to the theory of Barthes (1973) who suggests that signs do not operate singly or on their own but that they are usually combined together in groups to make meaning.
The signs in adverts are selected because of the connotations they ignite when combined.

The 'Nina' advert places great emphasis on the theme of 'beauty', specifically 'femininity'.
In order to create this image of feminine beauty many techniques are employed.

Colour is one of the main features used in advertising.
In the adverts that I have chosen to look at the image is the central feature of the text and these images are further enhanced by the choice of colour.

Williamson suggests that the 'use of colour is simply a technique, used primarily in pictorial advertising, to make correlations between a product and other things' (1978:24).
The colours are chosen specifically because of the connotations associated with them.

In the 'Nina' advert the colour pink is prominent.
This is featured primarily on the girls dress.

This colour is associated with sexual purity and romance.
White is also crucial in the advert symbolising innocence and purity.

The use of these two colours are gentle and clean, but also very feminine.
The 'Dove' adverts also place emphasis on the colour white, perhaps suggesting that the women in these adverts are equally feminine.

Bignell suggests that 'femininity is not a natural property of women but a cultural construct' (2002:60), as both adverts rely heavily on the use of such light colours associated with women it therefore suggests that such cultural constructs are dominant.
Despite the fact that the 'Dove' advert is claiming to disassociate itself with typical views of beauty and display 'real beauty' it does rely on traditional associations with femininity, namely the use of white.

Bignell suggests that 'the iconic signs denoting women in the media very often perpetuate oppressive ideological myths about real women' (2002:35).
The 'Dove' advert claims to overcome these ideological myths by using 'untypical' women in their adverts.

Goffman suggests that 'the females depicted in commercially posed scenes have straighter teeth and are slimmer, younger, taller, blonder and 'better' looking that those found in most real scenes' (1979:21) and this is certainly the case in the perfume adverts I have selected.
In the 'Nina' advert the figure is a young girl/ woman, very slim, tall, fair and almost certainly a model.

In the 'Lovely' perfume advert the model is Sarah-Jessica Parker who is renowned for her good looks and style after starring in the show 'Sex & the City'.
These two adverts support the dominant ideology of what equates to feminine beauty.

The 'Dove' adverts seek to overrule this dominant ideology.
The models chosen go against the 'typical' woman as Goffman (1979) described above.

The first advert depicts an mature woman and places emphasis on her age by focussing on her wrinkles.
This contrasts with the majority of beauty adverts which use young models unless they are emphasising the role of the product- for example: anti-aging cream.

This emphasises the impact of the 'ideological' view of women in that despite most women use perfume the models used to advertise it are all of a similar age.
It does however show the power of ideology, Bignell suggests that 'the perfume became a sign of feminine beauty, so that buying the product for ourselves seems to offer the wearer of the perfume a share in its meaning of feminine beauty for herself' (2002:34).

This suggests that women seek to be a part of the ideology that is being advertised, as Williamson suggests 'to possess the product is to buy into the myth and to possess some of its social value for ourselves' (1978:31).
Williamson also suggests that 'you do not simply buy the product in order to become a part of the group it represents; you must feel that you already, naturally belong to that group and therefore you will buy it' (1978:47).

This is particularly evident in the 'Dove' adverts whereby they are aimed at a wide audience.
By creating a range of adverts incorporating different age ranges, body types, skin colour and hair colour the campaign addresses a range of women.

This is clever in that women may feel inclined to buy the product as they feel they are part of the group that it 'represents'.
It is also clever as it widens the ideology if what equals 'feminine beauty'.

By using a range of models it is likely that you, as the audience, will relate to at least one of them.
Danesi suggests that 'advertising exalts and inculcates lifestyle values by playing on hidden fears- fear of poverty, sickness, loss of social standing, and unattractiveness'(1999:183), and this is exactly what the 'Dove' adverts do.

The Dove adverts seek to glamorise the women used by suggesting that they too are as beautiful as the 'typical' model.
They seek to prove that there is no 'typical' woman.

The perfume adverts do however do also rely on the 'hidden fears' that Danesi mentioned but in a different way.
The perfume adverts present a woman that the audience almost aspire to.

The Dove adverts use photographic codes in an attempt to further suggest that the women in their adverts are 'real', thus implying typical models are 'fake'.
This is particularly interesting as both sets of adverts are creations in themselves and thus not 'natural'.

Barthes suggests that photographs are carefully altered and edited in order to affect the way a person responds to the image.
On first appearances it would appear that the 'Nina' adverts have been subjected to more of this editing.

The photographs contrast in many ways.
The main difference is that the perfume adverts seek to tell a story, with the exception of the 'Lovely' perfume which relies on the famous face to sell the product.

These stories are told by the connotations of the other signs in the images.
The 'Nina' perfume uses the image of apples and a castle which on the surface conjure up images of fantasy and enchantment.

The apple appears to be particularly significant as they connote feelings of temptation, as in Genesis and Snow White.
In the bible Eve is tempted by an apple, as is Snow White in the fairytale of the same name.

The apple is therefore associated with wickedness but also implies sexuality.
The images combine to tell a story which indirectly help to sell the product.

In the 'Nina' advert the emphasis is on magical enchantment, almost implying buying the perfume will have the same effect on the consumer.
This links to the view of Cook who suggests that 'all descriptions create qualities for the perfume by fusing it with something else, and with the (variable) connotations of something else' (1992:108).

By giving the perfume a magical like quality it gives a further appeal to the consumer.
The 'Dove' advert however uses a contrastive approach.

Whereas the perfume adverts seem to enter a fantasy world the 'Dove' adverts place great emphasis on the role of 'reality'.
As well as the text emphasising the 'real' women being shown, the use of a white background seems to imply that the women used are natural.

The blank background does not attempt to tell a story of any kind and places emphasis on the women.
By doing this is attempts to create the idea that the photograph is natural and 'real'.

However it is important to note that, as Goffman suggests, 'A 'real' photographic portrait may be one that strikes the viewer as bad in various ways: it may be unflattering...
or be badly composed, lighted, printed and so forth' (1979:17).

Therefore, although the Dove adverts appear to be natural and not as posed as the perfume adverts, at no point in their viewing would you consider the picture to be 'bad'.
The women, although unconventional, (by current ideologies), are still photographed in a flattering manner, the lighting is professional and the composition is appropriate.

This photographic style is clever as it looks natural and supports 'Doves' claim that these are real women and not part of a fantasy world.
It is important to note that there are similarities between the two styles in the perfume and Dove adverts.

All the women in the adverts are not entirely 'natural' in that their hair and make up has been done and it would be expected that the photographs have been touched up to meet professional standards.
Both adverts also use 'symbolic signs' (Pierce 1958) to further enhance the feminine qualities of the women in them.

By dressing the women in certain clothes there are cultural connotations which affect how we as the audience interpret the image.
The women in the Dove advert are photographed in their underwear, this symbolises sexual attractiveness.

The perfume adverts 'Nina' and 'Lovely' both dress their women in ball gowns, these symbolise femininity and luxury.
These 'symbolic signs' are combined with 'indexical signs' (Pierce 1958), smiles, which indicate happiness.

By portraying this emotion the adverts draw in the audience and help to suggest that it is the products they are advertising that do this to the women.
The women in the Dove advert appear confident which further emphasises that breaking the typical myth of feminine beauty is not necessarily a bad thing.

The perfume adverts appear to offer unattainable worlds which Eco (1986) termed 'hyper-real'.
The hyper-reality stems from the desire to achieve something that appears unattainable.

The 'Dove' adverts seem to attempt to reverse this tradition by suggesting that beauty is attainable as previous beauty ideologies are outdated.
Barthes proposed that myth serves the ideological interests of a particular group in society which he terms 'the bourgeoisie' (Cited in Bignell 2002:24).

This suggests that current ideologies are controlled by those in power and those with the most influence in society.
Because of this it is important to note that 'the dominant ideology of society is subject to change' (Bignell 2002:25) depending on who has the most influence during different time periods.

Most ideologies become so embedded in society that they remain the same as they are unchallenged they seem natural to the untrained eye.
Danesi suggests that 'unravelling the meanings of the ad's subtexts and intertexts is perhaps the only way to become truly immune to any effects advertising might have on human consciousness' (1999:186).

To an extent the 'Dove' adverts have exposed the typical ideologies and suggested an alternative way of looking at beauty.
Bignell suggests that 'the myths which are generated in a culture will change over time and can only acquire their force because they relate to a certain context' (2002:22).

It is important to note that the connotations that do exist, exist because of the cultural and social factors surrounding them.
The ideology of 'feminine beauty' has changed gradually over the years and it may be that the Dove adverts illustrate another social change in the way beauty is viewed.

Danesi suggests that 'advertisers are not innovators.
They are more intent on reinforcing lifestyle behaviours already present in the system of everyday life than in spreading commercially risky innovations' (1999:185).

This suggests that although Dove appears to be groundbreaking in that it does challenge current ideologies, it is not the only company to be doing this.
The L'Oreal make-up advert advertises its product for a wider clientele by showing the range of foundation shades available.

Although the women in these adverts are models and are all young the brand does have products which appeal to mature women also.
In current advertising a lot of emphasis is placed on creating products which appeal to different age ranges, so although there are a range of products created they appear to be more personal and thus more specific.

This is also something that the Dove adverts attempt to do.
By attempting to distance themselves from established ideologies the adverts aim to personalise their brand, and do so by using a range of models.

They also personalise the brand by the use of linguistic codes.
The perfume adverts often rely solely on the name of the perfume to connote the main themes of the advert, for example 'Lovely', however as Cook suggests 'even when a perfume is given a simple manufacturer's name this carries connotations of its country of origin or other products of the same manufacturer'(1992:108).

This is the case with 'Nina'.
The brand 'Nina Ricci' is well established particularly in Paris.

The brand name differentiates it from other products and is associated with elegance and rich flawless craftsmanship.
By wearing a perfume named after the found of the company gives a sense of importance and worth as if you too are worthy of wearing the perfume.

The Dove adverts are not advertising specific products but are using the brand name in the same way as 'Nina' does.
The use of brand names often have associated meanings with them, it is important to note that 'these meanings are entirely constructed by society' (Myers 1999:18).

The dove bird is associated with peace, and this feature extends to the product by implying that the products can help you relax and are gentle.
The brand 'Dove' is associated with affordable luxury, and this is mainly due to previous advertising campaigns.

The brand is widely available and affordable.
This is one of the reasons why the Dove campaign has been successful in that it has also attempted to personalise its consumers.

By having a range of women representing different ages, sizes and skin colours it likely that there will be one of the women that you as a consumer would relate to.
This is supported by the text accompanying the images.

FORMULA The text in the Dove campaign uses the form of questions.
This is particularly personal as it invites the reader to answer a question, this involves you directly and asks your opinion.

The language used is also personal, the use of the word 'your' is directly aimed at you, and the word 'society' involves you as it suggests you are part of it.
The use of the adjectives 'real' and 'true' are also interesting as they directly appear to question established ideologies.

The ideologies in question are the idea that feminine beauty is in young, slim and busty women.
The success of the Dove campaign is that it seeks to expose the traditional ideology of feminine beauty and alter it.

This semiotic analysis has enabled us to make explicit the signs that advertisers combine in order to create mythic meanings thus influencing consumers and drawing them into established ideologies.
As Danesi suggests 'Semiotics ultimately allows us to filter the implicit meanings and images that swarm and flow through us every day, immunizing us against becoming passive victims of a situation' (1999:21).

The Dove adverts appear to expose the implicit meanings in established adverts and make consumers aware of the ideologies that exist in society and culture, however in doing so they attempt to create their own mythic meanings.
By suggesting that there are different features of 'femininity' the Dove adverts are in turn creating their own ideology.

It is also important to note that the adverts although claiming to be innovative actively use recognised codes - especially photographic codes- to promote their advertising.
This enhances the power of the codes in use and further emphasises the importance of semiotic analysis as a tool to unravel the components of texts and thus 'create structures of meaning' (Williamson 1978:11).

Introduction:
Libben (1996) describes the study of aphasia as 'by far the most important tool in the investigation of language in the brain' (1996:424).

In general it has been noted that the extent of a language deficit depends on how much the brain has been damaged and the area of damage.
Certain theorists claim that particular locations of errors result in specific language deficits such as production and comprehension.

I am going to look at whether research into aphasia proves or disproves these claims.
I will be looking at the viewpoints of the antilocalizationists in terms of lesion location, selection of patients and briefly a comparison with normal speakers.

I will also attempt to determine if generalizing group studies is reliable, finally I will consider the role of the right hemisphere in language.
Historical background- :

Interest in the location of language in the brain began in the 19 th century when the first scientific studies on the brain were undertaken.
The work of Paul Broca was innovative.

Broca's worked stemmed from the ongoing phrenological theory that 'the moral, intellectual, and spiritual faculties of man were each the result of particular portions of the brain' (Caplan 1987:43).
By the time Broca had begun his work in 1861 most of the phrenologists claims had been refuted however the claim that language was located in a specific area of the brain remained.

Broca undertook an in-depth and ongoing case study of a patient who had a severe language deficit.
His patient appeared to have good comprehension but their only verbal output consisted of the syllable 'tan'.

Upon the death of the patient an autopsy showed a severe lesion to the frontal lobe of the brain.
Broca's findings were innovative in that they claimed to have located a particular language function to an area of the brain.

Broca claimed that 'the expressive apparatus for speech is related to a small area of cortex' on the frontal lobe (Caplan 1987:46).
Broca also put forward the theory of lateralisation in that 'the left hemisphere of the brain is dominant for expressive language'.

He also discovered that the human brain worked via 'lateral asymmetries' (Caplan 1987:46) in that the right side of the body was controlled by the left side of the brain and vice versa.
The work of Broca concluded that language was localised in the frontal lobe and thus this area of the brain became known as Broca's area.

Shortly after Broca published his findings Carl Wernicke put forward a set of contradictory ideas when he discovered that 'other aphasic syndromes related to lesions elsewhere in the brain' (Caplan 1987:47).Cases of aphasia were discovered which had lesions elsewhere in the left hemisphere and 'cases came to light of patients whose autopsied brains showed lesions of Broca's area, but who had not had disorders of language in life' (1987:49).
In 1874 Wernicke suggested that as there had been discoveries of sub-types of aphasia in the brain resulting from lesions in different areas; 'different areas (of the brain) accomplish different tasks' (Caplan 1987:50).

Wernicke's case studies showed patients who's speech was fluent and plentiful but were unable to make sense.
These approaches to aphasia dominated the field until recent years.

Anatomical diagrams of the brain label Broca's area and Wernicke's area as crucial in the location of language within the brain.
Broca's area has been associated with the ability to produce language and Wernicke's solely with the ability to comprehend language.

There are however some criticisms to these particular findings.
Anti-Localizationists

The anti-localizationists disagree with the theory that language functions can be localized within the brain and suggest that 'evidence for widespread cerebral activation in higher functions has been accumulating physiologically and anatomically' (Kertesz 1983:3).
Lesion Specific:

One of the main problems with limiting the localisation of a language feature to one area of the brain is the fact that in the majority of cases there is a 'co-occurrence of symptoms' (Caplan 1987:33), that is that most if not all of aphasic patients have more than one language deficit.
One explanation as to why this could be is due to the anatomical structure of the brain.

For example, 'agrammatism', the omission of grammatical words, is often accompanied by 'dysarthia', a disturbance of articulation.
It is suggested that the reason these two disturbances occur together is due to the fact that 'the neurological areas responsible....partially overlap, or are in close proximity in the brain' (Caplan 1987:34).

Thus in most cases a lesion in the brain will affect both these features of speech, however it is also important to note that on occasion a 'lesion will spare enough of one region to allow one of these functions to escape without disruption' (Caplan 1987:34).
Another problem with the localizationists is that one of their main arguments claims that the site of a lesion will cause the exact same language problem in all individuals.

Basso et.al (1985) suggest that this is not always the case.
The research of Basso et al.

focussed on the correlation between lesion location and the type of aphasia experienced by the patient.
Their findings suggested that not all patients omitted expected results.

For example: Three of the subjects were classed as 'non-aphasic' but had lesions in the classic speech area.
There were also two patients that were aphasic but had lesions outside the classic speech area.

There were also patients that had been diagnosed with global aphasia but had lesions that spared Wernicke's area.
The researchers admit that the number of published cases is 'too small to consider as 'exceptional cases'' (Bassoal.

1985:225) and that more research is necessary offer a 'satisfactory explanation' (Basso et al.1985:226).
However their findings do suggest that the theory of localization is not as clear cut as 'current tenets would have them' (1985:227).

Cappaal. (2000) suggest that the diagnostic labels of aphasia, Broca's aphasia in particular, are too broad and that they should be replaced by 'more detailed linguistic and neurological descriptions of the clinical cases' (2000:27).
This suggests that the 'characteristics' of Broca's aphasia are not as clear cut as originally implied and that 'Broca's aphasics...show a highly variable pattern of impairment' (2000:27).

This supports the work of Ulman (2000) who claims that the ideas of localizationist Grodzinsky (2000) are 'hampered by problems of patient selection'(2000:53).
Grodzinsky, fundamentally a localizationist, published a paper claiming that the function of Broca's area was more localized that had originally been thought.

Testing patients regarding their comprehension skills primarily takes the form of assessing the ability of subjects to understand sentences both in the active and the, more difficult, passive form.
This comprehension skill is classified as a syntactic ability.

Grodzinsky (2000) claimed that Broca's area was responsible for syntactic ability, as this was affected by damage to this area.
Bastiaanse & Edwards (2004) however found that the same function was also evident in patients with Wernicke's aphasia.

This suggested that the localisation of syntactic functioning is not necessarily located in one specific area of the brain.
Ulman (2000) further criticized Grodzinsky's (2000) claim by suggesting that 'conclusions regarding the function of Broca's region would be less problematic if patients were selected solely on the basis of their lesions' (2000:52).

Ulman (2000) suggested that the patients upon which Grodizinsky's claims were based had additional lesions outside Broca's area, and 'even worse, have no reported lesions at all' (e.g. Friedmann & Grodzinsky 1997).
Subjects:

The selection of subjects has proven problematic in the study of aphasia.
Edwards (2005) notes that diagnosis is a fundamental issue in that individuals may diagnose patients differently depending on experience in the field.

Aphasics can also be diagnosed via non-aphasic testing.
Equally certain methods can be interpreted differently, Kerschensteineral.

(1972) for example found that the fluency continuum was problematic in diagnosis as six aphasic speakers remained unclassified.
Although there are standard characteristics associated with the different types of aphasia these can overlap or may not be evident under limited testing sessions.

Research into localization suggest that patients experiencing certain symptoms will have lesions which correlate.
For example: Non-fluent speech correlates with Broca's aphasia.

However making generalisations into the localization of all language is limited by taking into account the selection of subjects.
In order to limit the variables in research, patients are selected according to certain specific criteria.

However as Basso et.al (2000) note as the patients are 'essentially right-handed, unilingual, literate adults who speak a language without tonal oppositions and write with an alphabetic or kindred code' (2000:202) they only represent one-fourth of the human population.
However as humans have language and their brains are ultimately the same it should be presumed that the location of language would also be the same.

Therefore if localisation theories are to be accepted they should be expected to include bilinguals, illiterates and populations who speak tone languages but Bassoal.
(1985) suggest 'there is reason to doubt these concepts apply in toto' (1985:227).

There is also criticism that the localisationists' research fails to compare to normal speakers.
Young & Hutchinson (2000) suggest that there is no assurance that all 'neurologically normal competent speakers would perform ideally' (2000:54) to the tasks that aphasics struggle with.

This suggests that the research of localising language functions within the brain pays little attention to normal functioning brains.
Dick & Bates (2000) suggest that when research has been done on normal brains evidence has shown that 'Broca's area itself is involved in many different linguistic and non-linguistic tasks' (2000:29) This could explain why diagnosing the specific features of Broca's aphasia is difficult as it is suggested that the area performs many functions.

This weakens the localisationist argument that specific language functions are solely located in this area of the brain.
Reliability of generalizing from studies:

It is also important to look at how group studies and individual studies can inform our knowledge about the localisation of functions within the brain.
The argument adopted by the functional analysis approach claims that 'agrammatism' is a result of the 'dysarthia' because the patient simplifies his speech purposely.

This is so as to produce the fewest words possible as pronunciation is difficult for them.
It is important to note that if the functional account of the co-occurrence of symptoms is correct then 'the symptoms in question should always co-occur because both are a consequence of the same functional disturbance' (Caplan 1987:35).

If we look at group studies the symptoms should in theory be universal across everyone.
Thus all damage to that particular area of the brain should result in the same language deficiency.

In practice, matters are much more complex.
Caplan notes that often published studies do not 'show consistent groupings of symptoms or subjects' (1987:152).

It is often possible to find explanations for 'exceptions to symptom groupings' (1987:35).
Caplan (1987) notes how individuals may be exceptional in the way their cognitive functions are organized thus the disturbances they experience may lead to different overall disturbances.

For example; a person may speak by accessing purely motor representations bypassing auditory memories of the sounds of words.
Therefore a disturbance of permanent memories for the sounds of words might lead to a disturbance in comprehension with disturbances in spontaneous speech.

This indicates that although we can map certain language processes within the brain it is sometimes difficult to generalize as there are exceptions to how individuals may function.
The patient you are looking at could either be an exception or representative of many.

However, it is therefore interesting to note that 'there are very few examples of associations of symptoms which hold reliably across any number of patients' (Caplan 1987:36).
Inter-individual variability suggests that making inferences about groups of patients is difficult, as there can be exceptions which weaken the exact theory of localization.

Left Hemisphere Dominance:
Broca's original findings suggested that lateralisation of the brain meant that the left hemisphere was dominant for language use.

This was originally hypothesised by Broca when after his patient's death he discovered damage to the left hand side of the brain.
His patient had paralysis to the right hand side and severe language problems.

Therefore Broca concluded that language was controlled by the left hemisphere.
This has been further proven by experiments such as the dichotic listening task.

This test found that most of input into the right ear goes to the left hemisphere of the brain and this became known as the right ear advantage.
However recent research has claimed that suggestions that the left hemisphere is exclusive in language representation are 'no longer sustainable' (Cappaal.

2000:27).
It must be noted that speech can still be understood via the left ear too.

There are two key reasons as to why this is; firstly 'the auditory pathways to the brain are not completely crossed' (Libben 1996:424) and secondly information between the two hemispheres can be transmitted across a 'bundle of fibres'(1996:424) known as the corpus callosum.
Studies on 'split-brain' individuals whereby the corpus callosum is severed show interesting findings.

Libben (1996) concludes that from the behaviour of split-brain patients 'that although the right hemisphere does show some language understanding, it is mute' (1996:424).
For example when blindfolded the patients could name an object in their right hand but not when it was placed in their left hand.

This showed that 'the right hemisphere, which receives information from the left hand, knows what is there, but it can neither put this into words nor transfer the information across the severed corpus callosum to the left brain' (1996:424).
Both the above experiments supported Broca's theory of lateralisation.

However more recent research has placed more emphasis on the role of the right hemisphere on language.
Although the left hemisphere is seen to be the more dominant side in language the right side contributes more than was originally thought.

Libben (1996) for example notes how in some adults the removal of the left cerebral hemisphere results in most 'but not all of their linguistic competence' (1996:417).
This shows that the right hemisphere does play a role in language and that certain language features could be localised in the right hand side of the brain.

It is suggested that the right hemisphere has a distinct role in the normal language use of humans as those with damage to this side of the brain 'exhibited difficulty in understanding jokes and metaphors in everyday conversation' (1996:419).
Patients with such damage were only able to take the literal meaning of a figurative sentence.

Cappa et.al (2000) claim that neuroimaging work has shown 'right brain involvement in both language comprehension and production' (2000:28).
These findings suggest that language functions are located across the brain and may not just be limited to the left hemisphere.

Having said this it is important to note that it is generally agreed that complex grammar skills are localized to only one hemisphere (Lustepal.
1995).

Conclusion:
Research into the location of language within the brain is ongoing.

Using aphasic patients to help locate specific language functions within the brain has helped to suggest certain theories.
Over the years it has become more clear that although the left hemisphere is dominant in language the right hemisphere does have a role to play.

Lesions to particular areas of the brain do for the majority of the time cause specific language deficits however it is still unclear as to the 'precise topography and extent of cerebral damage required to produce Broca's aphasia' (Levine & Sweet 1983:185), thus it is difficult to make associations between lesion location and lesion damage.
As I have discussed, findings have shown that there can be 'exceptions' as to where the lesion is located and the type of aphasia found.

Although these findings are limited to single case studies and may be down to inter-individual variability they provide enough doubt to suggest that localisation of specific language functions within the brain is not as clear cut as originally thought.
Introduction:

The teaching of reading involve two main routes, the lexical route and the sub-lexical route.
The lexical route involves a teaching of words as a whole.

Goodman (1996: 91) who supported the whole-word theory of reading suggested that 'an efficient reader uses only enough information from the published text to be effective'.
This has been disputed by many who place more emphasis on the 'graphophonic system' (Gough and Wren, 1999:67) and the individual role of letters and words rather than whole texts.

Adams (1990:105) suggests 'the single immutable fact about skilful reading is that it involves relatively complete processing of the individual letters in print'.
This theory places more emphasis on the sub-lexical route whereby decoding via grapheme-phoneme correspondence is vital.

Goodman's (1967) most significant claim was that reading was a 'guessing game'.
His theory suggested that skilled readers use context to anticipate the words in a text therefore do not need to decode every word.

Contrary to this theory is the more recent idea that a skilled reader is someone who can decode automatically without being conscious of it (Field: 2003).
This theory also suggests that 'less skilled readers rely heavily on context' (Perfetti 1999:43) as a substitute for poor decoding skills.

It has been claimed that reading involves a dual-route, combining both the lexical and sub-lexical routes.
This essay will seek to identify to what extent readers rely on decoding and to challenge the misconceptions of the role of context.

These misconceptions include the varied uses of context amongst skilled and less skilled readers, eye movements in reading and the demands on working memory.
Varied uses of context in the skilled/less skilled reader

The extent to which a reader relies on context depends heavily on the readers level of skill.
Recent research has discovered that 'context aids poor readers more than good' (Nicholson:1991 cited in Gough & Wren: 1999:72) and that it is used by less skilled readers as a support for weak decoding skills.

Perfetti suggests that 'less skilled readers are slower in identifying words, because they lack basic word identification skill; thus they are able to benefit from the additional boost of context' (1999:46).
This emphasises the importance of decoding.

In the case of an unskilled reader external sources can be used to help give clues to work out what a word is likely to mean.
Once readers have mastered the skill of decoding context can be used in order to interpret the meaning of the text.

Perfetti suggests that 'the hallmark of skilled reading is fast context-free word identification combined with a rich context-dependent text understanding' (1999:46).
Stanovich (1980) suggests that to a skilled reader context can be used to 'enrich understanding' (cited in Field 2003:121).

Yuill and Oakhill (1991) expand on this by suggesting that 'skilled readers adjust their reading goals, makes inferences, use the context, and monitor their comprehension during reading' (cited in Perfetti 1999:44).
Context is a necessary component of understanding but unlike Goodman suggested it is not an effective substitute for decoding skills.

Eye movements
Goodman's (1967) theory that reading is a 'guessing game' suggests that readers do not need to decode every word as they can use context to anticipate them.

This implies that readers can skip over words.
Research on eye-movements in reading, however, suggest that this is not the case.

In order for the eyes to gain information they must be at rest, fixated on a specific region of the text.
When the eyes move in rapid saccades there is no information gained.

Perfetti suggested that the 'idea that skilled reading involves a strategy of sparse sampling words coupled with a heavy use of context' (1999:44) cannot be supported.
Rayner and Pollatsek (1989) demonstrated the limits of peripheral vision in actual reading.

They suggested that the eyes can only make out 4-6 spaces to the right of the eye's fixation.
This suggests that readers can only make out one word and possibly the first couple of letters of the following word.

The idea that context can be used to guess words is therefore unfounded.
Perfetti concluded that 'readers must read lots of words in order to read effectively' (1999:45).

However it is important to note that the readers technique can be affected by the purpose of reading.
Reading for gist, for example, can involve fewer eye fixations than reading to answer questions (Just and Carpenter, 1987).

Oakhill & Garnham suggest that 'skilled readers are therefore people who can get information that they want from a text efficiently' (1988:7).
It has also been found that there are shorter fixation times for predictable words, (Ehrlich and Rayner, (1981) however unlike Goodman suggested 'this context effect does not extend to skipping words' (Perfetti 1999:45).

Despite being predictable, words still have to be read.
Regression

As it has been discovered fixations are necessary in order for information to be gained from a word.
Interestingly both skilled and less skilled readers appear to make the same number of fixations, however, as Field notes 'what marks out less skilled reading is a much higher level of regression' (2003:75).

Regression is not only used more by less skilled readers but it is also used for a different purpose.
Field suggests that regression in skilled reading is often associated with building higher-level meaning, regression in unskilled reading however 'usually serves to check that words have been correctly decoded'.

(2003:75).
This again supports the idea that decoding is paramount in the process of reading.

Demands on Working Memory :
Goodman's (1967) key argument was that readers use context to predict the words they are reading.

There are two main problems with Goodman's argument.
Firstly is the idea that very few words are predictable.

Fischler and Bloom (1979) suggested that predictability must be near 0.9 before context can assist a word's recognition, however very few content words reach this level.
A study by Gough and Wren (1999) suggested that only 40% of function words and 10% of content words could be correctly anticipated.

This is a very low figure and the authors concluded that 'it would be a waste of the reader's time to try and predict them; he or she is better off simply recognising them' (Gough & Wren: 1999:73).
Ellis goes on to suggest that certain words may not necessarily be predictable but more familiar 'some words are recognised more easily than others' (1993:14).

The second problem with Goodman's argument is that using context makes more demands on the reader.
Context use is 'more under the conscious control of the reader...

because of this, they make considerable demands upon Working Memory' (Field, 2003:25).
Baddeley (1986) suggests that the capacity of the Working Memory is known to be strictly limited.

Because of this Goodman's theory does not hold much strength as it has been found that skilled readers make less use of context.
Decoding is highly automatic in a skilled reader 'this means that they make few demands upon Working Memory, leaving capacity for higher level processes such as building overall meaning' (Field, 2003:25).

This suggests that the aim of a skilled reader is automatic decoding.
Perfetti's (1985) 'Verbal efficiency theory' suggests that 'with less skilled readers the decoding process is more controlled (i.e. conscious), the burden upon Working memory is greater leaving fewer resources for meaning building'.

(cited in Field, 2003:121).
He goes on to suggest that the slow decoding in unskilled readers results in smaller amounts of information being supplied to Working memory.

'This leads to a focus upon local rather than global meaning relations' (cited in Field 2003:121.
This supports the idea that context is used in less skilled readers to support them when their decoding is weak.

Stanovich proposed that 'word recognition by skilled readers dealing with common words that are clearly written is so fast and automatic that contextual priming is unnecessary' (Ellis, 1993:20).
This supports the idea that in skilled readers the ultimate aim is quick and efficient decoding.

This can been seen through two tests (Appendix 2).
The first test, the stroop test, challenges the reader to overpower the automatic decoding they have been trained to do as a skilled reader.

When asked to name the word (not the colour) the reader will often struggle.
This is the same with the second test whereby upon closer inspection the shapes of the letters are similar.

The 'H' resembles an 'A' but reader can still recognise the intended phrase.
This shows how reading processes have become automatised.

Such automatic processes make few demands on working memory.
Miscue analysis:

Gough and Wren (1999) disputed the findings of Goodman (1973) in their own miscue analysis task when they found very few miscues in their own results.
They suggested that 'the fact seems to be that skilled readers, reading a text like those they usually read, do so very accurately' (1999:63).

By conducting a miscue analysis on 3 students I attempted to see if my findings supported the theory of Goodman.
All three students were considered to be 'skilled' readers as they had no learning problems.

They were all competent readers and had to regularly read university level texts as part of their degree courses.
None of the students were familiar with the linguistic/scientific text I asked them to read.

All three appeared to have difficulty with the two words 'sulci' and 'gyri', technical terminology for parts of the brain.
All three pronounced the words differently and paused before them.

The fact that the students pronounced them differently suggested that they were using analogy to read the words.
For example: FORMULA Apart from these words there were very few errors.

The students made a total of 3 false starts and occasionally paused before unfamiliar words.
The most interesting errors came when the reader misread an unfamiliar word, replacing it with a familiar word.

For example: 'automatically' for 'anatomically' Or made consistent errors, for example 'cerebal' or 'cerebrial' for 'cerebral' These errors seemed to me to occur because of the less frequent letter combination of 'ral' compared to 'al' or 'ial'.
This supported the idea that automatic decoding can become so strong it can be difficult to overrule.

Few of the words in the texts could have been predicted using the context despite the readers level of skill.
The findings of this small test showed that the readers used analogies in order to read unfamiliar words.

Goodman (1967) also supported his 'guessing game' argument with a test by Gollasch.
His subjects read a text containing 6 errors (see Appendix 3).

Despite being told that there were errors in the text very few were able to relay all the errors.
Goodman suggested that since the readers did not report the errors, those errors did not affect their reading or understanding of the text.

Gough and Wren however dispute this suggesting that 'while they are still able to see past (or through) these errors and arrive at a correct interpretation of the story these errors do make meaning construction more difficult' (1999:68).
They came to this conclusion by looking at reading-time data, sentences containing errors were read slower.

This again helps to suggest that decoding is an important part of reading, more so than context.
Conclusion

Context is clearly important in gaining an overall understanding of a text but it has been suggested that Goodman's account of reading 'more accurately characterises beginning readers than skilled readers'.
(Oakhill & Garnham 1988:33).

It is important to note that 'the use of context decreases as reading skill increases' (Oakhill & Garnham, 1988:86).
Crucially the way context is used also varies depending on the readers skill.

Oakhill and Garnham suggest that 'context is more important for ambiguous words', for example, 'bug' could refer to an insect or a spy/surveillance (1988:13).
The role of decoding therefore seems to be more important in current reading theory.

Automatic decoding means that readers can identify words with little effort and fewer demands on the working memory.
When this skill is mastered context can be used to enhance understanding, Perfetti suggests 'the hallmark of skilled reading is fast context-free word identification combined with a rich context-dependent text understanding' (1999:46).

It can therefore be concluded that decoding is an essential part of reading and that context has two roles, depending on the skill of the reader.
Introduction

System identification is a way of constructing mathematical models for dynamic systems from experimental data.
Recursive identification is useful for adaptive control, adaptive estimation and adaptive signal processing.

The examples of this are pattern recognition, linear prediction, adaptive filtering, monitoring and fault detection.
In control and system engineering, system identification techniques are used to obtain appropriate models of systems.

Recursive identification plays a vital role in many control systems such as aircrafts, missiles, automobiles and ships.
Dynamic characteristics of such system depend on many factors such as speed and loading, which might change over time.

Adaptive control is the area concerned with design of controllers and regulators in order to adjust controlled parameters.
This report relating to the use of least squares system identification is separated into three parts corresponding to the questions given and a condition to summarise the report.

In part A, a test system is created by producing MATLAB code which can perform the recursive least squares system identification for any given input and output data.
Some real data has been given in part B and this data is used to identify the system parameters for a mechanical "master" robot of a master/slave telemanipulator.

Part C is the extension of the existing RLS algorithm by including Instrument Variables or a PLS noise model which is based on the moving average model or pseudo linear regression.
Part A

Implementation and Test
The task of part A is to complete the MATLAB code which is given to be able to use the recursive least squares system identification using ARX model on any given input and output data.

In this part, the given skeleton code which was given to be able to represent the ARX model in term of being able to identify a model of any given input and output data (u and y).
Any input signal can be generated in order to test the ARX model.

This is given in the form of three commands such as a step input, sampled sine wave signal and random signal.
The filter function is used to filter a data sequence by using a second order system model which is stable.

In this model, the values of 'B' and 'A' are the numerator and denominator coefficients respectively.
This program shown in Appendix A is tested with all three input commands.

Result and Discussion
As can be seen from figure 1 to 3 and table 1, the results of this program seems to run correctly because the coefficients of a model in theta track towards the "true" values [A B] and the final value of the estimate is the same as the "true" value whether the input is step input, sine wave input or random input data.

Furthermore, this program is confirmed that it works correctly by testing the response using three command inputs when Gaussian noise is added to the output.
This code has been shown in Appendix B As can be seen from figure 4, 5, 6 and table 2, Gaussian noise has an effect on the coefficients of a model.

This causes that the final value of parameter estimates using different type of inputs are different.
The figure 4 shows the result of the program which use step input including Gaussian noise.

This result illustrates that estimated parameters converge to final values slowly, compared with the result of the program using step input without Gaussian noise (figure 1).
Similarly, the coefficients of a model resulting from the use sine wave and random data input also converge to final values slowly.

In addition, if the lengths of each input data are increased, this may encourage the adaptation of estimated parameters.
However, it ultimately depends on the level of noise.

The modification of the algorithm including the "Forgetting factor" may be the way to improve the performance of the program.
This is because the older data has less effect on the estimation of coefficients of a model.

The new implemented algorithm has been shown in Appendix C.
The sample result of the program being tested with a step input will be presented.

The figure 7 shows the result of the modified algorithm including the "Forgetting factor".
The convergent rate seems to be more efficient than the previous result (see figure 4).

The output, however, depends on the level of noise; consequently, it is difficult to compare the results with different noise level in order to know the performance of the "Forgetting factor".
Part B

Implementation and Test
The task of Part B involves identification of the system parameters for the master of a head controlled telerobot using the program called "loadass.m" in order to load data files containing the response to a 60mm step input.

This step response relates to Kp and Kd in a PID controller which can help to determine the accurate dynamics model of the telerobot.
The data sets used in this assignment are xp30d03.dat and xp50d20.dat which is selected by the loadass.m program.

The former data set means that the proportional gain was set to 30 and the derivative gain was set to 3 whereas the latter dada set means that the proportional gain was set to 50 and the derivative gain was set to 20.
The corresponding step input was generated by the command u = 60*ones (size(y)).

The MATLAB code has been changed (see Appendix D) before applying further parameters to generate tests using both data sets for the following four models to evaluate which the best model is.
FORMULA The development of the coefficients of each model has been shown in each data set.

The best model is also determined by comparing the actual response and predicted response simulated the response to a step input of 60mm.
In addition to this, the z transform of all models have been done in order to find the location of the final poles which are plotted on the unit circle.

This can show whether the predicted model is stable or not and also determine the best model for predicting this mechanical system.
Result and Discussion

The results of system response show that both actual and predicted responses seem relatively similar for all four models in each data set, as can be seen from the figure 8 and 9.
The actual and predicted responses are underdamped response as shown in figure 8, whereas the figure 9 shows that the actual and predicted responses are critically damped response.

Overall, these predicted models are effective in modelling.
The results of Model1 using data set xp30d03.dat

As a result of the figure 10, the model output can be shown as FORMULA where e(t) is a white noise with zero mean and variance 2.
In addition, the figure 11 shows a z transform of predicted response, which presents the stability of system model.

This predicted response seems to be stable because the two poles locate inside the unit circle.
The results of Model1 using data set xp50d20.dat

As a result of the figure 12 the model output can be shown as below.
FORMULA Furthermore, the locations of two poles are inside the unit circle of the z transform shown in figure13.

This means that this predicted response is stable.
The results of Model2 using data set xp30d03.dat

This model uses three coefficient of 'a', which is different from previous model.
As a result of the figure 14 the model output can be shown as below.

FORMULA The z transform in figure 15 shows that the predicted response is stable.
The results of Model2 using data set xp50d20.dat

As a result of the figure 14 the model output can be shown as below.
FORMULA The predicted response is also stable, but there are two poles which are complex values (see figure 17).

The results of Model3 using data set xp30d03.dat
This model is slightly different from those previous models.

In other words, both the number of coefficients of 'a' and 'b' are three.
As a result of the figure 18 the model output can be shown as below.

FORMULA Moreover, the predicted response is stable (see figure 19).
The results of Model3 using data set xp50d20.dat

As a result of the figure 20, the model output can be shown as below.
FORMULA The predicted response is also stable (see figure 21).

The results of Model4 using data set xp30d03.dat
This model uses 4 coefficients of 'a' and 3 coefficients of 'b' to predict system response.

As a result of the figure 22, the model output can be shown as below.
FORMULA Furthermore, a z transform shows that the predicted response is stable (see figure 23).

The results of Model4 using data set xp50d20.dat
As a result of the figure 24, the model output can be shown as below.

FORMULA The predicted response is also stable as shown by a z transform from figure 25.
The results of this model after changing in data set are similar to the previous model using the same data set.

In other words, both actual and predicted responses are critically damped response and the predicted response is also stable as can be seen from the figure 25 and 26, respectively.
As can be seen from figure 26, this figure plotted in logarithm scale shows the loss function FORMULA or the Mean Square Error for each model.

The values of the loss function for the model1 and 2 are far higher than the model3 and 4.
The model3 and 4 are, therefore, more preferable as the model1 and 2.

This is because the model3 and 4 are large enough to cover the true system.
In addition, the model3 is preferable as the model4, but not remarkably better than the model4.

The reason for this is that the model4 is more complex than the model3, but the values of the loss function resulting from both data sets for model4 are almost the same that for model3.
It can be said that the model3 seems to be the best model for these data set.

Part C
Implementation and Test

The task of Part C is to update RLS algorithm by including Instrument Variables (IV) or a Moving Average noise model based on Pseudo Linear Regression.
The model based on Model3 in part B has been modified by using the instrumental variable method in order to test in this task.

FORMULA First of all, MATLAB algorithm was tested in the same way with part A, as can be seen this code in Appendix E.
The algorithm has then been modified in order to test with the real data set as above (see the code in Appendix F).Moreover, two ARMAX models were developed based on Model3 in part B because this model seems to be the best model.

The algorithms for these models have been shown in Appendix G. FORMULA
Result and Discussion

The results of a test system including instrument variable are the same results as part A.
This means that the modified MATLAB algorithm works appropriately with a test system.

The results of IV model
As can be seen from figure 27 and 28, the estimated parameters resulting from the instrumental variable method are slightly different from Model3 (ARX model).

This leads to the slight difference in the location of some poles, but both predicted responses are stable and they are also similar to the predicted responses of other models which are underdamped response (see Appendix H).
The estimated parameters resulting from the instrumental variable method also seem to converge quickly, compared with Model3 (see figure 28).

In other words, the instrumental variable method leads to higher rate of convergence, compared with the result of Model3 using ARX model (see figure 18 and 20).
As a result of the figure 27, the model output can be shown as below.

FORMULA From figure 28, the model output resulting from the use of data set xp50d20.dat can be also shown as below.
FORMULA v(t) is system noise which is uncorrelated with the instrumental variable (vector z(t))

The results of ARMAX1 using data set xp30d03.dat
From figure 29, the model output resulting from the use of data set xp30d03.dat can be shown as below.

FORMULA The result of ARMAX1 model is slightly different from that of ARX model.
As can be seen from the figure 30, some values of predicted response are negative in the first periods of estimation.

This results from the prediction error (t-1) as shown in the figure 31.
However, both actual and predicted responses are underdamped response and they also seem similar.

In addition, a z transform shows that the predicted responses is stable (see figure 32).
The results of ARMAX1 using data set xp50d20.dat

As a result of the figure 33, the model output can be shown as below.
FORMULA From figure 34, both of them are critically damped response.

In addition to this, the predicted responses are relatively similar to actual response.
The z transform also shows that this predicted response is stable (see figure 35).

The results of ARMAX2 using data set xp30d03.dat
As can be seen from figure 36, the model output can be shown as below.

FORMULA In this model, the predicted response seems better than ARMAX1 in the first period.
Both actual and predicted responses are underdamped response.

Their responses are nearly the same, as shown in figure 37.
The z transform shows that predicted response is also stable (see figure 38).

The results of ARMAX2 using data set xp50d20.dat
From figure 39, the model output can be shown as below.

FORMULA In this model, the response and stability are almost the same as ARMAX1 model, except the pole locations.
As can be seen from figure 29 to 41, pseudo linear regression or the RLS algorithm for ARMAX model works appropriately, although the rate of convergence of estimated parameters is not significantly different from RLS algorithm for ARX model.

As the determination of model structure and model validation are very important for system identification.
This is because the estimated model should be accurate enough and avoiding over-parameterization should also be considered.

The reason for this is that an over-parameterized model structure cause complicated computations.
In determining an appropriate model structure, the loss function or the Mean Square Error are very useful for model validation.

The loss function or the Mean Square Error was, therefore, plotted in order to investigate whether or not the improvement in the fit is significant.
As can be seen from figure 42, both ARMAX models cannot improve the fit for those data sets.

The instrumental variable method is also not significantly better than Model3 using ARX model in case of fitting model for those data sets.
Conclusion

In this assignment, the use of least squares system identification was presented.
Recursive least squares algorithm was used for the tasks.

The test system in part A demonstrated the ability of these algorithms to ensure that the program will work appropriately with the given real data collected from a mechanical "master" robot of a master/slave telemanipulator.
Each model was then tested with the same two data sets so that each model can be compared in the same data sets in part B.

In general, the test results illustrate that recursive least squares algorithm is effective in parameter estimation and modeling.
Determining the appropriate model structure and model validation can be achieved by using the loss function or the Mean Square Error.

Based on these results, it seems that Model3 using ARX model is optimal model for model representation of this dynamic system.
In part C, the algorithms was implemented by using RLS algorithm including either Instrument Variables (IV) or a Moving Average noise model based on Pseudo Linear Regression in order to improve parameter estimation.

The Model3 which seem to be appropriate model in part B was compared with all models in part C called IV model, ARMAX1 and ARMAX2 model.
The result of this is that the IV model may do well on an increase in the rate of convergence, but it is difficult to see a noticeably improvement in performance.

Pseudo linear regression for ARMAX model and instrumental variable method may overcome the problems which the disturbance v(t) is not white noise.
To check this, autocorrelation and cross-correlation test should be implemented, but such tasks are beyond the scope of this assignment.

Abstract
The major concern of this assignment is to develop MATLAB code in order to design and compare controllers for various systems consisting of a proportional controller, the Modified Ziegler-Nichols (MZN) PID controller and the Bode's Maximum Available Feedback (MAF) design method.

The main feature of controller design methodology for effective feedback control system is that it has been designed to achieve high loop gain and stability for a given bandwidth and system specification.
The MATLAB codes proposed in this assignment have been implemented and tested with given relevant test systems.

The experimental results show that the Bode's Maximum Available Feedback (MAF) design method seems the best feedback controller design method.
1. Introduction

Control theory deals with the behavior of dynamic systems.
A closed loop control system or feedback control system play a vital role in the improvement of the control of dynamic system.

The main advantage of feedback control system is that a feedback loop can achieve the setpoint and reduce the effect of the variation of process characteristic of a control system, resulting from external disturbances, for example [1], [2].
The closed loop transfer function is FORMULA and the closed loop disturbance transfer function is FORMULA As can be seen from equation (1) and (2), if FORMULA or loop gain is high, the output will be independent of A and it will not be affected by external disturbance.

The feedback control system should, therefore, have high loop gain with stability.
To avoid instability, a controller should be added to the feedback control system.

A proportional control is a solution to this.
The property of the proportional controller is simply proportional to the control error in case that error is small [1].

The proportional controller (k) is 1/gain at angular frequency when the phase is -180+phase margin.
The loop transfer function will then be proportional controller gain*original transfer function.

The disadvantage of this controller, however, causes a steady state error.
The PID control can therefore alleviate this problem.

A PID controller is by far the most common feedback loops component in industrial process control system.
This controller provides three terms: the proportional term (P) is proportional to the error, the integral term (I) is proportional to the integral of the error and the derivative term (D) is proportional to the time derivative of the control error [1].

The advantages of such a controller are that the steady state error will be eliminated and the transient response when transfer function is a second order will be improved [2].
The Modified Ziegler-Nichols (MZN) PID controller is one of the more common way of tuning PID loop.

The controller can be shown as: FORMULA The need to effective feedback control system is a large loop gain over a wide range of low frequency.
In other words, the bandwidth must be wide.

The loop gain is, however, limited because of the dependence of the phase shift on change of gain.
To deal with this issue, the Bode's Maximum Available Feedback (MAF) design method was purposed.

The bandwidth over maximum possible loop gain, gain and phase margin must be specified for this design method.
This methodology provides a final loop transfer function: FORMULA

2. Implementation
This assignment was divided into 6 tasks.

Task 1
In this task, the function pf0607find in pf0607.m was implemented.

This function was used to find gain (m), phase (p) and angular frequency (w) at particular position (pos) which is near specified value (WHAT).
To achieve this, Bode(num, den) command was firstly called to obtain the values of gain, phase and angular frequency in vector form.

The value of pos can be found by using the command as following.
FORMULA Some codes were added to check the size of pos because the two or more position (pos) at the same values may occur.

In this case, only one value of pos was chosen in order to avoid the stopping of the operation of program.
Furthermore, the need to check for the pos at the beginning or end of array is important.

The codes for the extension of the range of angular frequency were added in this program.
This angular frequency logarithmically spaced in the range w[pos-1] to w[pos+1].

The new range of angular frequency can be obtained by using logspace(log10(w(pos-1)),log10(w(pos+1))).
Further iterations will provide more accuracy of results.

The Bode command was called all over again in specify angular frequency until the values of gain or phase reached the accurate results set at 0.0001 for the difference between gain or phase and WHAT.
The codes in this task, finally, return the values of gain, phase and angular frequency.

The more detail can be seen in appendix.
Task 2

There are two parts in this task.
First, the system was taken as specified in arguments to this program (pf0607) and then turned into a structure.

The structures for uncompensated and designed systems were defined as a 'struct sys' with the following.
FORMULA Second, a proportional controller for such system was designed with 45 phase margin by the following function.

function des = pf0607prop (sys, pm, dofreqtime) To find a proportional controller, a system 'sys' was turned to a design 'des' at first.
The function pf0607find from task1 was then used to find angular frequency and gain when the phase is -180+PM.

A proportional controller was then calculated by the following algorithm.
FORMULA The new numerator of loop transfer function of designed system including the gain of the proportional controller can be found by des.num = des.num*k.

The frequency and time domain performances were called by the following codes respectively.
FORMULA A string version was returned by using fnumstostr.

The more detail can be seen in appendix.
Task 3

The code in this task was written to find frequency domain performance which is d.c.
loop gain of the system (DCG), the actual bandwidth (b/w), the actual gain margins (GM) and phase margins (PM) and also to find time domain performance which is time to peak(Tpk), %overshoot(%os) and steady state output (Oss).

The d.c.
loop gain of the system can be found by using Bode command to find the gain where angular frequency is zero.

The actual bandwidth (b/w) can be found by using the function pf0607find to find angular frequency where the gain is FORMULA.
The d.c.

loop gain and actual bandwidth was, however, set to Inf and NaN respectively, in case that the d.c.
loop gain of the system is infinite.

The actual gain and phase margins can be also found by using the function pf0607find to find the gain where phase is -180 and the phase where gain is 1 respectively, before the actual gain and phase margins are calculated by the following algorithm.
FORMULA The codes for adding polynomials were generated in order to find the closed loop transfer function.

The time domain performance was then found by calling the STEP command on the closed loop transfer function.
The search for time to peak can be achieved by finding the time where the response is maximal.

The steady state output is the gain where angular frequency, whereas the percent overshoot was found by the following algorithm.
FORMULA where M pf is the peak value of response A string version was then returned by using fnumstostr.

The more detail can be seen in appendix.
Task 4

In this task, the Modified Ziegler Nichols (MZN) PID controller was designed by the implementation of following function.
FORMULA This function returns in 'des' with a PID controller.

The code for this function was implemented by using Bode command after a system 'sys' was turned to a design 'des' in order to find the gain and phase of uncompensated system at a particular frequency (w) and then find the difference between desired and actual phase (dph) at a particular frequency (w).
The MZN PID controller was designed in the following form.

FORMULA The values of K P, T i and T d can be found by the following algorithm.
FORMULA The new loop transfer function will then be C(s)*Plant P(s).

The frequency and time domain performances were called by the following codes respectively.
FORMULA A string version was returned by using fnumstostr.

The more detail can be seen in appendix.
Task 5

This task is to design the compensated system by using Bode Maximum Available Feedback method.
The codes have been written to find the maximum possible loop gain over a specified bandwidth, gain margin and phase margin and to find the corner frequency.

The code was generated to calculate the frequency when gain is 1 (w a) by using the function pf0607find.
After that, each parameter was calculated by the following formulae.

FORMULA, for finding relative phase margin FORMULA, for finding the reciprocal of the gain margin FORMULA, for finding the end angular frequency of the Bode Step FORMULA, for finding the start angular frequency of the Bode Step FORMULA, for finding the maximum loop gain over a specified bandwidth These parameters produce the loop transfer function in the following form.
FORMULA The frequency and time domain performances were called by the following codes respectively.

FORMULA A string version was returned by using fnumstostr.
The more detail can be seen in appendix.

Task 6
This task is to combine the design of an uncompensated system, proportional controller, the MZN PID controller and bode maximum available feedback in the form of struct array.

First of all, the codes for an uncompensated system have been written.
The three compensated systems were combined by calling the function pf0607prop, pf0607pid and pf0607bode in task 2,4 and 5 in order.

Finally, the function pf0607plot(syss) was written to plot the results as graphs.
The code for this is shown in appendix.

3. Testing
The code in each task was tested by system 0 which is defined as an uncompensated system (NUM/DEN) with BETA in feedback path as shown below: FORMULA with relevant data before testing all systems by using data in form as data = [pm for Prop, w, pm for PID, w, gm, pm for bode] which is FORMULA The results will be compared with the answer for test system-graphs If the results is correct, this program will be tested by five different systems as shown below.

FORMULA For system5, K and specified data must be found in case of Bode and PID: %os = 25; T pk = 1s ± 5% To achieve this, this system was, therefore, set by K = 408 and data = [45, 2.83, 41, 0.15, 15, 54].
4. Results

The string fields for the 3 designed systems have been shown as below.
FORMULA On system1, the results show that this system is stable on all designed methods.

A proportional control is the slowest system and the percent overshoot of response is quite high; furthermore, the steady state error is highest, compared with the other controller.
The MZN PID control is the fastest system, but the percent overshoot of response is still high.

This causes more oscillatory.
Whereas bode design is slower, but the percent overshoot of response is lower than that of the MZN PID control although it is higher than 20%.

In addition, the Bode design can reduce disturbance over more range of frequency as shown in figure 1.
The MZN PID control reduce more disturbance than bode method at low frequency until around 0.018 rad/s, while the Bode design reduce more disturbance than the MZN PID control between 0.018 and 0.12 rad/s.

The string fields for the 3 designed systems have been shown as below.
FORMULA On system2, the results show that this system is stable on all designed methods.

A proportional control is the slowest system and the percent overshoot of response is quite high; furthermore, the steady state error is highest, compared with the other controller.
The percent overshoot of responses of Bode design and the MZN PID control is slightly different.

That of Bode design seems lower than the MZN PID control.
The MZN PID control, however, appears the fastest system and also reduce disturbance over more range of frequency.

The string fields for the 3 designed systems have been shown as below.
FORMULA On system3, the results show that this system is stable on all designed methods.

The times to peak for these designs are almost the same, but the percent overshoot of response and steady state error of a proportional control are quite high, compared with the other controller.
The Bode design seems better than the MZN PID control.

This is because it appears faster than the MZN PID control and the percent overshoot is lowest.
Moreover, the Bode design can reduce disturbance over more range of frequency and seems no positive feedback over all frequency.

The string fields for the 3 designed systems have been shown as below.
FORMULA On system4, the results show that this system is stable on all designed methods.

The times to peak for these designs are almost the same, but the percent overshoot of response and steady state error of a proportional control are quite high, compared with the other controller.
The bode design seems better than the MZN PID control.

This is because bode design appears faster than the MZN PID control, whereas the percent overshoot of the MZN PID control is higher than bode design and also more oscillatory.
Furthermore, the Bode design can reduce disturbance over more range of frequency.

The string fields for the 3 designed systems have been shown as below.
FORMULA On system5, the bode design provides a slight lower phase margin than a desired phase margin.

In addition, the Bode design can reduce disturbance over more range of frequency as shown in figure 5.
The MZN PID control reduce more disturbance than bode method at low frequency until around 0.018 rad/s, while the Bode design reduce more disturbance than the MZN PID control between 0.018 and 0.159 rad/s.

5. Discussion
The three compensated system designs have been presented in this assignment.

These designed methods consist of the proportional control design, the Modified Ziegler-Nichols (MZN) PID control design and the Bode maximum available feedback design.
In case of pure proportional control, the control action is simply proportional to the control error.

However, there are some limitations in a proportional control.
Such a control method results in a large number of steady state errors as shown in the experimental results.

The MZN PID control might be the solution to this problem.
This control method includes the term of the integral action which is proportional to the time integral of the error.

This experimental results show that the steady state error always becomes zero.
The Bode maximum available feedback design method is an alternative way to design a controller.

Based on the experimental results, the Bode maximum available feedback design method seems the best design method.
The reason for this is that such a design method provides the lowest percent overshoot of response and also provide the fastest system except system 1 and 2 in which show that the MZN PID control is faster than the Bode maximum available feedback design method.

Moreover, not only is Bode's method fast and less oscillatory, but also the rejection of disturbance.
This methodology is more likely to reduce disturbance at a wide range of angular frequency, compared with the MZN PID control.

6. Conclusions
This assignment presents the design of controller with a single feedback loop and the comparison with each design method.

The MATLAB codes were developed in order to design of the parameters of feedback control system and tested with various systems.
Based on these results, it is more likely that a proportional control appears a slowest response and far more steady state error.

Furthermore, the effect of disturbance cannot be rejected and also more oscillatory.
The Modified Ziegler-Nichols (MZN) PID control improves in frequency and time domain performance such as high loop gain and the fastest response in some systems, but a bit more oscillatory.

The steady state error can be eliminated and the rejection of disturbance is moderate.
The Bode's Maximum Available Feedback (MAF) design method is generally the fastest response and provides maximum possible loop gain over a given bandwidth.

Moreover, this method reduce disturbance effectively over a wide range of angular frequency.
It can be said that the Bode's Maximum Available Feedback (MAF) design seems the best method.

However, the robustness of control system should be tested.
ABSTRACT

This assignment tried to predict the occurrence of the tremor in Parkinson's disease from the local field potentials (LFPs) recorded from sub-thalamus by using artificial neural networks.
This may contribute to design a demand-driven intelligent stimulator.

The data recorded from sub-thalamus was processed by using some statistic method before being trained by neural network.
The experimental results show that it is possible to use neural networks to predict Parkinsonian tremor by using the LFPs recorded from sub-thalamic nucleus.

1. INTRODUCTION
Parkinson's disease (PD) is a neurodegenerative disorder characterized by many forms of change in motor function known by neurological signs like tremor, muscle rigidity, bradykinesia, akinesia and loss of postural reflexes [1].

Such symptoms tend to be associated with basal ganglia, thalamus and motor cortex [1].
Parkinson's tremor is the most common form of resting tremor caused by idiopathic Parkinson's disease [2].

This tremor of Parkinson's disease is one of the involuntary movement disorders affecting many people, in particular, elderly.
There are several possible therapies for Parkinson's disease.

These approaches include L-Dopa, antioxidants, drugs which stimulate dopamine receptors, drugs which block glutamate, drugs which decrease apoptosis, neurotrophins, high-frequency electrical stimulation of the globus pallidus and transplants of neurons from a fetus, but such treatments have some limitations [3].
Patients usually have internal globus pallidus Deep Brain Stimulation (DBS) for surgical treatment [2].

Some patients are implanted a deep brain electrode into thalamus or sub-thalamus.
Such treatments are less dangerous than lesioning, particularly subthalamic nucleus stimulation [4].

The cost of Deep Brain Stimulation (DBS), however, limits the number of patients using this to stimulate deep brain structure because of the frequent replacement of batteries [4].
Demand-driven Deep Brain Stimulation may be a solution to this problem.

To achieve this, the occurrence of tremor must be predicted.
Artificial neural networks (ANNs), the field of artificial intelligence, emerge from the development of the first neural model by McCulloch and Pitts in the 1940s.

Since then many researchers have become increasingly interested in artificial neural networks, commonly referred to as "neural networks" in [5].
There are many advantages of neural networks such as machine learning, generalization, adaptation and fault tolerance.

Artificial neural network can be applied to medicine or biomedical engineering domain.
The medical applications fall into four basic fields: modeling, bioelectric signal processing, classification for diagnosis and prediction for prognostics [6].

A lot of research shows that neural networks have been used successfully in many areas in prediction such as the prediction of ovarian cancer and also several types of cancer [7].
It is likely that neural networks might be utilised in the prediction of the occurrences of tremor in Parkinson's disease from sub-thalamic nucleus (STN) LFPs signal.

2. IMPLEMENTATION
2.1 Pre-processing

Figure 1 shows the local field potentials recorded from sub-thalamus with a sampling rate of 800 Hz.
This data will be used for an input vector.

However, a large number of the input of a neural network may increase the size of a neural network.
The reduction in input dimensionality is achieved by pre-processing.

To do this, the data should be analysed.
Figure 2 presents the envelop signal of the electromyogram (EMG) with the same sampling rate.

As can be seen from figure 1 and 2, the pattern between the amplitude of STN LFPs and that of EMGs is an alternating pattern.
In other words, the amplitude of EMGs increases sharply when the tremor occurs while figure 1 show that the variance of STN LFPs decreases dramatically at the occurrences of tremor.

Consequently, it is necessary to perform pre-processing procedures.
First of all, the input data will be scaled.

The approach for this is to normalize the mean and standard deviation of this data and the new data will then have zero mean and unity standard deviation.
The next procedure is to find the variance (2) of the normalized raw input data every 800 sample; as a result, the new data will be 100 input data in time domain (see figure 3).

Not only was pre-processing applied for an input vector, but also target vector.
The target vector based on EMG signal was set into two states after finding the mean value of EMGs data at each second.

The two states are defined as normal or abnormal conditions.
The onset of tremor is set to 1, whereas the cessation of tremor is set to 0.

2.2 The types of neural networks
There are many way to implement neural networks.

The two supervised learning neural networks like a Multi-layer perceptrons (MLPs) network and a Radial Basis Function (RBF) network are most commonly used in a lot of research, particularly feed-forward MLP networks [8].
The reason for this is that a wide variety of problems can be solved by MLP networks due to the capacity of input-output mapping [8].

In addition to this, MLP networks are more likely to outperform RBF networks and also requires a smaller number of parameters than RBF networks in case of nonlinear input-output mapping for the same degree of accuracy [5], [9].
As a consequence, MLP networks tend to be more accurate than RBF networks, although RBF networks train faster than MLP networks [10].

However, the relative importance of selection of the type of neural networks depends on the application.
This experiment, therefore, uses an MLP network for prediction of the occurrences of tremor in Parkinson's disease from STN LFP signals.

2.3 The neural networks architecture
In designing an MLP network, some variables must be determined.

These variables are the number of input node, the number of hidden layer, the number of hidden neurons and the number of output neurons.
The selection of these parameters depends on the problem.

However, MLP networks architecture is not completely constrained by the problem.
This is because the number of inputs nodes is constrained by the problem and the number of neurons in the output layer is constrained by the number of outputs which is required by the problem, whereas the number of hidden layers and the number of neurons in the hidden layers depend on designer.

In this experiment, there is one input parameter which is STN LFPs; hence, there will be one input node in the input layer.
For output layer, this should be one output neuron corresponding to a target vector.

In determining the number of hidden layers and the number of hidden neurons, there are no formulae for this.
The most common forms of the architecture of neural networks are one or two hidden layers and it does not need more than two hidden layer because the accuracy depends on the number of neurons per layer [8],[10].

Furthermore, one hidden layer with sigmoid transfer function can train for any mapping between inputs and outputs with arbitrary accuracy [9], [10].
In addition, the number of hidden neurons was determined by trial-and-error, which is the common way to select the number of hidden neurons [10].

This experiment is firstly to compare the accuracy of the number of hidden neurons which are 50, 100 and 150 and the performance between one and two hidden layers was then compared by using two topologies with the same input data and the number of neurons such as 1-100-1 and 1-50-50-1 network.
The former topology means the one hidden layer network with 1 input node-100 hidden neurons-1 output neuron and the latter topology is the two hidden layer network with 1 input node-50 hidden neurons-50 hidden neurons-1 output neuron.

2.4 The selection of activation function
There is some consensus on which activation function should be used for hidden neurons.

In general, a large number of researchers mainly use standard sigmoid or logistic activation function for hidden neurons [8].
The equation for this function displays as below FORMULA Moreover, the target output values range from 0 to 1.

This exactly matches a logistic activation function which has a range of [0, 1].
This seems well suited for any neurons in this experiment.

Such transfer function may contribute substantially to the accuracy and speed of training.
As a consequence of this, a logistic activation function has been selected for both hidden neurons and an output neuron.

2.5 The selection of training algorithm
The standard backpropagation algorithm based on the gradient descent algorithm often has a poor convergence rate and dependence on some parameters which is set by the user such as learning rate and momentum.

Such parameters are sensitive to the performance of algorithm.
Several variations on backpropagation have been developed with faster convergence such as conjugate gradient algorithm and Levenberg-Marquardt algorithm [11].

A modified conjugate gradient algorithm known as the scaled conjugate gradient algorithm based upon the conjugate gradient method was designed to avoid the time-consuming line search by using the Levenberg-Marquardt approach called the model-trust region approach in order to scale an appropriate step size [12],[13].
This makes the algorithm more computationally efficient.

Such an algorithm is a fast algorithm and seems to perform well over many problems, in particular for large networks; therefore, this is a very good general purpose training algorithm and it also work well with using early stopping method [12],[13].
This is the reasons why a scaled conjugate gradient algorithm was used in this experiment.

2.6 Data partitioning
As can be seen from figure 2, the tremor occur around 18 th and 50 th second.

In this experiment, the onset of the tremor will be predicted around this time.
To do this, two groups of data will be set by the division of each group into training, validation and test sets.

The first group for prediction of tremor around 50 th second was divided by the ratios 4:1:1 for training, validation and test sample, while the second group for prediction of tremor around 18 th second was divided by the ratios 1:1:1 for training, validation and test sample.
The reason for this is that these ranges are the same as the target values which have both states of tremor (0 and 1).

2.7 Training and testing
In this experiment, both group of input data was trained and tested by neural network toolbox in MathWorks Inc.

Matlab version 7.0.
In addition, the mean squared error (MSE) was used for the measure of performance.

However, some problems that might occur when the neural network is being trained are over-fitting and also poor generalization performance to unseen data.
To avoid the over-fitting problem, the stopping criterion or 'early stopping' approach which works well with a scaled conjugate gradient backpropagation was used.

3. RESULTS
Table 1 illustrates the accuracy of each topology.

As can be seen from this table, the topologies with 100 hidden neurons have the minimum values of MSE for both one and two hidden layers, which are 0.0118 and 0.0132 respectively.
It is clear from Table 1 that the best topology seems to be 1-100-1 network.

Table 2 demonstrates the performance of both one and two hidden layers by the comparison between the 1-100-1 network and the 1-50-50-1 network.
The MSE of 1-100-1 network is slightly different from 1-50-50-1 network.

Figure 4 and 5 illustrate the results of 1-50-50-1 network and 1-100-1 network respectively, compared with the target values.
Generally, the results of both networks are almost the same although 1-100-1 network is more slightly accurate than 1-50-50-1 network.

The 1-50-50-1 network almost reaches the target at the 15 th second, while the 1-100-1 network can achieve the target at this time.
It seems that both networks predict the onset of Parkinsonian tremor at the 15 th second and again at the 44 th second.

4. DISCUSSION
Overall, it is more likely that the onset of the tremor in Parkinson's disease can be predicted by using artificial neural network and STN LFPs.

It is evident that selecting the appropriate number of hidden neurons has a significant impact on the performance of training and generalisation.
Moreover, it appears that an increase in the number of hidden layers can lead to a slight decrease in the accuracy of training and generalisation.

As a result of this, the two hidden layers cannot improve significantly performance, compared to one hidden layer with the same number of hidden neurons.
Nevertheless, there may be some errors in experimental results for both networks.

Poor training performance might be a major problem in this experiment.
There is some possible reason for this problem.

Initialisation of weights may cause such a problem if the magnitudes of weights are too large.
Another problem probably occurs is poor generalisation performance.

There are several possible reasons for this.
Over-training may be the reason why generalisation performance on the test set is poor.

This is because the noise on the training patterns might be learnt.
Another cause of poor generalisation performance may be over-fitting.

The possibly reason for this is that the neural network learns the detail of the training patterns and then fit the noise causing a decrease in the generalisation performance on the test data.
A local minimum on the error surface might also cause poor generalisation performance.

It is worth noting, however, that the other considered approaches can achieve a comparable, or even better, performance of training and generalisation after fine-tuning the values of the parameters including the number of hidden neurons, the number of hidden layer initial weights and some parameters in training algorithm.
5. CONCLUSIONS

This assignment presents the use of artificial neural network to predict the onset of the tremor in Parkinson's disease from the local field potentials (LFPs) recorded from sub-thalamus.
MLP neural network was used for the prediction task.

Based on these results, it seems generally clear that the MLP neural network using one hundred hidden neurons is more likely to predict the onset of Parkinsonian tremor, even though it is not more effective enough because of the existence of errors.
The experimental results obtained suggest that the impact of the number of hidden neurons is significant for the performance of training and generalisation.

Overall, selecting one hidden layer with approximately 100 hidden neurons including logistic activation function and being trained by a scaled conjugate gradient algorithm appear to be an appropriate choice for MLP neural network in this experiment.
6. FUTURE WORK

In a future correspondence, alternative topologies need to be investigated, as well as the potential benefits from employing genetic algorithms [14], [15] for training neural networks along with an implementation of pruning the networks.
Genetic algorithms have a better chance of finding a global optimum; hence, it can avoid local minima in the error surface.

Network pruning techniques can eliminate unnecessary synaptic weights in a neural network.
Such a technique is known as the Optimal Brain Surgeon (OBS) algorithm [16].

Introduction
Over the last several years, many chemists worldwide have tried to form desired products by stimulating chemical reaction.

Traditionally, control agents used in chemical synthesis are microscopic variables.
They change such variables like pressure, temperature and concentration or combine reagents in such a process.

These stimuli cannot yield products by changing such thermodynamic variables and the combination of reagents also forms unwanted byproducts (Dahleh et al., 1996).
One possible way to deal with this problem is to control the motion of molecular in chemical reaction.

Since the invention of ultrafast laser technology, it is possible to control the motion of atoms, molecules, nuclei or even electrons by using the laser pulse shaping techniques.
The duration of the shortest pulse changed from the nanosecond regime to the femtosecond regime.

The reason for this is that the maximum speed of electronic light modulation device limits direct shaping in time to nanosecond regime (Krumpert, 2004).
Currently, femtosecond laser pulses can be generated directly from an optically pumped Ti:sapphire Kerr lens mode-locked laser oscillator (Krumpert, 2004).

With the advancement in femtosecond laser technology, a large number of scientists attempt to develop optical methods which use coherent light source or laser radiation to manipulate and control the outcome of chemical reactions in the gas and liquid phase by mean of controlling the behaviour of atoms and molecules.
To achieve this, chemical bonds on a microscopic level can be broken or made suitably without harmful byproducts.

The optimal approach using in this process is the computer-controlled femtosecond laser pulses shaping combined with learning algorithms in feedback loop or a self-learning loop.
These algorithms called evolutionary algorithms are used to optimise the shapes of such laser pulses with durations on the same time scale as the motion of the atoms or molecules.

Evolutionary algorithms (EAs) are a set of global, parallel, search and optimization methods, which mimic the biological process of natural evolution.
The term of evolutionary algorithms was adopted to describe the algorithms like evolutionary programming (EP) by Fogel, Owen and Walsh, evolution strategies (ES) by Rechenberg and Schwefel, genetic algorithms (GA) by Holland, genetic programming (GP) by Koza and classifier systems (Michalewicz and Fogel, 2000).

Evolutionary algorithms are inspired by the idea of evolution using the principles of natural selection.
These methods are robust tools, which is able to tacking the problems that cannot be solved by using conventional means.

The main benefit of this optimization method in molecular control is no requirement of priori knowledge of the mechanism of the complex system.
This essay will describes in what way the evolutionary algorithm methodology has been applied to molecular control.

Fundamental of evolutionary algorithms will be firstly described and then evolutionary algorithms in adaptive femtosecond pulse shaping as well as optimal quantum control will be described later.
Fundamentals of Evolutionary Algorithms

Evolutionary algorithms are a class of search methods with noticeable balance between exploitation of the best solutions and exploration of the search space.
This methodology combines elements of directed and stochastic search.

They are, therefore, more robust than existing directed search methods (Michalewicz, 1999).
Evolutionary algorithms simulate the natural selection process by using a number (population) of random individuals (potential solutions to the problem) to evolve through certain procedures.

Each individual within the population is represented through some form of genetic code as a string of numbers (bit strings, integer, or floating point numbers) in similar way with chromosomes in nature.
Each individual's quality within the population is represented by a fitness function tailored to the problem to be optimised.

In other words, a fitness value expresses how good the solution to the problem is.
Evolutionary algorithms begin to work by generating randomly the initial chromosome population with their genes taking values inside the desired constrained space.

Each individual's fitness function is then evaluated after Initialisation of population.
A set of stochastic genetic operators, manipulating the genetic code is used to perform evolution.

Such operators are applied to the population, simulating the according natural processes.
These operators are selection, recombination and mutation, which are used to provide the next generation chromosomes.

The recombination known as crossover operator creates new chromosomes (offspring) by the exchange of genetic material between chromosomes (parents) or different potential solution to the problem.
Two randomly selected chromosomes are divided in the same (random) position, whereas the first part of the one is connected to the second part of the second one, and vice versa.

The mutation operator creates new chromosomes by a small change in a randomly selected gene of a chromosome.
This operator is used to introduce some extra variability into the population.

The process of a new generation evaluation and creation is successively repeated, providing individuals with high values of fitness function.
Evolutionary Algorithms in adaptive femtosecond pulse shaping

A lot of research in the field of quantum control was developed remarkably, resulting from the implementation of femtosecond laser pulse shaping, combined with learning loop.
Femtosecond Fourier-domain laser pulse shaping has been developed by Weiner and his colleagues (Weiner et al., 1992) and refined by Wafers and Nelson (Wafers and Nelson, 1995).

Fourier transform pulse shapers using spatial light modulators, based on spatial masking of the spatially dispersed optical frequency spectrum, play a vital role in controlling the waveforms of ultrafast optical pulse (Weiner, 2000).
Several different types of spatial light modulators have been used for femtosecond laser pulse shaping, including liquid crystal modulator arrays (LCMs) (Weiner et al., 1992; Wafers and Nelson, 1995), acousto-optic modulator (AOM) (Hillegas, 1994) and deformable mirrors (Garduno-Mejia et al., 2004).

The experimental implementation of the design of laser sequences which have the ability to drive a reaction into a desired final state has been difficult to achieve.
This is because of the complicated nature of molecular Hamiltonians.

The theoretical approach using optimal control theory to solve the problem of quantum control of chemical reactions requires precise knowledge of molecular Hamiltonian.
Currently, the molecular Hamiltonian can only be calculated to solve Schrödinger's equation for small model systems which contain up to two or three atoms.

This approach, however, limits for chemically relevant larger or more complicated molecules because the accuracy of such calculated or measured molecular potential energy surfaces is not sufficient.
Another problem is the uncertainties of experimental apparatus; hence, it is not technically feasible to generate the complex laser fields exactly the same as laser fields which is demanded by the theory.

This may cause the errors of laser pulse generation.
The design based on an approximate Hamiltonian may not be robust.

A solution to this problem was proposed by Judson and Rabitz in their work called 'Teaching Lasers to Control Molecules' (Judson and Rabitz, 1992).
This technique includes the output of the experiment in the optimization algorithm as closed loop feedback system in a learning algorithm.

The basic idea of this technique is that the quantum control system itself guides an automated search, via a learning loop, for a suitable coherent light fields or optimal laser field.
Such a methodology can iteratively improve the shape of the applied laser pulses until an optimum is reached by using the system itself to solve its Schrödinger's equation under the given constraints or laboratory conditions in real-time.

Their approach use genetic algorithms to search for the optimal laser field in iteration loop.
As a result of this technique, Baumert and his colleagues suggested a novel experimental methodology to reduce the effort to find the optimal laser pulse (Baumert et al., 1997).

This research emphasizes on femtosecond laser pulses compressed by a computer controlled pulse shaping apparatus using an evolutionary algorithm with feedback loop to control a programmable liquid crystal spatial light modulator based on the design of Weineral.
(Weiner et al., 1992).

This technique requires no previous knowledge of intensity or phase profile of femtosecond laser pulses.
Such a technique is extremely useful in the field of short pulse microscopy, two photon microscopy, ultrafast soft X-ray generation, in particular ultrashort spectroscopy such as femtosecond quantum control of chemical reactions.

Another example of femtosecond laser pulse shapers with learning algorithms used for automated pulse compression is reported by Brixner and his coworkers (Brixner et al., 1999).
In this experiment, a femtosecond laser pulse shaper based on a liquid crystal display spatial light modulator together with a feedback-controlled evolutionary algorithm was used to optimise complex phase-modulated laser pulses.

This leads to bandwidth-limited amplified laser pulses.
Feedback-controlled femtosecond laser pulse shaping has been developed later by Brixner and his colleagues (Brixner et al., 2000).

The combination of a frequency domain phase shaper with an evolutionary algorithm and different pulse characterization methods can compensate for any phase deviation.
In the design of arbitrary laser pulse shapes, an evolutionary algorithm can be employed to optimise generation of arbitrary laser pulse shapes.

Quantum systems and quantum-mechanical wave functions are three-dimensional objects, but conventional pulse shaping accesses only the scalar properties of ultrashort laser pulses.
The control of the vectorial properties of the electric laser field by the use of polarization as active agent can access the three-dimensional dynamics of quantum-mechanical wave functions (Brixner et al., 2001).

A novel computer-controlled femtosecond pulse shaping technique was, therefore, introduced.
In this methodology, the polarization state of light, the temporal intensity and the momentary oscillation frequency can be manipulated as a function of time within a single femtosecond.

This research uses a pulse shaper with a 256-pixel two-layer liquid-crystal display inside a zero-dispersion compressor to modulate the degree of polarization ellipticity and the orientation of the elliptical principal axes within a single laser pulse.
Brixner and his coworkers also demonstrated the generation and complete characterization of polarization-shaped femtosecond laser pulses (Brixner et al., 2002).

In this experiment, the light polarization, the temporal intensity and the momentary frequency were arbitrarily changed within a single pulse.
The generation and analysis of polarization-shaped femtosecond laser pulses was then combined with the optimization algorithm within a learning loop and the experimental feedback (Brixner et al., 2003).

This experiment uses second-harmonic generation (SHG) to serves as a feedback signal in the learning algorithm.
An evolutionary algorithm is used to iteratively improve the phase-modulated and polarization-modulated laser pulses.

Two optimization strategies were compared in this experiment: phase-only shaping and polarization shaping.
The optimization results of polarization shaping are better than that of phase-only shaping.

The experimental implementation of shaping of femtosecond polarization profiles within an adaptive learning loop raises the possibility of generation of femtosecond light field in which the scalar and the vectorial properties are optimised automatically.
This methodology can be also applied for adaptive polarization quantum control experiment with the feedback signal taken from quantum system itself.

Evolutionary Algorithms in optimal quantum control
There is a large amount evidence of the progress in using evolutionary algorithms for optimal quantum control in order to control the molecule in chemical reaction.

Feedback quantum control of the electronic population transfer in a dye molecule is one of such evidence.
Traditionally, time domain quantum control needs the calculations of quantum dynamic to find the optimal laser field.

This method requires knowledge of the molecular potential energy surfaces, whereas feedback quantum control in a molecular system uses a computer program based on genetic algorithms to analyses the experimental output from detector in order to optimise population transfer from ground to first excited state and then control an acousto-optic pulse shaping device in order to excite fluorescence from the laser dye molecule IR125 in methanol solution (Bardeen et al., 1997).
The advantage of feedback quantum control is no requirement of a priori knowledge of the molecule's potential energy surfaces.

Another advantage of such an approach is that the optimal solution given constraint can be automatically found by a control computer with optimization algorithms.
This feedback control approach might be extended to several physical and chemical processes.

Some research, however, shows that the achievement of the optimization without dealing with the electronic population transfer within the parent molecule is possible.
Assion and his coworkers, for example, introduced the automated optimization of coherent control of independent chemical reaction channel using a computer controlled femtosecond laser pulse shaper including an evolutionary algorithm as well as closed loop feedback system from the femtosecond laser-driven photodissociation reaction output (Assion et al., 1998).

Femtosecond laser pulses in their research are modified in a computer-controlled pulse shaper based on the design of Weineral.
(Weiner et al., 1992).

An evolutionary algorithm was also implemented to optimise the spectral phase of the femtosecond laser pulses.
In addition, a reflectron time-of-flight (TOF) mass spectrometer was used to record ionic fragments from molecular photodissociation and then feed feedback signal directly into a control computer together with an evolutionary algorithm.

Two substances: Fe(CO)5 (iron pentacarbonyl) and CpFe(CO)2Cl (dicarbonychloro(5-cyclopentadienyl)iron) were chosen to perform experiment.
The results show that a change from electronic population transfer to direct control of different reaction channels can be achieved by optimizing final product yields in the photodissociation reactions of two different organometallic molecules without prior knowledge of the molecular system of the sample substance.

This may be the advance in the synthesis of chemical substances together with reducing unwanted byproducts.
In addition, Bergt and his colleagues carry out their research on active control of the femtosecond photodissociation and ionization reactions of Fe(CO)5 (iron pentacarbonyl) in the gas phase (Bergt et al., 1999).

Their research used the combination of an evolutionary algorithm and feedback loop in femtosecond laser pulse shaping system to investigate the Fe(CO)5/Fe branching ratio, the possibility of using second- harmonic generation (SHG) between the output of a 800 nm pulse shaper and the molecular beam apparatus to optimise dissociation reactions with 400 nm phase-shaped laser pulses and the adjustment of the balance between the optimization of the desired branching ratio and the optimization of the absolute photoproduct yields.
The evidence shows that Fe(CO)5 dissociation reaction cannot be describe as intensity-dependent.

This is because a completely different behaviour results from different types of intensity variation.
In other words, the change in intensity by the variation of pulse duration leads to a behavior opposite intensity alteration by the variation of pulse energy.

This is called 'problem of inversion'.
Furthermore, second- harmonic generation (SHG) can be used after the output of a 800 nm pulse shaper to implement 400 nm phase-shaped laser pulses automate coherent control experiments.

Finally, tuning automated coherent experiments from an optimization of mainly the photoproduct yield ratios toward optimization of the absolute mass signal can be achieved by a suitable choice of weighting factors in the fitness function.
It can be said that evolutionary laser pulse shaping is a very useful to the processes of photoinduced chemical reactions.

All demonstrations as mention above have highlighted the control of the outcome of photoinduced chemical reactions in which molecules are identical.
Brixner and his coworkers report on controlling mixtures of different molecules, which is 'photoselective adaptive femtosecond quantum control in the liquid phase' (Brixner et al., 2001).

Two methodologies were investigated in this experiment.
These schemes are the use of single parameter quantum control schemes involving variation of wavelength, pulse energy and linear chirp, and also the use of many parameter adaptive quantum control using a femtosecond laser pulse shaper to generate complex pulse shapes by spectral phase modulation.

This research show the achievement of chemically selective molecular excitation in liquid phase by many parameter adaptive quantum control without using knowledge of molecular properties in optimisation procedure.
Such a technique may be applied to chemical analysis, bimolecular reaction control or multi-photon fluorescence microscopy.

Moreover, not only is an evolutionary algorithm also used for adaptive femtosecond quantum control experiments in molecules in which using specific optimisation goals within a given system, but it is also used to investigate if the optimised laser fields are specific to a certain molecule.
Bergt and his coworkers report their work on 'Time-resolved organometallic photochemistry Femtosecond fragmentation and adaptive control of CpFe(CO)2X (X=Cl, Br, I)' (Bergt et al., 2002).

This experiment shows that adaptive femtosecond pulse shaping is sensitive to discover and exploit the difference in the investigated metal complexes.
Adaptive control can also find pulse shapes which are specific even for chemically very similar molecules.

In the development of optimal quantum control, there is some experiment relating to adaptive femtosecond quantum control in a liquid-phase environment.
Such experiment apply femtosecond adaptive pulse shaping of 800-nm laser pulses to control over the multiphoton charge-transfer excitation of a [Ru(dpb)3](PF 6)2 (where dpb =4,4'-diphenyl-2,2'-bipyridine) coordination complex dissolved in methanol using adaptive femtosecond pulse shaping within 128-parameter search space (Brixner et al., 2003).

Their research show that the dominant intensity dependence of the multiphoton excitation process can be removed from adaptive quantum control experiments by using second-harmonic generation (SHG) as a general "reference" signal.
Hence, specific properties of molecular systems can be made visible.

This technique might be very useful in many applications such as multiphoton microscopy, chemical analysis and in the preparation of specific reactants for photocontrolled bimolecular reactions.
There is some advance in manipulating light-matter interaction in three dimensions using complex polarization-modulated laser pulses.

A new level of the control of quantum system is introduced by Brixner and his colleagues.
In their work, quantum control by ultrafast polarization shaping was carried out (Brixner et al., 2004).

This experiment use flexible and automated polarization and phase shaping of femtosecond laser pulses in combination with an evolutionary algorithm to control molecular dynamics and maximize ionization of potassium dimmer molecules (K 2) from a supersonic molecular beam.
Two types of adaptive control experiments to maximise the K 2 + yield: spectral polarization-and-phase laser pulse shaping as well as phase-only shaping were performed.

The comparison of optimisations of K 2 + yield shows that polarization laser pulse shaping is superior to phase-only shaping.
This is because the vectorial electric field is able to adapt to the time evolution of the polarization-dependent transition dipole moments.

In this way, quantum control in a molecular model system may be achieved.
Furthermore, some evidence shows that adaptive femtosecond pulse shaping is able to control isomerization reactions of a complex molecule in the liquid phase by optimizing the ratios of the cis and trans isomers in a multi-parameter optimal control experiment (Vogt et al., 2005).

This seems to be a first step towards controlled stereoselectivity in photochemistry.
Conclusions

This essay has presented the survey of evolutionary algorithms methodology and has sought to highlight the experimental application of evolutionary algorithms to molecular control.
The evolutionary algorithms for molecular control have been proved to be a very useful tool in adaptive femtosecond quantum control.

Many different scientific fields like physics, chemistry or biology can benefit from this methodology, in particular chemical reaction dynamics.
Evolutionary algorithms are flexible optimisation tool for the experiment of adaptive femtosecond quantum control.

The control parameters can be represented in a natural fashion, as can the performance specifications.
The optimisation method can be tuned to a great extent to the problem at hand.

Evolutionary algorithms have been proved to be significant and successful in the effective development of adaptive femtosecond pulse shaping and femtosecond optimal quantum control.
There are a number of examples where the technique has been used for automated pulse compression, optimised generation and characterisation of arbitrary laser pulse shapes and also adaptive femtosecond polarization pulse shaping.

In addition to this, there has been a growth in implementation of this technique for quantum optical control.
A number of example have included control of molecular electronic population transfer, control of photodissociation product distributions, control of photoselectivity in the liquid phase, control of organometallic molecules in the gas phase, control of multiphoton charge-transfer excitation in the liquid phase, control by ultrafast polarization shaping and control of photoisomerization.

Numerous quantum systems and particularly molecules undergoing chemical reaction in either gas or liquid phase have been controlled by the use of either a femtosecond phase-only pulse shaper or a femtosecond polarization pulse shaper in combination with an evolutionary algorithm as an optimisation algorithm in a learning loop with experimental feedback.
These methods can be applied to many challenges in physical, chemical and biological research.

The future use of evolutionary algorithms for molecular control is ties fundamentally to general advance in evolutionary algorithms itself and the novel technologies of femtosecond laser pulse shaping and adaptive femtosecond quantum control.
The continued progress in computer technology will permit further realization of the evolutionary algorithms method's potential.

The main problem with evolutionary algorithms is the large amount of computation required to formulate a good solution.
However, the increased availability of low-cost, high performance computing and the advancement in parallel architecture will be great advantage.

The combination of an evolutionary algorithm with complementary means to create hybrid algorithms as a novel evolutionary algorithm for adaptive femtosecond quantum control is seen as crucial in order to create a fully effective tool.
An example of this is that the use of genetic algorithm with differential evolution may provide the improvement of convergence rate.

A novel adaptive femtosecond quantum control technique is likely to open the door to a number of new experimental schemes exploiting certain aspects in quantum system such as the generation of attosecond light pulses or stereoselectivity in photochemistry.
Furthermore, evolutionary algorithms may play a vital role in the development of adaptive femtosecond laser pulse shaping for coherent control of complex biological and synthetic biomimetic systems in the future.

Introduction
This report is aimed to provide an extensive overview of parasitic plants.

This will be achieved by observing the different species, in particular the mistletoe families, and researching into how and why they paratisize their hosts.
The distribution, human uses and ecological benefits of parasitic plants will also be looked at briefly within the report, in order to gain a basic understanding of their importance in different habitats and cultures.

The different parasitic plant families are given in Appendix I.
Parasites

Parasites are organisms that are reliant on hosts for the production of nutriments.
There are two main types of parasites, as described in Table 1.

Table 1 - Types of Parasites
Facultative Parasite: A parasite that is able to adapt to a changing habitat.

It can be free-living, however, if conditions become unfavourable it can prey on host organisms to obtain nutrients.
Obligate Parasite: A parasite that is unable to develop and grow autonomously, therefore relying on a host for survival.

Parasitic plants either live in or on their hosts, penetrating through their roots or stems.
However there are different types of parasitic plants, as shown in Table 2:

Table 2 - Types of Parasitic Plants
Hemiparasite/ Semi-parasite: A parasite that relies on its host for half of its nutriment, but still able to photosynthesise, due to the production of chlorophyll.

An example is the Eurasian mistletoe (Viscum album).
Holoparasite: A parasite that relies completely on its host in order to obtain nutriments.

This parasite is unable to photosynthesise for itself and therefore lacks chlorophyll, for example Cuscata epilinum from the dodder family.
Approximately twenty percent of all species are holoparasitic [Press, Graves, 1995].

Host Plants
The majority of vascular plants, approximately eighty five percent [Hickman, Johnson, 2004], have mutalistic relationships with fungi, aiding with the absorption of nutrients from the soil.

The relationship between plant roots and fungal hyphae is called mycorrhizae of which there are two main types: endomycorrhizae and ectomycorrhizae.
Endomycorrhizae (Diagram 1) are the most common, however ectomycorrhizae (Diagram 2) are most common in temperate and boreal forest trees [Hickmanal.

2004].
The mycorrhizae create a larger surface area over which nutrients can be absorbed.

Root parasites take advantage of the mycorrhizae and insert their haustorium in order to absorb the nutrients.
An example is the holoparasitic Indian pipe (Monotropa uniflora), a relative of heathers and rhododendrons, and found in coniferous forests (Photograph 7).

It absorbs nutriments from the mycorrhizae of coniferous trees, and grows to approximately ten inches above the leaf litter [Attenborough, 1995].
Parasites rarely cause the death of their hosts, since the host is required for the very basis of survival, especially non-photosynthetic holoparasites that cannot generate energy and nutrients without chlorophyll.

Therefore holoparasites generally have an immense range of hosts.
Parasitic plants can, however, cause the host growth loss, but it may not be obvious until the parasite has infected a considerable amount of the host.

Haustorium
The haustorium is a chemotropic adaptation of the root system designed for parasitic plants to create a conduit, in order to transport water, carbohydrates and nutriments from their host.

The developing roots (radicle) of non-parasitic plants generally grow in a downward direction due to gravitational forces and away from light.
However the Viscum album radicle, for example, grows in the direction of the host, whether it is towards light or opposing gravity.

Once the radicle has reached the host plant it then transforms into a haustorium and perforates into the host's tissue, until it reaches the cambium.
The circumference then increases and cortical strands grow from the haustorium, parallel to the cambium.

Sinkers then form on the cortical strands as they touch the cambium, penetrating the xylem (Diagram 3).
The remaining cortical strands penetrate the phloem, allowing for a conduit through which nutriments and water travel from the host's vascular system to the parasite [Worral, 2005].

The haustorium gene is believed to have originated from either autotrophic, non-parasitic plants or from endophytic organisms, such as, bacteria/fungi [Penn State University, 2006].
Hemiparasites usually only penetrate the xylem to obtain water from the host, and photosynthesise to obtain nutrients for themselves.

A method in which various mistletoes' are able to extract water from their host is to allow for continual transpiration to occur throughout the day.
This process would normally, in non-parasitic plants, result in a considerable amount of water loss and as a result a wilting plant.

However, by maintaining open stomata the mistletoe establishes a "steep water potential gradient" [Mathias, 1995], withdrawing a significant amount of water and nitrogen from the host.
The haustorium (Diagram 4) has a multitude of functions other than the penetration of the host to obtain nutriments via translocation.

It also provides a structural support for the parasite given that it is the site of attachment to the host by means of hapteron, which "secrete a polysaccharide adhesive" [Mathias, 1995].
Hemiparasites

Mistletoes
Mistletoes are vascular, hemiparasitic flowering plants that are found worldwide and in different forms, including, trees, shrubs and herbaceous plants.

There are two mistletoe families, Viscaceae and Loranthaceae, which originally belonged to the same family Loranthaceae [Kuijt, 1969].
This division occurred due to the conspicuous differences between the two; the Viscaceae family all have relatively small, inconspicuous flowers, with red/white berries and are usually found in temperature regions of the Northern Hemisphere.

The Loranthaceae family have much more ostentatious flowers and are found in tropical regions, for example, Nuytsia floribunda, the 'Christmas tree' found in Australia which blooms in December/January.
Nuytsia floribunda is found in Australian heathlands, and can reach a height of up to forty five feet tall.

Its roots produce white suckers that encircle and cut into a host's roots, the haustorium then diverts water and nutrients (Attenborough, 1995).
Australia has a wide variety of different mistletoes amounting up to approximately seventy five different species, that are found in a variety of habitats.

Sir David Attenborough (1995), observed that many mistletoe species have very similar leaves to that of their hosts.
This is believed to be due to the parasitic plants obtaining various hormones from the host via the haustorium and coevolution.

Viscum album (Viscaceae family) has oval, green leaves and grows typically on deciduous trees.
Mistle thrushes' (Turdus viscivorus) feed on the berries, depositing the sticky, intact seeds, usually onto berry-bearing trees.

The northern populations of Turdus viscivorus (e.g. Scandinavia) are usually migratory in the winter to the Mediterranean, North Africa and Central Asia, and may deposit the seeds along these migratory routes.
The sticky berries are usually deposited in strings held together by viscin and only a few other birds eat them, for example, fieldfares (Turdus pilaris), redwings (Turdus iliacus) and black caps (Sylvia atricapilla) [Briggs, 2006].

The desert and dwarf mistletoe's however have scale-like leaves.
The desert mistletoe (Phoradendron californicum) either preys on cacti e.g. catclaw accacia (Acacia greggii) or desert shrubs.

The desert mistletoe produces red, sticky berries that many birds feed on, especially the silky flycatcher (Phainopepla nitens), which in turn distributes the seeds by wiping either its bills/droppings onto another host plant.
Tristerix aphyullus is another mistletoe which also infects a cactus, Trichocereus chilensis [Mauseth, 2003].

Dwarf mistletoe's (Arceuthobium spp.
) prefer pine trees as a host and produce white berries which burst when ripe, expelling their seeds at a high speed to other nearby trees.

"Seeds only 3mm long may shoot up to 49 feet laterally, with an initial velocity of about 62 miles per hour." [Armstrong, 2006] A number of mistletoe's create galls, which are masses of woody tissue that surround infected areas on their hosts.
The galls are where the haustoria once penetrated the host, and many remain even after the death of the mistletoe.

Coniferous trees obtain a gall-like structure known as a "witches broom" [Armstrong, 2006], which is where the infected branches become very dense.
Tropical mistletoe can cause mutations in the host's bark, called "wood roses" [Armstrong, 2006].

The local people in Mexico and Bali use these intricate imprints to create delicate woodcarvings for decoration or to sell to tourists.
Mistletoe also has an ecological importance as a source of food and shelter; mistletoe weevils (Iaxapion variegatum) feed on the stem during the larval stage, Hypseloecus visci, a rare sap-sucking bug and the mistletoe tortrix moth (Celypha woodiana) whose larvae mine through the leaf tissue, all feed on Viscum album, during the spring and summer [Gates, 2005].

Strangler fig
The strangler fig (Ficus spp) germinates within the boughs of trees in the canopy.

As it develops it grows down and around the truck of its host, attaching its haustoria to its host's roots, whilst constricting it.
A hollow structure remains once the host has died [Bradt, 2005].

The death of the host is usually due to the parasite gaining the majority of nutrients from the soil and shielding the hosts leaves from light in the canopy.
Occasionally more than one fig may paratisize the same host, entwining their roots and as a result appearing to be a single tree.

The different trees, however, tend to flower and fruit on separate occasions, subsequently providing important sources of food in the rainforests [Attenborough, 1995].
Holoparasites

Monotropa uniflora (Indian pipe) is a root parasite that when flowers pushes through leaf litter to expose a white/transparent, bell-shaped head.
This parasitic plant completely lacks chlorophyll and obtains it nutrients from the roots and mycorrhiza of coniferous trees.

Raffleisa arnoldii is an endoparasite, that lives completely inside the tissues of lianas (Tetrastigma) in the tropical rainforests of Borneo and Sumatra, although it erupts from the woody vine when flowering.
The Raffleisa arnoldii produces the largest, single flower in the plant world.

The female flower has five, thick and leathery orange/red petals that open to up to one meter in diameter.
The flower is pollinated by insects, in particular flies, which are attracted to the redolence of rotting flesh.

The male flower, however, is pollinated by small mammals and are much more abundant than the female flower [Attenborough, 1995].
Cuscuta spp.

(dodder/witches hair) is an obligate parasite, with an unusual appearance, considering that it grows as a mass of string-like strands, engulfing anything in its path to lengths of up to half a mile.
Cuscuta is very versatile and is therefore found in a variety of habitats, for example, Cuscuta marina is salt tolerant and is commonly found in and along salt marshes.

Pilostyles thurberi is an endoparasite, meaning that it lives in the stem of the host, apart from when it flowers.
The haustorium is fibrous and penetrates the host's vascular tissue by means of a sinker.

Fungus melitensis (Maltese fungus) is a parasitic plant that spends the majority of its life underground obtaining nutriments from its host (usually tamarisk or sea lavender).
In the summer months it obtrudes out of the ground and produces small flowers.

Once pollinated by small flies it turns black.
The Maltese fungus was extremely sought after during the medieval period due to its rarity and was used to staunch wounds, and to solve sexual problems [Attenborough, 1995].

Parasitic Weeds
Parasitic weeds can have catastrophic effects on crops worldwide, and for vulnerable populations in the semi-arid regions of Africa, and India, the lack of edible food crops may contribute to famines.

North American contaminated crops and forage seed have contributed to the dispersal of dodder (Cuscuta pentagona) to most parts of the world.
The spread of parasitic weeds have been helped greatly by human interactions, especially root parasites, for example the Cuscata and Striga families.

Therefore quarantine measures have been set up to prevent the spread of destructive parasitic weeds around the world.
Witchweed (Striga) is most damaging to maize, sorghum and a number of other grasses, whereas broomrape (Orobanche) devastates tomatoes and beans and dodder is a particular problem on alfalfa, for example.

Although quarantine measures are set up to help prevent the spread of parasitic weeds, once they have contaminated a crop it is extremely difficult to eradicate due to the minuscule seeds produced that are effectively distributed via the wind and persist in the soil.
Human Uses

Humans have used parasitic plants for a number of reasons over the centuries.
Numerous parasitic plants have and are been believed to have medicinal properties and curative values.

Others have been used as herbal teas, for example, Euphrasia and Pedicularis, [Pressal.
1995], or perfumes (sandalwood).

However some parasitic plants prey on poisonous hosts, and therefore the possibility of alkaloids (organic nitrogenous, commonly toxic compounds) passing from the host to the plant is likely.
Therefore care is needed before using parasitic plants in medicine and food.

The flower buds of Raffleisa tengku-adlinii, when boiled are said to induce the labour of pregnant women and to help mothers gain their strength after the birth.
A few root parasites accrue tannin (a substance in some plants used as a defence against herbivores and has a role in allelopathy) and subsequently have been used as anti-diarrhoeal remedies, for example, Prosopanche, has been used in Argentina, [Musselman & Cocucci unpublished, cited by Pressal.

1995 p.8] and Krameria has been used in Central and South America, [Simpson 1989, cited by Pressal.
1995 p.8].

Mistletoe has been used for many different medicinal cures all over the world throughout the years.
In Africa, it was used as a cure for convulsive distempers, to relieve digestive distress in Canada, and as aphrodisiacs for the Mayans and in the Mediterranean.

Not only have humans used parasitic plants for numerous medicinal reasons, but have also associated a number of folklores with them.
The European mistletoe (Viscum album) has been used to symbolise good fortune, and pagan rituals.

It is also used in Christmas festivities; when placed above doorways couples share a kiss when stood underneath.
This tradition is believed to have originated from the belief that mistletoe aided the fertility of women, and was used when conception of a child was desired.

European mistletoe was also believed to have been "sent to earth by Gods" and that the mistle thrush (Turdus viscivorus) was the "messenger" [Worral, 2005].
It was therefore believed to have healing powers and people drank it in tea and wore it as amulets.

European mistletoe when grown on oak trees was believed, by the Druids, to symbolise "human dependency on God", and the oak tree represented "God".
It was therefore used to scare away evil spirits, a good-luck charm and to create a warm atmosphere in the home within the winter months [Worral, 2005].

A few parasitic plants have also been used as a source of food for humans in different locations around the world, either as a staple part of the diet or when other food sources are scarce.
Parasitic plants are also a source of food for other animals; for example, mistletoe can often provide deer and elk as a source of food in the winter months.

Appendix I looks briefly at other uses of parasitic plants.
Conclusion

There is a diverse range of parasitic plant species found all over the world that live in a variety of habitats, and have adapted different ways in which to obtain nutrients from their hosts.
Although parasitic plants may harm their hosts, they are very important ecologically, economically and socially.

The provide food for a number of different species ranging from insects to humans.
The root structures of parasites have provided an income for some indigenous people who create and sell the handcrafted woodcarvings.

Parasitic plants have also provided a basis for local discussion derived from folklores, and festive traditions.
The word 'mythology' comes from Ancient Greece and using its derivatives it roughly becomes 'story-telling' in English.

Although, myths are stories and many of them focus on the epic deeds of heroes, 'Greek mythology admits a plurality of approaches' and it would be foolish to ignore the importance of the characteristics and the symbolism of the protagonists and antagonists.
The intervention of the Olympian deities in Greek myth is a recurring motif, and in many myths they play pivotal roles.

Furthermore, the idea of virginity is a major theme in a fair number of ancient Greek myths.
In particular, the virgin goddesses Athena, Hestia and Artemis feature dominantly in many of the virgin motif myths: Their sexuality and devotion to it is an antithesis to the Ancient Greek culture; where men dominated society and it was common practice for girls to be married as soon as possible.

By paying particular attention to the virgin goddesses and using a variety of sources, in this essay I will explore the figure of the virgin and their relevance in ancient Greek myth.
The idea of virginity, or one of the ideas, is that of 'bodily integrity' and Loven states the word 'virgin refers to a girl or to a young woman who has not yet had sexual experiences' whilst maintaining that for males, the idea of celibacy is more appropriate than the idea of virginity.

However, that is not to say that there are not male virgins in Greek myth, with the mortal, Artemis-worshipping Hippolytus being a perfect paradigm of a male virgin.
Furthermore, there considered two types of virgin in ancient Greek myth; passive or active.

A passive virgin is one who does not actively maintain their virginity.
An active virgin is one who will defend, if need be, their virginity should anyone threaten it.

The aforementioned three virgin goddesses are perfect models to enhance the distinction between the two types of virginity.
Artemis, goddess of the hunt and the wild, is arguably the best example of an active virgin, as she 'represents the impulse to asceticism and chastity." Not only does she defend her 'hallowed purity' numerous times, for example against Actaeon, but she also despises fellow goddess Aphrodite, a deity who is more than liberal when it comes to sexual relations.

Greenwood says there is a 'mutual jealousy and hostility' between the two, a point which is easily comprehensible considering that the two deities are complete opposites.
Michael Grant points out that 'she plays an inglorious role in the Iliad' and the reason for this may be down to the fact that 'she was, to the Greeks, the goddess of a conquered race." However, the fact there are at least four myths revolving around Artemis and the defence of her virginity, I feel, highlights how she was an important figure in ancient Greek society; as part of her duty 'she watches over women at childbirth' and 'the virginity of Artemis in her tenderest aspect makes her specially gentle to the very young maiden.' If there is any median between active and passive virginity then that median is Athena.

'Next after Zeus himself in Olympian precedence comes Athena the Grey-Eyed', her virginity could be considered a surprising characteristic considering her pre-eminence after Zeus and as I have previously mentioned, ancient Greek society far from favoured women.
However, it must be pointed out, that she was a deity and a powerful one at that so she was bound to be revered and worshipped beyond others.

Although not as active as Artemis, Athena did defend her virginity against Hephaestus, the god of craft and smiths, who attempted to rape her.
This could be seen as a reversal to the idea of male dominance in Greek society, however, Athena was a god of war, along with Ares, and Morford & Lenardon state that 'the masculinity of her virgin nature sprung ultimately not from the female, but the male." Although virginal, Athena had a particularly close relationship with Zeus and was a result was 'his favourite daughter' as well as being the patron of the hero Odysseus.

I believe this shows how she was open minded, furthered by the fact that she merely ran from Hephaestus rather than kill him as Artemis would have done, unlike the aggressive Artemis and Hestia, who did her utmost to avoid men.
The passive example is the goddess of the hearth, Hestia.

Unfortunately, there are very little remaining stories involving Hestia, and as such it is difficult to gauge the importance of her role as a virgin.
However, we do know she, like Artemis, was not swayed by the works of Aphrodite and swore to retain her virginity.

Furthermore, as every family hearth was her altar and she received the first offering of any sacrifice in a house she is arguably a pivotal god to Greek society.
Aphrodite, the goddess of beauty, love and marriage was in stark disparity to the virginal goddesses, a point which is reinforced with a reference to Harrison: 'In marked contrast to Athena stands...

Aphrodite." In ancient Greek myth Aphrodite caused the death of Hippolytus; Hippolytus was a favourite of Artemis and he only worshipped her and as a result he refused to revere Athena as he led a life of chastity.
As a consequence Aphrodite set in motion the events told in Hippolytus, which led to the brutal death of Hippolytus as a result of his rejection of Phaedra.

It is in Hippolytus, after the protagonist is told of Phaedra's love for him, that we can see an ancient Greek outlook on women, although admittedly, it is a strong view; "'Tis clear from this how great a curse a woman is' Furthered by the fact that Hippolytus, a male, is arguably the most famous mortal virgin in ancient Greek myth and that three prominent Olympian deities are virginal, it could be argued that the idea of virginity, which requires devotion, is generally an idea beyond that of the typical ancient Greek female, who in her culture was perceived as the lustful gender.
As a result, it could be stated that the virginity of the goddesses acts only to further separate them from the mortal woman.

However, Greenwood argues that is not Hippolytus' chastity that leads him to which 'his indignant horror is due.
Union with his father's wife would be both adulterous and incestuous and all respectable Greeks' would see such acts as terrible offences.

Even with this taken into consideration, Hippolytus' condemnation of women as a whole, not just Phaedra would surely render such a point obsolete.
In stark contrast with the ancient Greek cultural view of women as the 'lustful gender' is the fact that acts of rape more or less are always committed by males (aside from the rape of Anchises by Aphrodite, which once again reiterates her whorish nature).

On the other hand, three serial rapists in ancient Greek myth are Zeus, king of the gods, Apollo, 'second only to Zeus' and Poseidon, 'the equal of either Athena or Apollo' whose acts have often been said to be symbolic of the notion of male dominance.
At this point, it must be highlighted that an act of rape, although far from being positive, was not seen in same light of negativity it is today, which as a result meant that being characterised as a raping deity is no where near the unpleasant characterisation it would be today.

Rape was the primary threat to the virgin figure in Greek myth; although never were the virginal goddesses successfully raped, which once again further separates them from the mortal woman, who was frequently the victims in ancient Greek myth.
This point would also further support the idea of male dominance in ancient Greek culture.

To conclude, taking into consideration all the previously stated points, it is my view that the figure of the virgin in ancient Greek myth is a literary symbol used by the male ancient Greek writer to represent 'bodily integrity', or a level of 'purity' that cannot be attained, or maintained by the average ancient Greek woman.
This point is supported by how the three most renowned virgins are all female deities and not mortals.

A further condemnation of the ancient Greek female is shown by the threat of rape and in particular the threat of rape to virgins, which is, in my view as well as others, a symbolic motif subconsciously highlighting the ancient Greek cultural belief of the dominant male and the weaker female.
To many the word 'myth' will evoke the notion of stories of heroes and monsters, such as Theseus and the Minotaur; Stories which were intended to entertain, to scare and to thrill.

However, it would be ignorant for a mature mind not to look at the importance of the characters: What was their role?
Were they symbolic?

Did they make up part of a larger motif?
These are just a few of the questions that could be focused toward characters within myth; questions which, in this essay, I will ask of those hybrid creatures that feature in ancient myth.

Hybrid creatures were the predominant antagonist in many hero myths that date back to classical antiquity.
It is interesting that 'in the neatly ordered Greek universe, which has reserved distinct compartments for the 'bestial', 'human' and 'divine'' there is no such compartment for those hybrid characters in myth.

Even the word 'monster' is a vague description: The idea of 'the Monster is ubiquitous' and as such it remains unclassifiable, something which Lada-Richards argues is due to 'monsters' being 'culture-specific products.' I consider that, as there is no universal 'compartment' for the hybrid creatures in ancient myth, each and every crossbred character has been amalgamated into the manner they are because they are to be treated as individual cases.
This point can be maintained by how the many of varying fusions represent a different challenge to a different hero; for example, the Minotaur is a creature of brute strength, whereas the Gorgon Medusa relies upon intelligence.

On the other hand, in some myths hybrid creatures had another role, such as being symbolic of a social fear rather than playing the role of being the nemesis of the hero; the Sirens are an example of this.
Arguably one of the most famous of hybrids in ancient myth, the Minotaur, in the view of some, plays a far more complex role than that you would conclude from initial impressions.

Even its birth, I feel, is an area for discussion; Pasiphaë, the mother and husband to King Minos, became physically attracted to a great white bull, which was a sign from Poseidon.
Yet once Minos refused to sacrifice the beast, Poseidon sent Pasiphaë into a lust for the bull.

Finally, through the help of Daedalus, Minos constructed a way for the bull to mount his wife, which ultimately led to birth of the Minotaur.
On the other hand, the classical scholar A. B. Cook expresses the view that Minos, the king, and the character of the Minotaur are really the same individual in different outward appearances.

Cook and the anthropologist J. G. Frazier regard the union between Pasiphaë and the bull as a sacred ceremony, relating to Minoan religion, in which the queen of Knossos wedded a deity in the form of a bull.
The previous point aside, in the myth of the Minotaur, the unusual chain of events that led to the union between bull and woman can mean it is feasible to say that the Minotaur was a result of divine intervention.

Furthermore, I would like to point out that the physical sexual attraction by Pasiphaë towards the bull led to the creation of the archetypal physical challenge for a hero, the Minotaur.
Before the myth of Theseus and the Minotaur has even begun there is, debatably, a motif; trying to fool the Olympian deities, which will ultimately result in intervention from the Gods, is a course of action only for fools.

Whilst asking for help from the Gods can lead to unexpected and unwanted results.
If King Minos had honoured Poseidon by sacrificing the great white bull then none of the terrible events that followed would have occurred.

The Minotaur was used as an 'engine of vengeance', used by Minos in response to the death of his son, Androgeos; After signing a treaty with Minoan Crete Athens had to send 'seven Athenian youths and seven girls, children of noble families' as tribute to Minos, where upon the children were placed 'in the Labyrinth and devoured by the Minotaur'.
Historically, it is true that the fledgling polis of Athens was under the both cultural and political influence from Minoan Crete and for this reason, some modern mythologists consider Theseus' slaying of the Minotaur as a Minoan adaptation of the Baal-Moloch of the Phoenicians and as such consider it a solar personification being symbolic of not only the ending of Athenian tributes to Minoan Crete, but the end of the dominating influence of Minoan religion over mainland Greece.

The myth of Theseus and the Minotaur is not the only ancient myth with symbolic connotations; the myth of Medusa and Perseus is another.
With the rise of the patriarchal society all around the Aegean, the image of Medusa was far from acceptable.

The image I refer to here is not the one of myth where she was a chthonic monstrous hybrid with snakes for hair: Instead, the image is one of a woman, who symbolised female intelligence and wisdom and who was worshipped by Libyan Amazons.
However, she represented much of the qualities that patriarchal Greece did not want women to feel they had; intelligence, creativity and the power of destruction.

Her most famous re-representation from patriarchal Greece is that of the snake-haired Medusa, one of the 'three Gorgons, of whom only Medusa was mortal,' and they 'were of terrifying aspect, and those who looked upon their faces were turned to stone." It is this Medusa, who in myth, was beheaded by Perseus.
Similarly to the birth of the Minotaur, the events that led to Medusa's snake-hair also allow for interpretation.

After Medusa had intercourse with Poseidon in Athena's temple, Athena turned her hair to snakes and made her face so unbearable to behold that any man would turn to stone if they saw it.
It is here even more similarities can be drawn between the creation of the mythic Medusa we know and the birth of the Minotaur; interaction with the Gods, notably Poseidon in both cases, has resulted in dire consequences.

This connotation aside, Medusa is arguably also an example of a punishment of what should happen if a mortal angers the Gods.
It is worth noting that it in popular myth it is also Athena, responsible for the curse upon Medusa, who along with Hermes helps Perseus complete the challenge.

This occurrence is rare in myth and is something which Morford & Lenardon point out; the fact that 'two gods should assist him [Perseus] is remarkable." Furthermore, the fact two powerful deities actively help Perseus on his quest deducts greatly from the challenge faced by Perseus and for this reason, I feel the myth of Medusa is certainly more emblematic than any other; the myth is not so much a story as it is a symbolic connotation of male dominance over the female, a connotation which is full of 'male bias'.
The role of Medusa in myth does not end with her beheading at the hands of Perseus.

In fact, it is quite the opposite.
From Medusa's neck springs Pegasus, son of Poseidon and Medusa, in the form of a winged horse; this is, by modern definition, another hybrid creature.

Furthermore, Pegasus also later helped kill another hybrid creature, the Chimera; 'a thing of immortal make, not human, lion-fronted and snake behind, a goat in the middle, and snorting out the breath of the terrible flame of bright fire." Medusa's blood was drained and taken, and was later used to raise the dead.
Furthermore, her head was also taken and kept by Perseus and he 'turned his enemy Polydectes to stone by means of Medusa's head.' It is not only in the myth of Medusa that we see a hybrid creature representing the ancient Greek cultural and social fears of women and its condemnation of them.

The sirens, described as 'enchantresses', a hybrid between women and birds, who have been allegorised by classical writers 'as representing the lusts of the flesh...
the dangers of flattery', on the other hand there is another alternative existing view, which I agree with, that they are symbolic of the ancient Greek belief that women were trouble.

In Homer's Odyssey the Sirens lived near Scylla and Charybdis, two dangers so equally grave that 'between Scylla and Charybdis' is seen by some as the progenitor of 'between a rock and a hard place'.
Although Charybdis is not a hybrid, she was the 'formidable and voracious ally' of the hybrid Scylla, a 'fantastic monster with twelve feet and six heads'.

It is noteworthy to state that they are both named as if they were female in gender.
This, again, reiterates the social fear within Greek society towards women.

This point is supported by how Scylla later became 'an alluring woman' in later ancient Greek iconography.
The aforementioned Chimera was another hybrid creature, from Lcyia, in Greek myth, yet unlike Medusa, Scylla or the Sirens, it did not have any underlying negative attitude towards the female gender.

Its symbolism was completely different.
One possible allegory, as Harry Thurston Peck says, is that the Chimera was used as a symbol of the volcanic nature of Lycian soil.

On the other hand, Robert Graves puts forth the possibility that the Chimera was a symbol of the tripartite calendar, of which the lion (the Chimera was 'lion-fronted), the snake (the Chimera had a 'snake behind') and the goat (the middle of the Chimera was that of goat) are all seasonal emblems.
Although hybrid creatures were not always used as representation of social fears or as connotations to assert the notion of male dominance in society, for example the Satyrs, part man, part goat, were used as comedic uplifts in literature by the likes of Sophocles and Euripides.

It would appear that in ancient myth, particularly the most renowned ancient myths such as Theseus and the Minotaur, Perseus and Medusa and all the myth of the creatures found Homer's Odyssey have an underlying message, on top of their primary role as a challenge, either physically or mentally or even both, for a hero.
Unfortunately, despite the uniqueness of each hybrid we encounter in ancient myth it seems more often than not that they are used to condemn the female in Greek society.

Even the physical lust of Pasiphaë that creates the monstrous Minotaur could be seen as a criticism of the female gender, which throughout classical antiquity was seen as the more lustful of the two genders.
Introduction

This paper discusses the evolution in diplomacy in the Cold war period and analyzes the extent to which changes have occurred.
It is necessary to start with a definition of diplomacy where we distinguish between the actors, the functions and the agenda of diplomacy.

Then we proceed with a short description of Cold war diplomacy, as this will also provide us with a theoretical background for further arguments and comparison.
An analysis of the evolution of diplomacy thus relies on aspects identified by the definition and aims to elucidate to what extent changes have occurred.

The paper discusses changes in diplomatic actors, developments of multilateral and regional diplomacy, evolution of personal diplomacy and public diplomacy, and the shifts in agenda of diplomacy.
The most important development occurred in emergence of new actors, new style of diplomacy and evolution of new multilateral agenda.

It seems that the major change is the emergence of transnational diplomacy.
Where Cold war diplomacy reflected the interstate or intergovernmental negotiates and national interests, we can argue that the modern diplomacy involves a greater degree of complexity and we can meaningfully assert that major changes have occurred in the field.

While Nicolson argues that "to talk about new and old diplomacy is to make a distinction without difference," (Monsieur Jules Cambon in Nicolson 1988: 29) there are good arguments maintaining that more than just the 'outward appearance of diplomacy' has gradually changed.
The three major sources of change identified in the essay involve the end of the Cold war conflict itself, advances of globalisation and integration of the global society, and the 9/11 attacks on USA.

Definition of Diplomacy
Diplomacy refers to the manner of conducting one's relationship with other international actors, whereas foreign policy refers to the matter.

(Evans 1998: 180) It can be further said that diplomacy is the instrument employed to put foreign policy into effect, uses negotiation and dialogue and is concerned with management and conduct of relations with other states.
(Evans 1998: 129) It performs the function of a mean of communication and governments are the principal actors in the dialogue.

(Watson 1982: 120) When analyzing diplomacy, we need to distinguish between several things.
Firstly, the relevant actors, as they are the agents who participate in the conduct of diplomatic communication and negotiations.

Moreover, one needs to recognize the diverse themes and topics on diplomacy agenda and actors use distinct strategies or styles of negotiations.
Lastly, diplomacy fulfils several functions: negotiation, representation, identify and formulate state objectives and goals, establishing and renewing rules and procedures of the international system.

(Evans 1998:129)
Cold War diplomacy

Cold war diplomacy has essentially been the same for the whole duration of the conflict, and the evaluation of the changes that have taken place in international diplomacy since the end of cold war has to recognize following distinct features.
In terms of relevant actors, national government were the principal actors where bilateral negotiates behind 'closed doors' were the preferred.

Ambassadors and foreign ministers as representatives of government played an important role in diplomacy decision making.
International institutions predominantly served interests of the most powerful states and negotiations outcomes dependent on state's relative power.

Agenda of Cold war diplomacy addressed mainly state security and violence.
Evolution of diplomacy

There are several important developments in respect to actors of diplomacy in Post-Cold War period.
The demise of the Soviet empire brought new independent nations into the world politics ands started a period of realignment.

As the soviet iron curtain lifted, many states in the Russian sphere of influence were able to throw off the communist regimes, elect democratic governments and pursue independent diplomacy.
Changing balance of power fragmented the rigidity and cohesiveness of old coalitions.

This is best seen on the example of Warsaw Pact and NATO, where the former disappeared altogether and the latter is in the process of "reinventing" itself.
Disagreements between EU and USA like the split on Iraq war were unthinkable in Cold Wart.

It seems that the old rigid environment in which Cold War diplomacy operated has disappeared.
Secondly, Bayne (2003) argues that states as a consequence of advancing globalisation found their power fading away, while non-state actors (private sector and MNCs) gained power.

Governments, as representatives of the states and principal agents in diplomacy, were forced to change their negotiation strategies as a result of their decreasing power.
New actors were able to join and to a certain degree affect the diplomatic dialogue.

This was partly initiating or at least encouraged by some governments as the new non-state, in particularly the private sector was supposed to share the burden of conducting diplomacy, while trying to stay as agenda setters.
(Bayne 2003) Particularly in economic diplomacy businesses have been able to create various bodies and ad hoc coalitions to influence the diplomatic negations of powerful states.

For example, Business and Industry Advisory committee to the OECD provided business views and input into the deliberations of OECD.
(Bayne 2003: 59) American MNCs play an important role in formation of USA foreign policy.

Businesses play an important role in economic diplomacy as they can have indirect impact on policy via lobbying or direct via market decision.
(Bayne 200: 53) The rising importance of NGOs in Post-Cold War diplomacy is best illuminated in McRay (2001) by what he calls The New diplomacy.

Victory of democracy in the Cold War and advances of globalisation help to create a global civil society where NGOs are an important voice of world people.
The Ottawa treaty banning land mines and Rome treaty on ICC are notable successes of the new diplomacy and prove "that a coalition of governments, NGOs, international institutions and civil society can set a global agenda and effect change." (Kofi Annan in Davenport 2003) In the Ottawa Process "NGO pressure was cited as the No. 1 factor in states' decisions to support the ban of land mines." (Davenport 2002) While states remain principal agents of international negotiations and sovereign of international law, the domain of diplomacy is a more complex domain with a greater number and variety of actors.

"Perhaps, as one commentator suggested, we are entering a time of "poly-lateral" diplomacy, where major actors will have to balance and juggle a host of players and interests in shifting alliances." (Davenport 2003) This marks a major shift from superpower bilateral diplomacy of Cold war.
Multilateral and regional diplomacy have also evolved in the Post-Cold War era.

One of the major factors here were the end of Cold War and advances of Globalisation.
The end of the rigid alliance system and disappearance of the Soviet threat led to greater activity on multilateral and regional level.

While the diplomatic negotiations was previously conducted within the alliances or between the two superpowers, several initiatives have been launched both on regional and global level which used old international institutions like the UN or refurbished old ones like GATT into WTO.
On a regional level, Europe witnessed revival of EU in the Maastricht treaty and the Single European act, and North America concluded the NAFTA.

(Barston 2006) Whereas discussion platforms like the UN have been previously paralyzed by the Cold war hostilities, international organisations now have an increasing active role in the conduct of international negotiations.
States are too small for new problems facing them, so they try to collaborate to promote their joint interests.

The Kyoto protocol of 1997 on environmental protection is a multilateral solution to the international 'tragedy of the commons' conducted within the UN framework.
Here we can also observe the appearance of new topics on the agenda of international diplomacy.

On the regional level, NAFTA, AFTA, African Union and EU are examples of increased activity in regional diplomacy.
(Bayne 2003:97) The developments in EU are also affecting the multilateral level.

The Maastricht treaty introduced the Common Foreign and Security Policy to the community, which is now represented by the Commission on a number international negotiations platforms (e.g. WTO), and this in turn affects the dynamics of international conferences.
(Barston 2006:87) Greater reliance on regional institutions which are able to provide credible commitments in the negotiations process, for instance in the form of the ECJ, affects the manner of the diplomatic discourse.

While Berridge (1995) argues that the number of international institutions in international diplomacy has declined, several example show that they play a greater role in the new diplomacy.
A peculiar development is the emerging EU diplomacy.

Here we can contrast the diplomacies of individual states and diplomacy of the EU bloc as a whole.
This development is particularly marked by the Maastricht treaty which established the CFSP in 1992.

Since then the Commission has increased its importance as an international actor representing the block as whole in important organisations like the WTO.
The Banana wars in WTO are a good example of a dispute when EU was represented by the Commission.

Whereas international organisations played a limited role in the Cold war diplomacy, EU now fulfils the representational and negotiations function of several states.
One of the shifts in post cold war diplomacy is the increase use of personal diplomacy.

Thanks to the advances of globalization, heads of state are playing an increasingly important role in making and implementing of foreign policy.
Prime ministers and presidents are able and increasingly do get directly or indirectly involved in conduct of foreign policy.

This could also be a reaction to the internationalization of domestic policies, where heads of state try to take charge of domestically increasingly salient foreign issues.
Moreover, higher level of scrutiny and participation was possible through the technological advances where geographical costs are shrinking.

Rise of personal diplomacy is taken to undermine the role and influence of foreign ministries and related bureaucrats.
(Barston 2006) As key figures of state decide to get directly involved or use "cabinet secretaries and private envoys for negotiations", they bypassing the traditional ways of communication which involve foreign ministries leaving them without current information or just outside of the centre of decision making on foreign policy.

(Barston 2006:8) Greater scrutiny of foreign policy implementation leads to less freedom of action of foreign ministries.
For instance, the Blair Bush meetings are the major determinant of UK foreign policy where Blair has been taking an increasing role in formation of and conduct of foreign policy.

Also German chancellor Merkel engaged in the inter-EU diplomatic process and is playing a greater role in the EU policy making.
Public diplomacy is another form of diplomacy with growing importance and use.

Public diplomacy differs from traditional diplomacy in that public diplomacy deals not only with governments but primarily with non-governmental individuals and organizations aiming to cultivate public opinion in other countries by communicating a particular image or about an issue.
The 9/11 attacks has lead to particular importance of public diplomacy in US foreign policy.

While important of this form of power has already been recognized before, ""winning the hearts and minds" of Arab and Muslim populations has quite understandably risen to the top of the Bush administration's agenda."(Hoffman 2002) In this sense But winning the propaganda war isn't the only form of public diplomacy; soft power is an important form of power where public diplomacy has been used to nurture it.
W can argue that "once the stepchild of diplomats, public diplomacy has only recently taken its rightful place at the table of national security." (Hoffman 2002) Another important aspect of diplomacy where changes occurred is the agenda.

Previously noted the New Diplomacy addresses the topic of human security, moreover, environmental problems and world poverty are new topics of the international agenda.
The Kyoto Protocol, Rome treaty on ICC and treaty banning Land mines demonstrate that a growing group of actors of international diplomacy asserts an ethical dimension in the diplomatic discourse.

The change in post cold war diplomacy is that in this traditionally realist realm, we can now make a distinction between an idealist camp and a realist camp of actors.
(Davenport 2003) The new idealist agenda topics mark a serious diversion from Cold war.

Advances of globalisation and integration of world increase influence of international politics on domestic issues and policies.
But as the emergence of new idealist actors and the new diplomacy show, this works also vice versa as the domestic groups when united are able to set and promote multilateral political agendas.

A further phenomenon of the modern diplomacy is the transitional diplomacy.
As the end of Cold war started a new period of realignment and removed the lid on many conflicts, several states have been disrupted by the internal conflicts.

Transitional diplomacy refers to helping failed states or states torn by conflict to re-establish the peaceful process.
This has been particularly popular within the UN framework and peacekeeping and peacemaking are both very good examples this.

USA and member states of EU have been actively involved in mediating local disputes and rebuilding government structures to promote peace and stability, in countries like Rwanda.
Conclusion

The developments constitute a major shift from Cold war diplomacy to modern diplomacy.
A major development is the emergence of new non-state actors and their ability to affect international diplomacy.

As states try to cope with effects of globalisation and their fading power, they try to involve both sub-national actors as well as create new supra- or international actors which engage in diplomatic activities.
This is also reflected in states greater reliance on regional and multilateral diplomacy.

Several of the issues discussed can be summed up under the concept of transnational diplomacy.
The New diplomacy is an example of cross-border relations between state and non-state actors in promoting multilateral political agenda and transitional diplomacy involves state actors in helping failed states.

While the Cold war diplomacy invpolde interaction of states with other states, transnational diplomacy cuts across national; borders involves communication of state actors with non-state actors and intrusion of foreign states in previously domestic policy areas.
The rise of personal diplomacy where heads of state become actively involved in foreign policy decision making and conduct of diplomatic communication and increased importance of public diplomacy are major changes in diplomacy, as previously diplomacy was more confined to the foreign ministries and ambassadors.

Also communicating with other actors indirectly by influencing the public opinion is a recent development associated with importance of the soft power.
The widening of diplomatic agenda mainly reflects importance of public society and emergence of new actors.

It could be argued that globalisation created a global civil society which introduced an ethical dimension to diplomatic activity.
Still the essential functions of diplomacy, i.e. actors of diplomacy use it negotiation, representation, identify and formulate state objectives and goals, establishing and renewing rules and procedures of the international system, remain valid even though they are fulfilled by new actors on a wider area and complicacy.

Introduction
This paper assesses the explanatory value of the dependency theory and asks whether a theory looking at relations of dependency explain the severe inequalities in world economy.

While as Special report on Global Economic Inequality suggests there is a level of contention regarding how to measure and assess inequality, there is a clear divide between the rich developed and poor developing countries in terms of distribution of global wealth characterized by severe poverty in the Global South.
(Surin in Balaam 2006) The paper provides a synopsis of the argument associated with the dependency theory mainly based on the 1970 paper by Santos, addressing dependency relations as a source of the inequalities.

The dependency relations are illustrated in a critical analysis of three key structures of international economy, namely trade, capital and technology.
Following is a discussion of how plausible the explanation is when confronted with developments in world economy.

It is argued that dependency theory fails to explain a number of developments and makes wrong predictions in several cases.
It would seem that the theory is a partial explanation of existing inequalities at best.

A theory which limits the relevant causes to international level only does so at the cost of its explanatory value.
Dependency theory explained

This section provides an account of the dependency theory by exploring how relations of dependency in key global economic structures lead to inequalities in the world economy.
The dependency theory introduces an interpretation of relations in international economy characterised by dependency which makes economy of one country "conditioned by the development and expansion of another country to which the former is subjected to." (Dos Santos 1970:231) This leads to the idea that underdevelopment in a country is a direct consequence of expansion or development in other country on which the former depends on.

Proponents of dependency theory see structures and processes of world economy as having inbuilt biases leading to relations of dependency which directly cause the underdevelopment in the Third world.
The inequality can be explained in terms of dependency relations in trade, finance and technology.

"This system is a dependent one because it reproduces a productive system whose development is limited by those world relations which necessarily lead to development of only certain economic sectors, to trade under unequal conditions with international capital under unequal conditions, to the imposition of super-exploitation of domestic labour force with a view to dividing economic surplus this generated between internal and external forces of domination." (Dos Santos 1970:235) One of the arguments behind dependency theory is based on account of the international trade with trade dependency relations leading to underdevelopment.
"Trade relations are based on monopolistic control of the market which leads to transfer of surplus generated in dependent countries to the dominant countries." (Dos Santos 1970:231) In particular, developing countries have been forced to become mainly exporters of raw materials and primary goods, unable to compete with developing states in manufactured industrial products where developed states hold a comparative advantage, which has been further enhanced by the international trade regime bias in favour of developed states.

Developing countries are disproportionately affected by the tariff barriers.
While Uruguay round of WTO negotiations achieved substantial reduction of tariffs it has been more advantageous to the industrial countries as tariffs remain high on textiles, leather and agricultural commodities.

(Woods 1999) The relative openness of the global market in manufactured goods undermines the ability of LDCs to develop their own higher value added industries with manufactured goods and thus have to rely on imports.
Moreover, prices of the commodities and primary goods have been decreasing on the global market which has a negative impact on balance of payments of developing countries fur undermining their ability to develop.

The unequal trade relations thus leaves developing countries dependent on developed countries, where the unfavourable terms of trade undermine LDCs prospects for development.
Dependency theory identifies financial dependency of LDCs as a further factor leading to global inequalities.

"The financial relations are, from the view point of the dominant countries, based on loans and export of capital, which permit them to receive interest and profits thus their domestic surplus and strengthening their control over the economies of the other countries." (Dos Santos 1970: 231) As developing countries open their financial markets and integrate in the world economy, foreign capital is able to control their economic resources and rather than following developmental strategy it follows particular financial interests.
This leads to both the transfer of profits and interest and loss of control over its productive resources.

Consequently, reliance on foreign capital brings the risks associated with dependency.
Technological dependency relation also plays an important role in the theory's explanation of inequalities.

In particular, industrial development in the periphery is taken to be 'strongly conditioned by the technological monopoly exercised by imperialist centres.'(Dos Santos 1970: 233) The global regime on technology transfer is more beneficial for the developed countries as it protects their interests and leaves developing nations dependent on their monopoly as it is too costly for them to develop the needed technology independently.
International regime on patent rights severely limits the technology transfer and dominant states only deposit obsolete technologies in the dependent states generating a system of inherent industrial backwardness and dependency.

This leaves LDCs relying on developing countries for new technologies as major "technological innovations and gains in productivity largely occurred in the developed countries and the LDCs found themselves lagging and unable to compete in areas of new product development and production." (Balaam, 2006: 336) Technological dependency leaves LDCs unable to move up the production and technology ladder and thus contributes to their underdevelopment.
Evaluation of the theory

One of the implications of the dependency theory is that countries which decide not to integrate in the world economy are better off than those who fully liberalize and open their economies.
The extent to which this has been true can be judged when looking at relative success of countries which export-oriented growth and import-substituting industrialization strategies.

"Export-oriented growth is willing to risk dependency to gain benefits of development.
Import-substituting industrialization, on the other hand, views international market forces as threat and attempts to achieve development with less risk of dependency." (Balaam 2006: 340) While countries in Latin America have followed the latter, the East Asian countries which became to be known as the East Asian "tigers" (South Korean, Taiwan, Honk Kong, Singapore) and "Little dragons" (Thailand, Indonesia, the Philippines and Malaysia) have tried use the opportunities offered by the global market.

The Latin American approach was supposed to reduce its dependence on foreign capital, technology and markets by promoting home-grown industries, but in the end increased the foreign debt as deepening of import substitution required borrowing from abroad.
Once the countries were unable to service the loans as their exports declined, devastating debt crises followed.

Countries like Brazil and Mexico which adopted the ISI strategy overall experienced less growth, less stability and possibly greater inequality.
(Balaam 2006) The East Asian approach relied on strong state guidance of economy, supporting manufactured export oriented industries and attracting foreign capital to finance the development.

Contrary to prediction of the dependency theory, Asian countries which started as poor and underdeveloped have utilized foreign capital and trade to promote growth without experiencing underdevelopment.
Taiwan presents the dependency theory with a paradox.

It is both more integrated in world capitalism than the poor market economies and more developed.
(Amsden 1979: 342) Export oriented growth strategy results were very strong in terms of success in creating income and growth.

These developments undermine the dependency theory in two ways.
Firstly, dependency theory would suggest that countries which avoid the risks of dependence are more successful than developing states which liberalize and integrate their economies into the global system.

However, Newly Industrialized Countries in East Asia have clearly outperformed the Latin American countries which experienced greater level of inequalities.
Secondly, the success of NIC in achieving economic growth and eliminating national inequalities by relying on trade and capital relations with developed countries goes contrary to what would be expected by Dos Santos.

A development not predicted by the theory are the deteriorating prices of manufactured goods.
While the theory only talks about the volatility of commodities and primary goods, with an increase of global competition the prices of manufactured goods have also decreased making it cheaper of developing countries to import.

Thus the trade exchange of developing countries does not necessarily have to be negative based on imports from the developed countries.
A major factor which affected the balance of payments balance has been the volatile and increasing prices of oil rather than imports of manufactured goods.

A particular related issue when addressing a question about dependency theory is the development of the gap between Global South and North.
While World bank report talks about a positive net flow from developed to developing countries, Broad (1996) has suggested that once China and India are excluded majority of the 140 Southern countries are actually not getting closer to economic development levels attained by the North.

Broad (1996) has argued that contrary to World Bank 1994 report, majority of LDCs have not been part of the narrowing gap phenomenon envisaged in the report.
"The bottom line is that about a dozen countries have been doing well for the past few years, while the vast majority of the South is either slipping backwards, stagnating, or growing slower than the North." (Broad 1996: 9) It seems that it does come down to how we measure inequality.

If developing countries GDP growth is taken as the primary sign then Broads point seems to be valid however this seems to ignore that majority of developing world's population is concentrated in the key emerging markets countries.
As China and India are catching up so does the majority of world's population get richer and more equal.

As previously suggested it does come down to how we measure inequality, but concentrating on the sheer number of countries and ignoring the extent of Chinese and Indian success seems to ignore an important aspect of worlds economy.
Moreover, as the major success in the developing world has followed the liberal rather than dependency strategy, both countries [China and India] have consciously chosen to seize the opportunities afforded by the global economy, through both trade and foreign investment, the development of decreasing global gap makes the dependency argument less convincing.

The dependency theory does account for negative effects of integration into global financial markets and opening of domestic economy of developing countries to foreign capital.
A number of economic crisis have resulted from the globalization of financial structures and opening of economies to foreign investment.

This has had a particularly severe negative effect on the developing world.
As suggested by Dos Santos, foreign capital, particularly the short term investments follow financial interests and thus not interested in a countries development.

These investments have been highly volatile and unstable.
The collapse of the Mexican peso in December 1994 led to the flight of around $10 billion in short term US funds from that country in just the last week of 1994.

(Broad 1996: 15) As countries have relied on these investments to finance their development strategies, they have suffered serious negative effects when these funds have been withdrawn when a crisis emerged.
A particular case is the Asian financial crisis in 1997 which was partly caused by dependence on foreign capital.

"Although some analysts have argued that the crisis was caused by unwise state intervention in the economy, others argued that rapid adoption of Washington consensus policies especially free capital markets was more to blame." (Surin in Balaam 2006: 345) In this sense, this case supports the dependency case as integrating into the global financial structure by opening and liberalising the financial markets in Asian countries did not take into account the level of development of financial institutions in the countries and made it more susceptible to financial crisis.
Dependency on foreign capital has incurred considerable damage on the local economies.

However, liberals have blamed the crisis on a number of different domestic factors and generally bad government policies.
In this case, concepts like corny capitalism all help to explain the distortions of the market system which inevitably undermined the global market and led to the crisis.

(Gilpin 2000: 311-2) Still, it seems that the dependency on foreign capital and premature liberalisation of the financial markets were major causes of the crisis.
Surin suggests that many East Asian states have become increasingly reliant on foreign investments, especially portfolio investments which he views as producing the 'structural conditions', which brought about the Asian financial crisis which began in July 1997 in Thailand.

(Surin 1998) Another implication of the dependency theory is that developing countries which do not integrate in the global economy are better off than the poor integrated ones.
In particular, if the developing countries which integrated go down the vicious circle of underdevelopment, then surely developing countries which do not integrate experience greater level of growth as their economic surplus is not transferred to the developed countries and they don't suffer from unequal trade relations.

However, that would be an odd conclusion, given that sub-Saharan Africa's economies are so comparatively isolated from the rest of the world economy and yet they experienced the lowest levels of GDP growth with some of them being negative.
During the disastrous decade of the 1980s, gross national product (GNP) per capita increased only by an average of 1% in the South and in sub-Saharan Africa, it declined by 1.2%.

(Broad 1996: 8) If an explanation of underdevelopment relies purely on the global economy to provide causes for it and thus limits the explanation to make an account in terms of factors on the international, it fails to account for lack of development in countries which are not integrated or are have tried to avoid the dependency relations risks.
Given the examples of success of countries which have integrated, e.g. China and India, the point could be taken even further and it could be argued that Sub-Saharan Africa suffers not from globalisation, but from lack of it.

The dependency theory explanation of inequalities fails to account for underdevelopment where the international level of explanation is not present and thus at best only provides a partial explanation in a limited number of cases of underdevelopment.
Conclusion:

The dependency theory introduces an interpretation of relations in international economy characterised by dependency which conditions development of one country on another and thus explains inequality with the concept of underdevelopment.
The global regimes on trade, finance and technology transfers have a bias which favours developed countries over developing ones and leads to underdevelopment in the latter.

This explanation however leads to a number of implications which are not confirmed by developments in world economy.
Failures of the Import-substituting industrialization strategy and an immense success of export-oriented growth which relies on foreign trade and capital for development goes contrary to dependency argument.

Moreover, as both prices of manufactured goods as well as the prices of commodities have been decreasing, the unequal trade exchange is not as convincing as it fails to predict this.
Also the theory fails to account for the least developed countries in Sub-Saharan region as these are not integrated into the international economy.

The argument is further undermined by the decreasing gap between North and South.
Still, dependency theory does account for the negative effects of unstable global finance and investment on the developing countries.

However, this does not lead to development in the developed countries as the investors also loose in a financial crisis.
It seems that the theory can only provide a partial explanation of inequalities in global economy at best.

By concentrating on the global level as a source of explanation it ignores the effect of domestic level on prospects of development.
Global economy offers opportunities and risks where dependency theory only tries to account for the risks and even then overestimates their potential and effect.

Introduction
This paper discusses the rise of regionalism in the international economy.

In particular, the recent surge of regional trade agreements (RTAs):"about 162 RTAs are in force as of 2002 with over half of those coming into existence after 1995.
The WTO estimates that over 300 will be in effect by 2007." The paper starts with defining the concept of regionalism and delineates the timeline examined.

Following is a detailed discussion of causes of and motivations for regionalism.
The main analytical part is divided into arguments about end of Cold war consequences, advances of globalisation and existing economic interdependence levels, globalization of structures of international economy, economic and political motivation, failures of multilateralism and domino theory of regionalism.

It will be argued that a variety of factors gave rise to the surge of recent regional projects and motivated states policies on this issue.
While most of the regional projects are limited to economic integration resulting in limited institution building mostly with economic responsibilities, political factors and changes play an important role in the process.

There are several approaches when explaining regionalism.
In particular we can distinguish between international level, which can be contrasted with domestic level where domestic interest groups play an important role rather than the structures and processes of international economy.

As for theoretical perspectives, we can contrast neo-liberalism and neo-realism where each tries to explain regionalism in different ways.
Neo-liberalism stresses the economic benefits and ability of institution to enable international cooperation.

Neo-realism explains regional cooperation in terms of power distribution and national interests.
Definition of regionalism

This paper defines regionalism as a process which promotes "an identified geographical or social space as the regional project." (Stubbs and Underhill 2006:72) It is manifested by political and /or economic integration of policies leading to creation of institutions at a regional level.
It serves to fulfil specific shared interests of a group of states which decide to pool their power and sovereignty, in order to gain a greater degree of relative power.

The integration is instrumental to fulfil a specific shared interest determined by collective decision making.
Furthermore Deutsch (Laursen 2003) argues that regional integration has a security element as it creates a security community.

Thus apart from economic and political integration we can talk of integration of security policies or at least a common regional security goal.
Regionalism can be contrasted to multilateralism which involves a multitude of various states whereas regionalism tends to be limited to a specific regional or spatial area.

Regionalization, i.e. division of the international community into regions, is the product of the process of regionalism.
The most important examples of regional projects are European Union in Europe, MERCOSUR in Latin America, NAFTA in North America and the Association of South East Asian Nations in Asia.

The paper will mainly examine and use examples since the end of Cold War.
Causes and reasons of regionalism

This part of the essay discusses the various arguments which describe the causes and the motivations for rise in regional in international economy.
End of Cold war contributed to the regional projects building in several ways.

It acted as a catalyst as it induced changed in the international environment and state polices making regional projects more attractive and moreover, the disappearance 'Iron curtain' dividing line made regional projects possible on between states previously firmly aligned in rivalling camps.
Post Cold war period can be characterized by changes in alignment and disappearance of previous rigidity in alliances and groupings of states.

Changing balance of power initiated and also allowed for a rise in regionalism.
As new power centres started emerging and old ones have declined, a number of states were no longer limited by the traditional spheres of influence and engaged in regional projects.

This is particularly true for the 1992 and 2004 enlargement rounds of EU, where former Soviet bloc countries joined the regional projects started by Western European countries.
The EFTAN countries like Austria were no longer limited by the need to preserve its neutrality and Eastern European nations were now free to decide their own policies.

Furthermore, End of Cold War brought the Cold War rivalry and ideological conflicts to an end and thereby contributed to the rise of regional projects.
The conflict ceased to be the defining factor of in national policies.

Whereas Cold war was dominated by security agenda, the post Cold war policy agenda became increasingly, at least in Europe, dominated by the objectives of political stability and economic growth.
New policy goal, namely political stability, is an important political motivation for the EU 1992 and 2004 enlargement and NAFTA.

Both EU enlargement and NAFTA treaty were seen in the West as a project which would ensure political stability in both Eastern Europe as well as Mexico.
Regional projects commit the states to collective decision making in limited areas of policy and issue linkage makes deflection and abrupt change in policy very costly.

In this sense, new EU member states and Mexico were anchored to an established political system with a democratic and liberal tradition.
Moreover, US victory in CW made liberalist doctrine mainstream and the most attractive way of achieving economic growth.

Consequently policies of economic integration and open economies in the form of were considered the best way to induce economic growth.
Regionalism and associated projects can be seen as putting the liberalist doctrine into practice on a regional level, motivated by a recognition that economic interests are best served by regional integration.

The liberalist argument for regionalism is that regional economic integration and opening of markets leads to growth as it produces trade expansion, either in the form of trade creation or trade diversion, and static and dynamic efficiency gains.
(Balaam and Veseth 2005) The main point here is that as states integrate their economies and markets with its regional partners, individual states specialize in production of goods where they have competitive advantage and creates economies of scale.

Resulting efficiency gains also encourage trade creation as cheaper goods are more attractive abroad.
On the domestic level, we can identify several forces contributing to rise of regionalism.

Regional projects and liberalisation provide domestic companies with increased market access; consequently companies which are able to benefit from economic integration will pressure the government to implement necessary policies.
Economies of scale are particularly prone to pressure the government agencies for increased in market access.

Support of big national corporations was an important factor both in single market project in EU and NAFTA.
Economic integration on a regional level has several other consequences which have motivated states to engage in the regional projects.

It can be argued that economic regional integration creates a competitive advantage vis-à-vis non-members.
As goods of members become more competitive, trade between members increases at the cost of previous trade with non-members.

While economic regionalism tends to lead to more creation rather than trade diversion, regional integration offers a good way to gain a competitive advantage against non-members.
For example, one of US motivations for NAFTA was to keep Japanese investment and products out of the country and the FTA provides preferential treatment of US companies in Mexico.

(Balaam and Veseth 2005) The EU 1992 single market project can also be seen as a striving to make Western Europe more competitive on the global market in the face of strong competition.
A European wide single market and monetary union decreases costs of business operating in EU, forces European companies to be more competitive and brings efficiency gains.

This makes European companies more competitive on the global market and provides them with preferential treatment within EU against non member countries' companies.
Another factor which contributed to rise of regionalism are the failures of multilateralism.

As state recognize the benefits of liberal policies and are unable to pursue their interests on a multilateral level, they turn to regionalism.
Multilateralism in WTO has been particularly prone to breakdowns and government found it difficult to arrive at satisfactory agreement on further areas and higher levels of integration during the Doha and Uruguay round in WTO.

Dissatisfaction with multilateral economic integration led to rise of regionalism, as regional institutions are provide greater flexibility, a more secure environment and regional arrangements are politically easier to accept at the domestic level.
(Woods ed 2000) Regional arrangements involve fewer countries which promotes greater flexibility both at negotiating the deal as well as at later collective decision making level, whereas multilateralism requires a great deal of consensus seeking.

Countries find it easier to arrive at a favourable deal which comes closer to satisfying their original interests and are able to go deeper on specific areas of integration than multilateralism would ever make it possible.
Regionalism "allowed countries to lower trade barriers among neighbours and political allies, while retaining flexibility over which sectors to liberalize and which issues to negotiate." The threat of exclusion is more potent at regional level as the integration goes deeper countries also face to loose more if they opt out.

Furthermore, regional arrangements are easier to accept at domestic level than multilateral ones.
In particular, the US executive found it a lot easier to get the NAFTA deal passed in the Congress than the multilateral treaties negotiated within the WTO.

As the limits and failures of multilateralism became more evident in the Doha round, regionalism became a more attractive policy.
While economic motivations for regional projects played an important role in the recent rise of regionalism, it has to be understood in the context of advances of globalisation.

Existing informal regional and global economic ties and interdependence had of profound impact on national economic policies and power in external and domestic environment.
To a certain extent the loss of a degree of control over states ability to implement its policy objectives, marked by increased importance of translational relations and pre-existing levels of intraregional levels of trade led states to look for new forms of political governance.

Regional institutions and projects can be understood as aggregating states ability to control and decide in policy areas where individual states lost part of it.
By pooling their power, states gain a greater degree of relative power and in this sense external independence at least in relation to outside the region environment.

Rise of regionalism is to a certain extent a response to the effect of globalisation on states ability to implement their policy objectives.
In this sense, regional institutions like European Commission provide a regional level of policy output for member states and by representing the region externally is/are able to exert a greater level of relative power than single members.

Individual member states thus gain more power in the international field.
In particular, smaller states like Belgium and Cyprus gain power by joining powerful regional organisations with collective decision making like EU.

However, political institutions and integration of non-economic policies, e.g. in EU, are an exception rather than a rule as most of other projects are concerned mainly with convergence of economic policies.
While economic motivation as explained above are important, globalization and global economic structures and markets make regional projects more attractive as a way to deal with global market and associated problems on a regional level.

The Asian financial crisis of 1997 particularly illuminates the lacking global economic governance and regulations.
ASEAN as a response to these started to develop institutions to prevent or at least tackle market failures.

Development of a monetary union in EU is also motivated by states willingness to gain greater control over their monetary policies and avoid problems associated with weak and unstable currencies.
Regional projects can be seen as providing the foundation of political governance to match the expanding force of the market.(Balaam and Veseth 2005) Another factor contributing to the region projects is existing economic interdependence level between countries, as high levels of trade and volumes of transnational government create a high level of unregulated transaction where governments are unable to exert influence over these without inducing significant costs.

Neo-functionalism argues that "the expansion of economic activity generates pressure on enhanced regional or international coordination by enabling such organizations to fulfil functions that states can no longer perform." (Mansfield and Milner 1997) This can be also seen in NAFTA where existing levels of trade between USA and Mexico created a level of interdependence between these two countries which made the trade agreement easier to accept.
Domino theory of regionalism (Baldwin 1993) presents a particularly interesting explanation of rise of regionalism.

Regionalism here is seen as creating benefits for members involved and creating costs for non-members which makes membership particularly attractive.
In this sense membership in a regional block creates a competitive advantage against non-members companies as it guarantee preferential treatment for members companies.

Potential loss of competitiveness makes the option of remaining outside the block very costly as it can cause trade diversion, expanding trade between members at the cost of trade between non-members and members.
As it is better to be in that out of the regional block, regional integration is naturally expansive.

Once a group of countries decides on an economic regional project, it is rational for other countries to try to join the block or create one of their own (as seen with UK trying to create the EFTA).
Two important examples conform to this theory.

Firstly, creation of the FTA between Mexico and USA (NAFTA) led to other South American countries applying for membership and eventually led to creation of MERCOSUR as countries recognized the benefits of regionalism and possible costs of non-members.
Creation of NAFTA led to creation of further regional projects.

The Single Market initiative in the EU also started a domino effect in Europe.
The EFTA countries recognized the economic costs of non-membership, decided to apply and most of the actually went to become full members of the EU.

Conclusion
In conclusion, several lines of argumentation were considered.

In particular, the paper has tried to make a distinction between motivation for regionalism and changes which caused the rise of regionalism.
End of Cold war and related consequence both enabled and motivated regionalism.

Advances of globalisation, existing economic interdependence levels and globalization of structures of international economy in particular led states to engage in projects which try to deal with the forces of the global market.
Both neo-liberal as well as neo-realist theories provide a perspective on the arguments about economic and political motivation.

Furthermore, domino theory of regionalism is a theory which tries to build on existing theories and tries to explain the recent rise in regional projects.
It seems that international level provides a more important source of motivation for regionalism.

Global changes and developments enabled as well as made regional projects more attractive for states seeking both greater political stability as well as economic growth.
Still it seems that regionalism reflects more the national interests of individual group members rather than a general broad liberalization.

Introduction
This paper addresses one of the fundamental objections to Naturalist Moral Realism (NMR).

The argument runs that NMR moral judgements do not cohere with practicality requirement of morality and Humean picture of beliefs and desires.
The first part of the essay explains the nature of the problem and a possible solution, as argued for by Michael Smith in his book Moral Problem.

An important role in this plays his analysis of normative reasons.
The second part of the essay evaluates Smith's solution in reference to Brink's and Copp's papers.

They try to challenge Smith in various ways.
I consider four, in my opinion most influential, counter-arguments.

These arguments challenge Smith's analysis of normative reasons; in particular, it seems to ignore a fundamental level of human cognition.
Moreover, they question the connection between moral judgements and motivation he defends and the definition of 'full rationality', which he provides.

If Smith succeeds in making a link between moral facts and beliefs about normative reasons, and showing that normative reasons and rationality can be beyond reasonable doubt shown to affect how people are motivated to act, we can say that it possible to be a naturalist moral realist without denying that there is a connection between moral judgements and motivation.
Naturalist Moral Realism

To begin with, the paper explains naturalist moral realism.
It has in essence three main premises.

Firstly, realism believes that there are moral facts.
These facts are natural facts.

They are determined by circumstances and observable by science.
Finally, moral judgements are beliefs about these facts.

P1 There are moral facts.
P2 These facts are natural facts.

P3 Moral judgements are beliefs that are true when they are accurate to the moral facts.
Thus NMR aim at objectivity, objective in the sense that 'it is possible to converge in our moral views." Moralizing in normative ethics has to determine what the objective moral facts are; it supplies us with normative reasons about what morality requires us to do.

Moral judgements are based on our beliefs about what we have reason to do.
At this point, it is necessary to make a distinction between beliefs and desires.

According to D. Hume, beliefs represent the world and can be subject to rational criticism.
Desires, on the other hand, are about 'how the world is to be' and cannot be assessed in terms of truth and falsehood.

Discoveries about the nature of the world, i.e. beliefs about natural facts, have no effect on what we desire.
Therefore Hume claims that desires and beliefs are independent existences.

P4 Desires and beliefs have independent existence.
Another feature of moral judgements we need to take into account is that moral judgements also have an influence on our actions, i.e. the practical implications of our moral judgements.

It is a conceptual truth that if an agent judges an action to be right, he is also motivated to act accordingly.
According to Hume, a motivation requires two things: a desire to act and a means-ends belief.

An agent is motivated to act in a certain way if he desires to act in that way and believes that the action in mind is a means to his desire.
It follows that when an agent believes that an action A is right, he also desires to A. P5 If an agent A judges A's B-ing to be right, he would also be motivated to B-ing.

P6 An agent is motivated to do a particular action B, if and only if he desires to B and believes that the action A is a means to his desire to B. P7 It is a conceptual truth that if an agent A believes that A's B-ing to be right, he also desires to A.
However, the problem of NMR becomes clear once we consider both objectivity and practicality of moral judgements together with Hume's picture of desires and beliefs.

Since the rightness of an action is determined by beliefs about the world, the moral judgement is also essentially just a belief.
The problem is that while we might believe that a certain act is right, it is not clear how this belief about the natural facts is supposed to guarantee a desire ergo motivate us to act accordingly.

"The Humean picture [in P4] therefore appears to imply that a belief that one would be right to do a given thing could not guarantee motivation, even if we hold "other things equal", since nothing guarantees that the belief is accompanied by an appropriately related desire." Objectivity and practicality of moral judgements seem to be inconsistent in NMR as the former fails to make sense of the latter.
Smith's solution

Now that the nature of the problem of NMR has be explained we can set on to answer the question, whether it is possible to be a naturalist moral realist without denying that there is a connection between moral judgements and motivation, i.e. whether there is any solution to the conflict between objectivity and practicality.
It has to be noted that M. Smith is right to assert that practicality of moral judgements is not always the case.

An agent does not always have to have the desire to act, even if he recognizes the rightness of the action.
For example, clinically depressed or otherwise apathetic individuals can sincerely judge an action right and yet entirely fail to be motivated.

Thus a modification of P7 seems to be in order.
P7' If an agent A believes that A's B-ing to be right, were he fully rational, he also desires to A. Michael Smith "aims to solve the problem without denying any of the three propositions [P3, P4, P7'] by developing a rationalism that is consistent with Humean doctrine." An analysis of normative reasons is the base for Smith's solution to the problem.

"The core of the idea is that facts about our normative reasons for action - that is, facts about what is desirable for us to do - are facts about what we would desire ourselves to do if we were perfectly placed to give ourselves advice." In other words, normative judgements tell us what a perfectly rational copy of ourselves would do in the given circumstances.
"What we have normative reason to do is what we would desire that we do if we were fully rational." In this sense, they are normative as they tell us what is required to do.

P8 A has a normative reason to B if and only if a fully rational version of A, would want the A to B and B-ing is an act of the substantive kind.
In Smith's analysis, normative reasons are also practical as they cause agents to hold corresponding desires.

"Normative reasons are practical in the sense that someone who believes that she has a normative reason to act in a certain way in certain circumstances will have a motivating reason to act in that way in those circumstances," other things being equal.
The core of idea is that if A believes that A has a normative reason to B, if A were fully rational he would desire to B.

A desires B in virtue of A's believing that he has a normative reason to B. Psychological coherence of agents is the basis for another way to understand this connection between normative judgements and desires.
"Beliefs about normative reasons, when combined with an agent's tendency to have a coherent psychology, can thus cause agents to have matching desires." In Smith's account, normative reasons cause and therefore explain rational desires, as having beliefs about reasons makes it rational to have corresponding desires.

P9 It is conceptually true that if A believed he has a normative reason to B, were he rational, he would desire to B.
This account of normative reasons also seems to be in accordance with Humean theory about desires and beliefs.

Also the theory does not state any constraints on having the desires, i.e. whether they can be held in virtue of normative beliefs.
Therefore, practicality of normative reasons coheres with P4 - P6.

The last part of Smith's solution is in connecting moral judgements with normative reasons.
Given that the function moral facts is "to provide us with objective reasons for acting," i.e. to tell us how we are required to act, moral facts can be analyzed into beliefs about normative reasons.

Thus "whenever someone believes that an act is morally required, what they believe is that there is a normative reason to perform that act," and facts about moral requirement are facts about normative reason.
For example, if I believe that it is right to uphold my promises, I have a normative reason to uphold them.

P10 A's right if and only if A has a normative reason to B.
To put it all together "since our beliefs about normative reasons give rise to corresponding motivations, at least absent practical irrationality, it follows that if moral facts can be analyzed in terms of facts about normative reasons then our belief about what we are morally required to do must also give rise to corresponding motivations, at least absent practical irrationality." This solution accommodates also P1 - P7 as normative reasons are both objective, practical and cohere with the Humean picture.

It seems that Smith's solution to the problem is valid.
Criticism of Smith's solution

The second part of this paper addresses the counter arguments to Smith's solution and evaluates Smith's solution.
One way to challenge the picture Smith has drawn is to refute P9, i.e. the idea that "what we have normative reason to do is what we would desire that we do if we were fully rational." The problem with this is that it is not clear whether the normative reason is a pro-tanto (contributing) reason or an overall (overriding) reason, i.e. whether it only applies to overriding reasons or also to pro tanto ones.

For example, I can have a pro-tanto reason to keep my promise and meet my friend Joe.
If I, on the way to the meeting, see a drowning man, I will have a stronger normative reason to save him rather than keep my promise to Joe.

This shows that it is fully rational agent can have a pro tanto normative reason, which overrides a different one.
One might understand P9, as "a claim only about overriding reasons and that it says nothing about reasons that are overridden." However, Smith's aim is supposed to explain what moral reasons are.

To do this P9 needs to say more about pro-tanto level of cognition in relation to desire.
In particular, Smith's solution needs to specify in what sense of rationality I am rationally required to desire what I believe about my pro-tanto normative reason.

One can further argue that the analysis of normative reasons in P8 completely fails to make sense of the pro tanto level of cognition.
In a given situation an agent can have a pro tanto reason to do A, but if A is overridden by a stronger pro tanto reason to B; it would not longer make sense for a fully rational copy of the agent to want him to desire to A. Given the definition of a normative reason in P8, the defeated reason to A ceases to be a reason at all.

Yet we can say that we still have pro tanto reason to A. Thus, we can see that Smith solution fails to capture the pro tanto level of cognition about moral requirements.
There is also a problem with P7'.

P7 assumes that moral requirements are authoritative but Brink argues that moral requirements can well conflict with other requirements of rationality, e.g. prudence, and non-moral requirements can override moral requirements.
He has drawn an example of principled amoralist who is indifferent to moral requirements but not because of "a psychological interference with normal process by which results of practical deliberation affect an agent's motivational set," but because of principles he accepts.

Thus, he need not regard morality as having authority over other requirements of rationality and yet be fully rational.
Even if we make sense of what our moral requirements are, they can just play the role of pro tanto reasons.

While it still does not refute the idea of the principle that normative reasons carry motivational power, it does pose problems to Smith's formulation and to the idea that moral judgements need to have motivational power.
"Whereas the motivational efficacy of moral judgement reflects that many regard moral requirement as authoritative, the principled amoralist shows us that this assumption is open to challenge" Moreover, there is a problem with P8 and the definition of 'fully rational'.

It is necessary for Smith's theory to work that he specifies what it means.
Two Smith's ideas come to mind.

On the one hand, in order to have a normative reason an agent has to be fully rational, which is a "state that we are in when we are immune from reason criticism." In particular, fully rational agent's set of desires has to be "maximally informed and coherent and unified." One should note that this account of full rationality is naturalistic.
On the other hand, for an agent to be practically rational, i.e. so that his normative reasons guide his desires, he has to be a "good and strong-willed person." While this is not necessarily, what Smith means by fully rational it is clear that he considers these features essential for an agent to make moral judgements, which conform with NMR argument.

This account is non-naturalistic.
The problem becomes clear once we take a closer look at both of these definitions.

If Smith specifies full rationality in naturalistic way, it is not clear whether "by seeing what we would desire in a possible world in which we had a maximally informed and coherent and unified set of evaluative beliefs," we get a morally right set of desires.
Smith's definition is vulnerable to an Open question argument.

In practice, one can argue that it makes sense to ask whether a Smith naturalistically defined 'fully rational' set of desires is a morally good set of desires.
Full rationality need not equal rightness.

The problem with the non-naturalistic account is that it transfers Smith's NMR into a covert non-naturalism theory.
If Smith wants to find a solution to NMR, he cannot rely on non-naturalist moral realism.

Consequently, Smith is wrong to rely on a non-naturalist definition in his solution to the problem.
Conclusion

The nature of the problem is defined by the inconsistence of the three theories.
If we keep the Humean picture of desires and beliefs, then moral objectivity, as defined by NMR, is inconsistent with practicality of moral judgements.

It seems that in order to be naturalist moral realist we have to deny that there is a connection between moral judgements and motivation.
Smith also recognizes the problem.

His way out of this is a more sophisticated form of NMR, which keeps both objectivity and practicality of moral judgements but avoids the trap of Humean picture.
He makes a very intricate connection between desires and beliefs, which coheres with Hume.

The essence of his solution lies in an analysis of normative reasons and connecting moral facts with normative reasons.
The resulting normative moral reasons seem to have motivational power required and are objective.

While Smith analysis of normative reasons seems to be coherent, there are various objections to the solution.
Firstly, his it is not complete as he fails to specify how pro tanto normative reasons fit into the picture, in particular, their connection with desires.

In addition, the way they cohere with the full rationality criterion.
Yet these challenges need not refute Smith's solution, as they just require further explanation.

Another problem is the motivational power of normative moral reasons.
Copp's principled amoralist challenges to authority of moral requirements and shows that "a rational moral agent may believe she is morally required to do something without being motivated to do it." Authority of moral requirements is at least an open question.

This objection effectively challenges the practicality of moral judgements.
If it succeeds then there is no need for Smith's solution in the first place.

The decisive problem of NMR, which Smith fails to solve, is the definition of rightness.
Smith tries to avoid the issue by claiming that fully rational set of desires would also be necessarily right.

This is not obvious and more needs to be said.
Two accounts of full rationality, which I found in his work, can be considered.

However, the naturalistic one does not avoid the Open question argument and the non-naturalist one is not a viable option for NMR.
Smith needs to find a way around these challenges if his solution is to provide a plausible connection between moral judgements and motivation.

Introduction
This paper addresses the topics of particularism and generalism, where the main concern is the conflict between these two theories and its evaluation.

A particular form of generalism, Ross' deontology is firstly explained in more detail and some examples are given on how the theory is supposed to work in the process of moral judgement.
The particularist position and criticism of generalism is taken from Jonathan Dancy's paper.

Once the theories are defined and more fully explained, the essay presents Dancy's objection to Ross' deontology.
I take a closer look at three, in my opinion, most influential arguments Dancy makes: the argument from examples, holism of reason and argument from enabling conditions.

The second part of the essay evaluates Dancy's objections to particularism, Brad Hooker's paper and Sean McKeever's and Michael Ridge's paper were used as the basis.
The essay ends with some conclusions on the debate where the plausibility of particularism is given some thought.

The main aim of the essay is to establish whether Dancy's objections create serious problems for Ross' deontology to work, so that we have better reasons to rely on particularism, as it is more plausible.
One should note that argument against Ross' deontology also work as arguments for particularism, as these theories hold a completely different view of morality, refuting one makes the other look more plausible.

Ross' deontology
To begin with, one needs to clarify the position Ross deontology promotes.

Generalism, and Ross deontology as a form of it, maintain that "rationality of moral thought and judgements depends on a suitable provision of moral principles", ie that we should understand morality in terms of principles.
When an agent faces a moral judgement he ought to look at moral principles, which serve him to decide which action is right or false.

Thus it can be said that in generalist perception moral principles serve to connect or to specify the connection between the non-moral properties or features of the situation with moral properties, which are then translated as moral obligations.
Ross, in contrast to absolute generalism, makes a distinction between pro tanto obligations and overall moral duties.

When an action upholds a pro tanto duty then it is a pro tanto moral reason in favour of the action.
In practice whenever we make a moral decision its overall polarity, i.e. whether it is overall right or wrong, depends on the pro tanto moral reasons that are relevant to the case.

According to Ross, to figure out whether an action is overall morally right or wrong, we need to weight the Pro tanto moral reasons against each other.
To decide which of the moral reasons count most depends on the circumstance.

Now for Ross the polarity of the overall moral judgement varies and depends on the features of the case as they affect which pro tanto moral reasons are relevant.
He is then in this sense particularist about the overall moral judgements / reasons.

Polarity of the pro tanto reason, on the other hand, does not change and Ross is therefore generalist about pro tanot reasons.
This means that a pro tanto reason cannot count in favour of an action in one case and count against, or not at all, an action in a different case.

Once a feature counts in favour it always in any situation counts in favour of any action.
Ross believes that there are at least five pro-tanto moral reasons.

Consequently, what we have overall moral reason to do will always be some sort of a function of these five pro tanto moral reasons.
Ross' deontology:

1. what is a reason in favour in one case counts in favour in any case2.
judgement is the attempt to determine the balance of reason, so conceived.

Particularism, in contrast, holds a different view of the pro tanto reasons.
Particularist stress that pro tanto reasons have essentially variable relevance, i.e. they completely dependant on the circumstances of the situation and a pro tanto reason can count in favour in one case and count against it in different one.

This doctrine is called the holism of reason.
Therefore, no principle can determine or capture how a feature counts in a case.

Holism of reason:
what is a reason in one situation may alter or lose its polarity in another.the way in which the reasons here present combine with each other is not necessarily determined in any simple additive way.

Objections
The first particularist objection addresses the idea that pro tanto reasons can count only in one way, i.e. either in favour or only against an action.

This argument is essentially based on examples where moral reasons change polarity.
In addition, a property producing pleasure is normally a pro tanto reason in favour of an action.

However, where an act would produce pleasure for a sadist, it no longer counts in favour of the action, quite the opposite, the pro tanto reason changes polarity and counts against this action.
Dancy mentions that one of the reasons against having public hangings would be that it produces pleasure for the people who watch.

Another example where the traditional polarity of a pro tanto reason can switch is the property of keeping promises.
Consider when an agent makes a promise to break his next five promises or kill an innocent child.

All these promises are essentially immoral and count against an action, which would uphold them.
This is however in stark contrast with Ross deontology where a pro tanto reason can have only one polarity.

It seems that at least in some examples Ross deontology is wrong to rely on the principles, as they would not correspond to what we would normally consider moral.
Now that we have established that there are examples where the polarity of pro tanto moral reasons changes we can move on to second argument where Dancy tries to prove that holism of reason is valid for moral reasons.

This argument rests on two premises.
The first one is that holism of reason is true / valid for non-moral reasons, i.e. epistemic and practical reasons.

For instance in an ordinary situation, we have good reasons to trust what our senses like perception or taste tell us.
However, if we are under influence of psychedelic drugs the fact that we have a certain perception is no longer a reason for believing that the world is as we see it.

If we are under influence of a drug that makes blue things appear red, we cannot take the fact that we see a red object a reason in favour of believing that we actually see a red object.
Therefore epistemic reasons seem be contingent on the circumstances of a particular case.

The second premise establishes the connection between holism of non-moral reason and holism of moral reasons.
Dancy's main reason for that is that "nobody can say with any confidence just which reasons are moral ones and which are not." Since there is no obvious difference between non-moral and moral reasons, it seems more plausible to believe that moral reasons function just like non-moral reasons.

"If moral reasons, like others, function holistically, it cannot be the case that the possibility of such reasons rests on the existence of principles that specify morally relevant features as functioning atomistically." Therefore, since Ross' deontology fails to specify why moral reasons should not function just like non-moral reasons i.e. holistically, particularism seems more plausible.
The third argument against Ross' deontology is based on the idea that in order for some reasons to be reasons at all, they require something / a condition to be in place.

Some features act as enabling conditions, "they enable the features that are reasons to be reasons they are in this case, without themselves being among the reasons why the action is right." A good example of this is the idea that ought implies can.
This in practice means that when an actor is not capable of actually doing an action he cannot have the moral reason for it.

Thus for us to even start considering the morality of an action or the polarity of a pro tanto reason, we first need to have an opportunity to do it.
Consider an example where a disabled man sees a drowning person but has no way of actually saving him.

If the disabled man would try to save the person, it is very likely that he would drown as well.
In addition, a promise under duress would not be considered binding, as the agent was forced into it.

A morally binding promise would require the enabling condition that it was not made under duress.
Thus we can maintain that there are no moral obligations in these situations and particularist are right to make the "distinction of 'counting in favour' and 'enabling something else to count in favour.'" The enabling condition is therefore not a part of the reason yet affects whether the features even becomes a reason at all.

Ross' deontology seems to be mistaken to rely on principles, as they cannot incorporate the enabling condition.
Evaluation of objections

The next part of the essay evaluates the argument presented above.
The argument from examples seem to show that the polarity if reasons changes in accordance with the circumstances.

However, this does not mean that this is a rule and also Ross himself does not hold that all reasons are invariant.
He makes a distinction between derived and underived principles.

Where derived principles are variable and essentially function of underived invariable pro-tanto moral reasons.
Therefore Ross' deontology is not undermined by the giving examples where derived reasons change their polarity.

Yet the presented cases show that even the basic principle of pleasure can change polarity when it benefits a sadist.
Some generalists, e.g. Noah Lemos, would maintain that altruistic and sadistic pleasure is intrinsically alike and would argue that other moral properties, e.g. the fact that a wicked person who does not deserve the pleasure gets it, make this act not permissible.

This is not a plausible argument as surely "the moral status of the pleasure depends on, among other things, what kind of pleasure it is." This also goes against the common sense morality, as no one would maintain that sadistic pleasure is essentially a good moral property.
Brad Hooker, on the other hand, accepts that sadistic pleasure counts against an action.

This however only establishes that there are different types of pleasure, where non-sadistic always counts in favour of an action.
A similar argument can be made about the promise-keeping example.

Since the promise offends against other obligations, an agent has no moral reason to keep it.
Thus, the distinction can be drawn between immoral promises or promises made under duress and promises not offending against other obligations.

When the latter type of promise is involved, it is always a moral reason in favour of an action.
Therefore, the examples do not establish that underived pro tanto reasons can switch polarity, as Ross' deontology can always respond by making the principle and therefore also the reason more specific.

Generalism also responds with examples, which undermine the particularist theory.
Brad Hooker produces examples of pro tanto reasons, which always count in the same way, e.g. "killing of a conscious human being who is not threatening others physical security." However, these reasons look all but simple.

No Dancy seems right to point out that "invariant reasons should there be any, will be invariant not because they are reasons but because of their specific content." However, these complex reasons are not what we would normally use when making a moral judgement.
"The whole enterprise of defending one's reasons by complications begins to look strangely irrelevant, and its products unnecessary." The second argument concerning the holism of reason has at least two possible replies.

Firstly, Brad Hooker attacks the first premise that epistemic, practical and moral reasons are holistic, in particular that the example with psychedelic drugs does not hold.
This is because in the given situation there is additional reason why we believe that what we see is really as it seems, in this case that a red looking object really is red.

The additional reason is the belief that we are under normal conditions.
This however alters once we are conscious of the fact that we took a mind altering drug.

Therefore, the argument given does not establish that epistemic reasons are holistic.
Yet moral reasons can be holistic as Hooker only refutes the version of argument presented above.

Moral holism seems to hold in theory of choice.
Dancy gives the example where he has several options of action but none of them is painless.

In a situation where no painless alternative is present, pain is no longer a reason against an action.
Therefore, the alternatives present in a situation affect the status of a pro tanto moral reason.

This example at least seems to back the holism of moral reasons.
Another way to argue against the example is to maintain that the given reason is not a good enough complex feature to be taken as a reason for or against something.

This would however fall to the trap mentioned earlier where Ross responds to particular examples by making the principle and the pro tanto reason more specific.
McKeever and Ridge make the second reply that holism of reason is consistent with Ross' deontology.

As long as we can codify the circumstances, i.e. we can make a finite list of them, "Ross' denotology when suitably restated might cohere with holism of reason." Therefore, "holism itself provides no reason to suppose that context dependence cannot be codified in finite and useful terms." And in case a stronger version of holism is presented, i.e. "the context dependence of reasons cannot be specified in finite or helpful propositional way," Ross can argue that this in fact begs the question, as this version of holism is basically a negation of generalism, thus argues in a circle.
Therefore, "holism is compatible with the view that morality can and should be codified." But as we try to codify the circumstance in which a feature is always a pro tanto moral reason, we again make the reason too complex.

Dancy holds that the circumstance are essentially infinite, as we can always add one more feature which would switch the polarity, and therefore not codifiable.
All the examples given by particularism are essentially a prove of it.

Thus McKeever's argument ultimately fails.
The argument from enabling conditions can also have some possible replies.

Proponents of Ross' deontology can try to appeal to the idea that the reasons in question are not complex enough and that the conditions can also be incorporated in a reason.
However, this way of reasoning goes against common sense morality as no one would maintain that a part of the reason for helping someone is the fact that I can do so.

Conclusion
This paper started with elucidating both Ross' deontology theory as well as a brief statement of what particularists believe, i.e. in what way they differ from each other.

Three most important objections to Ross' deontology are explained in more detail and some examples are given on how they are supposed to work in practice.
In the main part, the essay evaluates the objections and replies made to them by Brad Hooker and McKeever and Ridge.

It seems that the argument from examples does not really refute the theory as also Hooker is able to produce counter examples to particularism.
The second argument from holism of reason is initially refuted however other examples Dancy produces make the point that pro tanto moral reason are holistic seem more plausible.

Also McKeever and Ridge 's argument that holism is consistent with codification of circumstances qua principles does not seem to hold.
Also the argument from enabling conditions shows that the circumstances are more complex than Ross originally assumed and that some circumstances, can only hardly be incorporated into a pro tanto reason ergo the pro tanto duty.

For the lack of space and time, this paper could not address some other objections particularist have made essay, e.g. the epistemic problem generalist have when trying to give a plausible account of the connection between natural properties and moral principles.
Also the question remains whether generalism and particularism also differs in practice, in overall judgement, from what particularist believe.

Whether a Ross' deontologist would at some point make a different moral decision from a particularist.
A possible answer would explore the fact that in Ross' deontology, what we have overall moral reason to do will always be some sort of function of these five pro-tanto moral reasons mentioned above.

It seems that Dancy makes a strong case for siding with particualrism rather than with Ross' deontology, as there are too many problems for a principle based approach to ethics to work.
Ultimately, particularism seems more plausible than generalism.

Introduction
This paper takes a closer look at the claim that semantic value of linguistic expression can be analyzed in terms of ideas in the mind.

In particular, it looks for a defensible interpretation of the central claim that meaning of an expression is an idea in the mind.
The paper proceeds in two parts where the first part gives a brief description of the basic interpretation in the form of Locke's theory of meaning, then discusses central objections to it and looks for solutions which would refine the interpretation and make it more defensible.

The central thesis is that Locke's claim is essentially faulty as it fails describe the concept of meaning in its complexity.
Any interpretation of the claim will fail as ideas are not sufficient to provide meaning for expressions.

Central objections to the claim address the referential problem of the theory for both words as well as sentences.
Further objections are based on understanding meaning as a public phenomenon, logical privacy of ideas and intentional content of expression.

Locke's theory - classic interpretation
To begin with, the claim "Meaning of an expression is an idea in the mind" needs some illumination.

There are several aspects which will be examined in this part.
Firstly, it is important to look at how ideas give meaning to particular words.

The classic interpretation of Locke is that words stand for or signify ideas and it is in virtue of this connection that ideas give meaning to words.
We use words as markers of the ideas we have and ideas in turn do the job of providing meaning.

Locke goes as far as arguing that "words in their primary or immediate signification stand for nothing but the ideas in the mind." (Locke) For example, if I say the word "red", I also have an idea of redness where the word "red" signifies this idea.
So there is an intimate relation of significance between words and ideas which serves as the basis for the claim in question.

Moreover, we need to look at how ideas can provide us with meaning in the first place.
Here it's necessary to look at Locke's theory of ideas where ideas are content bearing mental states.

Locke has been associated with the imagination theory where ideas are images or image like mental states, which arise from sensible experience and perceptions.
So we have the picture of image like mental states which stand as markers of particular words.

In this account, particular names like sunflower correspond to the image of a sunflower where this image is the meaning of the word.
Thirdly, we explain communication or use of words in terms of the analysis just provided.

Saying something is like expressing our thoughts which can be broken down to ideas, which in turn mark the words we expressed and thus give them meaning.
Others take words to be markers of ideas in our minds.

Here it's necessary to say something about privacy of ideas.
Ideas are mental states and by its very nature logically private and impossible to be discovered or known by anyone else.

As a consequence of this we can distinguish between speaker's meaning and hearer's meaning, where the former uses words and makes them stand for his own ideas and the latter also makes the words stand for his private ideas.
It follows that meaning on this account is a private phenomenon.

Objections to the claim and possible solutions
One of the objections to Locke's claim addresses the referential problem.

The connection or relationship of significance between words and ideas does not hold for all words.
While for many words we can think of an image corresponding to them, it is not true for all words.

Words like "as" and "then" have no mental images associated with them but we nonetheless considered them to be meaningful.
(Lycan 2000: 79) This undermines the claim that all words mark an idea; consequently words can have meaning without necessarily calling an image to the mind.

One way to reinterpret the central claim is by changing what we consider as the basic unit of communication (Lowe 1995).
While Locke takes words to be the units of communication, it would be easier to defend a view where sentences are the most basic units.

Words in turn will get their meaning in reference to their role in the structure of the sentence thus "as" will have its meaning in nature of its role in the sentence where it is used.
A defensible understanding of the claim thus holds that sentences alone signify ideas and individual words get their meaning in virtue of the sentence meaning as a whole.

Another problem with the Locke's claim is the existence of meaningful sentences that don't get their meaning in nature of an actual idea in anyone's mind.
We can think of several examples of very obscure sentences which have never been uttered before but still we would consider them to be meaningful.

For example, "Lenin and Stalin enjoyed wearing pink under flip-flops" is highly unlikely to have been uttered before still they had meaning before we uttered them (Lowe 1995) However, once uttered or thought of we also have an idea associated with them.
Still the argument can be made that there are sentences are meaningful despite lack of a corresponding idea in anyone's mind.

Therefore linguistic expressions don't necessarily require ideas to give them meaning.
One way to avoid this criticism is to take Locke's claim normatively (Lowe 1995) and claim that unuttered or not-thought-off sentences should not have meaning as there is none to understand them in the first place.

On this understanding, we could say that the sentences don't really matter because they have not been used in communication and therefore don't need to be understood or none needs to explain how they are to be understood in the first place.
However, as we are trying to give an account of meaning rather than understanding alone, this line of counterargument is not very persuasive.

It seems that there is no way to interpret the central claim to avoid this line of critique.
Furthermore, we can object to explaining a public and social phenomenon like meaning in terms of subjective entities like ideas.

We hold that words mean the same thing for "for entire community of the particular language speakers,"(Lycan 2000:79) but ideas are subjective and differ for every person.
Ideas are a reflection of mental framework and background.

Thus we can argue that subjective image like mental states /entities cannot account for sameness of meaning across a particular language community.
Therefore ideas won't do the job of providing meaning for expressions without ignoring the public aspect of the concept of meaning.

But when we talk about words as having the same meaning we don't necessarily mean that each language user shares the same image.
All Locke needs to keep the argument running is that the ideas we have are relevantly alike.

Surely, if we consider the idea of a pen, not every member of the community will have the same image of a pen.
What we are looking for here is sameness in concept, rather than sameness in Locke's imaginational ideas.

To avoid this line of criticism we complicate the account of ideas and allow for concepts rather than just percepts.
Concepts arise from reflection of the mind about ideas it has.

This way of understanding the central Lockian claim avoids objections based on meaning as a public phenomenon.
An additional central line of criticism addresses the privacy of thought conveyed by the theory.

Locke argues that "unless a man's word excite the same ideas in the hearer which he makes them to stand for in speaking, he does not speak intelligibly." (Locke) But how can we ever know that speaker's meaning is the same as hearer's meaning if we can only be in indirect contact with other people's ideas.
Locke's account of meaning only gives us access to words as sounds and our own ideas which we associate them with.

As a consequence, we can argue that we can never be sure that what we mean, the idea our words signify, is understood by the hearer.
Logical privacy of thoughts seems to undermine the prospects of successful communication.

One way to understand the theory and make it defensible to the last objection is by limiting the cases of meaning he talks about.
If we interpret the claim as only applying to speaker's meaning, ie cases where we only give account on how the speaker understand his words then it seems to hold.

But surely this is not enough and a theory this limited and unable to account for use of expression in communication looses a great deal of attractively.
Thus even if we can make a defensible interpretation of the claim it is not a very attractive one.

Another objection can be made once we consider proposition with intentional content.
Understanding of some expressions requires communication of intentional content.

(Ayer 1991) For example, 'desire to eat chocolate' cannot be accounted for only in terms of sensible images on our mind.
Quasi-physical images cannot hold intentional content of thoughts we try to express.

Thus ideas alone are not able to account for intentional part of meaning in communication.
There is little an ideational account of meaning can do to explain intentional content of our expressions.

Conclusion
In conclusion, several objections can be made to the classic interpretation of the claim that meaning of an expression is an idea in the mind.

There are defensible ways to understand the claim which avoid some of the objections.
In particular, we can take sentences as the basic units of communication which give meaning to words as some words lack the appropriate idea to mark.

However, it seems that not all sentences need to mark in idea in order to be meaningful.
Also it's possible to provide a defensible interpretation to the problem based on understanding meaning as a public phenomenon by making the theory of ideas more complicated.

However, the claim is ultimately faulty and impossible to defend once we consider logical privacy of ideas and the intentional content of expression.
Ideas are thus not able to give meaning to expression when we consider all aspects of the concept of meaning.

Introduction
This paper aims to establish what connection, if any at all, can be made between truth conditions (TC) and meaning of questions and commands (Q&C).

Consequently, we look at whether the connection is sufficient for us to maintain that knowing the truth conditions in question is sufficient for meaning.
The paper is divided into several parts.

A brief account of the Davidsonian Theory of meaning seems the best starting point as it aims to explaining meaning of all sentences in terms of truth conditions.
Furthermore, the theory provides a framework for the rest of the essay.

Secondly we examine what exactly is problematic when analysing meaning of Q&C in terms of TC.
Consequently, several solutions trying to show how a truth conditional theory of meaning can explain meaning of questions and commands are suggested.

We start by looking at ways to interpret Q&C in order to make the TC account applicable.
Secondly, we look at the truth-like bi-polar values of Q&C and lastly, we try to use the concept of inference to deal with semantic multi-polarity of possible answers to Questions.

The thesis of this paper is that it seems common sensual that assertions made by Q&C are not assessable in terms of evidence or truth conditions and therefore wither need to be interpreted or we need to rely on a different tool to make sense of them.
Ultimately TC account is problematic.

Davidson's theory
This section give an account of the Davidsonian project (Davidson 1979) as it argues that "knowing the truth conditions is knowing the meaning." It is a good starting point as it aims to deal with all sentence, Q&C included.

The broad idea of the theory is that we can explain meaning of language by accounting for meaning of individual sentences.
Meaning of individual sentence is explained in term of their composition or semantic structures.

Individual words only have meaning as parts or compositions of a sentence.
For Davidson, "providing a theory of meaning for a language is thus a matter of developing a theory that will enable us to generate, for every actual and potential sentence of the language in question, a theorem that specifies what each sentence means." An important distinction is between object language (OL) and meta-language (ML).

The language under discussion, i.e. language which is analyzed is called the object language and the language in terms of which we provide the analysis and meaning of the object language is called meta-language.
Davidson's theory provides us with a theorem for each sentence in the natural object language with a sentence in the meta-language which accounts for meaning of OL sentences.

This approach does not match meaning as an entity with expressions in questions, but rather the "theorems will relate sentences to other sentences." In this sense, the latter spell out the meaning of the former.
The theorem has the following form: "s is true if and only if p" where s is given in the object language while p is supposed to spell out truth conditions of s. D's theorems match OL sentences with what would serve as evidence for a proposition being true.

If the evidence obtains than it entails the proposition in question as true.
This brings us to D's reliance on Tarski's theory for analysis of individual sentence for extraction of TC and accounting for compositionality of sentences.

As sentence meaning is a function of its semantic structures, Tarski's theory of truth provides the necessary framework to deal with composition of natural language in order to identify truth conditions of individual sentences.
In this sense, D's theory of meaning directly relies on a theory of truth for an account of truth conditions.

The problem with questions and commands
This section gives a brief explanation why the TC approach to meaning fails to make sense of Q&C.

The theory relies on a general feature of language and declarative sentences in particular.
All declarative sentences make a statement which asserts a proposition describing certain state of affairs.

We can analyze the sentence to determine what has to obtain for the proposition to be true in other words its truth conditions.
However, this approach seems to fail when applied to interrogative and imperative sentences as they don't identify the eveidence which would make them true.

The former request information and the latter make a demand.
Since questions ask for information about a certain state of affairs we can't exactly ask what would have to obtain in order for that request to be true.

Commands also pose a problem for the theory as they demand that certain state of affairs obtains, which again fails to describe how the state of affairs is.
It seems counter-intuitive to ask for truth value of Q&C assertions and hence Tarski's theory of truth doesn't help in analyzing their truth conditions.

Possible solutions and comebacks - discussion
The rest of the paper discusses possible solutions to the problem of the theory.

One way of thinking about this problem is trying to interpret Q&C into declarative sentences which would be descriptive and fact stating.
Davidson had dealt with ambiguous sentences by providing an interpretation for every possible meaning of the sentence and then making a theorem for each of this options.

(Evnine 1991) By giving meaning of each possibility, he provided an account of meaning for ambiguous sentences based on truth conditions.
If we interpret Q&C in a sensible way we should be able to use the TC approach to determine their meaning where the meaning of the Q&C in question would be a function of the interpretation.

For example, "Is the sky blue?" can be interpreted as "The speaker of the sentence wants to know whether the sky is blue." This would lead to the following theorem: "Is the sky blue?" is true iff the speaker of the sentence wants to know whether the sky is blue.
"The speaker of the sentence wants to know whether the sky is blue" is a fact stating declarative sentence which can be understood in terms of the TC approach.

It also spells out meaning of the question.
An example including a command (""Go home!" is true iff the speaker ordered me to go home.") also seems to work as the interpretation/translation approach seems to provide us with translations of Q&C which in turn can be analyzed using the D's theorems.

The problem now seems to be that we need to base our interpretation on something.
A correct translation or interpretation is a translation which is meaning preserving.

(Miller 1998) But if we want to maintain that the translation is correct we need to rely on something to guarantee this.
Here we could rely on compositionality again but we still need to make sense of the 'what, when, where etc'.

To make a correct translation we need to rely on understanding of the meaning of Q&C in the first place.
But we have no way of justifying that our understanding is correct.

Thus more is needed to show that a truth conditional theory of meaning can make sense of Q&C.
Lycan proposes a different way to deal with the problem.

(Lycan 2000) One of his arguments suggests that at least some Q&C have bi-polar truth-like semantic values as they describe a certain state of affairs which may or may not be true.
Q&C have bipolar values in the sense that questions can be answered by "yes" or "no" and commands can be obeyed or ignored.

Thus they can have positive or negative semantic value.
These can be used in a semantic analysis as truth-values.

Consequently, we can analyze Q&C with bi-polar truth-like values in terms of the TC theory of meaning.
However, this solution does not apply to all types of questions as not all of them have bipolar truth-like values.

Questions like 'what, where, when etc' can be answered by a range of answers and thus take a number of different semantic values.
Multi-polar questions however cannot be analyzed in the same way as questions with bipolar semantic value.

Thus we now need a way to deal with multi-polar questions.
Still it seems that commands at least can be dealt in this way as commands can only be obeyed or ignored and therefore can be associated with a bipolar semantic value.

One way to deal with multi-polarity of sentences is to accept that there are several possible semantic values a question can take and try to rely on inference to solve the problem.
We still rely on Tarski's approach to make sense of fact-stating declarative sentences, but we make sense of question by inference based on the possible set of answers.

The meaning of a question and consequently understanding it would thus be a matter of knowing the possible answers to it.
This does not require knowing the correct answer.

If someone asks us for time we normally have to check our clock to give him the correct answer.
We infer meaning of the question as we know the possible answers to it and understand these based on the TC approach.

This avoids the multi-polarity problem but only at the cost of indirectly relying on TC approach and introducing the notion of inference which clearly needs a detailed account.
However, we still need a tool to identify the correct sets of possible answers so that we can infer the meaning of the question itself.

It does not really solve the problem as it is not clear how in particular are to infer from knowing about the set of answer that the question asks for one particular of them.
Conclusion

As I have shown in the first two sections, the traditional TC approach associated with Davidson fails to answer the question of how is that questions and commands have meaning?
The ultimate problem is that assertions associated with Q&C cannot be assessed in term of truth conditions.

Essentially the problem can be associated with the inability of formal theories like Tarski's truth theory to deal with certain aspects of natural language.
The suggested solutions looked at ways to interpret Q&C in order to make the TC account applicable.

But even though we can think of translation in the form of fact-stating declarative sentences, we can only base the interpretation on our previous understanding of the question.
But this would be circular.

Secondly, an analysis based on semantic bipolarity associated with Q&C had a degree of success with commands, but not all questions can be answered in terms of "yes" or "no".
And a different strand of solution relied on using the concept of inference to deal with semantic multi-polarity of possible answers to Questions.

Knowing the possible set of answers toa question seems to be a good enough inferential basis to understand the question.
It is however not clear how a question identifies this set in the first place.

An approach based on formal theories of truth try to deal with natural language in a too simplistic way, which ignores that not all of its features are so easily explicable.
In this sense, utility of truth-conditional approach is limited to dealing with fact-stating declarative sentences and possibly commands.

Introduction
This paper looks at the aspects of diplomatic methods of Russian foreign policy in the time period following the end of the Cold war.

The question has two parts and this essay rather than answering it in two parts will address highlight the patterns in the diplomatic methods and try to look at the ways in which they reflect the domestic interests.
The first part illuminates the concept of diplomatic method which is followed by an examination of Russian diplomatic methods and of their motivation, especially in relation to domestic interests.

I look at the role of coercion, both as a verbal threat and backed by economic tools, bilateralism in and diversification of diplomatic relations, role of personal diplomacy, cooperation with USA and EU and economic relations with a group of key countries.
The paper argues that Russian leadership does in a number of areas take the domestic political and economic interests into account or at least the policies are inspired by domestic context, however there are a number of other important factors and interests reflected in the diplomatic activity of Kremlin which come top on the agenda.

The concept of 'Diplomatic methods'
By 'the diplomatic methods' this paper understand the way in which a countries decides to use the tools it has, exerts its influence and implements the foreign policy goals.

Diplomatic methods are varied and reflect much of countries available resources, its international environment as well as general FP style.
An examination of the methods a country uses in its FP would look at how the country deals with other states and specific issues which confront it.

Essentially an analysis of diplomatic methods looks at the 'how?' of foreign policy.
To reflect on whether diplomatic methods are drawn from the consideration of domestic interests, it is necessary to looks at whether the main motivation to use a specific method is based on consideration of domestic context and interests and whether the consequences of using a particular method are directed at a particular domestic actor or to deal with a domestic issue.

Russian diplomatic methods and motivation
Use of economic diplomacy is one of the major patterns in FP of Russian Federation.

The Russian administrative has used the economic tools available in a number of ways and with various motivations.
Relying both on positive and negative effects of the diplomatic method.

Russian has been engaged in active economic cooperation with the EU, and particularly its Western members.
It has used a number of economic diplomatic methods to facilitate a greater level of cooperation with the EU.

Bobo Lo (2003) argues that Russia tries to improve its position in the European affairs in two ways.
It tries to developing strong economic ties in the energy industry, particularly the gas exports and increasing the trade volume with EU.

The Baltic gas pipeline project 'Nord Stream' is set to bypass the whole CEEC block illustrates both Russian efforts to increase its share in energy market of EU and undermine the position of the Eastern countries in relations with Kremlin.
The move with increase the reliance of EU on Russian energy resources strengthen the latter's position in negotiations with Brussels.

Increased importance of Partnership and Cooperation Agreement in Russian FP also demonstrates the significance of economic dimension of EU-Russia relations.
Here the motivation for using economic tools to facilitate greater level of cooperation with Europe is pragmatic and power interested.

Economic cooperation with Europe provides benefits for the Russian economy, namely market access which leads to higher levels of trade and attracts Foreign Direct Investment, and thus helps to increase the living standards in Russia.
Europe is Russia's major business partner in energy commodities trade.

"Energy exports, together with Russia's other extensive resource deposits, would be the foundation of its national revival.
They would provide funds to renew its infrastructure, diversify its economy and develop technologies." (Dmitry Medvedev quoted by FT, 5th July, 2006) As Russia becomes a major economic player in Europe, especially as a major energy supplier, its gains more influence over the individual states and their economies and consequently a greater role in Europe as a whole.

Another aspect of Russian diplomatic methods are the economic ties with countries like Iran, Iraq, Syria, Eritrea, and Sudan.
Barston (2006) Russia has been a major supplier of arms and nuclear technology to these countries, where its activity in the Middle East is particularly interesting.

Russia established commercial relations with Iran and previously Iraq, where it supplied the rogue regimes with weapons and nuclear technology in spite of criticism and negative reactions from its Western allies.
However, the commercial relations have their limits, mostly set by American tolerance levels.

(Bobo, 2003) By establishing commercial relations, Russia has also improved its diplomatic ties with these countries in general.
The motivation for the economic relations reflects both domestic interests as well as a general Russian interest in increasing its influence on International affairs or at least stop its current decline.

"By posing as the sole major power to take seriously (at least in public) Iran's protestations that its nuclear programme is innocent, Russia has gained both commercially and diplomatically." (The Economist; 2/10/2007) Kremlin has been able to sell arms and nuclear technology and thereby financing related domestic industries which would otherwise have it hard to find customers.
Bobo Lo (2003) argues that commercial interests assume primacy in the economic relations and consequently Russia has tried to depoliticize this issue by following money rather than political interests.

Moreover, commercial success in these countries is likely to attract more potential buyers.
Moreover, Russia's economic relations in Middle East have enabled Kremlin to increase its diplomatic influence in the region.

"As for Syria and Iran, Mr Putin's policies have a logic, up to a point.
With Iraq under new management, Syria is Russia's only real foothold to hoist itself into a seat at future Middle East negotiations.

Iran offers Russia the chance to do what China does over North Korea: play up its own diplomatic influence by offering to broker a deal." (Economist; 12/3/2005) Trade with and technology transfer from Kremlin create a level of dependence on Russia which gives the latter more diplomatic influence.
The increased importance of Russia as a consequence of its relations with Iran has been noted by the Economist (The Economist; 2/10/2007) following the Russian vote in favour of a UN Security Council resolution to sanction Iran.

"Russia would like to think that the recent slight softening of Iran's public tone and the rising domestic criticism of its president, Mahmoud Ahmadinejad, may owe something to fears of losing its only legitimate outside source of nuclear technology." (The Economist; 2/10/2007) Another general feature of Russian diplomacy is the reliance on coercion.
Here we can distinguish between verbal statements and speeches, like Putin's 2007 speech in Munich, use of force or a direct threat of its use and use of economic tools.

While Russia is well aware of the limits on its military force capabilities, as illustrated by a number of statements on state of Russian army, Putin still considers at least use of force a tool of the FP.
In the conflict with Georgia, where its territory has been used as a safe haven by the Chechen rebels, Russia has threatened to use force in order to convince the Georgian government does not deal with the rebels.

In addition, Russian reactions to US decision to develop the ABM system in Europe and establish military bases in Poland and Czech republic have been marked by a an aggressive and even threatening tone.
"In a speech that stunned most of the audience at an annual security conference held in Munich, Mr Putin also railed against US plans to build anti-missile defences in Europe, the expansion of NATO to include countries that were formerly part of the Soviet Union, and a host of other western policies." (Fidler & Sevastopulo, 2007) Poland and Czech Republic's representatives talked of Russian "blackmail." Coercion using economic tools is a persistent method of Russian diplomacy when dealing with countries from the former Soviet sphere of influence, namely the countries in Commonwealth of Independent States (CIS) and Central and Eastern European Countries (CEECs).

Using economic tools like trade and energy prices, it tries to incur significant economic costs and thereby muscle compliance in the target country.
Russian relations with Georgia, Ukraine and Poland illustrate well this diplomatic method.

"Last autumn [2006], the Russians imposed postal and aviation blockades, alongside the existing embargoes on Georgia's water, wine and fruit.
Then, with winter approaching, they doubled the price for Russian gas--in theory for commercial reasons, but with the real aim of taming Mr Saakashvili." (Economist; 2/10/2007) By isolating a country economically and affecting its oil supplies, Putin hopes to force the Georgian president to listen more closely to Kremlin.

In the Ukrainian case, Russia in 2007 has dramatically increased the energy prices and even resorted to a complete stop of supplies following the 'Orange revolution' which lead to a turn in Ukrainian FP orientation including the call for EU and NATO membership.
In the conflict with Poland, Russia has introduced a ban on imports of Polish meat, as one way to have a negative impact on Polish economy.

Russia has in a number of cases used its energy resources, particularly EU's dependence on Russian gas, and trade embargoes and their threat to influence countries FP.
Extensive use of coercive methods by Russian diplomacy reflects both a lack of alternatives and a continuation of the Soviet style.

Russia lacks a 'soft power' infrastructure to support its diplomatic efforts and thus cannot rely on appeal of its ideas and ideology in general.
Moreover, with Russian economic weakness, the country is unlikely to be able to offer significant economic benefits to affect others' FP.

While lower gas prices are useful bargaining tools, countries like Georgia and Ukraine show that economic interests do not always come top on the FP agenda.
As the military capability and utility of use of force declined, Russia has tried to rely on economic tools to back coercive diplomacy.

Russia is a major energy supplier to much of the Soviet empire and increasingly also EU, which provides it with more power to influence states economy by affecting supply and prices of gas and oil.
Coercive methods are also partly a continuation of Soviet diplomatic style where Russia's diplomats draw a considerable part of their experience and expertise from.

Russian diplomacy also heavily relies on personal diplomacy.
The past events clearly demonstrated the importance of the head of the state in policy making and diplomatic negotiations.

While Yeltsin's years were more chaotic and a number of actors have been engaged in Russian foreign policy making at the cost of its coherence and unity, recently it seems more appropriate to talk of Putin's foreign policy rather than a Russian one.
The president takes an active role in major diplomatic events, like the EU- Russia meetings and negotiations, and his statements, particularly the recent speech from February 2007 in Munich, highlights his importance in shaping the tone and direction of Russian FP.

Barston (2006:83) also mentions the increased importance of special presidential envoys.
This development demonstrates the importance of the Foreign policy in Putin's agenda and Russian affairs.

Personal diplomacy is a persistent feature of Russian FP with president's statements and visits as an important method to communicate Russian intentions.
The role of the Putin in Russian diplomatic activity reflects the importance of FP agenda in domestic affairs.

It could be argued the managed democracy regime existing in Russia on the one hand gives him almost complete power over the FP, but on the other hand makes him personally responsible for the successes and failures.
In this sense, personal diplomacy reflects more the importance of FP in domestic politics rather than the other way around.

Russia has in the post-2000 period developed bilateral diplomatic relations with a number of countries.
Barston (2006) identifies key group of countries, France, Germany, India and China, where Russian diplomacy relied on bilateralism to improve the existing relation.

High-profile visits by key officials and Putin were an important part in building up the relationships which were later institutionalized in treaties or other agreements.
"Russian Federation-Chinese relations were upgrade to a strategic level, with the Russian-Chinese Treaty of Cooperation and Joint Statement and similar arrangements concluded with India." Russian state energy-giant Gazprom and major German companies (E-ON and BASF) have been working on an important joint energy project, namely 'Nord Stream', which links the two states more closely together.

Barston (2006) has highlighted the role of cultural diplomacy in developing the bilateral relations, particularly the German-Russian cultural programme.
Russian efforts to establish a broader and deeper set of bilateral relations with key countries reflects both the pragmatic interests grounded in domestic economy as well as the significance of its diplomatic position.

"Though it makes sense for a country with as many neighbours as Russia has to be on good terms with as many as possible, China included, Mr Putin knows that, if Russia is to grow strong again, it needs to link its future with the world's rich democracies--the folk around the G8 table--not the world's few surviving dictators." (Economist; 07/22/2000) Diversification of relations and allies can also reflect Russian interest in improving its diplomatic power by limiting its reliance on the West.
"Russian officials warn westerners, with varying degrees of bad temper, that they have plenty of other friends--China, India, Iran, Iraq, former Yugoslavia--if NATO and the European Union again overlook Russia's interests in places like Kosovo and Eastern Europe." (Economist; 07/22/2000) The role of soft power, or public diplomacy, in Russian diplomatic methods has been limited but the recent shifts signal the growing importance of this area.

Soft power has been defined by Joseph Nye as the "ability to get what you want through attraction rather than through coercion" and which can "be cultivated through relations with allies, economic assistance, and cultural exchanges." (Nye, 2004) Russia has provided economic assistance to its closest allies in the CIS in exchange for their support of Russian FP.
In particular this has mainly taken form of low energy imports prices, at least in comparison to the consumer sin the West.

By providing economic benefits, Russia was able to maintain the pro-Kremlin orientation and general compliance with its policies in these countries.
The 'carrots' have been withdrawn once the countries changed their political orientation, e.g. the case of Georgia and Ukraine after the 'Colour revolutions'.

"As a result of the 'colour revolutions' in Ukraine and Georgia, Russia's leaders learned that crude manipulation might not be enough to remain in power, that ideas matter and that NGOs can make revolutions." (Popescu, 2006:2) This negative experience has led Popescu (2006) to argue that Russia has recognized the importance of soft power dimension in diplomacy and the methods associated.
"Russia-friendly and Russia-financed NGOs and think-tanks have emerged in many CIS states and even in the secessionist entities." (Popescu, 2006:2) Russians so far have a clear deficit in appeal of its ideas and Putin's doctrine of 'managed' or 'sovereign' democracy attracted a considerable critique.

However, there is a clear perception and the need to address this in the Russian administrative.
In Ivan Krastev words, the major objective of the Russian policy "is to develop an efficient infrastructure of ideas, institutions, networks and media outlets that can use the predictable crisis of the current orange-type regimes to regain influence not simply at the level of government but at the level of society as well." (Ivan Krastev in Popescu, 2006:2) As already mentioned above, Russia also engaged in cultural exchange programs with Germany (Barston, 2006:84) to facilitate a change in its image.

Russia as a part of interests abroad has been trying to influence the political process in the CIS countries in a number of ways.
Personal statements regarding the preferred candidate and visits to the country preceding elections have market Kremlin's involvement in the elections campaigns and politics in the CIS countries.

"It may now be called the Commonwealth of Independent States (CIS), but their independence goes only so far.
The Kremlin sends advisers to help its preferred candidates with election campaigns." (Economist; 5/22/2004) Particular example are the Ukrainian 2004 elections where Putin has made statements clearly supporting Russian-friendly Viktor Yanukovych.

Russia by interfering in the elections has tried to shape domestic politics of the CIS countries in order to create more Russian oriented governments and increase its influence over the countrie's FP.
One of the major patterns in Russian foreign policy is cooperation with the USA following the 9/11 attacks.

Putin's diplomatic efforts to bring Russia closer to US have relied on assistance in issues related to terrorism and diplomatic support or at least non-opposition on issues vital to USA.
"Putin declared that Russia will assist the US anti-terrorism campaign in the following way: by providing information about terrorist bases, allowing use of Russian airspace for humanitarian flights in areas of anti-terrorist operations, accepting US use of Central Asia airfields for antiterrorist operations, and by supporting the Northern Alliance in Afghanistan." (Buszynski, 2003:20) Russia has avoided major conflicts on issues which are vital to USA.

(MAcFarlane 2006:42) This was particularly visible in the run up to the Iraq war of 2003, RF did not want to state its position on future US actions and let other Security Council members lead the anti-war campaign.
Yet the partnership has its limits as Russia competes with US on issues which are vital to it but USA considers less important.

Putin's recent speech in Munich seems to be reconsidering the success of the US 'war on terror' and Russian support for US unilateralism in general.
Russian conception of world order is an important motivation for the diplomatic efforts.

While Yieltsin tried to balance the influence of USA and its NATO allies, Putin has put Russian in a closer relationship with the West and has cooperated with US in areas vital to the latter's interest.
As a consequence of Russian amiability, USA has supported Russian membership of the G8 which improves Kremlin's international standing.

As for the domestic interests, Bobo Lo (2003) notes that cooperation with USA attracted major opposition at home, especially in the legislative and military.
While certain are domestic groups that oppose alignment and cooperation with West, Putin has acted this way in the context of Russia's military, economic and diplomatic decline.

Russian economy has shrunk to half of its size following the break up of the Soviet empire.
(Bobo 2003) The decline makes opposition to and competition with US too costly and unlikely too succeed.

Moreover, "Russia's orientation to the West satisfies a deep longing among democrats and liberals, but it also serves the practical purpose of deflecting Western criticism of Russia's campaign in Chechnya.
(Buszynski, 2003:22) USA has in the aftermath of 9/11 and Russia cooperation minimized criticism of Russian campaign in the rebel province.

The Chechen war is a domestically a highly salient as it has been one of the most important issues Putin was elected on in 2000.
Conclusion:

This paper has examined various aspects of Russian diplomacy and discussed the motivation behind its diplomatic methods.
Both domestic context and associated interests, and external environment affect Russian methods.

On the one hand, domestic limits what methods are available, as Russia lacks of soft power dimension and had limited success with carrot approach when trying to influence countries' FP.
On the other hand, the utility of energy resources increases as Russia becomes a major exporter of gas to Europe.

Relationship with EU follows both domestic economic interest and Russian effort to increase its influence.
Economic ties with other countries, particularly the axis-of-evil members, follow profit interests as well as it serves to increase Russian influence in the region.

The importance of coercion with CIS and CEECs countries can be explained by lack of viable alternatives and large level of dependence on the Russia for energy imports.
Russia has tried to influence the domestic political process in some of the CIS countries, as Russia tries to keep its neighbours in line with Russian Foreign policy preferences.

Putin has been able to secure virtual autonomy in FP decision making process which puts him in a better position to get away with ignoring the domestic interest.
This has been also reflected by the role of personal diplomacy.

Russian reliance on bilateralism to extend and deepen its relations with key players and emerging powers in International relations reflects Russian interest in developing its diplomatic influence.
Having a more diverse set of relations can also make it less reliant on particular countries and improve its bargaining position.

The relationship with USA again reflects domestic weakness, but is also motivated by the benefits USA has to offer.
On top of that it makes the West less critical about Russian record in Chechnya and human rights record.

It seems that Kremlin's main objective is to enhance Russian diplomatic influence and power or at least stop the rapid decline.
Domestic interests are reflected in the fact that Putin has recognized that Russia can only be a major player if it has a strong economy.

Moreover, economic growth and rising living standards have an important influence on domestic stability and people's support for Putin policies at home and abroad.
(Bobo 2003) In this sense, economic pragmatism in Russia FP reflects both domestic economic interests and interest to increase Russian international diplomatic role.

Moreover, there are good reasons for Russia to develop a soft power infrastructure which serve its' public diplomacy in future activities.
Introduction

This paper analyzes the Representative Theory of Perception.
It starts by giving an account of a general form of the theory and then proceeds with objections to representativism.

In particular, I consider the sceptical 'veil of perception' objection, which holds that representativism undermines its intelligibility and that it collapses into phenomenalism.
Another objection denies that R represents a common sense view of perception and thus is less attractive as has been thought of.

The connection representativism has made between sense data and the physical world is also judged as false.
And the last objection refutes the idea that sense data alone can explain all perceptual experience.

The objections are also evaluated in light of possible R replies.
In the end, I try to make some conclusion about the plausibility of the theory on the whole.

The main thesis of the essay is that even though the Representative Theory of Perception has serious weaknesses, it is not the sceptical objections that pose the biggest problems to it.
Representative theory of perception explained

To begin with, it is necessary to give an account of the representative theory of perception (R).
Its' basic and a very simple thesis is that what we see resembles the world, as it is.

This seems to be quite a common sense approach to explain/identify/analyze our perceptual experience.
This is also one of the attractive features of the theory.

R could be called the "picture-original theory of sense-perception" (Mackie 1976:37) which implies that we get to know how things look "out there" (meaning how things look outside our mind) by looking at so called pictures of the physical objects.
The name of the pictures varies with authors, e.g. Locke calls them ideas or precepts.

I am going to use the term sense-data (SD).
The idea of sense data was introduced by the Argument from Illusion which aimed to prove that all we ever have direct access to are the sense data.

In other words, the mind can attend to the SD only.
To understand this better we need to look at the R metaphysics of perception.

R works with three distinct parts or concepts.
Firstly we have the world of physical objects where each of them has their own individual qualities (E.g. Tomato has the quality of being red and round).

Secondly we have the perceiver or the mind, i.e. us, with our sense-perceptual experience to account for.
R adds the sense data to this picture.

They act as intermediaries between our mind and the external world.
In R account they are truthful picture of physical world and their qualities.

For instance, if we see a tomato, in reality we are directly aware of a sense data which has the qualities of a tomato, i.e. redness and roundness.
In this sense SD have the qualities of the physical objects, which are being perceived (red object -> red sense datum).

An important feature of R is that it is an indirect theory of perception.
Since all we all directly aware of / perceive are Sense data, the external world stays hidden behind a "veil of perception" and consequently all information we have about the physical objects and external world are indirect and mediated by SD.

SD act as intermediaries which stand between us and the perceived.
We become directly aware of the nature of objects only through the SD.

The existence of intermediaries between us and the perceived has been established by the Argument from illusion, which R relies on.
This invites the question about the nature of sense data.

At this point, two possibilities, or version of them, are viable.
They can be "physical real pictures somewhere inside one's head" (Mackie 1976:42) or "mind dependent in the sense that they exist in and by one's awareness of them." (Mackie 1976:42) Both of these have implication for plausibility of the theory as a whole, which I am going to explorer in the next section.

Objections evaluated:
The sceptical 'veil of perception' objection poses two problems for R.

This objection is mainly based on the idea that there is a veil or sense data between us and the reality, which has implications for overall coherence of the theory.
The first problem concerns the meaning of assertions about the physical world and heavily relies on verificationism.

Since all that the R picture grants us a "direct acquaintance with ideas" (Mackie 1976:55), A. Quinton has argued that this destroys the intelligibility of expression about the nature of physical objects (Jackson 1977:138).
Verificationism "requires that a (non-analytic) meaningful sentence be either verifiable or falsifiable." But if we never get into direct contact with physical objects, then there is no way to verify or confirm their truthfulness.

Thus one can argue that R makes it impossible to meaningfully express our speculations about the physical world.
(Jackson 1977:138) It seems obvious that a reply to the problem of meaning depends on one's theory of meaning.

If we accept the verificationsit thesis, which is by no means unproblematic, then the solution to the problem would depend on whether we can prove or at least justify that the external world exists in the first place and consequently whether our statements are thus verifiable.
This issue will be covered shortly.

As for V itself, one of it's' problems is that it is self-refuting.
If we accept its thesis then there is in practice no way to verify whether it is true and thus the thesis itself is meaningless.

Thus objections based on an unsound theory of meaning are bound to have the same problems.
If we hold a non-verification doctrine, e.g. empiricist and natural constructive theory of meaning, then the limits on what is meaningful don't exclude speculations about physical objects.(Mackie 1976:58; Mackie 1985:220) As Mackie and Jackson argued, once we drop the verificationist criterion, which is not an essential part of the R theory, then this objection is refuted and R does not destroy "the conditions of its own intelligibility"(Jackson 1977:138) by granting only indirect access to the external world and physical objects.

The second problem concerns the justification of R project.
Sceptics argue that R is not justified in claiming that the physical objects in the external world really resemble the picture which we get from the sense data in our mind.

In particular, S attack R leap from SD to certainty in claims about the indirectly perceived physical world.
Surely, just because we can and normally do make direct judgements about the external world, this does not mean that the picture we get truthfully resembles the original.

If all we ever directly perceive are SD (as proved by argument from illusion), "how shall the mind know that they agree with things themselves?"(Mackie 1976:39) The R theory is thus vulnerable to a total scepticism, as we cannot be even sure whether the world which is supposed to stand behind the SD exists and whether "sensory states ever represent the objective realm?" (Mackie 1976:52) If this objection succeeds, R might be forced into phenomenalism as reality becomes "a logical construction out of appearances." (Mackie 1976:51) One way R have tried to respond is by incorporating a Causal Theory of Perception into R. Locke asserted that the only possible explanation for our direct awareness or perception of SD is a causal relationship between them and an external world.
They "must necessarily be the product of things operating on the mind." (Mackie 1976:39) Thus a world which conforms to the picture of SD is necessary to make sense of our awareness of the SD.

This refutes a total scepticism argument, as we have established the existence of the originals for our pictures.
But Mackie argued that even if this is true, it still leaves space for a limited form of scepticism.

"Causal immediateness introduces real possibility of systematic differences between appearance and reality." (Mackie 1976:44) Therefore, SD still might not resemble how things really are.
Another way to respond to the sceptic objection is by appealing to "inference to the best explanation".

The best explanation, that is, the one most likely to be correct, is that "the perceptual experiences are caused by and, with certain qualifications, systematically reflect the character of a world of genuinely independent material objects, which we accordingly have good reasons for believing to exist." There are two features which point to the sense-data model.
Firstly, "its involuntary character, i.e., the fact that it simply occurs without any choice or control on the part of the person having the experience," points towards causality of perceptual processes, which further supports Locke.

Another feature is the "systematic order or coherence of that experience." While other coherent formulations of explanations of the details of our experiences are available (e.g. Descartes evil demon and God hypothesis (see Mackie 1976:144)), "no such rival hypothesis yield nearly so good an explanation of all the details of our experience." (Mackie 1985:220) Also, as Mackie has argued, no one has been able to back up the claim "by actually giving in detail an alternative hypothesis." (Mackie 1985:144) Our common sense also seems to embody a realist view.
(Dancy 1988:93) Additional objection to the theory, and to the best explanation reply in particular, disagrees with the idea that R represents a common sense view of perception.

The main appeal of R is in the idea that the physical world resembles what we normally perceive.
This is an ordinary account of perception, which many take to be true by common sense.

However the second feature of R theory, the SD, is not what we normally consider when talking about perception.
The idea of perceptual intermediaries and an indirect theory of perception, both essential parts of R, seem to be quite alien to the way we normally understand our perceptions.

In everyday language, watching television is understood as a form of indirect perception, but one of the reasons why seeing [something alive] is believing, is because we take this perceptual experience to be direct, while R don't.
Therefore R cannot be taken to represent a common sense approach.

While this does not necessarily undermine the theory, it can make it less attractive as the "best explanation" available and therefore make the appeal to "inference from best explanation" argument less plausible.
Another objection denies that Sense data are able to resemble the physical world, i.e. have the characteristic of the physical objects.

Armstrong has argued that "any characteristic that sense impression have is a characteristic that physical object cannot have." (Mackie 1985:139) His argument was based on a false premise that physical objects and therefore also their characteristics/qualities cannot be directly perceived.
I want to argue that the conclusion of his objection holds but for different reasons than he suggested.

I would like to do so by looking at two possible ways the SD can be understood/described, either as physical causal intermediaries or as mind-dependent entities, and then showing that both of these accounts are ultimately incoherent, which undermines the whole R theory.
The first possibility of SD, as "physical real pictures somewhere inside one's head" (Mackie 1976:42), is easy to refute.

An example of causal intermediaries are retinas.
The problem arises when we consider that R requires that we are directly aware of the SD, and thus perceive the qualities directly.

On this account, when I have a perceptual experience of a square object, my SD is also square.
But this seems to be impossible in the case of retinas.

"My retinas won't be square, since retinas are always curved" (Mackie 1976).
The causal intermediaries will not do as SD because they just don't have the right sort of properties.

(Mackie 1976:47) Thus it is incoherent to assert that SD are physical intermediaries.
The second account of SD, which takes them to be "mind dependent in the sense that they exist in and by one's awareness of them," (Mackie 1976:42) also runs into problems.

In this version we don't see objects, but only our ideas of them.
R requires the SD to have the same kind of qualities as the objects they are supposed to resemble/represent.

But it is hard to conceive that mind dependent entities can have qualities of physical a kind.
For instance, if I see a tomato, R would require that I have a round and red idea.

This account of SD is thus incoherent and as R fails to produce a plausible account of SD, which would also suit the R theory itself, their whole project seems to be problematic.
Additional objection to the theory refutes the idea that sense data alone can explain all perceptual experience.

As Hanson argued, there are examples where two persons can be said to have same/different sense data and different/same visual experience.
For instance, two persons can perceive a rabbit-duck figure and one judge it to be a rabbit while the other a duck.

Thus "it is meaningful to speak of situations in which two observers having indistinguishable visual sense data nonetheless have disparate visual experience." (Hanson 1972:188) It seems that there is more to our perceptual experience than SD only.
But this refutes the picture R tries to make, as we can show that "perceptual experiences can't be captured in terms of sense data."(Hanson 1972) The objection proves that there is a degree of relativity or subjectivity about the kind of perceptual experience we get from the SD, which is ignored by R.

It can be argued that this is a direct consequence of our interpretative power, i.e. the power of the mind to shape the content of our experiences.
It has to be noted that even thought this undermines the representative theory; it does not lead to total scepticism, as all we are able to deny is that SD alone can do the job R wants them to.

Even if argument from illusion is true for perceptual experience, SD can still exist.
This project invites us to a discussion of more relativist account of perception, where the resulting theory would keep SD, which would truthfully resemble the physical objects qualities.

The new theory might allow relativism in interpretation of the qualities.
Not in the sense that we the qualities of the SD can be individually interpreted differently but rather that the perceiver can make different conclusion from the sum of the qualities he perceives, i.e. different conclusion about what we perceive on the whole.

Such a theory would allow same/different sense data and different/same visual experience.
(This might also help to make sense of why crazy people "see" things differently.) Thus it seems that a more sophisticated version of R could cope with the problem, but ultimately it would depend on whether we can make a coherent relativistic version of representativism.

Conclusion:
In order to evaluate the theory I have considered several objections to the theory.

Each takes a different approach in trying to refute the Representative Theory of Perception.
Firstly, I have tried to show that the 'veil of perception' objection fails in both of its attack.

It does not show that R cannot meaningfully speculate about physical object and also there are good reasons which R assertions/speculations about the physical world despite that we are only in indirect contact with them.
Yet the theory is not as simple and common-sensual as some of R promoters have argued.

This is mainly due to the complications caused by the SD.
Sense data are the main weakness of the theory for two reasons.

They are a very problematic concept as neither their physical nor mind-dependent version evades incoherence.
In addition, SD alone are not enough to capture and explain the perceptual experiences.

This is proved by the 'same/different sense data and different/same visual experience' example.
While the latter might be solved by a more sophisticated version of R, which would incorporate relativism to account for interpretations, the former, i.e. the concept of SD, undermines the whole project as an essential part of the theory is incoherent.

Introduction
This paper essentially analyzes just how much of relativism is left when we critically examine the theory in the light of the objections that have been made to it,.

The paper thus aims to establish a defensible form of relativism.
In particular, I look at objections to and consequently forms of epistemological relativism (ER).

The essay starts with clarifying the theory of relativism in general and clearly stating what form of ER is used in the essay.
The rest of the essay looks at the objections which can be made to this version of general form of ER.

Each objection, and to what extent is it able to withstand the objections which were made against relativism.
The objections suggest that ER is mistaken as: it suggests that reality is shaped by our conceptual framework, the thesis of relativism--is true in some non-relativistic sense, the ER collapses into subjectivism and that it cannot evade that there is, after all, an absolute notion of truth.

The project aims to refine the theory so that it coheres with our intuitions.
This is mostly done by specifying aspects of the form of relativism which have been criticised.

The last part tries to sum up what is left of relativism after all objections are considered, and thus preset the viable / defensible form of ER.
Epistemological relativism

To begin with, more needs to be said about the relativist theory.
Any relativist theory is a version of the main idea of relativism: "For all X-judgements, some feature, F, of an X-judgement is relative to Y." This essay considers cognitive or alethic version of relativist theory, where the scope of relativism is limited to relativism about truth variable of a judgement.

The truth of a judgement is then relative to the conceptual framework of the thinker's epistemic community, which in other words is the domain of the judgement.
(AR) "The truth of any judgement is relative to the conceptual framework of the thinker's epistemic community." (Preston 1992:61) In other words, whether something is true really only depends on the epistemic framework, ie the concept scheme the thinker has.

Truth variability of a judgement then does not only depend on how thinks are but also on what things mean.
Relativised truths can consequently tells us something about the epistemic framework.

One very useful argument to demonstrate how the theory is supposed to work in practice is the argument from paradigms.
One can understand the argument in the flowing way, if one start with different proposition or standards of what counts as a good argument, it is obvious that one will also come to different conclusions.

Objections
One of the fundamental objections to AR, as argued by Whyte, is based on the distinction between what we hold for true and what really is true.

AR asserts that truth of a proposition depends on the conceptual framework, i.e. the concepts which one holds.
But this contradicts the equivalence principle (EP: "for any proposition 'p', the assertion that p is true if and only if p"), as relativism would grant that people were true about various astronomical theories, e.g. the geocentric model of universe, which then turned out to be false.

Consequently, if relativism about truth has an unrestrained scope to which proposition it applies, then a relativist must hold that while geocentric model was true in the times of Copernicus, Heliocentric model is true in our times, which is a blatant contradiction as only one of them can/could obtain.
Relativism also needs to make sense of epistemic transitions, i.e. why people decided to abandon believes which were truth.

Another incoherence which follows from AR + EP is that since in 14 th century it was held to be true that the earth is flat, then people in that time must have lived on a so called Discworld©.
If, as EP holds, reality copies truth and, as relativism argues, truth changes with our conceptual framework, then it would follow that reality is shaped by our conceptual framework.

This, in the light of examples mentioned, seems counterintuitive, as it has inconsistent consequences.
What is truth - Kirk argued that "holding a particular belief, or thinking in terms of a particular theory or system of concepts, does not make that beliefs or theory true." It seems that relativism in this sense amounts to coherence with the system of beliefs in question.

But "for us to have that system of beliefs is one thing; whether or not the system is true is something else." There are a few ways how relativism can respond, i.e. forms that it can take to avoid that this objection applies.
In particular, a relativist can question whether the EP applies to relative truths as defined by the ER.

While EP seems to hold for truths of absolute nature in certain areas, relativism on the other hand redefines the concept of truth in a way that makes it meaningless to apply EP.
As ER subverts the concept of 'true' for 'true for', i.e. "p is the case relative to that person's conceptual scheme", "absolute categories are inapplicable." But for this to be possible, AR needs to limit the scope of judgements it applies to, or at least exclude the areas where EP clearly applies, e.g. physics, which tries to describe world as it is, in contrast to relativist world which is always relative to our framework.

Thus a defensible form of ER would hold that p judgement is true in a specific way, namely p true for the thinker if it is true relative to the conceptual framework of the thinker's epistemic community, where this excludes knowledge where EP applies.
Aesthetics is an area where ER can be applied without worries about inconsistency.

Second objection to relativism, as presented by Putnam, is the charge that total ER is inconsistent or self-refuting.
This is based on the idea, implicit in Relativist theory, that absolute truth, i.e. truth that is valid in every framework, is not possible, as any thinker can only / is able to make statements regarding his own epistemic framework.

This however leads to inconsistency problem for the relativist thesis.
"If I assert that all judgements are only true relative to some non-privileged standpoint, the objection runs, I am implicitly claiming that this judgement--i.e.

the thesis of relativism--is true in some non-relativistic sense." The argument is based on the inconsistency of two statements: A1 There are no absolute truths, i.e., no consistent point of view is more justified or true than any other.
B2 It is absolutely true that (A1).

The problem of the relativism is that the relativist thesis "is itself something absolute".
Relativists have no other way to deny absolute truths but by making a statement which itself has to be taken as an absolute truth.

There are two possible ways to respond to this.
First, we can try to redefine the concept of absolute truth.

A defensible form of total relativism would hold that absolute truths like the relativist thesis itself are possible in the world as painted by relativism.
Absolutely true statements, in the sense that they are valid for every framework, are coherent with the relativist thesis / idea.

For a statement to be valid / true for every framework, it would have to be true relative to each of the epistemic frameworks.
This is possible if the statement is based on a part of the framework which obtains in every framework, this can be called the shared framework in the sense that each shares this part with every other epistemic framework.

Thus a defensible form of ER redefines the notion of absolute truth, into truth which is true relatively to the shared framework.
This changes the picture form a number of separate frameworks to a web of frameworks which share a common middle ground.

While this means that a total relativist thesis does not necessarily have to be self-refuting, a redefinition of absolute truth does not mean that the shared framework is a sufficient base for the thesis.
The shared framework, or the middle ground, needs to sufficiently complex so that ER can be told to be true for it.

For this to work, we would have to take a closer look at what actually obtains as a shared epistemic framework.
The whole project of total relativism would thus rely on being able to asses from one's own framework what holds or is part of another framework.

However, it is not exactly clear how we can access other frameworks.
Other frameworks might be unintelligible for us, as they might hold different paradigms of rationality.

Diverging secure attunements, i.e. "the places where the community draws the limits of rational discourse," will produce differences in what counts as a reason and consequently disagreements, as to what exactly is true relative to which framework.
It seems that this project cannot avoid "the spectre of unintelligible complexity." Yet if this can be solved, R would be coherent with the concept of absolute truth, which is the main objection used against it.

While supporting the very concept of absolute truth seems to undermine the relativist project, one has to keep in mind that only a limited part of any framework will be the shared framework or middle ground.
Thus many areas remain isolated to the possibility of any absolute truth, as all that can be said about these is that a judgement can be true for a particular framework.

Further way to respond to the self-refutation objection is by denying the notion of absolute truth at all.
This would make the ER thesis true for relativist framework only, which s could make the whole Relativist project less attractive, as relativists seem to have "no prospect of converting anyone else." However, this is not necessarily true.

Even if R loses the option to appeal to the notion of absolute truth, it does not mean that "there can be no rational grounds for coming to be a relativist." Relativists can convince non-relativists by "showing inadequacies in their beliefs".
This would still require that relativists and non-relativists are able to make intelligible conversations, and have the same notion on what counts as a reason.

Thus a defensible form of ER would "presuppose the possibility of rational alteration of belief." This however, among others requires that relativists are able to make a distinction between being right and merely thinking that we are right.
Additional objection presented by Putnam, which based on Wittgenstein's Private language argument, denies this distinction to the relativists.

"Wittgenstein denies him the distinction between being right and thinking he is right." In his sense, AR collapses into subjectivism, as his judgement amount to beliefs only, which have little appeal and are self-refuting in the same way as subjectivism.
This point is further demonstrated by what Putnam calls 'Garfinkel's one-liner'.

If AR fails to present any objective notion of rational acceptability, then there really is no difference between being right and thinking that one is right.
If this is the case and Garfinkel thinks that R is false for him then it really is false for him.

Any unconstrained form of subjective relativism thus seems to be self-vitiating.
However, this does not mean that any form of relativism has to degenerate into subjectivism.

A defensible form of ER would have to be constrained a rationalist criterion which would guarantee that a distinction between 'being true' and 'thinking that one is true' can be made.
"Objective relativists don't seek to deny that there is objective knowledge, but rather to give an account of what that objectivity consists in." Objective reasons are possible within the standards of the framework itself, as "the application of its standards is objective and does not depend on what people think." The standards themselves are in turn equally justified for every framework, as there is no point from which anyone would be able to assess and judge them according to some objective criterion of rationality.

This is based on the argument that all justification has to at some point run out of reason, i.e. reach the "places where a community draws the limits of rational discussion." These limits are not themselves justified, rather "they are matter of training or enculturation." While this seems to make the notion of objective rationality completely arbitrary, it is sufficient to make the distinction between being right and merely thinking that one is right.
As for 'Garfinkel's one-liner,' it is by no means clear whether it is in any sense objectively true that relativism is false for him, as all his argument now amounts to is the 'I think that...' and not 'it is true for me...' In addition, Putnam objects that "if statements of the form 'X is true (justified) relative to person P' are themselves true or false absolutely, then there is, after all, an absolute notion of truth (or of justification) and not only truth-for-me,..., truth-for-you, etc." It seems that Relativism has to concede that there are some absolute truths, as it is hard if not unintelligible to relativize the judgement that 'X is true relative to P'.

To relativise the 'true for' concept would be open complexity objections, e.g. Reducito ad absurdum.
Therefore to make AR possible, a relativist has to embark on the project of described above.

He has to find a way to make some form of absolute truth consistent with relativism without making the relativist project abundant or too complex.
A defensible form of relativism would thus describe how or at least whether a shared framework is possible, and whether it is sufficient so that AR can claim that its thesis is true relative to this shared framework..

Conclusion
So what is left of alethic relativism, or what form does it have to take.

The first objection shows that AR needs to specify the relation between reality and our conceptual framework by explaining the concept of 'true for'.
This requires that AR limits the scope of areas in which it applies.

To avoid the self-refutation objection, AR can either become limited and deny any notion of absolute truths, or try to redefine the concept of absolute truth in a way which is coherent with relativist picture.
While the former solution seems to avoid the objection more easily without too much complexity, the latter way seems to be necessary once we look at what Preston calls the "most successful of Putnam's objections." The charge that AR cannot evade absolute notion of truth, the statements that X is true for a person P is itself absolute.

Moreover, the collapse to subjectivism can be avoided by imposing an objective criterion of rational acceptability.
In this sense, a defensible form of epistemological relativism has to be limited in scope, objective and total (cohering with the concept of an absolute truth) Yet for the project of shared framework to work, a lot more needs to be known about exactly how much of rationality is left in the shared framework.

Introduction
Since the introduction of the United Kingdom Biodiversity Action Plan (UK BAP) in 1992, 427 UK action plans for specific species was introduced, 34 of them relating to both oceanic and terrestrial mammals (Mammal Trust UK, 2005).

This is because there has been a dramatic decline in the numbers of native UK species, including the red squirrel, pine marten, water vole and common dormouse.
This is due to a change in agricultural systems affecting habitats and feeding site, competition with other native and non-native species, and previous hunting pressures over the previous two centuries.

There is also a threat of hybridisation among species, and therefore merging of distinct species (eg. polecat and feral ferret) may occur in the future if no active management of species is to happen to the species.
Some of these 34 animals are flagship mammal species that indicate the local health state of the environment.

For example the presence of healthy bat species shows that European Bat Lyssavirus (EBLV), which can be transmitted to humans, is free from the environment (Battersby, 2005).
National 'flagship' species

High profile UK BAP species include the red squirrel, dormouse, polecat and otter.
The read squirrel used to be widespread throughout the British Isles until the introduction of the American grey squirrel in the 19 th century.

The grey squirrel is better adapted at collecting resources when compared to the red squirrel, therefore, was more successful in the red squirrel's habitat.
The grey squirrel is also immune to the squirrel poxvirus, and when the non-native species brought the virus to Britain, it resulted in a mass decline in red squirrel numbers.

There is thought to be 161,000 red squirrels inhabiting mainland Britain, mainly in Scotland and northern England (Figure 1) but is reducing in fast numbers as the distribution of grey squirrels spreads from the south (Harris et al., 1995).
The prevention of grey squirrels populating Scotland is a high priority in keeping the red squirrel from being extinct in the UK.

This can be achieved by culling grey squirrel if they are seen approaching habitats that are held by red squirrel.
However, the Scottish Habitat Action Plan has set out guidelines that have aims to change coniferous forests and woodlands to broad-leaved species (Scottish Forestry Forum, 2002).

As the red squirrel thrives in these coniferous woodlands, it is one step backwards on the survival for this native species.
One success case for the BAP is the re-population of England by otters in pockets throughout the country, mainly from Scotland.

This suggests that waterways are becoming more habitable for this species since the UKBAP introduction.
The 2006 update from the Tracking Mammals Partnership has stated that there is a significant increase of otters in the UK from the 2003 report by Jeffries which provided evidence of 9,470 otters in mainland Britain.

The BAP does not drive conservation work for some of the UK mammal species.
These include the grey squirrel (as discussed), mole, fox, badger, non-native species of deer, rabbit and rat.

This is because population numbers of these animals have either been increasing or are at stable populations (except rabbit numbers that are declining).
These animals are regarded as agricultural pests that predate on poultry (fox), damage forestry, agriculture and biodiversity (deer, mole), and have correlations to health problems for humans and livestock (badger is believed to promote TB in bovine livestock, and the rat can transmit Leptospirosis to humans).

It is noted that many of the rare UK species are actually rare in mainland Europe.
The red squirrel is more common in Europe, and it is these animals that have allowed the re-population of red squirrels in the UK.

All of the British bat species are found in Europe, although some species, such as the Barbastelle and Bechstein's bat are rare through all of Europe.
On the other hand, the UK accounts for 17% of the badger population in Europe, and the majority of Chinese water deer, which is thriving better in the UK than it's native Asia.

This shows that comparisons between the UK and Europe must be taken into account when identifying species to help promote.
If we were not to protect all non-native species, this would affect the biodiversity of the whole of Europe and the rest of the globe.

Local Biodiversity Action Plans
Biodiversity Action Plans are divided up amongst local authorities in the UK.

Each individual authority identifies local priorities and determines the contribution they can make to the delivery of the national Species and Habitat Action Plan targets (UKBAP.org.uk, 2006).
Berkshire, unlike the major counties that surround it (Oxfordshire, Hampshire, Buckinghamshire, Gloucestershire), doesn't enforce any Local Action Plans for certain species, including mammals.

Instead the county is focusing on providing correct habitat areas that are suitable for the region.
This will help all wild species indirectly by providing suitable sites to nest and provides adequate feed sources for them.

Using the information provided by Berkshires neighbouring counties, there is main focus on the establishment and stabilisation of populations of dormice, water voles, polecats, stoats and weasels.
All species of bat that are found in southern England are also of a conservation concern.

These animals, as well as the West European hedgehog, (an example of an insectivorous mammal with population data that is more extensive than other insectivorous species such as the common- and water shrew) are to be used to apply conservation practices at Sonning Farm.
Sonning Farm

Sonning Farm is located in the Wokingham district of Berkshire.
It is a 180ha site, of which 110ha is pasture and 70ha is arable ground that grows oilseed rape and wheat.

The farm also rears dairy herd replacement heifers and often fattens lambs ready for market.
The River Thames runs to the north of the farm, which provides the area with water through streams and ditches.

As the farm is located on the flood plain of the river, it is prone to flooding of the fields that provide silage, and flooding of the overgrown marsh area known as the Shacklefords.
A map representing the farm in the current state is shown in Figure 2.

At the current state there are isolated islands of wooded areas that can only sustain small population numbers of mammals, birds and insects.
A few hedges line the edge of few of the fields, but are mostly of a couple of species of hedge plants, with wide gaps in between.

As these are also isolated from each other, they can only provide a habitat for small numbers of populations as well.
There are however, in-fill trees that provide suitable nesting sites for bats that hunt over the neighbouring fields at night.

A ditch that caries excess water off the track is presently overgrown by bramble species.
A permanent pond is also located in the centre of the farm and is overshadowed by trees.

In order to increase wild mammal numbers on the farm, you need suitable information on each individual species' habitat, breeding systems and food resources.
An increase in populations will not occur if one of these factors is not provided.

For the animals that have been chosen for conservation need to be able to travel in safety to the farm.
This will most likely happen using hedgerows as corridors from neighbouring homes and fields.

It is important to join up and maintain the hedgerows on the farm to provide a corridor for access to the central woodland areas.
Many species will not travel far from hedges (except for species such as deer and badgers that travel freely across open fields).

Bats also stay close to, and use hedgerows to travel.
In-filling gaps with a variety of native species, especially those that fruit annually, will provide a range of different feed environments.

Different plant matter will increase the species of animals and insects that eat the leaves.
Having a larger range and number of insects will bring insectivores to the farm to predate.

Examples include the hedgehog, dormouse and all the bat species where each individual bat can eat up to 3000 insects in one night (RSPB, 2007).
Hedge species that fruit (eg. bramble and dog rose), provide food for small mammals such as the dormouse, and water vole (which eats fallen fruit).

Five hedgerow species every 30m is sufficient to increase insect and small animal numbers.
Creating a hedgerow that extends north-west and joins with the vegetation that lines the Thames will open up the possibility of a wider range of animals and insects to enter the farm, and hopefully stay.

This is because many animals and insects use the fast flowing river to either feed or provide a habitat.
Cutting hedges in rotation every two years enables part of the hedgerow to mature and provide suitable habitats for mammals to hide.

Hedgerows require fences that prevent the large ruminants from grazing the undergrowth that would otherwise destroy habitats for voles, mice and shrews.
Cutting back on the bramble species that line the track will open up the ditch and allow access for maintenance.

Properly dredging the site's ditches should allow better access for those species that inhabit the waters edge, such as the water vole.
Also, reducing the numbers of ornamental plant species that are growing in the current hedges will remove the chances of animals eating any potentially poisonous plants.

Adjoining of the woodland islands using hedges and filling in the corner of the field currently used as set aside with trees and shrubs allows the habitat to support and sustain larger viable populations.
This will lead to the introduction of the larger carnivorous mammals such as the weasel, stoat and polecat.

As the field entrance is located in that corner that will be re-developed, making a new entrance east of the pond will ensure that animal species will not be disrupted by farm traffic.
This woodland area also needs to be fenced to stop grazing by ruminants.

Introducing scrub species (use tree guards to stop them getting eaten by rabbits etc.) in the woodland areas that separate fields 'straighthanger' and 'gravelpit' will introduce an understorey, providing better habitats for mice, hedgehogs and weasel family.
Introduce woodland management schemes to start coppicing trees again.

This will provide better habitats for dormice and other small mammals.
Reducing the numbers of conifers may be of beneficial use to some animals and to create the right woodland habitat for the Berkshire Downs.

However, dormice are known to live within them, so this isn't a main priority for conserving small wild mammals.
Branches and standing dead trees provide a feed source for detrivorous insects, which in turn can be eaten by insectivorous mammals.

Coppicing the south-facing trees around the pond area will allow light into the pond, allowing for more plant life to grow, and provide habitats for flies and waterborne insects.
The Daubenton's bat almost exclusively feeds over water sources (Battersby, 2005), so allowing enough manoeuvrability for bats to enter and leave the area over water should provide sufficient space for other species, such as birds to feel secure on the pond during the day.

As the north of the farm is on the Thames floodplain, it may be possible to establish a reed bed in the Shacklefords.
The reed bed could allow the introduction of a new habitat for water voles, shrews and mice.

The bed also provides shelter against stoats, weasels and polecats that might be introduced to the area.
The reed beds provide a source of insect species that have juvenile stages in water, such as the dragonfly.

Currently, there are tall trees lining the shacklefords, and the building of bat boxes will provide roosting sites for bats, such as Daubenton's bat that hunts primarily over water sources (Bat Conservation Trust, 2006).
Bat boxes can also be situated to the south of the farm, in the wooded area of next to the cow sheds.

As a recent report by The Mammals Trust UK (2005) showed that bats tend to congregate around cattle during night time grazing.
This is most likely due to that cattle dung attracts insect and fly species to the area, providing a constant food supply to bats.

This is especially important during rearing of juveniles.
Beetle banks that join the River Thames to the centre of the farm will aid travel routes for small animals and insects from the river into the farm.

The banks also provides over wintering habitats for insects and the smaller rodents.
Insect diversity is also increased with the addition of field margins around the largest fields.

This also provides suitable habitats for shrews, mice and voles that take refuge in heterogeneous grass and flower species.
The layout of the farm, with the added hedgerows, field margins, and beetle banks is shown in Figure 3.

Monitoring species
For the farm to sufficiently monitor the population abundance of mammal species, it is important to get volunteer involvement.

There is a large number of people in the UK that want to take an active part in collecting information on the flora and fauna of the British Isles.
It must be noted what type of surveys are to be monitored and whether the volunteers need to be taught how to use specific equipment (eg. bat echolocation devices).

Health and safety of the volunteers is more important than conserving mammal species, so procedures must be taken not to put the participants in unnecessary danger (e. wading in deep water, or climbing trees with no protective equipment).
Volunteers and skilled surveyors must use a variety of sampling methods and background knowledge or to successfully estimate the numbers of wild mammals present on the farm.

For example bats are primarily nocturnal, so counting bat numbers during the day is inefficient at monitoring their numbers.
Field surveys using echolocation devices can be used, as well as colony counts as brooding females leave their roosting sites at night to feed.

The Mammals on Roads (MOR) survey identifies road casualties along certain stretches of road, and can correlate this to the abundance of the species present in the local environment.
This may be possible to introduce to the farm, except that there is only one major road that travels past the farm, so species aren't likely to cross the highway.

A suitable number of deaths need to be recorded before statistical analysis can be achieved.
Therefore only species that are common (eg hedgehog, rat, fox and badger), can be suitably correlated to abundance.

A more efficient way in counting abundance is by using clay tiles that create footprints as the animal treks across the tile (Game Conservancy Trust).
Hair trap tunnels can also provide hair samples for identification either by using guides or keys.

A DNA library of hair samples (cytochrome B sequence) is being established that will allow DNA identification of hair samples (Battersby, 2003).
At the present time there are 24 mammals sequenced, but a quicker way to analyse samples is needed for this type of surveying to be used nationally.

This will in turn allow for the recruitment of more volunteers that are not required to have special skills in identifying the type of hair from species.
Since 1992, there have been 6 different bills that have attempted to ban the use of hunting with hounds in the UK.

The final bill was rejected 3 times before the use of the Parliamentary act was used to ban hunting.
Since the 2004 ban, it is believed that the ban is a target against the rural communities that participate in the activity.

It is claimed that hunting with dogs is the least cruel method for culling foxes, but comes with criticism from the Committee into Inquiry into Hunting with Dogs which have evidence that a dog bite doesn't always initially target the neck of foxes and hares, and the constant pursuit of deer doesn't correlate to natural behaviour.
A study on the numbers of foxes during the foot and mouth outbreak proposes the idea that hunting with dogs isn't an adequate way to regulate numbers of foxes.

Deer hunting with dogs only accounts for 15% of culling needed.
Also, hare and mink hunting make an insignificant impact on the number of the species.

Therefore the ban on hunting with dogs can have suitable justification that animal welfare is seriously compromised, and not a conspiracy against the class divisions.
Introduction of the ban

The Hunting Act (2004) was put into practice on the 18 th of February 2005.
This Act came with criticism, especially from the pro-hunting groups because the current Labour government had to evoke the Parliamentary Act in order for the bill to be passed through the House of Lords.

Tony Blair's statement on the BBC programme 'Question Time' in 1999 that, "hunting will be banned before the next election" (supportfoxhunting.co.uk, 2006) showed the desperation that the party was in order for a complete ban on hunting.
Many pro-hunters believe that using the Parliamentary Act was the last attempt that the Labour party could use in 2004 for the bill to pass before the 2005 general election.

"With the world in the state it is in...
people will not understand Labour's warped priorities and their fixation with the issue of banning hunting with hounds." James Gray, Shadow Environment, Food and Rural Affairs minister (BBC News 2004) Mr Gray's remark came after 6 different bills- since 1992- all tried to ban the use of hunting with hounds, especially that of fox hunting, by the Labour party, (Wild Mammals (Protection) Bill, 1992; Fox Hunting (Abolition) Bill, 1993; Wild Mammals (Protection)- Amendment, 1995; and The Foster Bill, 1997).

These bills never passed with the favour to ban hunting, due to the majority Conservative government opposing the bill.
After the 1997 general election, where the surge in Labour MPs caused the Conservative government to back down as the main political party, the then-Home Secretary Jack Straw initiated the Hunting Act in 2000 after the results of 'The Committee of Inquiry into Hunting with Dogs in England and Wales' (termed hereafter as 'The Burns Report').

This bill was 'thrown out' by the Lords, which had a majority of Conservative members.
The Hunting Act was proposed again in 2002, and once again thrown out by the Lords.

The Act was finally passed in 2004, after Alun Michael, the Rural Affairs Minister initiated the Parliamentary Act over another rejection from the Lords.
The constant attempts by the Labour party in the last decades over hunting (with dogs), may be due to a form of 'class conflict'.

Class and its definitions
Mentioned in the Oxford Dictionary (2005) as "a system that divides members of a society into sets based on social or economic status", and was traditionally perceived as high earning members that were powerful and affluent ('upper class'), and those members that were less economically well-off (the 'working class').

As a stereotypical judgement, upper class members were usually wealthy landowners that tended to vote Conservative.
Whereas the working-class are perceived to work within towns and cities with a low income.

Labour's 1945-97 coalition of the working class and progressive middle-class allies (Galloway, 2006), has stood as a party with the views of the working class, winning support for the party's MPs in the majority of the UK's towns and cities in the 1997 and 2001 general elections (Figure 1).
This has caused a division by the wealth of person, the political views they hold, and also whether they live in an urban or rural environment.

It can therefore be assumed that the repetition of anti-hunting bills brought forward to the government may be an attack on the rural Conservatives after once common working-class sports such as cock-fighting, dog-fighting, bull and bear-baiting were banned in the 19 th century under the power of the then Tory (which are now modern-day Conservatives), and the Whig (now modern day Liberal Democrat) parties(number-10.gov.uk, 2006) However, it was then, and can still be believed that fox hunting, although enjoyed by the wealthy, was a form of countryside management.
The different attitudes to fox hunting between urban and rural dwellers can be summarised in Table 1.

From the above table, it is clear that the views of fox hunting differ between the urban and rural populations.
This may be due to media influence, because many people that live in towns and cities have never seen or been on a hunt.

Hunting with dogs versus Shooting
Hunting with dogs is claimed to be the best method for culling foxes.

This is because the second most common method for culling wild animals is by the use of shooting the animal from a distance.
The riflemen must be highly accurate in order to successfully kill the animal without causing a prolonged pain response in the animal.

Shooting an animal inaccurately can seriously cause pain to the animal.
Shooting from a distance also allows the animal to have a high chance of escaping with an injury, unlike hunting with dogs, which specifically targets a single animal and will constantly pursue it.

"However, this concern is currently not applied to other species of animals during open shooting season, or the irregular shooting of animals or birds by farmers" (Wheeler 2000).
It is claimed that the dogs hunting the animal will not cause injury prior to the main neck bite:

'Instant Killing of Fox by Leading Hound'
"The kill occurs as a swift, almost instantaneous, procedure made possible by the considerable power weight advantage the hound has over the fox" (Thomas and Allen 2000) Hunting with dogs doesn't always attack the animal with a powerful bite to the neck after a chase, as many pro-hunting groups tend to believe.

The Burns Report (2000) obtained the carcasses of four out of five foxes killed during 15 individual hunts (the fifth carcass was not presented to the Committee by the Hunt).
Two of the carcasses were shot after being dug out by terriermen when the 'fox went to ground'.

The post mortem of one of these carcasses showed the presence of past bullet wounds- supporting the idea that shooting animals isn't the best/ most effective method of controlling numbers of foxes with the minimum amount of stress or pain to the animal.
Two other fox carcasses that were killed, were by hounds above ground, and were examined to find "little tissue damage in the head, neck and shoulder region" (Figure 2).

Therefore these two foxes couldn't have been killed by a swift kill to the neck as stated by Thomas and Allen.
The University of Bristol concluded that these deaths occurred from "probable...

trauma to the abdomen, hindquarters or chest...by repeated dog bite".
The two carcasses examined with no factures to the cervical vertebrae that were presented to the inquiry may have been a coincidence, but information presented to the Burn's Report for the same result re-occurring was both supported and opposed.

Jones et al, 2000, own studies on 3 post mortems show that the main cause of death was a bite wound over the chest region, and severe breakages of the spinal cord may have resulted from the fox being shaken by the foxhound.
As there was no debris, or frothing in the trachea, Jones has stated that the kill, although not on the neck, still killed the foxes instantly (most likely due to the shear size difference between the fox- 7kg, and foxhound 30-40kg)

Chase of foxes and deer
The fox is classified as a top predator in the UK.

Therefore, being chased may not be part of its natural behaviour and may cause stress to the animal.
Also, The Burns Report state that approximately a third of foxes that are chased by Masters of Foxhound Association dogs are shot after the fox has 'gone to ground'- therefore it doesn't seem necessary to chase them, and just shoot them in the first place, without the need of a chase.

Huntsmen agree that a fox being chased isn't part of its natural routine, but foxes have evolved to be able to travel long distances, to enable them to catch and find their prey.
Deer and hares are characteristic of prey species, in that the "whole anatomy of the species with its muscle mass concentrated at the top of long slender limbs and the bipedal feet is a conformation clearly evolved for flight and speed" (Thomas, 2000), and therefore the animal can travel great distances with the same type of stress levels induced similar to those which occurred in horses and humans which had exercised intensively (Bateson and Bradshaw, 1999 as cited in Burns et al.2000).

However, when Bateson worked with Harris for the Burns Report, they both concluded that deer experience heighten activity levels when being chased, greater than deer would experience when being chased by wolves in other countries.
Hunting with dogs often targets those animals which are weak or old, especially in the case for deer hunting (with dogs).

This selection, in a way, stops those that are weak and old, suffering over the winter months from its illness.
With foxes, it is often for hounds to catch foxes that are diseased (most likely due to mange).

If disease is allowed to spread, it could infect domesticated animals or decrease the population of the hunted animal, especially if a ban where to be put into place (Eddy, 2000).
This targeting of the weak and old allows a good way of regulating the numbers of wild animals that would otherwise increase in numbers and harm the effectiveness and economy of farmed animals.

Regulation of fox numbers in England
It is speculation that numbers of animals will increase if a complete ban on hunting with animals.

In the case for regulating foxes, it is estimated that there are 240,000 adult foxes in UK and produce 425,000 cubs each year (Harris 1995, as cited in Baker et al., 2002).
Therefore, a 64% mortality (276,250) each year needs to occur to stop the numbers of these animals from increasing in population.

However, Pye-Smith (1997 as cited in White et al.
2000) calculates that 15,000 foxes are killed directly from fox hunts and Burns et al., (2000) estimate that only 21-25,000 foxes are killed each year.

Therefore, hunting has never been a huge part in the culling of foxes, and the main killing method is due to motor incidents (Pye-Smith, 1997, calculates 100,000 are killed from traffic), and environmental factors (e.g. starvation).
Baker et al., (2002) studied the numbers of foxes before and after the FMD in 2001, by calculating faecal density in sites located throughout England and Wales.

The report concluded that the numbers of foxes didn't increase during a nationwide ban on hunting with dogs (and other types of culling methods to a certain extent).
The report also found no association between any reduction in hunting pressure that was observed, and the change in fox density.

If this report is to be taken into account, it cannot be valid to justify that hunting with dogs is a good regulatory method.
The Burns Report also reports that hunting with hounds doesn't regulate other wild mammal species.

It is believed that deer hunting with hounds only accounts for 15% of the number of deer needed to be culled to maintain a stable population.
Although the numbers of hares and mink populations haven't been statistically measured, Burns et al.

believe hunting these animals make an insignificant impact in regions that they are hunted, due to the constant reproductions of the animals.
Price per kill

Averaging out over the year and across all foxhunts, each fox killed costs £930.Looking at it by region, the cost per fox killed varies from less than £100 in six hunts that participate in Wales, but over £3,000 for each fox that is killed in seven hunts in the South of England.
This is an un-economical cost to culling in the South, and this evidence, together with collections and memorabilia that is available, reflects to a certain extent that hunting with dogs can be seen as an unnecessary recreational activity.

The high cost isn't as high for the farmers who want foxes removed from their land.
As White et al.

quote "The major advantage to farmers and landowners from the hunt is that it operates without any additional expense on their part, unless they choose to hunt themselves", is one of the main reasons why farmers choose to allow hunting- because the price for pest control is free.
If the farmers do not want culling of wild animals on their land, it has been estimated that non-culling approaches may cost more than £3000 per fox killed.

For example, Hodge and Pepper (1998, as cited in White, 2000) calculated that the price to erect a fence that would prevent large animals, such as deer and foxes, to enter onto agricultural property would cost in the region of £4.10 per metre of fence, with an extra 14pence per metre every year for maintenance.
A fence would also stop the general movement of wild animals across the countryside, and with animals such as deer that move to graze, fencing them off might cause them to starve- therefore bringing in another welfare dispute.

Conclusion
The suggestion that the ban on hunting with dogs is entirely due to class, than animal welfare seem implausible.

The ban is/will affect the rural population, mainly because they are mainly the 'class' that participate in the activity.
However, I believe it is right that the rural community should be incensed by the ban.

There was little, or no scientific evidence given to The Burn's Report that a ban on hunting with dogs would result in an increase in wild mammal populations, only speculation from both sides of the debate.
As Leader-Williams (2002) states: "The best way to test these predictions would be...

a temporary, medium-term ban in random areas...
and with careful planning, it would provide a firmer scientific basis for legislation than existing evidence" I believe that this statement is right, but from what evidence is presented in this report, I would agree that the affect on class is a result on a ban, and not the cause.

Wild mammals do need to be regulated- especially the over-populated deer in the south-west - but modern-day hunting with dogs has been waived from regulation of animals into a cruel sporting activity.
48 Simmental x Holstein-Friesians (mean bodyweight 424.2kg) were fed a mixture of grass and maize silages ad libitum (100% grass, 33:67% maize: grass, 67:33% maize: grass, 100% maize) during the finishing period.

Crude protein was calculated, and compound feed added to provide all steers with isonitrogenous diets.
Once the liveweight of 560kg was reached the animals were slaughtered and compared against the pre-treatment slaughter group of 8 steers.

Using ANOVA, there was a significant increase in dry matter intake and metabolisable energy with diets based on maize silage (P<0.001).
The number of days on trial decreased with the use of maize silage, by approximately 6 weeks (P<0.001).

There was also an increase in daily carcass gain (P<0.001) and killing-out percentage (P=0.05).
There was no significant difference in the daily gains of meat or bone tissue between the diets.

Fat deposition per day increased with maize silage (P<0.001), with the colour becoming less yellow, and more of a whitish-cream (P=0.004).
There was no change in meat colour or pH change with the diet changes.

This experiment shows that using maize silage as the main feed for silage produces a higher carcass weight in a shorter time period, with more appealing visual characteristics of the beef product
Introduction

When comparing grass silage with maize silage, maize can have a benefit over grass when feeding as forage to beef cattle.
Such examples are that as maize matures, the digestibility remains relatively constant at around 70% as the grain develops.

Whereas grass on the other hand, looses its digestibility as the grain develops (Jones, 2001).
Another example is that currently the British weather has a sunnier, drier summer periods, which can cause grass to burn and wilt in the hot weather.

Maize tends to cope better in this heat due to the plant's origin in Mesoamerica.
The use of maize instead of grass can cause different rates of gain in cattle due to the different nutritional values of the types of silage.

Figure 1 shows the different chemical make-up of the two types of silage on a percentage basis.
The column chart shows that grass contains a higher proportion of neutral detergent fibre compared to maize.

This is due to the higher cellulose, hemicellulose and lignin composition in grass silage.
Grass also contains lower, or no starch content compared to maize silage which has a high proportion of starch.

The lower NDF and higher starch leads to higher intakes of dry matter in maize is provided as the main source of fodder.
The high dry matter also ensures that there is a rapid drop in pH when put into silage, aiding the fermentation processes.

There is however, a lower proportion of crude protein in maize silage.
Therefore, a higher concentrate is needed to supply the same amount of crude protein that is present in grass silage.

When contemplating the change to a different feed source, it is best to ensure that the composition and appearance of the final beef carcass will not deter the consumer from purchasing the product, which will cause a reduction in the carcass price.
A 2004 survey carried out asked the consumer to identify and rate each of the qualities by which they might choose beef products on a scale of 1-10.

The results are shown in Table 1.
The top priority is that of safety (e.g. sell-by date, and any damages to packaging), which has always been the top selling point in regards to food products.

The next four priorities are to do with the preparation, cooking, and eating satisfaction, which the consumer cannot be provided with at the point of purchase.
Therefore, the middle priorities on the scale are the most informative decisions that sell the product.

The visual characteristics such as the colour of the meat and fat, and how much fat is shown are therefore very important in selling the beef to the desired consumer market.
To assess the difference in finishing beef cattle with either grass silage, maize silage, or a mixture of the two, the following method was used.

Materials and methods
Fifty-six Simmental cross Holstein-Friesian beef steers (with a mean start weight of 424.2kg) were split unequally into six pens into similar bodyweight groups.

Ten steers were put into four pens, and eight steers put into another two pens.
Two random steers from each of the pens with ten cattle in were then removed to form the pre-treatment slaughter group (PTSG).

These PTSG cattle were analysed by a full or sample dissection to identify the proportions of muscle, fat and bone tissue present in the thoracic limb.
The channel, thoracic, kidney knob, and cod fat depots were also measured.

The forty-eight remaining steers were divided up within the pens so that two steers in each group were fed either a whole grass silage, whole maize silage, or a mixture made up of 33% maize to 67% grass, or vice versa.
The silage mixtures were regularly analysed, and the average composition can be seen in Table 2.

Diets were made isonitrogenous by calculating the amount of nitrogen present in the silage and adjusting using different compound feeds to allow all diets to provide 140g CP/Kg DM.
The feed analysis for the compound feed can be seen in Table 3.

All steers were fitted with an electronic tag, which activated only one gate into each feeding trough.
This allowed each steer to only eat the type of silage mixture it was allocated, and to measure intake and refusal.

The steers were kept in the indoor, rubber mat-based cubicles and were measured bi-weekly until they reached approximately 560kg, where they were transported to the University of Bristol and slaughtered using a captive bolt followed by exsanguination.
Once slaughtered, the internal thoracic and abdominal viscera were removed, but the fat depots were left intact, similar to the PTSG.

The lean tissue and fat colour were also recorded using a chromameter.
The PTSG results gave the estimated start proportions of the final forty-eight carcasses.

This allowed a general linear ANOVA to be used to compare the different tissue gains, as well as overall carcass weight and killing out percentage.
A significant difference is regarded when P<0.05.

Results
Changes in bodyweight

Table 4 shows the intakes of dry matter.
It significantly shows that as the amount of maize silage replaces grass silage, the dry matter intake increases- a mean increase of 1.41kg extra of silage from complete grass to complete maize.

The increase in dry matter intake reflects the increase in liveweight gain with an extra 363g of tissue laid down on the steers fed maize compared to those fed on grass.
The animals fed on the maize diet reached the desired slaughter weight of 560kg quicker than that of the animals fed on the other diets, especially the steers fed on the grass silage which took, on average, an extra 6 weeks on the trial to reach the target liveweight to be slaughtered.

Once slaughtered, the beef carcasses had a linear increase in carcass size with a significantly higher killing-out percentage as the amount of maize increased in the diet.
The daily carcass gain of the steers fed maize silage was approximately 285g more than the grass silage-fed steers.

These results would indicate that the use of maize silage increases the liveweight and carcass gain of finishing beef steers when compared to those fed on a whole grass silage.
Tissue gains

Table 5 shows the depositions of lean muscle, fat deposition and bone growth over the experimental period.
Numerically, there is a negative trend in lean muscle gain as a total gain per day, when the amount of grass silage decreases.

This would show that less lean tissue growth occurs with the inclusion of maize silage in the diet.
However, lean muscle gain by day increases linearly from a grass to a maize diet.

Neither of the sets of results are statistically different.
Bone gain as a total approaches significance (P<0.1), but the linear relationship isn't clear.

The results show that a diet with complete grass silage produces a higher rate of bone growth, but there is a large drop in growth when a third of the grass is substituted for maize silage.
There is then a positive trend as the amount of maize increases.

This may suggest the mean value given for the bone gain in a grass diet may be erroneous- and the large standard error may support the proposal.
The total amount of fat that was deposited in the thoracic limb produces a significant linear increase as the silage changes from grass to maize.

Fat gain per day is even more significant, with a very low standard error, showing that fat deposition is highly effected by the intake of maize silage.
Fat Deposits

There was no significant increase in weight of the channel or thoracic fat depots between the carcasses from the different diets.
There was a numerical increase in lipid accumulation in the kidney knob as the maize silage is increased.

The main increase in weight was that of the scrotal cod fat depot, which increased significantly.
Between diets of grass and maize silage, there is approximately 778g more cod fat in the steers fed maize.

Table 6 summaries the results.
Appearance and pH of carcass

Table 7 shows the colour determinants of the tissue types.
There is no significant difference to the lightness of the meat or the fat when samples were analysed.

There was also no difference in the muscle HUE or saturation- determinants of the colour of the tissue.
There is, however, a significant increase in fat HUE with increased maize substitution, making the appearance of the fat less yellow, and more of a creamy-white colour to the naked eye.

The saturation of the fat had a significant difference between the diets.
However, it is not possible to judge any correlation between the saturation and changes of diet, due to the values fluctuate as the amount of maize silage increases in the diet (Figure 2) Muscle pH is also shown in Table 7.

The results show that there is no significant change in the pH value of the meat, and between the diets, the values stay within 0.03pH.
Discussion and Conclusion

Replacing grass with maize silage increased the dry matter intake and lead to increases in metabolisable energy intake, daily liveweight gain, and thus carcass gain.
This can be attributed to the nutrient composition of maize silage.

The reduction of neutral detergent fibre, and increase in starch content, in maize silage leads to an increase in dry matter intake providing more energy for digestion, and therefore, growth- all in a shorter period than a grass silage diet.
The energy that is provided to the steers is used for lean tissue growth first, with the excess being stored in the fat depots.

Forrest and Vanderstoep (1985) found that all carcasses fed a maize diet received the highest carcass classifications, whereas only 80% that were fed on grass-based silage received the same standard.
This supports the results given of a higher carcass weight, and killing out percentage, producing a higher carcass standard in maize-fed steers.

The results showed no significant increases in lean tissue between the diets, indicating that the steers were provided with enough energy- with all the diets producing layers of fat.
In the case of maize, which is high in energy, a larger proportion was stored in the cod depot.

Although higher fat deposition in these steers, the appearance of the fat may have become more desirable to the consumer by the whitish-cream appearance.
Knight et al.

(1998) found that the carotenoid concentration in fat accounted for 60% of the variation, and not by the fat depth of the depots.
Therefore, the low carotenoid concentrations present in maize silage may have resulted in the whiter fat.

The level of marbling in the lean tissue was not measured in this experiment.
Albrecht et al.

(2006) concluded that Holstein- Friesian cows had a more numerous, finer level of intramuscular fat than other breeds of cattle that were studied.
Consumers visually prefer both low and high levels of marbling in beef- depending on individual preferences (Killingeral.

2004).
Therefore, the amount of intramuscular fat that was present in the meat from this study would still be desired by a certain consumer market.The low bone growth can in all the diets is due to that the steers were being finished-off, therefore were reaching maturity and most bone growth would have already occurred prior to the experimental trial.

Any small bone growth would have been in a lateral direction to strengthen the bones of the steer.
The experimental results showed no change in the pH of the lean muscle tissue.

This shows that the change in diet doesn't result in a loss of glycogen stores that are available to be converted to lactic acid and reduce the pH to the acceptable range of 5.4-5.7 once the steer has been slaughtered.
A low pH would have also resulted in the colour of the meat to darken.

However, the trial concluded that there was no change in the colour of the lean tissue content.
As low glycogen levels would be an indicator of stress or poor nutrition (MLA, 2006), the change from grass to maize silage during the finishing period doesn't affect the well-being of the animal.

This experiment believes that where geographically possible (such as southern Britain), growing maize as the main source of silage will give an advantage in carcass yield with a decrease in weeks to finish that steers would have to be on.
This would give a higher profit to the farmer, and maintain consumer acceptance of the final meat product.

Dogs evolved from wolves between 15-100,000 years ago.
They share similar genes, and anatomical characteristics.

Dogs have strong carnassial teeth that shear flesh in a scissor-like fashion, and use powerful muscles to aid in this action.
The shape of the dog skull is similar to the omnivorous bear's.

Herbivores however, use a developed masseter muscle to masticate plant food in a circular, lateral movement- a procedure that carnivores do not posses.
Similarly, carnivores do not have the same microbial activity to efficiently degrade cellulose before absorption, making green plants ineffective in the diet.

Carnivores demand a high intake of protein requirement in comparison to herbivorous and omnivorous animals, with essential amino acid deficiencies more common in carnivores.
Modern-day dog food consists of a large proportion (50-70%) of carbohydrate, which dogs are able to digest.

There is proportionally less protein, and usually comes from cereal and vegetable crops which provide an almost constant nutritional composition allowing the right ratios of essential amino acids to be absorbed, with an excess of limiting amino acids to reduce nutritional protein disorders.
Nutrients from meat sources are much lower (~4% minimum) in complete diets, therefore dogs can be healthy with a low meat intake diet.

Introduction
It has long been debated whether to categorise the domesticated dog as a carnivore, omnivore, or an omnivorous carnivore.

This is due to the vegetation-eating behaviour that is observed in dogs, and also due to the high carbohydrate intakes that are being fed to the nation's pets.
The Oxford dictionary (Soanes, 2005) states that a vegetarian is "a person who does not eat meat for moral, religious, or health reasons".

Some evolutionists and nutritionists believe this comment can be extended from 'a person' to 'an animal'.
Evolution of the domestic dog

It has recently been shown, by studying the genetic construction of mitochondrial DNA that dogs originated from East Asia, due to the high genetic diversity of the DNA base sequences.
It is calculated that his would have occurred less than 15,000 years ago (Vilà, as cited in McGourty, 2002) - far less than the previously calculated 100,000 years ago (Vil àet al., 1997).

All studies have concluded that dogs originated from wolves, and not coyotes.
The clusters of similar base sequences in many breeds of dogs, give reason to believe that in the past there was constant back-breeding of dogs with wolves.

Wolves and other canids are known to prey on species such as moose and elk, along with smaller mammals such as rabbits.
Once the prey has been caught, the predators tend to eat the internal viscera of the prey, which would, in most cases contain [partially] digested vegetable matter.

Therefore the evolutionary ancestors of dogs do, to a certain extent eat vegetable matter, but are not vegetarians because they are also consuming meat.
Dentition

Carnivores have specialised masticatory apparatus to allow the animal to ingest certain sized pieces of food efficiently.
Carnivores, like herbivores have a front row of incisor teeth that are generally used for nibbling.

Either side of the incisor teeth are large canines which have a root that is usually even larger than the tooth.
This root firmly holds the canine in place to allow the animal to firmly hold the tissues of its prey when tearing it apart.

The characteristic features of all carnivores are that of the carnassial teeth.
These sharp, multi-rooted teeth do not occlude, supporting the idea that canines tear the food, instead of biting, and chewing food like most herbivores.

When comparing the dentition of dogs (Figure 2) with that of the Brown bear (Figure 1), there isn't an obvious, visual difference between the shape of the teeth.
Brown bears still have the same jaw structure as dogs, and powerful masseter muscles (described below), yet is easily classed as an omnivore due to that the bears are known to often eat roots, sprouts, and berries, along with a large variety of vegetation.

The only difference is that the upper jaw of the bear aligns closer to the bottom jaw, giving more of a chewing mechanism than the dog, despite showing similar multi-cusped polar and molar teeth.
Brown bears that inhabit the coastal regions of America are much larger, and heavier in weight due to the high protein diet that they consume from fish and crustaceans (Macdonald, 1992).

This gives suggestion that bears too, might be omnivorous, but only when they revert back to their carnivorous instincts that the animal shows the maximum size of the species.
When using the bear as an example, it is plausible to believe that despite the evolution of the jaw for a carnivorous diet, dogs can live on a herbivorous diet, but may have a nutritional imbalance resulting in stunted growth.

Jaw Muscle Differences Between Carnivores and Herbivores
Herbivores and omnivores have evolved specialised muscles and dentition in order to sufficiently masticate their vegetation to allow for adequate digestion of the feed intake.

Herbivores, when compared against carnivores, have a pronounced masseter muscle, which initiates and aids the movement of the jaw in a lateral movement, which grinds food on the flat molar dentition at the back of the mouth.
An animal that can live solely on a vegetarian diet should have a pronounced masseter muscle to help break down the strong cellulose walls that would otherwise prevent the absorption of proteins and fat within the plant cell.

Carnivores, however, have a masseter muscle that is smaller in size, due to the fact that the muscle cannot move the mandible in a lateral movement due to the shape of the jaw.
It is noted in Dyce (1987), that the only movement of the mandible in a lateral movement will cause trauma, and is 'occasionally so severe that the coronoid process engages the zygomatic arch, locking the jaws in the depressed position'.

This problem occurs due to that the mouth of evolved carnivores, such as the dog and cat having the scissor-like movement of the jaw bones, and the postglenoid process prevents the temporalis muscle from dislocating the jaw.
This would evidently be possible because the temporalis muscle is a powerful muscle that joins the mandible to the top of the cranium by stretching from the large coronoid process in carnivores to the occipital region.

The pronounced zygomatic arch on a carnivore's skull shows that it is bowed to allow the large muscle to work properly.
It is believed that the temporalis muscle evolved to restrain prey species for consuming (Dyce, 1987; Pough, 2002).

A strong temporalis muscle wouldn't have evolved as much as the cat, which is defined as an obligate carnivore.
Digestion

Dogs, as well as other carnivores contain no salivary amylase enzymes in saliva; therefore the salivary glands' main function is to lubricate the mouth and oesophagus, and not digestion.
The stomach of the dog is comparatively large for the species.

Colin (as cited in Sisson, 1953) gives a capacity range of 0.6-8.0 litres, depending on the breed of dog.
The large expansion that the stomach can undergo reflects the eating mechanism that dogs engage in.

Dogs tend to gulp their food in large quantities, and are known for their voracious appetites.
This may relate back to the wild ancestor of the dog, where the wolf would sometimes spend long periods of time without food, during times of low food availability or when in constant pursuit of its prey.

The cardiac sphincter is generally wide, which may be related to the ease of which dogs can vomit.
Once the food bolus has been broken down in the stomach by hydrochloric acid, the chime passes in to the pancreas, where enzymes aid in the digestion of the food.

Dogs can, and are more efficient at digesting carbohydrates, in that they produce three-times as more pancreatic amylase to breakdown starch (Frisby, 2001).
Proteases and lipases are also released, similar to all monogastric mammals.

The chime then passes through the relatively short intestinal tract.
It is believed that the dog's intestine is approximately 5 times the length of the dog, whereas the intestinal length of the sheep can be 25 times the body length (Dyce, 1987).

This short intestinal length, and simple when compared to ruminants, is similar to the cat- supporting the idea that the dog is a carnivore.
Dogs are hind-gut fermenters, in that the presence of fermentable microbes are located in the intestines of the animal, and not located before the digestible stomach like fore-gut fermenters, which include all ruminant animals.

This is believed to be a primitive digestive vertebrate characteristic of the animals due to the same digestive patterns in birds and lizards.
Since no mammal possesses the genetic information to produce cellulase enzymes within the body, the animal has to rely on microbial activity to breakdown neutral detergent fibre such as cellulose, hemicellulose and lignin-based structures.

It is only until these structures have been broken down, that the nutrient composition within the plant cell can be digested and absorbed.
In the dog, microbial digestion of cellulose occurs in the colon, an organ that is mainly involved with the regulation of the absorption of water from the chime into faeces.

There are little or no sites of absorption of cellulases that have been broken down.
Therefore microbial activity doesn't have a great effect in the dog's nutrient intake.

Coprophagy may occur, similar to that of rabbits, which digest cellulases in the caecum.
However, in the dog's case, it isn't true coprophagy, in that the animal has to re-ingest the food as soon as it emerges from the anus (Pough, 2002).

The dog would find this process hard to perform every time it defecates.
Removal of the colon, along with the microbial activity, in dogs is relatively successful (Findlay, 1998); supporting the idea that microbial fermentation isn't necessary for the dog.

Amino Acid Requirements
A true carnivore, such as the cat and mink, have a specialised metabolism that is adapted to the ingestion of animal tissues which is devoid of carbohydrates and contains an excess of protein relative to energy (Morris, as cited in D'Mello, 2003).

Dogs, on the other hand, are able to cope more with an intake of carbohydrate, (as noted above with the amount of amylase that is released during digestion), but still requires a high intake of protein compared to true omnivores such as the pig and human.
Dogs are still susceptible of an arginine deficiency that can lead to hyperammonaemia- but will not be as severe as the susceptibility in cats.

Arginine is present in meat sources such as pork and beef, and not very common in plants and vegetable.
However, it is common in soybeans- which are one of the main sources of plant-derived arginine in dog food.

Unlike animal sources of protein, soy does not vary widely in amino acid content and availability (Purina, 2006).
This is one of the main reasons why dog food companies use soybeans to ensure a constant supply of amino acids in the right quantities to prevent any limiting amino acids from reducing protein synthesis, which would cause non-limiting amino acids to become oxidized.

Prolonged reduction in protein synthesis would lead to muscle depletion, and eventually death (Hendriks, as cited in D'Mello, 2003).
Cats and dogs tend to excrete more nitrogen-containing material than herbivores and omnivores.

This is due to the animals' evolutionary ancestors consuming high quantities of amino acids, and because amino acids cannot be stored for long periods of time in the body, any excess are excreted by the small and large intestine.
Behaviour

It is possible to watch domestic dogs eat grass.
It was originally thought that eating grass was used as an emetic, to force the dog to make itself vomit to empty the stomach from disturbances.

However, due to the cardiac sphincter is wide, with little muscle closing the oesophagus from the stomach; dogs can vomit with relative ease.
Cats sometimes eat grass to provide folic acid into the diet, and may account as the same principle (Morris, 1998).

Dogs don't swallow all the grass that they chew; therefore the animal might enjoy the taste of the plant sap that is present in the leaves.
Nutritional Analysis of Pet Foods

Table 1shows the nutritional analysis of some of the pet food products that are available in most supermarkets.
The results show that the majority of complete and mixture products contain a high percentage of carbohydrate, with the average composition of 45.6% (FW) in complete diets, and even higher (64.3% FW) in mixer diets.

There is also an extra 2.5-3% more carbohydrate as indigestible fibre.
Most dog food carbohydrate that is digestible comes from cereals and vegetable origin.

The only meat that is contained in the ingredients is usually stated at a minimum of 4% (14-24% minimum in more expensive/ specialist foods).
The low meat content reflects the low percentage of protein that is present in dog food.

The highest protein content is present when feeding a dog is in a compete meal.
However, on a dry matter basis, canned food has the highest composition of protein, but is made up of over 80% water.

Canned food usually contains a gelling agent, which is known to aid in the total gastrointestinal tract nutrient digestibilities of amino acids, especially the essential amino acids (Karr-Lilienthal et al., 2002).
Therefore, supplying canned meat to the dog may have a lower protein content, but the digestibility of the molecules is increased.

The high carbohydrate diet, which herbivores and omnivores usually eat, (because vegetation has more polysaccharides than meat), it is possible to believe that domestic dogs can be classified as a vegetarian, especially when the animals are being fed on the cheaper brands of complete food (e.g. 'ASDA Smart Price'), which has low amounts of nutrients that derive from animal tissues.
Conclusion

The debate over whether the domestic dog as a species should be classified as an omnivore does have scientific support with the similarities to the anatomy of the dog with the omnivorous bear.
Both consume berries and vegetable matter.

Dogs fed supermarket pet foods consume a very low percentage of protein, from an even lower source of animal-derived meat.
Dogs fed on these diets are therefore omnivorous, and not a vegetarian because of the small inclusion of animal matter.

Dogs are also not vegetarian because the animal has no ethical, religious or health reasoning that we know of.
They are being fed on a diet by humans, who think about the prices of food, and it is cheaper to produce and buy products that are mainly made from cereals, and not freshly packaged meat.

Abstract
Horses have developed over the last 55 million years from a small forest dwelling browser, to a long legged plains grazer.

However, the animal has still retained the 'primitive' hindgut digestive fermentation.
This has caused the animal to become more sensitive to what it ingests, as it does not have the ability to neutralise toxins like the ruminants foregut microbes.

The physiology of the stomach prevents eructation and vomiting, resulting in the horse being susceptible to colic from the build up of indigestible matter and gaseous molecules from the result of fermentable feed products.
Plants that are high in fructans also affect horses by disrupting the hindgut bacteria, causing a cascade that eventually affects the lamina in the hoof.

Laminitis occurs when the lamina is degraded, allowing the rotation of the pedal bone.
Good stable management, together with the knowledge of the horse anatomy is needed as prevention of these conditions is more effective than treating the symptoms, as fatalities are high among cases.

Introduction / Evolution
Horses are thought to have evolved 55 million years ago from a small mammal creature, the Hyracotherium (Roberts, 1991).

This animal was believed to be more of a browsing than a grazing animal, where it would eat the thicker, more succulent leaves of the Eocene vegetation.
Over time and climate change, the tropical forests decreased and increased grassy plains appeared.

The horse ancestor slowly evolved to adapt to these surroundings by losing the primitive mammalian formula for teeth- with the loss of pronounced canines, and the flattening of the molar cusps to create a grinding surface to help masticate the more coarse grass (Hunt, 1995).
The horse evolved to have longer legs, a straighter back, and stronger bones to allow the species to survive the grassy plains by using its speed as an escape mechanism, rather than camouflage that the smaller ancestor used in the dense vegetation.

The instincts and the behaviour of a gregarious herbivore has not changed in the horse since the animal evolved from Equus caballus (Auty, 1998) around 1 million years ago in the Pleistocene epoch.
However, humans have been breeding and feeding horses for over 3000 years, and yet problems still arise in the nutritional difficulties that face the horse that can lead to health disturbances if the correct requirements are not sufficiently fulfilled.

Digestion
Horses are hind-gut fermenters, in that the presence of fermentable microbes are located in the intestines of the animal, and not located before the digestible stomach like fore-gut fermenters that include all ruminant animals.

This is believed to be a primitive digestive vertebrate characteristic of the animals due to the same digestive patterns are present in birds and lizards (Pough, 2002).
However, if the horse were to have a rumen like cows, sheep and goats, the high speeds that the horse can make would be severely compromised due to the shear weight of a specialised fore-gut (Hintz, 1993).

Since no mammal possesses the genetic information to produce cellulase enzymes within the body, the animal has to rely on microbial activity to breakdown neutral detergent fibre such as cellulose, hemicellulose and lignin-based structures.
It is only until these structures have been broken down, that the nutrient composition within the plant cell can be digested and absorbed.

In the horse, microbial digestion of cellulose occurs in the large intestines, especially that of the caecum, a blind-ended sack that is over a metre long (Douglas, 2000), which is host to the symbiotic, fermentable microbes.
As the large intestine is an organ that is primarily involved with the regulation of the absorption and secretion of water into and out of the chyme to maintain homeostasis in the animal, the digestion and absorption of the cell contents after breakdown of the cellulose cell walls isn't as efficient as ruminant animals.

The large intestine has sites for absorption of volatile fatty acids (VFAs) that are produced by the micro-organisms, but there are no sites for absorption of amino acids from the plant cell contents (Bowen, 1996), as the contents have passed the primary site of absorption in the small intestine (Hintz, 1993).
Other monogastrics such as the dog, pig and man have a smaller caecum relative to their body size, so digest even lower plant material content to the horse and ruminants.

For the horse to effectively utilise the digestive capabilities of the fermentable bacteria, the horse would have to perform coprophagy, a mechanism successfully used by rabbits and hares (Collinder, 2001).
However, the anatomical structure of the horse would hinder the animal to be able to reach round and re-ingest the food as soon as it emerged from the anus every time the animal defecates.

The horse is one of the few monogastrics (with the panda and elephant being other examples) that can comfortably consume high fibre diets of low nutritional quality by being able to ingest very high feed intakes to obtain their requirements from non-fibre feed sources (Collinder, 2001).
The horse grinds the high fibre using specialised teeth for the coarse feed, and therefore breaks the fibre down into smaller pieces before entering the stomach.

This allows for a more rapid transit for undigestible plant structures through the alimentary tract, with digestive efficiency remaining constant (Pough, 2002).
The horse is more suited to a high fibre diet when compared to ruminants as undigestible fibre suppresses the chemical activity of rumen microbes on the material that can usually be digested.

This results in a slower transit time of digestible matter through the ruminant tract.
This is represented in Figure 1.

Due to that the horse is a hind-gut fermenter and does not absorb as much nutrients from plant material as ruminants, the horse has to continuously consume food throughout the day to obtain the animal's nutrients.
The comparatively small stomach of the horse, which can hold 8-15 litres of food, shows that the horse needs to consume little amounts of food, frequently throughout the day- as shown in the natural state of wild horses.

As the majority of stable management practices impose specific feeding times for the horse each day, health problems may become associated with this due to over- and underfeeding or general poor feeding routines.
An example of a poor stable management is the sudden change of feed.

When the type of feed needs to be changed, it should be introduced gradually to the horse to allow the microbes in the caecum to change gradually to help digest the new feedstuff.
This is because the microbial population is dependent on the nutrients provided by the chyme produced in the small intestine.

The sudden change of feedstuffs can cause electrolyte imbalances due to acidosis (Auty, 1998) which can lead to both enteral and extraenteral problems in the horse (Zeyner, 2003), which are described below in more detail.
Another example of poor stable management is the feeding of mouldy or pathogen-contaminated hay and concentrates.

A horse is more susceptible to infection since the horse is incapable of vomiting any food that the horse feels is disrupting its digestion.
This is because the cardiac sphincter of the stomach is a more muscular, one-way valve compared to other mammals such as the dog, which can vomit with relative ease.

The horse is also at a disadvantage when compared to ruminants since the microbial organisms can detoxify some toxins and moulds in the fore-gut before the molecules are absorbed by the ruminant (Hintz, 1993).
The cardiac sphincter also prevents eructation; therefore gases can build up within the alimentary tract, causing colic that is one of the most common and lethal health problems in the horse.

Enteral problems of the horse- Colic
Colic, which refers to a 'severe pain in the abdomen caused by wind or obstruction in the intestines' (Soanes, 2005), is a term used for a wide range of causes and symptoms that all affect the gastrointestinal system.

There seems to be no gender that is more prone to colic, although types of colic may be gender specific, i.e. uterine torsion in mares.
There is conflicting studies regarding breed and age risks.

For example, Tinker et al.
(1997) identify Arab horses to be at a reduced risk of developing colic, whereas Cohen et al.

(1999) describe the opposite, where Arab horses are actually more prone to colic.
However, in all horses that develop colic, the risks of death still remains high, despite improved medical and surgical techniques (Greet, 1993).

Colic tends to occur mostly in the large intestine due to the layout of the colon in the lower abdomen of the horse.
The large intestine is attached to the abdominal wall at two points.

The first is at the ileal-caecal junction, and the second attachment is at the pelvic flexure where the colon shortens to form the rectum.
This results in the ileal ingesta having to pass two sharp, 180° u-bends in the large intestine to successfully pass through the horse (Best, 2006).

These two sites are prone to impactive colic, which is a build up of food at the tight flexure.
The irritated intestine contracts spasmodically around the lodged food material, tightening the obstruction, therefore causing more pain in the animal.

Impaction usually occurs in stabled horses that have eaten a large quantity of bedding, which cannot be broken down into smaller digestible segments, resulting in a thicker chyme that is liable to build-up in the colon.
Another cause of impaction is a large amount of parasitic larvae present in the lumen blocking the route of passage (Best, 2006).

As with all types of colic, veterinary attention is needed.
A large amount of liquid paraffin is most likely used in this situation, in conjunction with painkillers (Greet, 1993).

As the horse cannot eructate, gas has to move caudally through the digestive tract.
Therefore, gaseous distension tends to occur with an impaction, as the gas cannot adequately pass through the tract.

Gas build-up in the stomach and intestines may be the result of feeding the horse a high proportion of brewer's grain/yeast.
These supplementary feeds provide a good source of B-vitamins (Auty, 1998) to the horse, but the molecules ferment in the intestines causing a production of gas to be released in the intestines, causing both abdominal pains and a laxative effect if fed too much.

Gaseous distension also occurs when the abdomen becomes twisted from the lack of attachment of the intestines to the abdominal wall.
This is known as intestinal catastrophy.

It is the most dramatic causes of colic, which often leads to the horse either dying, or needing to be euthanized (Greet, 1993).
However, intestinal catastrophy appears to be causative without much relation to the kind of feedstuff that is fed to the animal (Smith et al., 1972), and therefore cannot prevent against this type of colic in regards to monitoring nutritional intake.

Spasmodic colic from muscle spasms of the intestinal wall tends to be the most common, but luckily the least harmful form of colic in horses.
Causes could be due to parasitic larvae penetrating the endothelial lining of the intestine.

However, the most common cause in healthy animals is that they are fed too soon after exercise.
As blood flow is diverted away from all areas of the gastrointestinal system during exercise (Manohar et al., 1995) by the sympathetic nervous system, it results in the reduction of digestive functions (including peristaltic contractions).

Feeding the horse too soon after exercise, before the parasympathetic nervous system controlling the resumption of digestion is signalling again, incorrect nervous connection occurs, leading to spasmodic contractions that is painful to the animal.
Spasmolytic drugs usually dissipate the problem when administered by a veterinarian (Greet, 1993).

Extraenteral problems of the horse- Laminitis
Magner (1980) states that laminitis can occur in over-worked horse and ponies on hard ground (such as roads and pavements).

Johnson et al.
(2004) have described that 'laminitis can be a clinical component of equine Cushing's syndrome associated with a dysfunction of the pars intermedia' in the pituitary gland.

However, it is mostly overfed animals that are usually kept in rapidly growing grass paddocks (Dyson, 1993), usually those that contain high fructan levels that produces a high content of rapidly fermentable carbohydrate-based material when digested.
This digestion can affect the acidity of the hindgut, making the animal susceptible to endotoxaemia.

The decrease in pH can kill the symbiotic bacteria that usually allow the horse to stay healthy and absorb VFAs.
The death of these microbes produces a range of toxins that lead to a cascade mechanism once absorbed in the colon.

This is shown in Figure 2.
The toxins result in vasoconstriction of blood vessels to the lamina between the pedal bone and the hoof wall, with the disruption starving the vessels and lamina of oxygen.

Pain results as the cell induces inflammatory fluids in the local area, where most pain to the animal occurs when pressure is placed on the injured foot.
Chronic laminitis can cause the pedal bone to drop within the hoof due to the supporting lamina have been separated.

Preventative measures
The above conditions are the two main reasons why a horse requires a call-out for veterinary attention in the UK (Harris et al., 2006).

Prevention of the noted cases, as colic is frequently fatal, and laminitis casualties very rarely get back to normal exercise routines if the horse has managed to be treated effectively, is better than treating the cases when the symptoms arise.
Generally, to lessen the risk of colic occurring is to maintain a regular worming programme, regular attention to teeth and good stable management, especially the change of the feeding programme to be bought about gradually.

Preventing laminitis is less clear as causes of the condition isn't mutually exclusive between certain molecules or systems (Bailey et al., 2004).
Although animals that may be predisposed to laminitis can be confined to stabling from pasture that is rich in fructans is one way (for example, during mid-day in spring where the rate of photosynthesis is highest in most plant species), it may not be beneficial for the animal in regards to welfare, resulting in behavioural problems or other health risks that are associated to confined animals.

Research doesn't provide a guideline to suitable levels of fructans that can be eaten, as individual horses have different levels of tolerance (Harris et al., 2006).
The horse or pony should be provided a diet that does not have excessive carbohydrates, such as the over-consumption of cereals.

This should allow the animal to stay at a reasonable weight, with any weight loss being carried out slowly.
Horse owners must understand the anatomy and behaviour of the horse to suitably look after its health, with the risk of developing problems kept to a minimum.

Introduction
It has been suggested that egg production is costly in energy for the growth and maintenance of the ovary and oviduct (Stevenson and Bryant, 2000 as cited in Williams and Ames, 2003), with examples of the resting metabolic rate in bird species increasing by 22%, during the breeding season (Vézina et al, 2003).

Therefore the complexity of the structure and movement of the egg from initial development in the ovary to the final release during laying (Figure 1) must have gone through a range of processes in order for the egg to have a possibility in hatching.
Ovary

During the embryonic state of the female bird, there is a migration of germ cells from the right to the left ovary.
By the time of hatching the left ovary is considerably larger than that of the right, with the former usually becoming the only fully functioning ovary (however rare occurrences do arise of both ovaries and/or their oviducts functioning normally as in the Accipiter, Circus and Falco genera).

It is believed that the presence of one ovary instead of two reduces weight in the flying bird.
The right ovary is still present and can be seen joining the cloaca.

If the left ovary is naturally removed/ malfunctions, the right ovary changes to form a testis-like structure.
The left ovary itself is compared to a bunch of uneven grapes, with the largest 'grapes' more posterior of the bird (Figure 2).

Each grape is actually a primary oocyte, wrapped in vascular tissue forming a follicle.
There are approximately 25,000 follicles at the time of hatching, but only 5 to 7 hundred develop, and released into the oviduct.

During the breeding season, the ovary enlarges greatly, due to some follicles rapidly increasing in size by the laying down of fat and protein around the primary oocyte, which originated from the liver.
These are used for energy if the embryo can grow.

In the rook the size of the follicles increased from 0.05mm to 3.5mm in diameter over a 9-month period, but then increases rapidly to 14.6mm in just 4 days (Benoit, 1950 as cited in Welty and Baptista, 1988).
When the follicle is mature, ovulation occurs- the outer envelop of blood vessels breaks, releasing the ovum- or yolk of an egg- into the oviduct.

This is stimulated by the release of an already produced egg out of the cloaca.
Oviduct

The oviduct in the fowl weighs approximately 5g and has a length of 15cm, but increases to 75g and 65cm in length when the bird comes into season (Welty and Baptista, 1988).
During the production of the egg the proximal regions of the oviduct regress back to its original weight and length, but the distal regions remain enlarged until the passing of the egg (Williams and Ames, 2003).

The oviduct can be split into five different regions, all of which have specific functions to create the structure of the egg.
The peristaltic route of the egg lists the following regions below.

Parts of the egg referenced in the following text can be seen in Figure 3
Infundibulum

The infundibulum is the more anterior of the oviduct and can be divided into two parts.
The first is the ampula, which is shaped like a funnel, with a 9cm long entrance in the domestic fowl (King and McLelland, 1975).

The ampula 'catches' the ova released from the follicle when the outer envelope breaks.
The ampula does not always catch the ova, and internal laying can occur, where the ovum is released into the internal air spaces- the celom.

It is thought that the ovum is reabsorbed within 24 hours (Welty and Baptista, 1988).
Cork balls have been substituted for ova, which the infundibulum then catches, resulting in eggs that contain cork instead of the yolk showing that the ampula does not have any recognition functions, and responds to any stimulus.

If two ova are released together, a double yolked egg arises Following the ampula is the chalaziferous part of the infundibulum, which is a narrow tube, with folds consisting of tubular glands located at the base of them.
These secrete the final layers of the vitelline membrane, which started forming around the yolk when it was present in the ovary.

The membrane is mostly a protein coat, and stops the fat and protein mixing with the albumen once it has been secreted.
Once all three sub-layers of the vitelline membrane (the inner, continuous and outer membranes) have been added, the membrane is approximately 15µm wide.

In the chalaziferous layer, the chalaza, and the inner thick membrane of the albumen are also added.
The chalaza are rope like structures that merge into the inner thick membrane (Romanoff and Romanoff, 1949 as cited in Burlet and Vadehra, 1989) that helps maintain the central position of the yolk within the final egg(Welty and Baptista, 1982), and also helps keep the ovum DNA (the blastodisc) above the fat and protein layers of the rest of the yolk.

The chalaza contain antihemagglutinin, which indicates the presence of ovomucin- an antibacterial enzyme that reduces the chance of microbial proteases from entering the yolk (Sugihara et al, 1955 as cited in Burlet and Vadehra, 1989).
Following these additions, between 18 and 30 minuets later in the fowl (Welty and Baptista, 1988), the ovum travels by peristalsis or cilia movement to the magnum.

Magnum
The definition where the infundibulum joins the magnum is difficult to find.

The magnum is a long coiled tube (about 34cm in a fowl (King and McLelland, 1975)), with a higher number of folds increasing the quantity of tubular glands, which occupy 90% of the cells of the oviduct (Burlet and Vadehra, 1989) and secrete larger granules than those of the infundibulum.
There are mainly two types of tubular gland cells in this region- A and B. Type A gland cells produce ovalbumin- the most common protein found in the albumen- from amino acids that are removed from the blood supply.

Type B synthesises the other proteins, and carbohydrates (Wyburn et al, 1970 as cited in Burlet and Vadehra, 1989).
Albumen is about 88.5% water, and 11.5% solids, which are mostly made up of proteins (91%) (Burlet and Vadehra, 1989).

The albumen is stored in the tubular glands at ovulation until the ovum passes into the magnum.
Due to the high quantity of albumen in the magnum tissue, finding the lumen is almost impossible.

As the ovum is pushed from the infundibulum, the light pressure causes the release of albumen into the lumen, and around the ovum.
The albumen provides two main functions- to provide the growing embryo with proteins and nutrients, as well as water; and to also protect against bacterial infections to the yolk.

The albumen is secreted all at once as a jelly substance.
Once the final egg has been laid, the albumen has settled into 4 distinct layers.

The outer thin and thick, the inner thin and inner thick (which was produced in the infundibulum).
The albumen secreting process takes about 3 hours, after which the egg passes onto the isthmus.

The infundibulum and magnum regions reduce by 56% in zebra finches, once the ovum has passed through them (Williams and Ames, 2003).
Isthmus

The 10cm long, narrow isthmus is highly distinguished from the magnum by a 'translucent band of tissue' (Williams and Ames, 2003).
There is less folding of the oviduct walls, although different from the magnum, there are secondary folds present.

This region of the oviduct produces the shell membranes around the albumen, before calcification occurs.
The ovum will stay here for around an 75 minuets (Pesek, 1999), while tubular gland cells, similar to those of the previous regions- except that they contain sulphur containing amino acids- secrete the fibrous keratin-based membranes.

Before the first membrane- egg membrane- is laid down, about 10% more protein is added to the albumen.
Another layer of keratin is laid down onto of the egg membrane further down the isthmus, allowing for an air gap to be left at the 'blunt end' of the egg.

This will occur when the albumen contracts as the growing embryo becomes larger in size.
The shell membrane is approximately three times as thick as the first membrane- both of which are permeable to gases and water vapour, allowing the chick to breathe.

The egg then passes to the uterus (otherwise known as the shell gland).
Once the last egg has passed through the isthmus, the region regress by approximately 38% in zebra finches (Williams and Ames, 2003).

Uterus
There is a characteristic 4cm long 'red region' at the proximal isthmus and uterus (Williams and Ames, 2003).

This is the region of the oviduct is where the start calcification of the egg occurs.
Calcification is the longest process compared to the other regions, and it takes between 18 and 20 hours for the egg to become fully calcified and pigmented (depending on avian species) (Welty and Baptista, 1982).

The shell is the first line of defence against a predator, whether and animal or a microscopic organism.
Tubular glands from the distal region of the uterus secrete a watery solution, which travels up towards the red region.

The solution contains salts and carbonates.
This dilutes the albumen, increasing its size to the required amount.

This method is known as 'plumping' and takes around 15-18 minuets.
Once this stage has finished calcium carbonate ions are deposited slowly onto the membrane of the egg, now located in the main uterus (15-30 min later).

It is a very slow process, but accelerates 12-18 hours later (Clunies et al, 1993 as cited by Reynolds, 1997).
The presence of the protein osteopontin on the surface of the shell membrane signifies where mineralization should not occur (Fernandez et al.

2003).
The main sources or calcium are from the bird's blood supply, which it must replenish daily.

Hurwitz and Griminger, 1962 recommend that an intake of 3g of calcium should provide enough minerals to sustain the production of eggs, and its own health.
Laying birds retain more calcium than non-laying birds, which the latter expels with its excrement.

40% of calcium for the calcification in the zebra finch comes from its medullary bone (Reynolds, 1997).
As the glands secrete the ions, the calcium crystallises into the shell membrane forming vertical crystalline structures.

The whole surface of the egg is not completely covered with the final shell due to the osteopontin- pores are formed that allows gases and water vapour to diffuse in and out of the egg.
There are approximately 6,000-17,000 single pores covering the domesticated fowl egg (Welty and Baptista, 1982).

As the eggshell is calcifying, the epithelial cells surrounding the inside of the uterus store-up pigments originating form bile and blood pigments.
The secretion of pigments onto the egg may be a reason to remove waste products from the body, due to that once laid, eggs do not posses any camouflage, making them easy prey.

In the last three hours before oviposition (egg laying), the pigments are released (Butcher and Miles, 2003) along with another fluid called the cuticle that lines the outside of the final egg.
Thomson, 1923 (as cited in Welty and Baptista, 1982) describes that the different speckles, streaks, and colourings depend on the motion of the egg when pigments are added.

After the pigments have been added, 25 hours after entering the uterus, the egg passes into the vagina.
Vagina

The egg can pass very quickly through this 8cm long s-shaped region (King and McLelland, 1975).
There are relatively little folds of the oviduct wall, and contains little amount of tubular glands, that can aid the secretion of the cuticle.

However these glands tend to be located towards the sphincter that divides the uterus from the vagina.
In this area are tubular vaginal fossulae that store spermatozoa, which can be released up to 10 days after entering the female, to fertilise the ova at the correct time (Burlet and Vadehra, 1989).

Once the egg passes through the vagina, oviposition occurs.
The female has direct control over when she wants to lay her eggs, and can rotate the eggs inside the vagina using the smooth muscle due to that the egg passes through the oviduct sharper end first, but are laid blunt end first (Radfiels, 1951 as cited in Burlet and Vadehra, 1989).

The eggs, once finally released through the cloaca- 20-25 hours later, need to be looked after for an even longer period of time by the parent for the embryo to survive.
Sugarcane is a typical example of a C 4 photosynthetic plant- that is, the plant fixes carbon dioxide from the air into a four-carbon sugar, instead of a C 3 sugar- that occurs in approximately 99.6% of flowing plants.

In a change to use C 4 fixation, the plants have evolved a different set of stages to C 3 photosynthesis- that overall makes photosynthesis a more efficient system for sugar production.
In plants, the major photosynthetic organ is the leaf.

For photosynthesis to occur, there needs to be a supply of carbon dioxide, water, and light energy.
A C 3 dicotyledonous leaf (TS, shown in Fig 1) is designed to get the most of these materials.

At the top of the diagram is a layer of simple, unspecialised (no chloroplasts present), flattened cells called epidermal cells.
These cells secrete cutin, which is a polymer of fatty acids, with cross-links that form a network that is embedded in a matrix of waxes.

Cutin is deposited to reduce water loss through transpiration, due to the waxy layer being impermeable to water.
The layer also prevents most pathogens entering the leaf.

Below the upper epidermis is the palisade mesophyll, which is the most important photosynthetic tissue of the plant.
The cells are tightly packed together, in a columnar arrangement.

Inside these cells there are chloroplasts, that are too, tightly packed together.
Using the cytoplasmic stream, the chloroplasts are able to move within the cell to get the most sunlight possible.

In high light intensities, and quality, the chloroplasts are evenly distributed around the cell, because they can all get the light needed.
However, when the light intensities are reduced, the chloroplasts move closer together, at the top of the palisade cell, to get the maximum possible light energy.

In most C 4 plants, the Kranz Anatomy is present (Fig 2).
The arrangement of the mesophyll cells are that the palisade cells are never more than two cells away from a bundle sheath cell that surrounds the vascular bundle.

This is because the first stage of photosynthesis is in the mesophyll cell, and these products passed to the bundle sheath cells through the plasmodesmata where it is converted to sugars and passed into the phloem.
The chloroplasts of the bundle sheath cells have far less grana, and longer thylakoid lamella.

The bundle sheath cells are also present in C 3 cells, but are smaller in size and just allow the products of photosynthesis from the mesophyll cells into the phloem.
The second major tissue for photosynthesis in C 3 plants is the spongy mesophyll tissue.

There are less chloroplast present in these cells, compared to palisade cells, due to that light intensities will be less because the cells are located towards the lower down in the structure of the leaf.
The arrangement of these cells is so that they create air spaces between them.

This is due to their irregular shape.
The intercellular air spaces are so that gaseous exchange can take place- carbon dioxide in, and oxygen out for photosynthesis.

The air spaces are also needed for water balance within the plant, and the transport of materials around the plant.
At the bottom of the leaf are the lower epidermal cells.

These are the same as the upper layer, except there are specialised cells located within them.
These are called guard cells of the stomata.

The opening of the stomata allows carbon dioxide, oxygen, and water vapour to all enter and leave the leaf.
The stomata have only a few chloroplasts present.

The Biochemistry Of PhotosynthesisLight- Dependent Reaction (Fig 3):
In both C 3 and C 4 plants, the chlorophyll molecules are arranged in the membranes of the thylakoids so that the haem group projects from the outer surface.

The groups are called light harvesting complexes.
At the base of the funnel like structure, is a Photosystem.

The chlorophyll molecules in the light-harvesting complex collect light energy and feed it to the chlorophyll molecules in the photosystem.
Once light shines onto the plant, the light harvesting complexes in the chloroplasts absorb the visible light corresponding to the wavelengths accessible, and give the energy to either photosystem I or II.

This quanta of energy excites the electrons in the photosystem, and instead of the electron immediately loosing its electron, and returning to its photosystem, it is captured by one of the electron acceptor.
This starts the conversion of light energy into chemical energy.

The loss of an electron causes the photosystem to become oxidised, and the gaining of an electron causes the acceptor to become reduced.
The electron starts to lose its energy, and therefore travels downhill in energy levels, passing between electron carriers, and every time releasing energy.

This process provides energy to join ADP with a phosphate group to form ATP.
In the non-cyclic pathway reduced nicotinamide adenine dinucleotide phosphate, NADPH 2 is also produced.

In the non-cyclic photophosphorylation the light shines onto the photosystems, and electron leave and reduce the acceptors that they join to.
PS II is neutralised from its oxidised state by 2e' from the photolysis of water (H 2O 2H+ + ½O 2 + 2e-).

PS I is neutralised by electrons from photosystem 2 that move downhill.
The energy from this causes ATP production.

The electrons lost from photosystem 1 pass down electron carriers to NADP and combine with H+ from the photolysis of water to create NADPH 2.
In cyclic photophosphorylation, electrons that were lost from PS1 are recycled back to its original photosystem via a chain of electron carriers.

This produces ATP but does not reduce NADP.
The ATP and NADPH 2 move out of the thylakoids membrane, into the stroma for the light independent reaction.

Light Independent Reaction:
The light independent reaction does not need the use of light directly, but needs ATP and NADPH + H+ to reduce carbon dioxide.

Carbon dioxide gas enters the plant through the stomata and into a mesophyll cell.
In C 4 plants (Fig 4.2), the gas is fixated to phosphoenolpyruvate (PEP) using PEP carboxylase to form a 4-carbon molecule- oxaloacetate.

The enzyme malate dehydrogenase then reduces the oxaloacetate molecule into malic acid.
The molecule then travels out of the stroma of the mesophyll chloroplast- and through plasmodesmata into the neighbouring bundle sheath cell's chloroplast.

Here, the malic acid reacts to produce CO 2 and pyruvate (as well as reducing NADPH+).
The pyruvate then exits the bundle sheath cell, and can be regenerated into PEP with energy from ATP.

The remaining CO 2 is then used in the same way as C 3 plants- in the Calvin Cycle (Fig 4.1).
The gas is the joined to an acceptor- ribulose bisphosphate RuBP.

This occurs using the enzyme RuBP carboxylase.
The product formed is a 6 carbon sugar, but is unstable and breaks down immediately to 2x 3-carbon 3-phosphoglycerate.

This is the first true product of photosynthesis.
The reducing power of NADPH + H+ and energy store of ATP causes an oxygen atom to be removed from the 3-phosphoglycerate.

This changes the molecule into Glyceraldehyde 3-phosphate.
This sugar contains more chemical energy than 3-phosphoglycerate, and is the first carbohydrate made.

For every 6 molecules of triose phosphate produced, 5 of them are used to regenerate the RuBP.
This involves a complex cycle, containing 3-7 carbon sugar phosphates.

The last of the ATP energy from the light dependant reaction is used to phosphorylate ribulose phosphate into RuBP.
The remaining triose phosphate is synthesised into its final products that can include starch, lipids, amino acids, and sugars.

The only difference between the Calvin cycle in C 3 and C 4 plants is the location.
It, along with the rest of photosynthesis takes place in the mesophyll chloroplasts, whereas the Calvin cycle takes place in the bundle sheath cell's chloroplast.

The reason why the Calvin cycle takes place in the bundle sheath cell is to prevent Photorespiration.
This occurs due to that RuBP carboxylase fixes O 2 as well as CO 2.

When O 2 fits in the active site of the RuBP carboxylase, the ribulose bisphosphate doesn't break down evenly into 2x 3-phosphoglcerate molecules but into one (where it will continue with the stages above).
The other molecule produced is phosphoglycolate.

This molecule cannot follow the stages, but instead has to travel to the cell's peroxisome, then mitochondrion to convert to a 3-phosphoglycerate.
However this process causes an overall loss of a CO 2 molecule.

This faulty mechanism due to RuBP carboxylase can cause a loss of 30-40% of carbohydrate that they should be producing if they are photorespiring.
The transfer of CO 2 into the bundle sheath cell separates the light-dependant reaction from the light-independent reaction- therefore no oxygen is produced in the bundle sheath cell- increasing the productivity of carbohydrates.

Also, the PEP carboxylase that is used to initially fixate the carbon dioxide from outside the chloroplast works very efficiently and is not inhibited by high oxygen concentrations, therefore a higher concentration of substrate (CO 2) is built up in the bundle sheath cell, making RuBP carboxylase more efficient also.
The high build up of the carbon dioxide usually stops the CO 2 concentration being the limiting factor.

Because sugarcane grows optimally in high light intensities, a high quantity of oxygen molecules is produced.
The plant therefore has an advantage of C 3 plants because its RuBP carboxylase is not being inhibited; therefore more sugars are being produced.

The C 4 system has an economical importance for the agricultural industry.
If C 4 photosynthesis and a Kranz anatomy were to be able to become incorporated into terrestrial C 3 plants that are grown in the warmer climates- a substantially higher biomass will be recorded, therefore decreasing the length of time before harvesting.

This could allow for extra crops to be grown (depending on the type of crop, nutrient levels, climate etc).
The C 4 system also shows the limitations towards C 3 photosynthesis, or mainly RuBP carboxylase.

If this enzyme were modified so it would not photorespire, the consequences may be similar to that noted above.
Part One- What is the literary and historical context of this passage?

Euripides' 'Suppliants' was written in the late 5 th century, speculatively dated at around 423 BC.
By this time the democratic state of Athens had gained great power in Greece as leader of the Delian league, and as a result was shortly to become involved in the Peloponnesian war.

This tragedy was produced as part of the Great Dionysia, held in March of each year.
The week-long Dionysia was a grand state-run religious festival that involved ceremonies, processions, and animal sacrifices as well as daily performances of plays.

'Suppliants' deals with themes such as war, divine interference, the importance of burial for the dead, and perhaps most intriguingly, democracy as a ruling style.
Part Two- What beneficial aspects of Athenian democracy does Theseus choose to mention in this speech?

How interesting is his choice?
The extract is from near the beginning of the play.

Prior to the Herald's entrance, the King of Argos, Adrastus arrived in Athens representing the families of the Argives who fought against Thebes for Polynices (the titular suppliants).
The city's new king Creon had refused the dead Argive warriors burial, prompting Adrastus to seek outside help.

After much debate Theseus was won over and proposed taking the dead from Thebes by force to the people of Athens, who quickly assent.
Theseus firstly explains the balance of power in a democracy; '...the poor man has an equal share in it.' He is quick to correct the herald, and proudly asserts this fundamental difference between democracy and monarchy (and also oligarchy), that every citizen has the right to a say in how Athens is run.

Here and later in the passage, Theseus' language stresses the importance of the concept of equality over all else, making it seem something a city must strive towards; 'One man has power...equality is not yet'.
By extolling the worthy ideals that underpin democracy, he aims to make Athens' model of government seem admirable, even enviable.

He goes on to link equality with the law, which once laid out gives rich and poor 'the same recourse to justice'.
He argues against the herald's argument that democracy is easily swayed by the self-serving, who evade notice as no single leader can be blamed.

'A man of means, if badly spoken of, will have no better standing than the weak'- the system's inherent equality means no one can seize enough power to abuse.
Theseus also depicts democracy as a natural progression from monarchy, which is made to seem primitive; 'In the earliest day, before the law is common...'.

To win the listener over to his side he finds fault with the second system.
He condemns monarchy, saying 'Nothing is worse for a city than an absolute ruler', who would 'make the law his own'.

It is implied that there is greater potential for wrongdoing by the powerful in systems other than democracy.
'The people reign, in annual succession', as opposed to a monarch abusing his power unquestioned for a long period of time.

However, Theseus notably doesn't elaborate on the actual institutions of democracy that facilitate the fairness and equality he so values.
He could have explained to the herald how poorer citizens were given financial aid so they could travel to the assembly, how officials were chosen by lottery and could only hold power for a year.

While he makes great reference to the importance of law in democracy, Theseus doesn't clarify just how the law courts are fair.
But this lack of detail is understandable given that the extract is from an emotional drama performed in a poetic style.

Endless facts would break the narrative, and Euripides wouldn't have had to explain democracy to 5 th century audiences.
It is also interesting that Theseus doesn't directly counter the Herald's argument against the poor having any say in a state's government.

He claims that a poor man is too ignorant to be capable of using power correctly, and has no right to it in any case.
Theseus' only comment is that any man with 'good advice to give the city' is free to do so.

These vehement allegations could perhaps be similar to those made by supporters of oligarchy or monarchy in the active political debate in Athens at time Euripides wrote 'Suppliants'.
Theseus also inexplicably misses the chance to pick a hole in the herald's argument, who states that Thebes 'is controlled by one man, not a mob' ('mob' here presumably again referring to people of lower social standing).

Until Creon's accession Thebes had been in the throes of a bloody civil war, and was in turmoil even before that because of Oedipus' downfall.
Theseus could perhaps have asked the herald to reassess just how well his town was really being 'controlled' by its unsettled monarchy.

Theseus concludes his speech with a classic showman's device, a rhetorical question; 'For the city, what can be more fair than that?'.
The modern reader might take issue with the character's definition of 'fair'.

He declines to mention that women, metics and slaves still had no vote in a democracy, and ignores completely Athens' other injustices, such as an absence of legal rights for women of all classes, and the slave trade itself.
And for all his grand talk of equality for all men in Athens, Theseus still demonstrates a snobbish sense of place, as shown by his attitude towards the other speaker- 'What a bombast from a herald!'.

Theseus' Athens seems an incredibly fair city to live in- if you were a male citizen.
Also of interest is Theseus' early statement 'the city is free, and ruled by no one man'.

But his version of democracy seems to operate in quite a different manner to that of Euripides' time.
In 5 th century Athens all the members of assembly made took decisions concerning Athens' well-being, aided by the council.

After Theseus' mother and Adrastus have persuaded him that battle is the best course of action, only then does he put this serious matter to the people he says all have equal decision-making power; 'The city gladly and willingly took up this task when they heard that I wished them to do so' D.Kovaks (1998: 53).
Just how democratic was Theseus' Athens?

Theseus' defence of democracy certainly outlines its main aspects in a favourable light.
He mentions its 'fair' system of votes and legal structure, and speaks proudly of its equality.

He strengthens his argument by slating monarchies and systems in which the undeserving have great power to abuse.
Yet his speech largely consists of vague statements rather than factual argument; 'The people reign, in annual succession'.

What purpose could Euripides have had in writing Theseus' dialogue in this way?
He surely did not intend this 'Suppliants' to be a discussion of the relative merits of systems of government.

The rest of the play elicits a more emotional reaction from its audience by its depiction of human suffering.
So although Euripides does seem to have intended to stimulate thought and debate among his audience by his insertion of a 5 th century political system into a mythological setting, he did not choose to examine the topic too deeply in this extract.

The epic poem, the Aeneid, is considered not only Virgil's greatest work, but perhaps the greatest of Rome's Augustan Age.
Written during the final period of Virgil's life, it follows the Trojan hero Aeneas, who overcomes many obstacles to found Rome and fulfil his destiny.

Based on the Iliad and Odyssey, the Aeneid draws many a flattering comparison with Augustus and Aeneas, and makes much of their shared descent from Venus.
Propertius, an upper class love-elegist was also writing at this time, sometimes considered subversive.

The extract, 1.1-8, is from a poem typical of his sophisticated style.
Both passages are concerned with love, namely its effect on people.

The first extract, from Book Four of the Aeneid, presents love as a destructive force.
Dido has fallen in love with Aeneas, a situation orchestrated by Venus.

The goddess, wishing to protect Aeneas from Juno's wrath, manipulates the Carthaginian queen mercilessly throughout Book Four.
Right from the start, the reader is aware that for Dido, the story cannot end happily, as Aeneas is destined to leave Carthage.

In Book One, Jupiter confirmed that Aeneas would succeed in founding Rome; a destiny that doesn't leave room for a Carthaginian consort.
Virgil uses syntax with negative connotations to make the transgressive nature of Dido's love very clear; it is described as a 'wound' and a 'fire biting within her', almost parasitic in the way it saps her life force.

Virgil pinpoints aspects of obsessive love; driven to distraction from her duties to her people, she is convinced this 'heaven-born' hero can do no wrong.
It is also one-sided at this point, as Aeneas' emotions towards Dido aren't mentioned at all in this extract.

Dido is aware of how ridiculous her situation is, that she loves 'this stranger' Aeneas, and has to watch herself fall victim to this madness.
Aside from the fact that Dido and Aeneas are simply not meant to be together, Virgil implies that love is a bad force overall.

Dido suffers under the power of 'love's disquiet', disturbed by haunting dreams, allowed no relief from this unwanted feeling day or night.
She is succumbing to, and being destroyed by what the Romans called 'furor', a state in which a person was controlled by their extreme emotions and lost their sense of rational thought ('ratio').

Furor was considered dangerous, and although Virgil presents emotions like rage as appropriate in the battles of later books, they must be balanced with ratio.
In Book Two, Venus has to step in to stop Aeneas from killing Helen in an angry fit, and remind him of his duties to his family and the city (pietas); 'Why this raging passion?...

Will you not first go and see where you have left your father?' So, for Dido to be in love is for her to be irrational, out of control, neglectful of her duty and consumed by passion.
This was exactly the type of behaviour Augustus did his best to discourage among Roman citizens, especially the women.

Aeneas' passion for Dido/submission to furor is depicted as nothing but wrong by Virgil.
It keeps him from his destiny, and he has to be twice reminded of it by Mercury, who chides him for 'idling your time away in the land of Libya' If left unchecked, this furor could have jeopardised the future of Rome.

It should however be remembered that Virgil's purpose in writing the Aeneid was not to present a view of love, romantic or otherwise, but to create a great Roman epic poem as propaganda for Augustus.
It constantly reminds the reader of the heroic origins of Rome, and frequently refers to Augustus' divine heritage.

Love is not a major theme or driving force in the poem, and only features prominently in Book Four, as a detour from the important matters of founding and protecting Rome.
The inclusion of such a negative, sinister portrayal of love in the extract succeeds in creating a sense of foreboding for the reader, and adds to the drama of Book Four.

Though the second extract is on a less epic scale than that of the Aeneid, it too depicts love as a powerful force.
The voice of the poem is, like Dido, lowered by love, 'trampled', made 'pitiable' and humiliated.

He personifies love, presenting it as a 'he', a figure with the might to reduce him.
The love-struck narrator is driven to a 'frenzy' and lives 'without taking thought', obsessed much as Dido was.

Yet in this extract, love isn't forced on the narrator by the gods, rather he is infected by its 'contagion'.
Much has been made of Propertius' subversive stance, who, as an upper-class Roman, was expected to be unmoved by love.

His use of extreme language and imagery highlights how atypical his situation is- making the poem all the more emotional.
He makes use of a feature of elegy, militia amoris, 'to capture with her eyes...' Interestingly, the one captured in this extract is a man; once under love's spell he is an abnormality in Roman society, a male citizen acting as slave to a woman.

Although the idea of servitium amoris was another common feature of love elegies, Propertius makes it clear that this 'perverse' state of affairs is disapproved of by the gods.
He himself doesn't seem happy with his helplessness.

While these ideas of slavery and infection don't coincide with today's idea of romance, we should consider what the Augustan idea of romance was.
Perhaps in an age of the 'stiff upper lip', the only romantic view was an illicit, violent, extreme one.

If this was so, Propertius' unpleasant, 'perverse' sense of being in love may have seemed romantic at the time.
Both Roman poets suggest that to be in love is to suffer, and be manipulated by forces out of your control.

In Dido's case, love is the means by which cruel gods get what they want, another hardship humans can do little to counteract.
Propertius is equally vulnerable to love's force, but presents it as an affliction with consequences contrary to Augustan social norms.

Although the love described here is not romantic in today's sense (i.e. a positive, life-affirming force), a clear sense of the darker side of love is presented by both extracts.
It cannot be disputed that Virgil's character Aeneas is a hero; portrayed as a brave, wise leader, he overcomes huge obstacles to found a new city, from which the great Roman Empire would later develop.

What sort of a hero he is however is an issue of debate.
Virgil chose his main character of the poem as Aeneas, a Trojan from a Greek epic.

While this gave his poem (written a great deal later than the works of Homer) a degree of credibility, it also left him with the problem of fitting a character with features of a Greek hero into a poem that was intended to be the defining Roman epic.
Virgil manages to use this conflict to his advantage, to advance the plot and develop Aeneas' character, giving the story an emotional depth and resonance.

The ways in which Aeneas the hero is characteristically Roman also reflect the underlying purpose of the Aeneid; to promote the greatness of Rome and Virgil's patron, the Emperor Augustus.
When attempting to interpret the Aeneid on any level, it is vital to first understand the circumstances under which it came to be written.

During Virgil's formative years Rome was in a state of unrest, which culminated in the assassination of Julius Caesar in 44 BC.
The period which followed saw Virgil's lands seized as a result of the disorder, and his pastoral poems, the Eclogues and Georgics display his love for the Roman countryside, and the pain the upheavals around him caused.

The accession of Octavian, later Augustus, in 29 BC, was a turning point; the victor of the Battle of Actium promised to build up Rome to greatness once more, and bring back the stability and prosperity of the past that Virgil so craved.
This, and the fact that Augustus became Virgil's patron, accounts somewhat for the nature of the Aeneid- it is an epic written to emphasise the heroic origins and past greatness of Rome, while also praising Augustus as the regenerator of the Empire.

In some places this praise is quite obvious (Book Six), but is mainly woven into the poem subtly by way of comparisons drawn between Aeneas the hero and Augustus the Emperor.
The Aenied was part of Augustus' long-term propaganda campaign in which he made every effort to present himself as the ultimate Roman hero.

The Res Gestae documents Augustus' selfless (or so he claimed) achievements at length- his victories in battle, huge financial contributions, gifts made to the gods- in short, the many ways he kept Rome safe and improved it during his lifetime.
These heroic deeds are easily comparable to those that Virgil describes Aeneas performing.

So it seems the study of Aeneas as a Roman hero is fairly convoluted- to discern how much of a 'Roman' hero he was, we must also look at his similarities to Augustus the hero.
For purposes of this essay, I shall make a few basic assumptions about Roman heroism.

Any Roman hero would have to adhere to the moral standards laid down by Augustus as part of his regeneration of Rome.
Some idea of these standards is given in Book Six of the Aeneid.

Adulterers, those causing civil disruption, and those committing crimes against their families were interred in Tartarus with figures from myth who had heinously offended the gods.
Not only would a Roman hero have to avoid transgressing this new moral code, he would also have to fulfil the other more traditional requirements of heroism, those seen in Homeric heroes e.g. prowess in war and leadership.

There are several characteristics of Roman heroes that Aeneas has from the start of the poem, and others that he develops as it progresses.
Although shared by Greek heroes, Aeneas' inherent prowess and courage in battle are certainly Roman attributes.

The Romans were, by definition, a fighting, conquering force, as declared by Anchises at the turning point of the poem, the parade of future Romans in Book Six.
He tells Aeneas, 'Your task, Roman, and do not forget it, will be to govern the peoples of the world in your empire...

to impose a settled pattern upon peace...
to war down the proud' So, to be Roman is to be a warrior, and Aeneas is certainly this, as shown by his prowess and ferocity in battle in the poem's later books, especially its final lines.

The vengeful and bitter ending to the poem seems to justify MacKay's thought that 'heroism, in the conventional sense, involves loss of humanity'.
The final sacrifice that Aeneas has to make in the long series of sacrifices required for the founding of Rome, is that of his mercy.

Driven to extreme rage by Turnus' treatment of Pallas' body, Aeneas loses any of the compassion he once had.
The immense toll of his responsibilities and fate is never more apparent.

Heroic and Roman though this fighting ability is, Virgil makes it clear early on that it must not be simply mindless, rather tempered with ratio.
Ratio, the Roman concept of clear thinking and good sense, is not a quality Aeneas has in great amounts in the first half of the Aeneid.

In Book 1, he is almost overwhelmed by the constant deluge of problems Juno forces on him; 'Groaning, he lifted his palms upwards to the stars and cried: 'Those whose fate it was to die beneath the high walls of Troy with their fathers looking down on them were many, many times more fortunate than I'.
Despite this inner distress, Aeneas is a natural leader and is still able to help his people, immediately taking charge and providing food for them when they land in Libya.

The prowess at hunting he demonstrates is an interesting Greek trait, and also comparable with Augustus' oft mentioned provisions of food for Rome in times of crisis.
At this early stage, we are already being presented with a 'dutiful Aeneas', with Roman traits.

To aid him in achieving his destiny, and his transformation, guidance from outside forces keeps him on track, and from giving in to despair.
Venus consistently aids her son, pointing him in the direction of safety in Carthage, protecting him from Juno and stepping in to prevent him making mistakes.

In Book Two's flashback to the fall of Troy, it is only her intervention that stops him taking the life of Helen in an almost catatonic state of rage.
Aeneas has to be pulled out of his 'ranting and raving' and reminded of his pietas.

This duty to his family, the people of Troy, and the gods is a heavy burden on Aeneas throughout the poem, and one he is not able to attend to fully at first.
His later move towards rationality and selflessness (see below) is markedly different to the attitudes of Greek heroes such as Achilles, who seek only glory through war, channelling their furor into gaining as much fame as possible before their inevitable early death.

So, despite the obvious need for heroes to be violent in battle, Aeneas' furor must be moderated, tempered with ratio.
Quinn goes as far as to say that 'it is only when there is scope for initiative that moral responsibility and moral judgement become significant." i.e. outside battle situations.

Aeneas' struggle with ratio is shown further in Book Four.
He is distracted from his duties during his stay in Carthage, and for a short while his emotions for Dido take precedence over the founding for Rome and his destiny.

To be more accurate, Venus and Juno manipulate the situation so that Aeneas is kept artificially in Carthage, in spite of his already decided fate.
Aeneas' behaviour after being reminded of what his purpose by Mercury is certainly more Roman than Greek.

His cold, almost emotionless speech to Dido utterly fails to 'humanely acknowledge and respond to the despair of a beloved person', rather gives her the bare facts, that fate compels him to leave, she is not his wife, and what he wants most of all is to be in Troy again.
Unlikeable though this makes him, this change in attitude is what is best for the Trojans and as Lyme notes, 'to be compassionate is irrational, unhelpful, even dangerous: it may divert you from reason'.

Aeneas' transformation from Greek to Roman hero is a gradual one, at least until roughly the middle of the poem.
During the first half of the Aeneid, he is not fully committed to achieving his destiny, rather going along with it under duress; 'It is not by my own will that I search for Italy'.

Fuhrer, having studied the scholarly opinion, states there is a general consensus that 'there is a change in the personality of Aeneas, from a lower degree of heroism to a higher one, in a sense of morality and inner maturity'.
This graduation to higher standards of morality (i.e. leaving Dido and not being a slave to lust/ love) is an indication of Aeneas' increasing 'Romanisation', for lack of a better word, as he comes closer to achieving his destiny.

There is some debate over when exactly the turning point for Aeneas' is, with Heinze naming it as Anchises' speech to him at the end of Book Five, and Howe instead suggesting 'a gradual change of character which is completed in the sixth book...
(after which point) he is no longer uncontrolled, uncertain and aimless in his actions, as he was earlier." In any case, from Book Seven onwards, Aeneas fully pursues his destiny, having accepted it, made his mistakes and been put right by the gods.

In summary, throughout the course of the Aeneid, Aeneas demonstrates several characteristics of a Roman hero.
He is a born leader, whose skills only improve with time, an accomplished warrior who learns to temper his fighting instinct with ratio, and puts aside the fruitless furor of his time with Dido, governing his emotions as a Stoic Roman would.

Aeneas becomes increasingly diligent in fulfilling his pietas, and truly suffers for the Trojans and future Romans.
Despite his early Greek attributes that were necessary for the plot, Virgil's Aeneas is drawn as a Roman hero- he founded the city, and as founding father certainly embodies the ideals Augustus was promoting when it was written.

Aeneas heroic deeds are inextricably linked with Augustan Rome; symbolically, the shield he uses in battle to found Rome is decorated with what Augustus presented as the defining event in Roman history, the Battle of Actium.
Aeneas is named a Roman by Anchises in Book Six before Rome even exists.

Perhaps Virgil was painting his poem's hero Aeneas, the first of all Romans, as the archetypal Roman.
In today's society, as in that of Ancient Greece, the name 'Helen of Troy' symbolises the epitome of female beauty.

One of the few female characters in the Iliad, an epic dominated by men and their concerns, namely war, Helen remains something of a mystery to us.
While the written sources agree that she was the most beautiful of all women, they contradict each other on almost everything else about her.

Her personality and the question of how much she is to blame for the Trojan war has been disputed throughout antiquity and the more recent past.
With so little about her known for certain, just how has Helen continued to fascinate western authors and audiences for so long?

To consider this question, I shall use classical sources in conjunction with the recent novel 'Troy', a reworking of the Iliad for older children and teenagers.
Spanning the final days of the siege of Troy, the book is written from the viewpoints of young Trojans, newly added characters, as well as the figures present in Homer.

Perhaps the continued interest in Helen can be attributed to the varied ways she is presented in Classical epic poetry.
Helen's first appearance in western literature is, unsurprisingly, in the Iliad.

Perhaps the bloodiest ancient epic, it stresses the immense human suffering that Paris' abduction of Helen caused.
Homer introduces us to her in Book Three, where despite the kindness of Priam and Hector, she is not seen positively by the Trojans; 'for all her beauty, let her go back in the ships, and not be left here a curse to us and our children' In Geras' Troy, Andromache expresses her feelings rather more strongly- 'I hate the war and that woman for starting it' That Helen has been repeatedly presented as an object of hate highlights one paradox surrounding her; despite her outward beauty, she brings misery to others, or, as Suzuki puts it, 'for some, Helen is like a goddess, for others a grief' The Helen of the Iliad is highly aware of the damage her desertion of Menelaus caused, calling herself a bitch several times during Book Six and behaving humbly towards Hector.

Yet she also blames Aphrodite for her sorrows and those of the Trojans, passionately attacking her when the goddess is compelling her to go to Paris; 'Why so eager to work this seduction on me?
Go sit by him yourself..." This gives the impression that Helen was coerced by the power of the gods to leave Menelaus, or at least regrets the mistakes of her youth.

Her disgust for Paris by this stage is also made clear; 'since the gods have decreed that these miseries must be so, then I wish I had been the wife of a better man than this' These speeches inspire sympathy for Helen in the reader, sympathy for a seemingly moral, passionate woman made a victim.
Her depiction in the Odyssey however is rather less appealing, yet still intriguing.

Appearing in Book Four, Helen is again living with Menelaus after the war, and acts as hostess to Telemachus.
She tells of how she was filled with joy at the Greek attack on Troy, though Menelaus disputes her tale with an accusation that she was still helping the Trojans right until the end.

She is further shown to be duplicitous by her dosing of the mens' drinks with a calming potion.
This suggestion that Helen, daughter of Zeus, had godlike, magical qualities, coupled with her 'almost supernatural ability to enchant and beguile' makes her all the more captivating.

Helen's character is also of significance in the Aeneid.
Aeneas describes the last night of Troy, and seeing Helen hiding in the palace, stressing not her beauty, but 'how she lurks and crouches', a 'portentous and sinister' depiction.

She is the subject of Aeneas' blind hatred, and he intends to avenge Priam's death on her, until Venus reminds him of his duties and the fact that 'the cruelty of the gods' is really to blame.
Aside from this brief appearance in the poem, Virgil transposes aspects of Helen's character onto other prominent female characters.

Dido and Lavinia are 'diametrically opposed to each other' yet both share qualities with Helen; Dido, manipulated by Venus, loves Aeneas and threatens the building of a new Troy, as Helen brought about the destruction of the old one, while Lavina is seized and controlled, fought over by two warriors.
From these Classical representations of Helen we get quite a negative view.

Indeed, 'from the Odyssey on, Helen is always presented...
either as an emblem of doubleness and duplicity on the one hand, or a trivial cardboard figure on the other- to be scapegoated and repudiated." Today however, in 'Troy' at least, Helen is starting to be portrayed as a more complex character.

The epic setting is imbued with modern concerns; that Helen and Paris' marriage is now loveless, his infidelity, and her grief at being separated from her child.
While these aspects of the story create more sympathy for Helen, her marked disloyalty to Troy as time goes on (as declared in the Odyssey) does not; 'It's not my city.

It never has been my city.
As far as I'm concerned the whole place can fall apart stone by stone' By the end of the book, many characters are dead, the remaining few engulfed in misery, manipulated to breaking point by the gods- the actions of the humans involved seem rather insignificant.

The few other women from antiquity to have sustained interest also have reputations as 'bad' women, and seemingly survived in collective memory because of their transgressions.
Cleopatra and Boudicca for example, show manly qualities, lead armies, scheme and die heroes' deaths.

Yet not only is Helen likely a mythic figure, from a much earlier period than these women, she is famed for 'feminine' qualities; her tragic beauty and helplessness compared to men, and conversely her scheming, deceptive nature.
Perhaps Helen's survival can be partly attributed to the dominance of Christianity over Western civilisation in the centuries after the classical period.

Some parallels between Eve and women from Greek myths can be drawn, for example Pandora, the first woman created being dangerous and an evil for men to deal with.
Helen, a sexualised, dangerous figure would likely also have been identified with Eve, due to her reputation for tempting and leading men astray, and thus 'kept alive'.

It is probably also worth noting that many of those who have analysed, interpreted and reworked sources on Helen over the years are men (e.g. Joseph of Exeter).
Hughes is of the opinion that for men, she's always been 'an archetype of beauty, of womanhood, of sex, of danger' so sexual fascination could have contributed to her continued presence in Western culture.

Added to this is the timeless fact that any object of male desire will generate jealousy, and a different kind of fascination among women.
'Troy' highlights this; 'her heart contracted whenever she looked at Helen, even though she knew it was fruitless to long for something you couldn't have, her beauty filled Andromache with envy." So, when dealing with absolutes, like the most beautiful woman in the world, a large degree of sustained interest can be expected.

'Helen of Troy, alluring in her multifaceted nature, has become an archetypal image...' Today we remember Helen of Troy as 'the face that launched a thousand ships', the stimulus or scapegoat for a horrific epic war.
The question of her guilt can perhaps never be resolved due to the varied presentations of her character in epic poetry.

Not only has this generated interest in her, it has also given authors a great deal of scope to interpret Helen to their desired effect.
Helen can be seen as one extreme or the other, powerful, or powerless because of her ultimate beauty.

This paradox has meant that throughout history, the opinions and desires of the time have been projected onto her, and she is by turn seen as manipulative whore and remorseful victim.
Added to this, Geras raises the point that stories of tragic romance are hugely appealing to audiences.

The idea of a love so strong it started an epic war seems to have timeless appeal; 'For love of Helen...
that's what the stories said.

More romantic, said the singer.
It's a poet's first duty to give the public what it wants.'

Annotated Bibliography
-Morford, M. P. O., Lenardon, R. J., 2007, Classical Mythology, Oxford University Press.

I used 'Classical Mythology' as a starting point for my essay, finding the basic information on Helen via the index.
Though I only used one quote from it, to open my conclusion, it was important in that it pointed me towards Helen's status as an archetype, an important theme in my essay.

-Suzuki, M., 1989, Metamorphoses of Helen- Authority, Difference and the Epic, Cornell University Press
I found this book the most useful of those I used for the essay, as it was concerned with the changing perception and representation of Helen through and beyond the Classical period.

It summarised and analysed the Classical depictions of Helen, highlighting in particular the paradoxes surrounding her.
I had to look elsewhere however for an understanding of her more recent literary interpretations, as the latest texts the author looked at were Shakespearean.

-Geras, A., 2000, Troy, Scholastic ltd, London.
As this novel reworked the Iliad for a modern, younger audience, I found it useful as an example of Helen's enduring popularity and continued reinterpretation.

It was especially good for emphasising the emotional reactions towards Helen from other characters.
-Hughes, B., 2005, Helen of Troy: goddess, princess, whore, Jonathan Cape, London.

The extracts of this book I was able to obtain were interesting, given the author's passionate, literary style, and provided useful quotes to underpin arguments on.
For example, I was able to illustrate the importance of the gender of those interpreting sources on Helen by the quote 'an archetype of beauty, of womanhood, of sex, of danger'.

-Hammond, M., (trans) 1987, The Iliad, Penguin London
As it was the first appearance of Helen in literature, I paid close attention to the Iliad's interpretation of her, and found it interesting alongside later interpretations.

-West, D,.
1990 (trans) The Aeneid, Penguin.

The Roman perspective of Helen was not greatly different to the Homeric one, of rather more interest was the fact that aspects of her character were incorporated into other female characters, demonstrating Helen's wide-reaching influence.
Today, some consider that 'there is no such thing as 'Greek religion''.

In 'On making sense of Greek religion' John Gould disputes the notion that we can simply 'make sense' of the evidence left- 'data which we put together and call 'Greek religion': the best we can hope for will be no more than piecemeal explanations of a particular ritual or a particular myth' We have only a fragmented picture of ancient Greek religious practices from the evidence of art and descriptive texts.
We have an even less precise idea of the reasoning behind Greek rituals- religious belief, as with other aspects of ancient cultures, leaves little physical evidence remaining.

We do know that much of Greek religion was secretive and an enigma to the uninitiated even at the time, with widespread secret cults and 'mysteries'.
It is also difficult to come to conclusions on 'Greek religion', as the area we now call Greece wasn't a unified country until long after the 5th century BC, rather an area of land on which separate city states co-existed.

Although it seems there were beliefs commonly held across Greece, we can't assume there was religious uniformity.
To compound this, unlike religions more familiar to us, there was a lack of Greek scripture or an 'organised body' to dictate standard practice.

Greek religion was 'an open, not a closed system', with potential for individual interpretation.
So it seems that the answer to the question 'Why did the Greeks perform sacrifice?' is unlikely to be simple.

This essay will trace the emergence of Greek sacrifice from prehistoric times to the 5 th century BC.
By examining and interpreting the evidence for sacrifice in ancient Greece, I hope to conjecture a few of the beliefs behind this ritual activity.

The ideas behind sacrifice can be traced back to prehistoric Greece.
By the Palaeolithic period, or even prior to it, mainland Greece was populated by nomads from central Asia, who lived in hunter gatherer societies.

These Indo-Europeans brought religious ideas that would endure to the Classical period.
Burket has put forth the idea that later ritual sacrifice was rooted in this early 'condition of man the hunter', when hunting and killing an animal was a spiritual, primal experience.

It also seems that these nomads placed religious significance on the sky, the only constant aspect of their landscape.
This seems a likely origin of the later belief that the gods dwelt high on Mount Olympus.

Etymological links abound too; the word 'Zeus' has its roots in the Old Indic word 'devah', with the names of other Olympians such as Hera, Ares and Poseidon 'formed form Indo European roots'.
By the Neolithic period, the early Greeks began to form settled farming communities, again influenced by the East, where farming was first discovered.

None of the domesticated animals and 'grains, barley and wheat' required for this development were native to Greece.
Perhaps it was at this stage that Greece's preoccupation with nature and fertility, seen in its art and mythology, emerged.

Now the Greeks could cultivate food and manipulate nature- yet the success of this was somewhat out of their control.
A good harvest relied on favourable weather conditions, natural forces they didn't understand.

When their farming efforts failed, they must have questioned the cause of their misfortunes, wondered what had such control over them.
The natural assumption to make was that a higher power was dictating matters.

How then to have some control of their own lives?
Burkert explains that, 'In distress and danger man seeks to find deliverance through a voluntary act of renunciation...

He seeks to master the uncertainties of the future by means of a self imposed 'if-then''.
If efforts were made to appease forces with power over them, the Greeks could hope to keep the gods benevolent rather than vengeful.

We must remember that in contrast to our modern view of deities as distant forces, to the Greeks, the gods had a direct influence over everyday life.
Right up to the Classical period they had potential for great good or harm, to raise pious, worthy mortals up as demi-gods themselves (e.g. Heracles), or destroy those who incurred their wrath (e.g. Euripides' Hippolytus).

Considering this, the concept of supplicating oneself and offering items to the gods seems logical.
'First Fruit' offerings involved the concept of 'ap-archai', beginnings taken from the whole, for the god comes first'.

Before consuming food, the first portion given to the gods, to the force which provided it.
The foodstuff was often destroyed (i.e. sacrificed) by way of transmission to the deity.

According to Plato, 'Socrates define(d) piety as 'knowledge of sacrificing and praying' and sacrificing as 'making gifts to the gods''.
To achieve the social norm of piety and avoid personal disaster, sacrifices were essential.

Although many sacrifices involved the death of a victim, (see below) there were other ways the Greeks made sacrificial gifts to the gods.
'A more lasting testimony than a sacrifice' when appealing to or thanking the gods was the dedication of a votive offering, such as a statue.

Girls coming of age gave their childhood toys to sanctuaries, and craftsmen dedicated 'the tools of their trade' on retirement.
Libations often occurred alongside sacrifices, while still significant in their own right.

The most dramatic sacrifices were those that involved the ritual killing of a victim.
The type of victim was specific to the deity concerned, with major Olympians receiving the 'most noble sacrificial animal', the bull, and lesser gods other common domesticated animals such as goats and sheep, although birds and fish were also used.

The bull's religious significance can be traced back to its appearances in Minoan and Mycenaean art.
At Knossos wall frescoes depicted bull leaping, and the 'horns of consecration' iconography is common in Cretan architecture.

The bull was of some unknown significance to these Greeks, a significance that endured in a different form to the Classical period.
Descriptions of sacrifice indicate a great wish to please the gods.

The animal received by the deity must be a 'perfect' specimen- anything less would be an insult or harbinger of doom.
Docile animals were preferred, as a struggling victim was a bad omen.

Much ceremony was involved in sacrifices, presumably to show the proper degree of respect to the gods.
Victims were garlanded, even gilded with gold, and led to the temple's altar in a procession, accompanied by music, as depicted on the Parthenon frieze of the Panathenaic festival.

Their throats were slit at the altar, the blood sprayed over the altar, the body then dismembered.
Internal organs and thigh bones wrapped in fat (as in Homeric texts) were placed on a fire to be burnt for the god, other edible parts eaten at the customary feast, as significant a part of the ritual as the actual killing.

However, there is some suggestion that by the 5 th century BC, sacrificial feasts had become more occasions for indulgence than holy rites.
In Menander's comedy, 'Dyskolos', a character attacks the 'scoundrels' who place on the fire 'the tail bone and bile, because they are inedible...then gulp down all the rest'.

Perhaps by this period, some did view sacrifice as a social event.
However, we must bear in mind that the above evidence is from a comic play, and may not be intended to be taken literally.

In some instances the animal was not in fact eaten, but entirely burnt, a 'holocaust' in which the whole of the offering was transmitted to the god.
Pausinias states that '...among the ancients custom ordained in regard to sacrifices that the victim on which an oath was made was not to be eaten by any man' This tells us that sacrifices were in fact taken seriously, used to seal important oaths, and that tradition was a major factors in shaping the religious practices in Greece.

Its rituals, including sacrifice, were dictated by what had gone before.
Indeed, when examining depictions of sacrifice from centuries prior to the Classical period, we see that 'almost all the elements of the later Greek sacrificial ritual seem already present." The Ayia Triada coffin, dated around 1450-1400 BC, has painted on one side a bull sacrifice just the same as that in 5th century Athenian ceremonies, with 'only the fire on the altar missing' An important function of sacrifice was providing entrails to soothsay by.

This is said to have helped Xenophon make decisions during his military campaigns in the 5 th century BC; '...since he could not decide he thought it best to consult the gods...
Xenophon offered sacrifice and the god signified clearly that he should not seek the command, not accept it if he were selected.

And so the matter ended.' Just as the early Greeks sought to make sense of their world, those from the Classical age still tried to read signs from the gods.
'Divinity, it seems, speaks to man, but in a language that he cannot understand'; sacrifices were one way the Greeks attempted to understand this language.

For grand festivities, or issues of supreme importance, it was believed that a great gift was required for the gods.
In the case of the Athenian Great Panathenaia, a great festival in Athene's honour, over 100 'sheep and cows are slaughtered at the great altar'.

However, in Greek mythology and literature, sometimes matters of dire importance required human sacrifices.
In Euripides' 'Iphigeniea at Aulis', the Greeks were prevented from sailing for the Trojan war until Iphigeniea, Agamemnon's own daughter was sacrificed.

There is also evidence for actual human sacrifice in Greek prehistory.
Several hundred administrative clay tablets (dated to the 14 th/13 th century BC) inscribed with the language Linear B have been found at Pylos.

One tablet is conspicuously messy, and appears hastily written, perhaps at a time of crisis such as an attack on Pylos.
It records gifts for Olympian deities, 'gold vessels and men and women'.

An unknown word in the text is suspected to mean 'victims'- it seems possible that human sacrifices were made to the gods by the people of Pylos during a disaster.
Disturbingly, physical evidence has also been found at Knossos; 'a deposit of children's bones with clear knife marks'.

Sacrifice was an integral aspect of Greek religious festivals, the Thesmophoria especially so.
The 'most widespread Greek festival', Burkert describes it as the 'principal form of the Demeter cult', the central aspect of which appears to have been sacrifice of piglets.

Demeter was the goddess of fertility and 'closely associated with crops'.
She is now widely believed to be a form of the 'earth mother' figure from Greek Prehistory, who had power over fertility, the suffix 'meter' meaning 'mother' Demeter was honoured widely by women, to ensure their personal fertility and that of the city's crops.

Mackenzie states that it was mainly 'as a provider of the food-supply that Demeter was addressed.' At Athens the Thesmophoria was conducted near the Pnyx, at the top of a hill.
Women alone were allowed in the area for the three day period of the festival.

While separated from their husbands and families, the wives of citizens (it is not known if women of lower status were allowed to participate) performed mysterious rituals, including the sacrifice of piglets.
In the myth of Demeter, which appears to have influenced much of the Thesmophoria's ceremony, her daughter Kore was abducted to the underworld by Hades.

Demeter's misery halted the growth of crops and brought starvation to Greece.
Only when Kore was returned to earth for half the year was the land's fertility restored.

To the Greeks, Demeter was a powerful goddess with control, ultimately, over life and death, and needed to be treated as such.
The principal of 'First Fruits' still stood; 'She was asked for gifts of cattle and corn and fruit, and bulls and cows were sacrificed to her' Bremmer has stated that only gods considered 'impure' and on the 'margin of social order' were given pig sacrifices, as pigs were considered cheap animals.

However, Burkert notes that when Kore was taken into the earth, 'the pigs of the swineherd Euboluleus were swallowed up as well'.
It seems more likely in this case that the sacrifice of pigs at the Thesmophoria was not a reflection on the perceived importance or purity of Demeter, rather an aspect of the original myth incorporated into the 5th century festival.

It was also usual for pigs to be sacrificed to gods associated with the earth, of which Demeter was one.
Some features of the Thesmophoria suggest that participants were re-enacting other such aspects and themes of the myth.

On the second day, the women would mourn as Demeter had, fasting and living in 'a primitive state' before feasting.
A contemporary account in Burkert's book describes the sacrificed pigs or piglets being thrown down into a cave, and then brought back to the surface again, as Kore was.

The decayed remains were then honoured at altars, perhaps signifying rebirth and the return of the crops.
Burkert's source claims it was believed that '...whoever takes of this (the pig remains) and scatters it with seed on the ground will have a good harvest'.

Although festivals were special periods during which normal standards of living were suspended, the continued performance of activities at certain times of year created a sense of stability.
In some cases these seasonal sacrifices, processions and rites had persisted since the Stone Age.

According to J. Gould, making offerings to the gods was 'the central ritual of Greek Religion...
and its most characteristic form animal sacrifice." Though overall the evidence is patchy, we can state that the Greeks believed in a number of all-powerful deities that were potentially harmful to them.

Repetitive honouring of the gods via offerings and sacrifice was intended to influence their own circumstances, and avoid misfortunes interpreted as divine anger.
Certain significant aspects of sacrifices derived from prehistoric ideas, such as the perceived nobility of bulls as victims.

Sacrifices were closely connected with the old themes of nature and fertility, as shown by the Thesmophoria.
Bremmer points out that Greek religion was an embedded aspect of society, rather than an entity separate from the concerns of state as it is largely today.

Sacrifices were not only one's religious duty, but part of Greek culture, and had been for centuries.
So another significant reason for Greek's sacrificing was that Greeks before them had.

Ancient Greek religion was 'a system of explanation and response', and though to us sacrifice seems a strange response to the world, it appears to have made perfect sense to the Greeks.
The 5 th to 8 th century was a formative, transitional period for Western Europe.

The collapse of the Roman Empire's power structure, in the 5 th century, was caused by internal pressures, but also by the waves of 'Angles, Saxons, and Jutes..." that attacked the Empire on every front.
These invading groups were to be instrumental in the development of many of Europe's modern languages and borders, yet 'documentary evidence about even the most important people in early medieval society...

is at best sparse and frequently non-existent'.
So, to understand the changes taking place, and the people living at this time, we must look to archaeological evidence, explicitly the 'particularly rich' burials of the period, which are our main source for material culture.

For example, in the Alemannic region of south west Germany, there are currently 'only a few fragmentarily excavated settlements', while vast numbers of graves with 'hundreds of thousands of grave goods' are available for study.
Much study has been devoted to the changes in burial ritual in different areas of Europe during the period, with the aim of inferring information on many topics, including social structures, population movement, and religious belief.

This essay will examine the burial rites and discuss their usefulness in drawing conclusions about early medieval society.
There are a few British historical sources that comment on the period of confusion after the Romans abandoned the country, and hint at the situation in Europe.

It seems that as early as the 3 rd century, neighbouring tribes were raiding and encroaching on Roman borders.
Inconveniently for us, 'the end of roman Britain and the beginning of Anglo-Saxon England were not directly consecutive or datable events'.

Gildas' 'De Excidio et Conquestu Britanniae' names the Picts as the worst threat to the Romano-Britons, who actually employed Saxons from the continent as protection, and even gave them land to settle on.
Gildas claims that soon eastern England was overrun by a threatening number of Saxons, who eventually banded together with the Picts.

This seems to be the start of a period of Saxon dominance in Britain.
While the document must be used carefully due to its 'demonstrable errors...(and) derivation from oral tradition', it is roughly contemporary to the period in question, and one of the only written sources for it.

In conjunction with consideration of the archaeological evidence, it is likely to prove useful.
The political and cultural changes taking place in Europe are reflected in the differences between 4 th and 5 th century burials.

In the years of the late empire, inhumation, sometimes with vessels, was the common Roman burial practice, with 'grave goods becoming increasingly rare' However, excavations of remains interred after the end of Roman rule across Europe produced evidence of a variety of new funerary rites being practiced simultaneously, notably inhumations with extravagant assemblages of grave goods.
5th century burials containing goods of similarly high status have been found in Britain, northern Gaul and southern Germany.

Across Europe, furnished burials shared common orientation (west-east) and a lack of standardised grave shape.
Appearing earliest in Gaul (in Roman cemeteries at first), such burials often included gender-specific goods, jewellery with women and weaponry with men.

The type, style and distribution of these artefacts however varied by region.
In Britain, the weapons buried were almost exclusively spears and shields, and generally more females were buried with goods than men.

By contrast, in northern Gaul swords were favoured heavily, and there was a bias towards burying men with goods.
The influence that European societies had on Britons can be demonstrated by Lucy/Reynolds' observation that 'different areas of the country can be seen to adopt varying facets of a broadly north European/ Scandinavian burial practice..." The appearance in British graves of Scandinavian cruciform and square style brooches demonstrates communication between groups in different areas of Europe at this time.

The influence these groups had on each other is implied by the fact that simultaneous to the adoption of lavish burials, urned cremations, (also with grave goods) like those in north east Germany were too being taken up again by Britons.
These changes can perhaps be explained by a direct transfer of religious ideas, that 'where they settled the Anglo-Saxon conquerors followed the religious practices they had observed in their homelands' That these practices were influenced by those of their European neighbours in the first place is possibly indicated by the frequent parallels between Anglo-Saxon grave goods and practices with those from many areas of Europe.

Early Anglo-Saxon burials were nothing if not varied, as demonstrated by evidence of other, more unusual burial practice dating to this period.
Charring inside graves appears to indicate a process of 'half cremation'; 'a fire was lit in the grave before an inhumation took place', perhaps as a form of ritual purification.

Perhaps more noticeably, large burial monuments for the elite were being built from the 5th to 7th century, signifying a substantial change in how social stratification was represented in death practices from the Roman period.
Reynolds and Lucy cite barrows such as Sutton Hoo as evidence of 'the increasing distance in ideological terms between ruler and subject' Anglo-Saxons seem to have perceived a need for this distance to be made clear, and used their burial rituals to do so.

The idea of multiple inhumations in the same grave however (there are repeated instances of double, even treble burials across the continent), did not appear to travel to Britain during this period.
The 'susceptibility to frequent change' of mortuary practices is demonstrated by another shift in burial rites; 'a virtual disappearance of the objects with human burials during the course of the late 7th and early 8th centuries..." This is generally attributed to the rise of Christianity, as it was known to have spread back to Britain by this time.

Instead of barrows, crosses and even churches were adopted as above-ground burial markers.
The burials were often located in churchyards, close to settlements in contrast to the earlier Anglo-Saxon burials in liminal areas, on boundaries between territories.

Adoption of Christian views of the afterlife (that a person's body had to be in tact so it could be resurrected on the Day of Judgement), are almost certainly the reason for the abrupt discontinuation of cremation.
Furnished burials seem to have been phased out more gradually, as goods were still appearing in 7th century 'final phase' Christian burials.

The 'significant ritual diversity' that characterises Anglo-Saxon burial customs is demonstrated by the famed site Sutton Hoo, which also shows several parallels between European and British burial rituals.
The 'group of low circular burial mounds' in Suffolk were first excavated in 1930's, and found to contain high status burials from the mid 6th/early 7th centuries.

The largest of the mounds contained the remains of a huge ship, and a very wealthy assemblage of grave goods.
Although not an especial source of burials with rich goods, Scandinavia is known for its 'lavish boat burials' (i.e. Valsgarde).

A purse containing Merovingian Gaulish coins was also found near the presumed body (no trace of which was found due to acidic soil conditions).
This raises the thorny question of the ethnic descent of those buried at Sutton Hoo; can we perhaps conclude they were Germanic Saxons, influenced in their burial practices by those of other Europeans?

Probably not, given that 'the idea that the artefact types themselves are significant in ethnic terms is one that is increasingly challenged." While Halsall, is absolutely certain that 'the introduction of cremation into lowland Britain was the result of migration from northern Germany', due to the conspicuous similarity of the rites in both regions, he is reluctant to use grave goods as evidence of ethnicity.
He suggests that ethnogenesis (the creation of 'new ethnic identities') was widespread in the early middle ages, arguing that there would have been obvious benefits for local people who imitated the high-status newcomers to their area who 'dominated post-Roman society' Others have also observed that it would be irresponsible and closed minded to assume that, for example 'the wearing of Germanic brooches was exclusively confined to people of Germanic origin' Perhaps the most we can assume in this case without further investigation is that trade contacts with Gaul were developed, or perhaps sustained from the past during the 6th-7th centuries.

Evidence of influence from several areas of Europe is implied by the other burials of Sutton Hoo.
Mounds 2, 3 and 4 contained, among other things, remains of cremated bones of humans and a horse; while a few horse burials exist from north Gaul, they were frequent in Germany, with animals buried too, like deer.

Finally, perhaps some religious significance can be inferred from the ship's burial assemblage, which has been described as containing '...possessions that reflected every aspect of his (the deceased's) life in order to provide for his needs in the afterworld' This could imply a pagan belief in East Anglia at this time, contrasting as it does with the stark Christian burials of the late Romano-Britons, and equally stark later medieval ones.
Yet inferring religious belief from funerary practices is never simple; spoons inscribed with 'Saul' and 'Paul', names with an 'ultimately Christian significance' were part of the 'pagan' assemblage.

Adding to the confusion is the consideration that while the spoons are 'probably symbolic of someone's conversion to Christianity' it cannot be proved that the deceased was that person.
This resurgence of certain funerary rituals in Britain, sometimes subsequent to such resurgences in other parts of Europe, combined with the common styles of artefact imply that Britain was being influenced heavily by outside communities, as covered above.

Though it is certain that there was an influx of Saxons to Britain, it has been debated whether large-scale population movement in Europe was the cause of 'sudden and drastic (cultural) change from Romano-Britain to Anglo-Saxon Britain' Traditionally, it has been assumed that there were large scale migrations into Britain from the continent, while others (see the 'New Archaeology' of the 1960's and 70's) have put forth the idea that 'the adoption of new cultures could occur by trade or the influx of a small ruling elite with minimal or no impact on the gene pool' Recent scientific developments have made it possible to test both theories by means of a genetic study.
That Y chromosomes from the current population of Central England were 'statistically indistinguishable' from samples from Friesia supports the idea of a mass-migration.

It was the conclusion of those conducting the study that 'the Anglo-Saxon cultural transition in Central England coincided with a mass immigration from the continent' 5 th to 8 th century burials across Europe have, and will continue to be an invaluable source on this period, and without written evidence, possibly the only ones we will ever have at our disposal.
The development/ readoption of a variety of rites after the end of Roman rule indicates the relative disorganisation of society, and the rise to power of separate groups in each region.

Europe and Britain shared several 'new' burial rites different to those of the preceding period, from which we can infer both communication, and also significant migrations between different areas.
That there were regional variations in the details of these rites also hints at a degree of individuality, and the future emergence of defined areas that would become the later states of medieval Europe.

The significant change in burial rites from the 7 th century is indicative of the religious developments throughout the period; from 'pagan' practices staunchly different to those of the Romans, to the prevalence rites demonstrative of Christian beliefs that would shape Europe in the coming centuries.
However, our interpretations of the burial evidence from this period of population movement en masse, are somewhat limited by blurred lines of ethnicity.

Without written records to define their (the Anglo-Saxons and other Europeans) definitions of ethnicity, we may never understand the significance of certain items found buried with human remains.
Aside from Indiana Jones-esque discoveries of treasure hoards, the aspect of archaeology that most captures public interest is surely contact with physical remains of people from the past.

Seeing the bones of a person who lived hundreds, even thousands of years ago, makes us feel a strong a connection with the past, and reminds us of our own mortality.
Archaeologists expend much time and effort on studying human remains, due to their significance in interpreting sites and drawing conclusions with far-reaching implications.

When remains have been deliberately disposed of by a society, they can perhaps tell us just as much about the living who performed the disposal as the deceased.
A body can inform us of society's attitude towards that person, members of their society once dead, and death as a concept.

Prior states that for all of us, the body is 'our most fundamental source of contact with death' and explains that it is 'not merely an object over which people vent their emotions but also one which is utilised to convey a representation of death' Bodies can be incredibly rich sources of evidence for many aspects of life in the past as well as death.
Pearson is of the opinion that human remains are 'most likely to reveal information about the life of an individual and not about their death." By examining how the body was disposed of (burial or cremation, inclusion or absence of grave goods, pathology and more) archaeologists are able to make inferences on social status, occupation, their position/role in a family, religious beliefs.

Such inferences as these, when clearly supported by the evidence, can lead archaeologists to make wider ranging inferences to do with the society the deceased belonged to; its social and economic structure, standards of behaviour, differing levels of wealth, population size and movement.
However, caution must be exercised when using interpreting human remains, as I shall explain during the course of this essay.

I chose the Eastern cemetery of Roman London as a case study, as the excavation (from 1983 to 1990) produced a large number of deliberately disposed human remains.
In use from the 1 st to 5 th centuries AD, burials and cremations were found, with many dating from the 3 rd/4 th centuries.

Located outside the Eastern boundary of the Roman city, the cemetery is so large (encompassing 12 sites, with upwards of 29 burial plots) that archaeologists have been hesitant to confirm that it is in fact a single cemetery.
Due to the scale of the excavation an in-depth analysis of its wide-ranging findings would be inappropriate.

Instead I will provide examples from the site of inferences archaeologists have made at the site.
The cemetery is a valuable source for death and funerary practices in Roman London, containing 'ample evidence for a variety of burial rites'.

The existence of evidence of cremation in the site's later contexts was of interest to the archaeologists, as it was 'thought to have been generally superseded by inhumation'.
Buried urns containing burnt fragments of bone were found across 7 of the 12 sites from which at least 92 individuals were identified.

Unurned cremation burials were also excavated, and debris from funeral pyres.
The rare discovery of an in situ bustum (Plot 3) is sinificant, given the 'general paucity of evidence for pyre sites from any period in which the practice of cremation was in use'.

Considering cremation's popularity, this absence of evidence for the actual process is surprising, yet pyres don't leave substantial deposits, rather 'very shallow evidence of their existence' The bustum is evidence of a complex funerary rite, in which a pyre was cremated over a grave pit, and filled by the resultant debris of charcoal, wood and bone.
Others are believed to have existed on site, but have not been preserved due to their delicacy.

Other pyre debris deposits were found, dumped leftovers containing only fragments of bone; it is presumed the majority of remains were removed from the pyre and buried, possibly in an urn.
These deposits were concentrated in two plots (21 and 28), which could imply rough contemporaneity, and that a group of people used certain burial rites in a separate area of the cemetery.

However, conclusions about burial practices should not be drawn too hastily.
Archaeologists involved with the cemetery explain that the site was one of the last to be excavated, 'by which time excavators had acquired the experience to recognise the significance of such deposits and the need to sample them'.

Prior to this, deposits weren't recorded as extensively, a discrepancy rendering it impossible to make developed inferences from their locations.
The poor survival of deposits in the first place only added to this problem.

One inference that was tentatively made related to possible aspects of funeral ceremonies.
Pottery fragments from the debris deposits were lacking in mortaria and storage jars, implying that food wasn't prepared or consumed for the cremation/burial ceremony.

Many amphorae however were present in the deposits, suggesting that ritual drinking did feature in ceremonies, in 'marked contrast with the kinds of rituals which appear to be represented later on in the cemeteries' Objects placed deliberately with human remains can be very useful for providing information on the person they were buried with, or indeed those who buried them.
One inhumation (site reference B291) had an extensive selection of accompanying goods, which were in this case useful sources on religion.

A wooden casket placed at a young woman's feet contained among other items, several ornamental intaglio were found, one made of coloured glass inscribed with the image of Mercury.
Myth tells us that he led souls to the underworld, and that his image was found in this grave suggests his importance to Romans in the area at the time of the woman's death (around the 4 th century).

Another carnelian intaglio, and also a coin depicts Sol, implying regeneration, while other coins show figures associated with the 'concept of victory over death' like Hercules and Mars.
The specialists cataloguing the goods noted that practical and recreational items (shoes and gaming dice) were also buried with the woman.

This, coupled with the appropriate religious imagery led to the conclusion that the deceased's relatives buried her with provisions for the afterlife.
In other instances, burial goods have been cited as evidence of ethnicity, another area of interest to archaeologists.

Distinctive belt fittings and a crossbow brooch similar to those categorised as foreign in origin elsewhere were found with a body perhaps implying the wearer was an immigrant from the continent.
Yet archaeologists must be considerate of possibilities other than that the wearer of a foreign object must be foreign.

It could symbolise a trader's connection with another country, or simply be a gift.
Other evidence such as the 'heterogeneity of burial rites across the cemetery' and skeletal similarities suggest that those buried there were largely of Romano-British descent, and were 'born, lived and died locally' The archaeologists analysing the cemetery call burial goods 'symbols that are unreadable now' They hesitate to use them as indicators of status, referencing one paradoxical burial, of a child that was more 'materially rich' than most others, yet whose bones showed evidence of disease and poor diet.

The fact that burials were ceremonial, religiously significant, atypical events perhaps suggests that the objects involved are too atypical of a dead person's life.
As Pearson attests, 'The dead do not bury themselves, but are treated and disposed of by the living' Who knows what reasons the living had for selecting, or omitting items to be buried with their loved one.

The bodies themselves are perhaps a more appropriate source than grave goods on the everyday activities of those buried in the Eastern cemetery.
Although the inhumations contained bones more intact than those of the cremation burials, both helped archaeologists to draw conclusions on Roman lifestyles in the area.

Yet the inhumations dated from the entire period of the cemetery's use, and for the archaeologists, 'phasing them within this time span proved difficult' The result was that all inhumations were taken as a single sample, and no comparisons between skeletal evidence of different periods could be made.
Truncation across the site and varying levels of detail in recording also rendered analysis of family groups extremely difficult.

However, the inhumations were informative on other topics of interest, firstly the appearance of the deceased.
The sample was just shorter than today's population, with average heights calculated for males and females similar to those at other Roman sites Dental analysis found that as with other Roman British populations, a small percentage had minor tooth decay, suggesting relatively good hygiene and a diet containing little sugar.

The high levels of wear on older teeth indicate consumption of coarse foods that ground the enamel down.
Only a small number of skeletons showed evidence of dietary deficiencies, with rare cases of rickets, stunted growth among the young and enamel hypoplasia due to dietary stress.

This suggests that the general population had a diet that was at least adequate, and two cases of gout imply that a small percentage were even consuming 'excessively rich' foods.
Whether this is an indication that the elite level of society was not very large would have to be investigated, possibly by further osteological analysis of the bodies in question, or by studying distribution patterns of luxury grave goods.

Stress and wear on bones can indicate which physical activities a person performed regularly and any trauma they have undergone.
Osteoarthritis was widespread throughout the inhumations, particularly on the vertebral column, shoulder and hip joints, possibly suggesting 'repetitive activity', likely manual labour.

A number of individuals in Plot 3 of the cemetery had 'bony changes consistent with repeated controlled sweeping movement of the arm' interpreted as resultant of either scything or sweeping.
An assortment of (often healed) fractures resulting from everyday activities and accidents were observed across the sample, with very few suggestions of violence- most damage to bones were caused by the 'degenerative complaints of old age' Despite the multitude of inferences that can be made from it, the study of human remains does have its limitations.

While in osteology lesions, breaks, tumours and infections can be studied, sometimes disease and trauma is only evident in soft tissue.
Another problem is that some inferences can only be made from bone in good condition, and as the destructive cremation process leaves bone fragments, cremation burials 'curtail pathological discussion' Pearson bemoans the fact that 'most ancient funerary rites seem to be archaeologically invisible, leaving no direct material trace." We must always contend with the fact that samples cannot be fully representative of populations and their practices.

The small numbers of women and especially infants buried in the Eastern cemetery compared to males is typical of Romano-British cemeteries, and possibly a result of differing, 'less-visible' burial customs for these groups.
The idea that infants were perceived as having lower status than adults in death was documented by contemporary sources, and backed up by their burial in cemeteries separate from those for grown members of society.

The fragile nature and poor preservation of immature bones also accounts for the infrequency of infant cremations found.
That only 27% of burials at the Eastern cemetery were excavated undisturbed, and 165 graves were missing human remains implies that we will never see a great percentage of the bodies once buried there.

It's steady use over several centuries resulted in much truncation and destruction of older contexts, 'notably inhumation graves cutting cremation burials'.
While a total of 642 bodies, both cremated and inhumed were studied, an estimate based on the cemetery's size placed its actual population between 13,536 and 180,480.

The deliberately disposed human remains in London's Eastern cemetery were used by archaeologists to make multiple inferences on typical subjects.
They acted as indications of funerary practices common to the area during the cemetery's period of use, with evidence of these practices being of use in reconstructing Roman attitudes towards death.

Inferences on the perceived nature of life after death, and what was religiously significant to the area's inhabitants were made possible by the burial goods placed with the deceased.
Where the body had not been partially destroyed by cremation, the remains were also indicative of the everyday lives of Roman Londoners, of social stratification, health and ethnic origin.

In the case of this Roman cemetery, the human remains have proved most valuable when looked at as a whole, as a sample within which comparisons can be made and conclusions drawn.
Despite the incomplete and perhaps misleading nature of the sample, the inferences made from it are nonetheless important for the interpretation of Roman London as a whole.

Methods of absolute (or chronometric) dating have developed greatly over recent years, with dendrochronology and radiocarbon dating in particular being used extensively by archaeologists.
While relative dating techniques (such as typology and frequency seriation) can be used to put objects and sites in a chronology relative to each other, we must look to other methods to provide specific calendar dates.

One such method is dendrochronology, the study of tree ring growth, and was first put to notable use by A.E.
Douglass in the 1930's.

Soon after William Libby made another pivotal discovery while examining unstable carbon isotopes- radiocarbon dating.
In this essay I hope to outline the actual processes involved in both methods, compare their merits and give examples of their use in dating sites.

I shall also examine the use of dendrochronology as 'a successful means of calibrating or correcting radiocarbon dates' (Renfrew, Bahn, 2000.
p135).

Renfrew and Bahn name radiocarbon dating the 'single most useful method of dating for the archaeologist' (Renfrew and Bahn, 2000.
p138) By measuring the number of carbon 14 atoms present in a sample of organic material, such as wood, bone or leather, its age can be determined.

The element carbon is absorbed constantly by living organisms as carbon dioxide, which is taken in during respiration or photosynthesis.
There are three isotopes of carbon, that is, atoms that have 'the same atomic number, but different atomic weights', (Bowman, S. 1990.

p10) carbon 12, 13 and 14.
Of the three, carbon 14 is the rarest in the atmosphere, and the only carbon isotope that is radioactively unstable.

Like other unstable isotopes, carbon 14 has a half life and so decays at a rate known to archaeologists.
An element's half life is the period of time it takes for half of the atoms present to decay away, which for carbon 14 Libby calculated as 5568 years, although more recently this was adjusted to 5730 years.

So, the amount of carbon 14 left in a sample indicates how long ago it ceased to live.
However, it is not the case that a sample can be analysed and a specific calendar date easily ascertained.

The presentation of radiocarbon results requires some explaining.
The level of carbon 14 in the atmosphere is not the same as that of the past, having been changed due to an carbon emissions since the industrial revolution and the use of nuclear weapons.

For this reason all results are standardised to the archaeological 'present', set at 1950, and presented as however many years before the present, or BP.
There is also always a margin of error attached to these dates, a standard deviation calculated by the archaeologist.

Factors such as contamination and decay of the sample, or inaccurate testing make it highly likely that the date provided is not exact.
The real figure can only be shown as lying between two dates; 2600 400 BP This result would indicate that the age of the sample lay within a period from 400 years before 2600 to 400 years after 2600, i.e. 2200 to 3000 years before the present.

With recent developments in radiocarbon dating, such as the use of AMS (Accelerator Mass Spectrometry) to detect carbon 14 atoms (Bowman, S. 1990.
p34), the technique has been put to use in a number of high profile archaeological investigations.

Radiocarbon dating of the Turin Shroud was possible in the 1980's because only a small sample size was required for AMS.
The cloth (made from plant fibres), which had previously been suspected of covering Jesus Christ's body, was found to actually date from the medieval period.

(Renfrew, Bahn, 2000.
p145.

Bowman, S. 1990.
p36) As well as the substantial margins of error attributed to radiocarbon results, the technique has other limitations.

Contamination of samples gives incorrect results; for example, if during excavation modern organic matter came into contact with a sample, when tested the number of carbon atoms present could have been altered.
The use of paper in the packaging and storage of samples too would render them of little use in radiocarbon dating.

Aitken goes against the views of Renfrew and Bahn, crediting dendrochronology with 'an accuracy far surpassing radiocarbon' (Aitken, M.J.
1990, p36) in dating sites; instead of a margin of several hundred years, samples from the Sweet Track have been dated to within a matter of months (Baillie, M.G.L.

1995 p17).
It is perhaps an easier concept to understand than that of radiocarbon.

The yearly growth of a tree is visible as rings of varying widths in the cross-section of its trunk.
This variation in width is caused by environmental factors such as temperature and rainfall, which can stunt or increase growth year by year.

These environmental conditions will have the same effect on the growth of trees of the same species in the same area, meaning that the same yearly pattern of rings is visible in their trunks.
This is highly useful to archaeologists, who are able to cross date tree remains from overlapping periods found in the same area.

Measurements of the exact size of each ring in a sample are made and recorded in a computer database, to which other samples can then be compared.
If roughly contemporary, these other samples would hopefully have rings matching those already on the database, and ideally would stretch even further back in time.

This adds to the database and forms the beginnings of a tree ring chronology.
With enough well preserved samples available in the area, and assuming the environmental conditions had a notable effect (but not a detrimental one, see bellow) on ring growth, different trunks can be linked up to establish, 'a year-by-year record of average...growth, for a particular region'.

(Baillie M.G.L.
1995.

p17) Once this chronology has been established, 'floating sequences', that is, undated ring patterns from sites can be compared to it and are often successfully dated.
Remarkably the geographical area within which samples can be compared can be quite large; a European oak chronology stretching back 7272 years has been achieved.

(Aitken, M.J.
1990.

p45) In the 1980's, scientists were able to link up the Belfast Palaeoecology Centre's Irish bog-oak chronology with German oak sequences by Huber and Hollstein.
This successful amalgamation of their two sequences was possible because both used oak samples, a species highly suitable for radiocarbon dating.

Oak has long been a commonly used building material across Europe, with a strong presence in the archaeological record.
Oak trees are sizeabe, enabling the large samples required to be taken, and even allowing for radiocarbon dates to be taken as well.

However, a substantial number of samples are needed to build a chronology in the first place, as oaks are short-lived.
The relatively low sensitivity of oak makes it 'less prone to abnormalities', (Aitken, M.J.

1990.
p45) abnormalities that can limit the success of dendrochronology as a dating method.

Aitken makes special mention of sensitive conifers such as pine and spruce as being problematic to analyse.
(Aitken, M.J.

1990.
p37-8) In years of especially harsh conditions for growth, conifers are prone to have missing rings, with other instances of double rings occurring.

He highlights the paradox presented to archaeologists; that in sensitive trees that react extremely to climate, growth patterns are very clear to see, yet 'there is highest risk of these abnormalities'.
(Aitken, M.J.

1990.
p38) However, one species of conifer, the bristle cone pine, has been of great importance to dendrochronology in North America.

The unusual environmental conditions in the Californian White Mountains has left the ancient wood there near-perfect for use in dendrochronology.
The high altitude and low temperature created an inhospitable environment for the micro organisms that would otherwise have speeded decay in the trunks, which were also extremely well preserved by their high resin content.

(Bowman, S. 1990.
p19) The pines were also highly suitable because of the great number of narrow rings in each trunk; samples with 100+ rings are preferred in dendrochronology to guarantee a unique sequence.

Dead and alive pines, some up to 4000 years old, were used to create a master chronology stretching back 6700 years.
Interestingly, this was validated by another from the area, and calibrated also by radiocarbon dates of the pines, as well as those of the 'lowland oak chronology in Europe' (Aitken, M.J.

1990. p44).
Suess used the bristle cone pine chronology in constructing the first calibration curve, which shall be discussed later.

Aside from irregularities present in the wood, more difficulties can arise just through sampling it.
As with all archaeology, sampling is destructive of the material.

Dendrochronology has especial potential for this destruction, given the importance of getting 'samples which run out to the bark surface' that indicate the all-important date of when it was felled.
Baillie cites the MC18 chronology that was constructed using late medieval painting boards, valuable objects in themselves (Baillie, M.GL.

1995.
p45); to avoid unnecessary damage, the technique of coring was used.

Even less destructive methods still have been utilised in dendrochronology.
Taking a photograph or mould of a tree ring sequence ('contact lifting') can be just as effective.

However, even delicate sampling methods are of no use with samples that have not been preserved well.
Wet environments have the best survival rates for ring samples, hence the use of bog oaks by the Belfast chronology.

And of course, if the species of wood being sampled doesn't have an existing chronology to be compared to, all archaeologists can judge is whether it is contemporary to any other ring sequences in the area, no calendar date can be ascertained.
Aitken cites a 'second radiocarbon revolution', the first being its invention, the second being its use in conjunction with tree ring analysis.

The aforementioned first calibration curve by Suess in the 1960's was a significant development in chronometric dating, that proved 'major discrepancies between radiocarbon age and calendar age' (Bowman, S. 1990. p17).
The older a sample is, the more inaccurate the radiocarbon dates are likely to be.

One of Libby's initial assumptions that he based radiocarbon dating on was that the percentage of carbon 14 in the earth's atmosphere has always remained the same- this turned out to be untrue.
As a result of this, radiocarbon dates from before 1000BC are erroneously young and need to be calibrated using a curve.

These curves are based on radiocarbon dates taken from individual tree rings, the dates of which have already been calculated.
There was a confusing 'proliferation prior to 1985' (Bowman, S. 1990.

p17) of more such curves.
Bryony Orme's case study of radiocarbon dates in the Somerset Levels demonstrates the importance of using two methods of chronometric dating when analysing a site.

It looks at Garvin's Track, from which conflicting sets of radiocarbon dates were obtained from brushwood samples, reproduced below.
The track forks off into two sections, West and East, which the archaeologists had reason to believe were built around the same time, but the radiocarbon dates are confusingly spread over a long period: By performing a tree-ring analysis of the samples, it was ascertained that the tracks were in fact contemporary; 'sometimes virtually identical patterns of growth' (Orme, B. 1982.

p18) were found.
The construction of Garvin's Track was subsequently placed within a narrower period, between 2470-2330bc.

Orme later notes that 'the two approaches to dating are therefore very complementary', with radiocarbon placing the context in a large timescale, and tree samples identifying it within a shorter period.
She also states that young tree samples are especially useful in proving 'exact contemporaneity of material' (Orme, B. 1982.

p20) in a way that radiocarbon is simply incapable of doing.
The use of both methods is also wise considering the limitations of dendrochronology.

Even after a sample of wood has been linked up to a master chronology and its date of felling ascertained, there is still no certainty that this is when it became part of the archaeological context.
As with the issue of heirloom artefacts giving a site or context an inaccurate early date, so a piece of wood may have been left to mature after felling, or have been reused in several buildings.

For this reason, brushwood rather than timber was used to date Garvin's track way.
It would not be of much use to label one of the discussed methods of chronometric dating as 'better' than the other, it may in fact be impossible to judge which that is.

Radiocarbon dating is invaluable in giving us a long term picture of human activity, and as the scientific techniques become ever more sophisticated, its range and accuracy can only be improved.
Dendrochronology can have an extremely high level of accuracy, informs us of human events on a short timescale, and can even go further by indicating an area's environmental conditions thousands of years ago.

Both methods have inevitable drawbacks, and are only helpful if used properly.
In conjunction, radiocarbon dating and dendrochronology have great potential for dating sites, and adding to our knowledge on the subject, as the work on the successful bristle cone pine chronology has shown.

Task One
The first part of this assignment looks at the idea of using corpora as an analytic tool.

Corpora can be defined according to Guy Cook as " a databank of language which has actually occurred, whether written spoken or a mixture of the two." However the following investigation concerns itself only with the use of an electronic online Corpus and therefore all the results are taken from written accounts.
The initial aim of the study is to look at the semantic prosody of a word or phrase and to determine whether it is positive or negative.

The first phrase used in the study was chosen as "set in" when meaning that something moves in, for example when 'depression set in'.
Therefore due to the way that the online corpus works the facility is unable to differentiate between different meanings of the same phrase and subsequently this needs to be done manually with each new set of results.

The first search was done using a database from The Times newspaper from March 1995 with,as aforementioned, the phrase "set in".
Of the 55 concordance lines which were produced only 9 lines related to the correct meaning of the phrase.

In each of these cases but one it was clear that the phrase had a negative prosody as the concordance lines dealt with, to name but a few 'panic', 'recession', 'infection' and 'tiredness', all words which are associated with negativity.
In line 26 the word reaction precedes the phrase "set in".

Standing alone the word 'reaction' can be said to have either a negative or a positive prosody however in this case once again it appears to have a negative prosody.
In order to extend this investigation it was decided that a number of different databases should be used to see whether this would alter the semantic prosody of the same phrase, these being a database of The Times newspaper from a month earlier in February 1995 and the other an extract from one of the Sherlock Holmes novels.

With the database from February 1995 a similar result was expected as that from The Times database of March 1995 as they are both of the same genre.
As predicted, of the 58 results only 6 of the results were used in the correct context and once again each of these had a negative prosody.

This was determined to be the case as more often than not stories of current interest in the news deal with events that have had a negative impact on the world.
It was due to this that it was therefore necessary to investigate whether changing the genre of the text would alter the way in which the phrase was being used, having taken into consideration the idea of poetic license it was altogether possible that in a fiction novel the writer may have used this particular phrase in a metaphor.

However the two results that were produced from doing a search of a database taken from the famous Sherlock Holmes novels once again were shown in a negative light.
However this was in a slightly different way than had been seen before in the newspaper extract.

This time the term "set in" had been used to describe the movement of weather, but yet it was simply describing, 'gales' and 'frost' both elements of weather which are associated with a cold and unpleasant climate and not for example 'sunshine'.
This lead the investigation to continue further, looking at ways in which subtle changes to the original phrase used would be able to produce a different semantic prosody.

As originally, the database from The Times in March 1995 was used.
Firstly the phrase was altered to "setting in" and this search only managed to produce one concordance line which didn't use the idea of "setting in", in terms of movement as required and therefore no conclusions could be drawn from this.

The last investigation again looked at a variance of "set in" which was "sets in".
There were four concordance lines found in the database from The Times in March 1995 which contained this phrase.

One was used in the specified manner and referred to the word 'disillusionment' which naturally is a word with a negative prosody.
In conclusion there are a number of things which can be learnt from this investigation.

Firstly we can see that the genre of text that we look at has very little bearing on the 'collocations' as described by Cook in Applied Linguistics as "frequent combinations of words" within a language.
In addition to this the study showed that small alterations to a phrase will not influence its collocation either as all it is simply doing is changing the tense of the phrase for example 'set in' is the past tense of 'sets in'.

Task two
The second section of the assignment looks at the idea of ' Discourse Analysis' which is described by Jan Renkema in Introduction to Discourse Studies on page one as "the discipline devoted to the investigation of the relationships between form and function in verbal communication." In this particular case it is the idea of persuasion within the media that is being investigated.

A piece found on the Times newspaper's website on Sunda y 26 th November 2006 entitled "Word on the street.....
they're listening" is of particular interest, it can be found in the appendix.

The article written by Steven Swinford and Nicola Smith looks at the idea of introducing CCTV cameras on the streets of London in order to intercept cases of crime before they happen.
The authors have used a number of linguistic tools in order to persuade the reader to engage in their means of thinking.

Firstly the use of Noun Phrases within the article is a prominent choice of the authors to strengthen the meaning of their message.
These phrases are combinations of words which very often go together and due to their familiarity evoke emotion in the reader.

A number of these can be seen in the article, two examples of which would be "growing concern", found in the fourth paragraph and secondly " further discussions" found in the eighth paragraph.
Another concept used by Swinford and Smith is that of dysphemism.

A dysphemism is that opposite of a euphemism and refers to the choice of an author's words to emphasise in this case a negative occurrence.
Renkema talks on page 256 of an Introduction to discourse of how "using milder words will result in a milder attitude" to explain why a euphemism or equally a dysphemism would be used.

Once again this linguistic technique can be seen in the piece.
The noun phrase "uncharted territory" has been purposely selected where perhaps the word 'new' could have been used as an alternative to uncharted.

The same can be seen later on where the word "warned" has been chosen in favour of 'explained' or again in the case of the use of "escalates" instead of 'grows' perhaps.
Further to this Swinford and Smith have chosen to use many attributive adjectives within the text in order to emphasise the meaning of the noun in a way that makes the reader perceive it as the writer had intended.

In this article the word 'aggressive' is used in the first paragraph to highlight the fact that the exchanges between those concerned are in fact of a violent nature.
This same technique is also used later on when the authors refer to 'subtle' changes in the noise so that audience is able to see exactly how the camera is working.

The use of quotations is also a prominent feature of the article in order to re-inforce what is being said.
By doing this the writer of an article is more likely to get the reader on their side as it no longer becomes simply the view of a journalist but instead reliable fact given by someone of considerable authority.

In this piece it can be seen that Swinford and Smith have taken quotes from Graeme Gerrad who reiterates their distaste for the ideas put forward in the article.
Not only have the writers very cleverly selected who to interview regarding their topic in relation to their particular point of view but they have also ensured that by using his position as 'chairman of the chief police officers' video and CCTV working group, that their article suddenly becomes more reliable and subsequently persuasive.

The methods mentioned above are only some of the ways in which a writer can incorporate linguistic techniques in their work, in order to create a piece that is persuasive.
It can also be seen that they are very successful in moulding the mind of the reader, as instantaneously when you read this article you have been lead to believe that the idea of CCTV installation is an invasion of privacy and that you should be sceptical over its benefits.

Task one
Figure 1 In this task the concept of the use of a variety of modes in the media to express a message is being investigated.

A television advertisement is an excellent source for an analysis of such an idea as they often incorporate a number of different uses of language be it visually or aurally.
The advertisement that has been chosen for analysis is the 2005 'Diet Cherry Vanilla Dr Pepper' campaign which is a particularly good example of a combination of the aforementioned modes.

As can be seen from the table in figure 1 this advert contain four elements of the use of language in the form of the picture, the voice over and finally the text.
They work together to convey the ideas of the producers and directors, making them easily accessible to the audience without much conscious thought.

The main concept of the advert is a young beautiful and successful couple are going out to a stylish restaurant on a date.
There are a number of connatations which highlight this for the viewer, for example the choice of wardrobe, the young lady is wearing a fashionable top and the man is dressed in a casual but smart shirt and jacket.

This could be said to refer to the idea that although Dr Pepper has elements of class it is still a fun drink and not too conservative.
The scene seems to be perfect, the young man talks of his successful job in a hospital, which is not only well paid but shows he is sensitive and caring towards children, all of which would fill all the obvious clichés of attraction the woman should be having.

However it is here that the plot turns and irony sets in as instead of being totally consumed with this 'dreamy hunk' the woman takes one sip of the Dr Pepper and all around becomes irrelevant.
It is at this point that 'Mahna Mahna' by the Muppets begins to play, which is a successful choice of music as the lyrics of the song are non sensical and it shows that woman is no longer making any sense of the real world all due to this amazing drink.

The advert then cuts to a an ice cream cone surrounded by falling cherries, which evidently is a representation of the flavours of the drink however when the cherry lands on the top of the scoop of ice cream it could be said this image refers to the saying ' with a cherry on top' which is synonymous with the idea of going that extra last step in order to reach perfection.
The last use of a mode in language is demonstrated by the use of text in the penultimate scene where the slogan ' tastes so good you'll get lost in it appears behind the glass of Dr Pepper'.

It is the last thing you see and it is a catchy slogan which will stick in your head but moreover it sums up all the ideas which as a viewer have been conveyed to you subconsciously throughout the duration of the advert.
Task 2

The aim of this part of the assignment is to investigate the opinions of three informants who do not have a background in linguistics on a topic which has been researched in linguistics.
It is therefore the intention to use the finding to see how closely the ideas conveyed by these informants relate to theories which have been previously developed.

This particular study concerns itself with the concept of dialect which according to Matthews in the Oxford Concise Dictionary of Linguistics refers to "Any distinct variety of a language, especially one spoken in a specific part of a country." However it is not only the idea of regional dialect which will be investigated in this study, it is also the intention to analyse the ideas of the affects of racial and social background on the development of dialects and lastly the role of dialects in the education system.
In light of this, the questionnaire used in this study is comprised of three main sections each of which consisted of a main question and then a number of sub questions which were posed according to the each informant's responses to the main question.

A complete set of the answers can be found in the appendix.
The formation of the questions themselves was in its own right an integral part of the success of the questionnaire and to ultimately receiving comprehensible answers from the informants.

This was especially important as those who were being interviewed did not have a background in linguistic studies and so it was vital that the use of subject specific terminology was kept to a minimum.
Another factor which played a major role in the planning of the questions was the idea of structure as it was important that each of the questions was an open question allowing for the subject to be able to elaborate their answers as opposed to simply giving one word answers.

This was particularly paramount as the aim of the study was not to see how far people agreed with a study but to gather their personal opinions and to subsequently see whether they correlated to a particular theory or piece of research.
Having said this even with careful planning some problems were encountered during the conducting of each of the interviews, the first issue being with the comprehension of the concept of the word dialect as opposed to accent which can be easily confused.

It was necessary on occasion to explain that the study was concerned with the words which were used in the language and not they way that they were being pronounced.
The first question asked " If you encounter someone who has a regional dialect different to your own, how does this affect your ability to communicate with them?.

This question was answered by all parties with little hesitation, however two of the informants were vague with their answer as they felt that it was different depending on which dialect was being spoken.
On the other hand another informant gave an answer which was expected, they said "I suppose you have to listen harder and concentrate more," which indicates that they have acknowledged that there is to an extent an amount of physical inconvenience incurred when communicating using different regional dialects which is a concern for English in society.

As the informants expressed a degree of varying attitudes towards different regional dialects it was then necessary to ask them which particular dialects if any they found difficult to understand and it was seen here that the 'Geordie' or Newcastle dialect was one which two of the informants found difficult to understand.
Synonymously with it's reputation of strange expressions, one informant pointed out their choice of words often made it difficult to understand the conversation as to them they were unrecognisable.

It was however stated that the 'general jist' could still be had.
This is particularly interesting as it clearly demonstrates the idea of mutual intelligibility which is the idea of " dialects which can be understood, or understood sufficiently for ordinary purposes", Oxford Concise Dictionary of Linguistics.

The second section looked at the concept of social and racial dialect asking "In what ways do you feel social class and or racial background affects dialect?" The overwhelming response indicated that the informants believed social class had a strong influence over dialect.
They felt that someone from a lower class was more likely to be speak using a dialect as opposed to standardised English.

They also all found it difficult to link racial background with a distinct dialect, concluding that it would be the fact that the person had grown up in an area of a lower social class that was the determining factor, whether or not it was in a community of this same racial background.
The last section looks at the way dialects play a role in education.

Firstly the informants were posed the question " What are your feelings towards the way being taught by a teacher with a regional dialect affects a child's development of English?
" The general consensus to this question was hesitant, with the informants all only thinking that the children would pick up different words for things.

To lead on from the previous question they were then asked " In this case do you believe that all children should be taught in Standardised English?" Two of the informants were extremely defiant and believed it was vital for children to be taught in this way, however the third believed that by doing this children will miss out on vital social experiences and an ability to communicate with other English speakers on a wider level.
The other two respondents were then asked how they felt the use of solely Standardised English in schools would affect the survival of dialects and they both believed it wouldn't endanger them as they would still be spoken within the family home.

A number of conclusions can be therefore drawn from this study.
As aforementioned this study has shown evidence to support the idea of Mutual intelligibility.

In addition to this there is evidence to show that the ideas discussed by Spolsky in Sociolinguistics pp 35 concerning the connection with lower social class and an inherent use of slang appear to be perceived to be true in everyday life.
However in the same chapter Spolsky also discusses the idea of racial solidarity which according to the answers given have little standing in everyday life.

Lastly it is of paramount importance to remember that the own regional, social and racial dialects of those interviewed will have played an important role in influencing the answers given.
In this case it may have been more beneficial to conduct the study with a greater of variety of social classes as although the informants had different regional dialects they were all from middle class backgrounds.

This literature study will explain the reasons for carrying out this research project and highlight why it is necessary and relevant to today's scientific audience.
The following points will be addressed:

Problems associated with an organic farming system Weeds and techniques used in their controlSmoke water and its applicationsConclusion
Problems associated with an organic farming system

The success of an arable production system rests on many important stages within the cultivation process, such as disease and pest control, sowing rate, fertiliser application, seedbed preparations and the control of weeds.
For conventional farmers, the degree to which they are able to influence these processes is far greater than farmers who farm in an organic system, where there are restrictions on inputs.

Hodges (1982) defines organic agriculture as "a system of agriculture that attempts to provide a balanced environment, in which the maintenance of soil fertility and the control of pests and diseases are achieved by the enhancement of natural processes and cycles, with only moderate inputs of energy and resources, while maintaining an optimum productivity." Whereas Lamkin (1990) defines organic agriculture as "a production system which avoids or largely excludes the use of synthetically compounded fertilisers, pesticides, growth regulators and livestock feed additives.
To the maximum extent feasible, organic farming systems rely on crop rotations, crop residues, animal manures, legumes, green manures, off-farm organic wastes and aspects of biological pest control to maintain soil productivity and tilth, to supply plant nutrients and to control insects, weeds and other pests." These two definitions both share the aim of producing crops and raising livestock with minimum input and non-reliance on man-made, synthetic inputs and agree that the requirements for crops and livestock should be sourced 'naturally', however to successfully achieve this and expect the same results as conventional farmers is not possible.

Due to the restrictions organic farmers face on inputs, problems arise in combating disease and the following from Lampkin (1990) highlight other difficulties:
- Reduced yields (crops)- Reduced output/ slower growth (livestock)- Weeds- Pests- Prices of output products - Labour requirements- Fertiliser application and releaseSource: Lampkin (1990)

For the purposes of this study, the concerns over weeds from the above list will be discussed by looking at the current techniques used to control weeds and then propose and explain a possible new technique for the control of weeds in organic arable crops.
Weeds and techniques used in their control

A weed, simply, can be defined as a plant that grows where it is not desired (McGinley, 1996).
However, a more comprehensive explanation is giving by Roberts (1982) who stated that weeds: "- Establish themselves without deliberate action by man, and when present are difficult to eradicate - Tend to be competitive and adaptable - Capable of exploiting habitats created or modified by man - Can form extensive populations that interfere with agricultural activities - Have efficient reproduction combined with mechanisms that permit survival under temporarily unfavourable conditions" In agricultural terms, a plant that is growing within the crop for example; poppy (Papaver rhoeas) is not where is it desired because by it being there it is ultimately causing a threat to the yield of the crop.

Studies such as Nielsen et al.
(1993), and Patriquin, et al.

(1986) both suggest that the presence of weeds in a crop significantly reduces yield, however, the reliability of Nielsen et al.
(1993) should be viewed with caution, as no control plots were used in their studies.

Although the above studies found a relationship, no quantification has been made on the impact of weeds on a crop (Rasmussen, 1995), apart from Nors (1993) and Nielsen et al.
(1993) who suggested crop yield losses of 20% due to weeds.

Although this is undeniable, the simplicity over a weed plant being present, by choice or not, it should also be considered in context.
A 'weed' plant that is present on an intensive arable farming system could be deemed as undesirable, however, in an organic arable farming system, the presence of a weed may well have benefits, ie; biodiversity within a field, which could contribute to the long-term sustainability of that farm.

Therefore, the total eradication of weeds in an organic system shouldn't be the objective of the farmer, rather a tolerance of weeds, but not at the expense of the crops viability.
Obviously, weeds shouldn't be allowed to grow freely, therefore weeds in an organic system can be controlled through the use of the following processes, such as: rotation, tillage techniques, seed rates, stale seedbeds, inter-row hoeing, harrowing, hand weeding (Rasmussen, 1995), catch cropping, and a manure application (Rasmussen, 2006).

The single use of one of the above weed control measures would not avert weed problems but conversely the use of a number of techniques in a holistic approach would achieve the best results, due to some of the processes above suppressing weed seed germination and emergence (stale seedbed and seed rates), and other processes removing the weed seeds that have emerged (Inter-row hoeing and harrowing).
In the paper Rasmussen (2006) wrote, he goes on to conclude that the soils on individual farms present different challenges to farmers converting to organic, saying that a combination of techniques may be effective in reducing weed biomass at one location, however not effective, or to a lesser degree at another location.

Therefore farmers need to plan which techniques would produce the best results, however suggestions for doing so were not mentioned.
A stale seedbed is the tillage technique used to deplete the soil seed bank.

This occurs by forming a tilled seedbed, but not planting a crop, and due to the disturbance of the soil weed seeds are promoted to germinate.
These weeds can then be eradicated before setting seed and then the crop is drilled.

For the purposes of this study the stale seedbed technique will be investigated, which as Rasmussen (2003) concluded proved effective in reducing weed seed numbers in the soil seed bank.
However, a further development, smoke water, for this application will be investigated.

Smoke water and its applications
Stimulation of seeds by smoke has been observed for many years, however it hasn't been until the last six to eight years that discovery and experimentation of the chemical within smoke has been studied and the product 'smoke water' been created.

Initial work has been carried out in Australia, which is one of a number of countries that is prone to natural fires, and it is due to this that scientists have observed the stimulation of plant seeds.
There are known to be 170 plant species from 37 families, which show germination enhancement by smoke derived from plant material (Roche et al., 1997).

At the time of publication Adkins (1997) was unable to identify the compound in smoke, however it was mentioned that Keeley et al.
(1985), Keeley & Pizzorno (1986), Baxter et al.

(1994), & Enright et al.
(1997) knew the compound was produced by the combustion of plant matter.

In later articles, Adkins et al.
(2001) & Adkins et al.

(2003), products such as 'Seed Starter', 'Regen 2000' and 'Kirstenbosch Instant Smoke Plus Seed Primer' are mentioned as marketed products of smoke solution used in their studies.
'Regen 2000' is said to "increase germination rates, resulting in stronger, healthier seedlings with improved and uniform germination" '(Regen 2000' website, 2006).

Marketing jargon aside, the product information does align with the literature sited above.
In Flematti et al.

(2004) and more recently Merritt et al.
(2006), the compound which is responsible for promoting germination is named as butenolide, 3-methyl-2H-furo[2,3-c]pyran-2-one.

This synthesized chemical in the above studies has acted at a level similar to smoke water and its ability to remain stable at high temperatures, be soluble in water, and be active at a variety of concentrations is proof enough for the authors to conclude this is the active germination stimulating chemical found in smoke water.
Work on developing this concept has been driven by scientist's vision to use this application in the restoration of native habitats.

Wok carried out by Keeley et al.
(1998) and Van Staden et al.

(2000) show that many species of plant have adapted their strategies for germination, where by the cue for germination is associated with fire.
Therefore in such countries as America and Australia particularly, where natural fires occur and plant species has adapted, the restoration of habitat in, for example urban areas or derelict land could be successfully implemented by the use of smoke water.

Research has been carried out to investigate species that would respond in such a scenario, however little work has been commissioned to investigate the agricultural application.
Stimulation of arable weed seeds by smoke water was investigated by Adkins et al.

(2003) when a number of weed species were investigated and it was noted that half of the species that responded originated from countries that contain fire prone habitats, however some species which didn't originate from such areas also responded.
Earlier work by Adkins et al.

(2001) demonstrated, with varying degrees of success, that some species (Avena sterilis - monocot) responded positively to a smoke water application and others (Sinapis arvensis - dicot) did not respond at all.
The physical differences mentioned of those species above and their contrasting responses to smoke water is something is of interest and the knowledge of such a phenomena may prove important information for this project.

Conclusion
In summary of what has been said, the purpose of this study is to investigate a new application, smoke water, to enhance the stale seedbed technique used in organic farming systems to reduce weed populations within the crop.

Although work has been carried out using smoke water and its effects are know for many species, it is apparent that its relevance and application for such in an agricultural context are not known of.
From what is know and has been mentioned above, the theory of the suggested title for this project should deplete the soil seed bank through more weed seeds geminating and therefore a more successful stale seed bed produced, within the stipulated regulations for organic farmers.

What are roundworms?
- Round worms are common sheep parasite which predominately effect lambs, but also older ewes - There are several species of roundworm & more than one species can be present in the same animal, however only one will be dominate - There are two important diseases caused by roundworms:

Nematodirosis - caused by Nematodirus battus
Nematodirosis is when damage is caused by burrowing larvae in the gut.

Problems can be severe & can end in death.
Lambs can build residence to the parasite between 3 to 6 months of age.

Adult ewes are fully immune & don't carry N.battus worms, therefore cannot contaminate pasture
Parasitic gastroenteritis (PGE) - caused by:Ostertagia, Trichostrongylus, & Nematodirus

PGE is the passing of tricostrongyloid eggs through the gut.
It causes energy & protein to be diverted away from vital processes such as meat, milk & wool production to repair the damaged gut & to maintain protein circulating in the blood

What are the symptoms?
Diarrhoea dehydrationReduced food intake - 10-20%Reduced food conversion efficiency

Particular problems for lambs include:
Reduced weight gainLower rates of growthReduced fleece qualityReduced carcass quality

What is the life cycle of the parasites?
Diagram 1 shows the life cycle for Nematodirus battus.

The 1 st, 2 nd & 3 rd stage larvae develop in the ground & are then consumed by the sheep Diagram 2 shows the life cycle for Trichostrongylus.
Notice that the 3 rd stage larvae on the soil surface & are then consumed by the sheep

What are the organic regulations?
The regulations stipulate that preventative livestock management techniques are used instead of the prophylactic use of chemical medicinal products A farm 'health plan' must be constructed to outline the appropriate measures to be taken if particular circumstances arise Animals that become sick or injured must be treated by first using medicinal products that comply with organic farming principles; (plant extracts or plant, animal, mineral products) If the above products don't work & the animal is in distress or is suffering, chemically synthesised medication may be administered, a record made and there is a legal 'withdrawal' period (28 days)

How do you control them?
Select an appropriate breed - some breeds show great resistance, eg; Merino & Finn Dorset Encourage the natural immunlogocial defences of the animal by: high quality feed, exercise & access to pastureAppropriate stocking densities (10 ewes/ha )Host Immune Response - allowing some degree of exposure to promote resistance Clean grazing by rotating animals between fields for grazing to achieve a balance where by problems with parasites are minimised & the life cycle for the nematodes are brokenBiological Fungus - a recently developed fungus which can be incorporated into feed supplement & break down the larvae stages of Trichostrongylus, once the eggs have been deposited in the dung

Clean grazing explained:
Clean grazing does not aim to eradicate roundworms but rather achieve an acceptable level of control based on the known ecology of the nematodes Diagrams 1 & 2 show the cycle of both nematodes.

By rotating sheep between fields & ensuring that the 'new' field the sheep enter in the rotation has been free from grazing sheep, the likelihood of there being egg larvae present is small & therefore that field is deemed to be 'safe pasture'.
By carrying out this technique the life cycle of the nematode is broken & the threat posed by these parasites in a low input system are reduced, however, completely clean, uncontaminated land is impossible to achieve.

Acceptable larvae egg numbers: Regular egg count samples should be taken from fresh dung.
Lab testing should identify the number of eggs per gram (EPG).

A good result would produce an EPG of >100, however an EPG of between 500-1000 will mean productivity losses will begin to become apparent, and EPG values <2000 will possibly result in lamb deaths.
Biological fungus developments:

A micro fungus known as Duddington flagrans has been found in recent studies to survive passing through the digestive tract of sheep & therefore is able to infect the egg larvae passed by the sheep & prevent the larvae spreading in the field.
Administered in sheep feed, this noval non-chemical approach would be well suited to organic livestock farmers if found to be economic.

Clean grazing explained:Clean grazing does not aim to eradicate roundworms but rather achieve an acceptable level of control based on the known ecology of the nematodesDiagrams 1 & 2 show the cycle of both nematodes.
By rotating sheep between fields & ensuring that the 'new' field the sheep enter in the rotation has been free from grazing sheep, the likelihood of there being egg larvae present is small & therefore that field is deemed to be 'safe pasture'.

By carrying out this technique the life cycle of the nematode is broken & the threat posed by these parasites in a low input system are reduced, however, completely clean, uncontaminated land is impossible to achieve.
Acceptable larvae egg numbers: Regular egg count samples should be taken from fresh dung.

Lab testing should identify the number of eggs per gram (EPG).
A good result would produce an EPG of >100, however an EPG of between 500-1000 will mean productivity losses will begin to become apparent, and EPG values <2000 will possibly result in lamb deaths.Biological fungus developments:A micro fungus known as Duddington flagrans has been found in recent studies to survive passing through the digestive tract of sheep & therefore is able to infect the egg larvae passed by the sheep & prevent the larvae spreading in the field.

Administered in sheep feed, this noval non-chemical approach would be well suited to organic livestock farmers if found to be economic.
Food security can be defined as when all people, at all times, have access to sufficient, safe and nutritious food to meet their dietary needs and food preferences for an active and healthy life, (FAO, 2007).

China's rapid growth in population, development and industrialisation in recent years has not just surprised many people, but the consequence of such accelerated growth has implications that dominate the country's government and will have continued global effects.
The following points will be addressed to consider China's current situation with regard to food security:

Background to China General introduction Sustainability concerns for food security Factors contributing to China's sustainability concernsPotential solutions for the present and the future 3.1 Stakeholders 3.2 Development pathways4.
Conclusion

Background to China
General introduction

China is a country located in East Asia, covering a land area of 9,596,960 sq km, of which 9,326,410 sq km is land and 270,550 sq km is water.
In 2005, the land use was calculated at approximately 15% for arable land, 1% for permanent crops and 84% as other, ie; urban or industrial land use, (CIA, 2007).

As was mentioned, China is host to an extensive source of water, however as Changming (1998) commented in his work, the problem is it is not located geographically where it is required.
The climate in the country is diverse; in the North, where there is roughly up to 500mm rain annually, it is sub-arctic and the soil type ranges from black chernozems to chestnut brown pedocals to gray brown pedalfers, (Vermeer, 1977).

The south, where the inter-tropical convergence zone brings about seasonal changes to produce a tropical climate Dennett (1984), receives between 1500-1750mm of rain annually and is host to predominantly red and yellow pedalfer soils, (Vermeer, 1977).
Dramatic changes in population have taken place in China in the last 50 years.

Quoted figures in the literature estimate the population to be approximately 1.3 billion (CIA, 2007 and Richardson, 2006).
Not only has the population increased, but so has China's economy, which has seen a steady rise between 1978 and 2006, when in the measure of purchasing power parity, the country was the second largest in the world, although in per capita terms this was nearer the average, (CIA, 2007).

China currently has problems with poverty, however this issue is being addressed through the Millennium Development Goals (MDGs).
An example of their success is seen in the substantial fall in the number of undernourished people, from 303.8m (30% of population) to 119.1m (9% of population) between 1981 and 2000, (Anderson et al.

2003).
However, problems are far from over as China continues to develop and despite advances in food security, many more solutions need to be found.

The uneven development across China is benefiting some and not others.
Sustainability concerns

From the above introduction to China, it may be noted that the country has some underlying problems regarding the rate of growth, both in terms of population, economy and industry and whether these rates can be sustained.
For the purposes of this report, however, the production of food, the problems that have occurred or are occurring at the present time and China's ability to sustain food production for its' large population, will all be addressed.

The first concern is that of increased development, where as Richardson (2006) reports, China is currently loosing 1 million acres (approx 40,4600ha) of farmland to development per year.
A large number of the population have and are still moving from rural to urban areas.

In the last 25 years, 350 million people have migrated from rural areas.
This therefore has resulted in a fall of 75% to 43% of the national force employed in agriculture, (Gairdner, 2006).

China has prided itself in being able to produce 95% of its' total grain requirements, however, as the country has become more developed and people's life styles have changed, their demand for new products such as vegetables, meat, dairy products and oilseeds has caused a change in farming practices, (Index-China, 2000).
Rice, wheat, soya and maize are by no means unwanted, but the ability for Chinese farmers to provide all the desired products from a decreasing area of land is impossible and therefore the reliance on imports looks certain to increase, (Gairdner, 2006).

Fernández & Fernández-Stembridge (2007) include figures for the year 2005, when China was exporting $2781m of vegetables, $4056m aquatic and seawater products and $1062m of fresh vegetables, whilst importing $6979m of soyabean, $1649 of wheat and $2230m of cereals and flour cereals, clearly demonstrating China's emphasis on higher value products, (both exported and consumed within the country) and as a result, their importing of products for which they would previously have been virtually self-sufficient.
A second concern, closely linked to the agricultural sector is the availability and use of water.

As mentioned earlier, China's water resource is fairly extensive, however the location of that water in relation to the land used for cultivation is not ideal.
This is due to the unfortunate topography and climate that spans China causing the South to be water rich, but too hilly for extensive farming and the North, which has flat pains and is well suited to agricultural cultivation, to be water impoverished, (Changming, 1998).

Richardson (2006) put some numbers to the situation, clearly demonstrating that with two thirds of the cultivatable land in the North but only one fifth of the water, the sustainable use of what water there is and the provision of more from the South, still raises concerns for the future.
25% of all Chinese agriculture is irrigated, Gairdner (2006).

China has always been successful in producing rice and has seen huge increases in its' ability to produce high yields from multiple crops throughout the year, however the need for water to irrigate those crops does put pressure on future production, as the amount of water required varies from 533-1533 cubic meters of water per mou, (Vermeer, 1977).
Demand for water is an ever-increasing concern, as today there are around 450 million people who depend on Huang or yellow river and the Yangtze river for agriculture, fishing and other uses, (Karasov, 2002).

A report carried out by the World Bank (2001) showed these afore mentioned rivers were, in many places, huge health risks and unsuitable for human contact.
The Yellow river, in particular, has suffered from over extraction for irrigation and the construction of dams has caused the flow to be reduced, so much so that it sometimes does not reach the ocean, (Karasov, 2002).

Sustainability concerns regarding China's food security are important, not just on a national level but on an international level, as food supply problems could compromise the country's rate of economic growth and political reform it is currently experiencing and an increased demand for imported food could dramatically influence global markets, especially for feed grain and the countries exporting it, (Heilig, 1999).
Climate change, as for any developed country at the present time, is highly topical and this is particularly true of China as Zhang (2006) reported late last year that the country had slipped down the Climate Change Performance Index (CCPI) to be 54 th of 56.

This not only has ramifications for the environment, further water degradation and risk of acid rain, but the global effect of climate change may affect vegetation cover, particularly with regard to the large areas of grassland which is used for livestock production.
Climate change can exacerbate natural disasters such as floods and periods of drought, thereby posing an additional threat to China's food security, (Heilig, 1999).

Factors contributing to China's sustainability concerns
In order to understand China's present situation, it is important to consider how the country has developed and worked to achieve the agricultural system seen today.

China has a long history of agricultural production.
Although its' achievements are impressive, such as successive cropping, intercropping and inlaid cropping (that multiplied the cropping areas within the limited cultivatable area) and good soil husbandry that allowed the crops to be continually grown year after year, the drawbacks are intrinsically linked to these successes.

Chinese agriculture has historically concentrated on increasing the yield per unit area and has not taken into consideration the advantages of increasing labour productivity.
This didn't seem to matter because there were enough people to enable production, but, as population continued to increase, the pressure upon the limited amount of land to deliver intensified, (Guohua & Peel, 1991).

China had successfully fed one fifth of the world's population from one fifth of the world's cultivatable land FAO, (1999), and still manages to produce vast amounts of grain with which to feed its own population.
China's recent agricultural history has seen many changes in land reform, such as the farmland revolution in 1950 which redistributed land from landlords to landless peasants.

Following this, the next reform came in the mid-1950s when the people's commune was established, encouraging people to group together as cooperatives to farm the land that had centrally controlled property rights, which ended in the collapse of the agricultural sector as farmers lost their freedom to farm the way they wanted, (Chen et al.
1999).

Further reform came in the 1970s, when China decided to introduce family based contract systems where the farmer had land use rights and the right to make decisions, therefore allowing the farmer freedom to farm, even though there was no land ownership.
Not only this, but the Chinese government offered incentives that were linked to production, which as we know already from the old CAP subsidy in the UK, has dramatic effects on production.

However, there were problems associated with this reform, which were firstly the fragmentation of cultivated land, due to each household being entitled to plots of land, and with such a large population, this meant only a very small area of land per family, roughly 0.466 ha, (Ministry of Agriculture of China, 1993).
Different areas of land were also more productive than others, which meant that the land a family owned could be located at multiple locations around a village, raising concerns about labour and time efficiency and large areas of potentially cultivatable land were wasted because of paths and boundaries separating people's land, Chen et al.

1999).
The way land has been managed and cultivated in the past, which was shaped strongly by policy, is not the only factor affecting China's present problem regarding food security.

As has already been mentioned, the current state of China's water supply, chiefly in two of the country's largest rivers, has been recently described as unsafe for human contact.
The main cause for this sorry state of affairs is again due to the rapid industrialisation occurring in the country.

Large investment in industry is not being met with the required infrastructure and services to process waste or emissions.
A figure to highlight the severity of the problem comes from World Bank (2001) where 23.4 billion tonnes of sewage and industrial water was dumped into the Yangtze in the year 2000, which was 11% more than in the proceeding year.

Organic and inorganic pollutants found in both the Yellow and Yangtze includes human excreta, industrial chemicals, heavy metals, cyanide and solvents, most of which originate from paper, steel, silk and chemical factories.
Agriculture is also highlighted as being a contributor to this pollution and in some places, sediment run-off due to erosion was causing problems, both in the rivers and from cultivatable land, (World Bank, 2004).

This is not only a problem in itself, but an ongoing problem with regard to agriculture, as water is required for irrigation and obviously the use of contaminated water in this instance would not be allowed.
Potential solutions for now and the future

Stakeholders
The following people are involved, would play a role or would be affected by plans to addresses the aforementioned problems of food security in China:

Local and national governmentThe rural farming communityCountries exporting food to China on the world marketThose involved in industry, if responsibility is put on them to reduce emissions etc.
Development pathways

There are many ways and means of instigating change to fulfil an objective, particularly one as big as the achieving a sustained level of food security.
One successful 'bottom-up' project that was implemented in the Loess Plateau with help from the World Bank aimed to improve the potential for agricultural production, reduce erosion and equip farmers with knowledge and a long term strategy for management of the area.

The land use of the small watersheds in the area prior to the project were as follows: uncultivated wasteland (40%), cropland (40%), mostly on low-productivity slopeland, trees and shrubs (10%); gullies (5%), and roads, villages, etc (5%).
To improve this situation the project terraced 90,500 ha, afforested 90,900 ha and shrubbed 136,000ha, planted 26,700 ha for timber production, planted 30,890 ha orchards, re-established 100,140 ha of grasslands, irrigated 7,100 ha and installed sediment control dams.

The World Bank played an important role in the project preparation and implementation and the total cost was $250 million; the cost per hectare was about $160.
The objectives of the project were sustainable and coordinated social, economic, resource and environmental development of small watersheds and this proved successful.

It was aimed at a local scale and it proved that land conservation is compatible with sustainable and productive agriculture and that they are mutually reinforcing.
It achieved the creation of sustainable crop production on high-quality terraces and the protection of steep slopes with trees and shrubs.

The farmer has also benefited, not just from having a more sustainable method of production, but legal protection for land-use rights and management techniques in the form of a land contract for the minimum of 50 years, ensuring that the long term benefit from the project and the initial investment lasts, (World Bank, 2004).
As was mentioned earlier, poverty in China is still a problem, and small scale projects such as this help relieve people from that low level of living so that they can support themselves.

Other projects such as FAO's South-South Cooperation initiative, part of the Special Programme for Food Security (SPFS), aim to strengthen cooperation between developing countries at different stages of development to improve agricultural productivity and ensure access to food for all, (Buerkle, 2007).
The recent dispatch of experts to China to help and advise with water control, crop production, animal diversification, aquaculture and processing is to help China achieve it's Millennium Development Goals of halving poverty in China by 2015.

Change and influence can also come 'top-down'.
Policy and government decisions will also have an important role to play in the future of China's food security.

In 2001 China became a member of the World Trade Organisation (WRO) Baihua (2006), which meant another period of change for farmers, when prices for domestic cereals fell.
Although this may have had negative effects at first, excessive agricultural inputs and demand for water to produce the once large quantities of food have receded, meaning the land can now be used for other, more appropriate uses, such as the farming of cash crops and/or livestock, (UN, 2001).

The UN is involved together with the World Bank to help China in this period of transition by improving agricultural infrastructure and service systems and enhancing protection of agricultural environment and resources.
When considering development pathways, a holistic view of the problem and an awareness of other sectors and industries, which use of similar or the same resources, is important.

Chenggui & Hongchun (2002) commented that to focus on the continued development of growing cereals is not sustainable in the long run, when problems such as water loss and soil erosion are happening.
Therefore, it is important, not to out-source blame to other sectors, but jointly communicate with each other, so that development is equal across all sectors.

For example, the need for a cleaner water supply for irrigation will involve stakeholders from more than just the agricultural sector.
It would be important to see China continue to address problems at a local scale, with the help from the World Bank and FAO, enabling farmers to take control and be responsible for changes in agricultural production, protection of soils and contribute positively to improving water quality.

Government should be responsible in maintaining reserve productive capability (Changgui & Hongchun, 2002) as the economic strength of the country has grown, the ability for its population to eat a more diverse diet has increased.
With a population as large as China's and the uneven distribution of wealth and food, the capability for producing grain should not be something lost in the hast to supply and improve the diets/lives for those how can afford it.

The ability for China to produce grain is deeply important, as this is important for the survival of many 1000's of people.
To provide food security for everyone, China needs to retain the ability of producing grain when required, and the government will need to address this, and not be quick to rely on imports.

4. Conclusion
Monitoring of progress towards achieving improvements in food security is important.

Millennium Development Goals (MDGs) are important and should still remain, however small scale indicators of improvement could be monitored by farm surveys, covering things such as measurements of rainfall run-off, rates of erosion and also could include the health and level of food intake of people on that farm.
Information from all farms could then be collated and progress monitored at this local scale.

Areas not progressing as well as other areas could then be addressed accordingly.
Continued monitoring of water quality and water safety would also be advised, not just for agricultural use, but for the interest of the country.

Food security for any country is important and the sustainable production of food in an ever increasingly developing and urban world is even more important.
For a country such as China, whose huge population is exerting pressure on the environment and encroachment of urban sprawl is reducing land area for production of food, it seems impossible to propose sustainable methods of production.

However, local projects to aid food production in rural areas should target issues such as soil erosion and to some degree water use, but the later is also highly dependant on sustainable industrial and domestic use of water.
Government policy has its part to play, however, local scale projects will reach the isolated rural people quicker.

For a sustainable future, China must broaden its thinking in giving agriculture the same importance as its newly developing industry, other wise, the future of China's food security will be reliance on imports and a demise of the people who skillfully work China's productive land.
Organic produce available for the UK's general public has, in recent years increased and nowadays is a common phenomenon.

Reasons for apparent interests in this market of food are mixed, however the market is here to stay, and will probably in the next few years continue to grow, if the demand remains.
The following points are the intended outline for this report in which the organic sector will be appraised and suggestions for biocontrol put forward.

The shape of the organic food sectorBiological control backgroundWhat is organic farming and can biocontrol help?What is integrated farming and can biocontrol help?Conclusion
The shape of the organic food sector :

The amount of organically certified land has grown by over 1000% since 1997, from 60,000ha to over 688,373ha, including land in conversion.
The organic market overall has grown from £100 million to £1.12 billion between 1993/94 and 2003/04 and in the year 2003/04, organic sales grew by 10.2%, equivalent to almost £2 million a week (Green, 2004).

These figures are impressive and to suggest that the organic market will continue to grow is realistic.
In a recent report by Horne, (2006), the quota for organic milk alone is said to increase by 25% a year for the foreseeable future, with the sector having gone from 7m litres (1995/96) to 300m litres (2005) of milk per year.

So why are people so interested in buying organic food?
Work carried out by Jones & Tranter, (2004) compiled data from a number of countries in Europe using focus group studies and found that, for the UK, 17% of the people surveyed regularly bought organic food and 44% bought sometimes.

The level of understanding of organically produced products was, in most cases, shallow or nonexistent, except for Denmark, which has the most mature organic market and consumers, accordingly, have the deepest knowledge and understanding of what they are buying.
When asked the reason for buying organic, the majority said either for environmental or health reasons, the latter, together with food safety, was an important issue for pregnant mothers or young children.

The common response by all who were surveyed across Europe mentioned the environment and therefore an implicit erroneous assumption of restricted use or complete absence of chemicals.
There are two fundamental misconceptions in this response - firstly, there is occasional, last resort use of pesticide sprays, principally copper, sulphur, natural pyrethroids, and derris, Shepard (2003) and secondly the misguided association of 'unnatural' with 'harmful', when actually there is no relationship and it is known that some natural chemicals are very harmful compared to some unnatural chemicals that pose a lesser threat (van Emden & Peakall, 1996).

Therefore, it seems that the logical way forward is to educate the public to the realities of organic food production through the use of advertising, factual information leaflets, and labelling of food products.
A cogent example is the Sheepdrove enterprise based in Lambourn, Berkshire, which has two farm shops and an informative website which explains their method of production and provides appropriate recipes.

(Sheepdrove organic farm, 2003).
Biological control background

A formal definition of biological control is a method of controlling pests and diseases in agriculture that relies on natural predation rather than the introduction of chemicals (Wikipedia, 2006).
Increased awareness of the chemical inputs used for controlling pests and diseases in agricultural and horticultural enterprises has, since the publishing of Rachel Carson's Silent Spring, been the driving force behind what could be termed as today's 'environmentalism'.

Carson's holistic view for the development and use of powerful chemical pesticides, insecticides and herbicides, such as DDT, highlighted the problems associated with their over use in the industry, resulting in resistance, toxic residues on food, soil and animals, undesirable deaths of nontarget organisms and damage to the environment (Emden & Peakall, 1996).
With the general public's view on farming and it's effect on the environment (sometimes irrational and uninformed), it may then be a quick assumption that biological control would be a good thing to integrate into the organic system of agricultural production found in the UK today.

However, little information can be found regarding this integration in the literature, for reasons explained later.
What is organic farming and can biocontrol help?

Hodges (1982) defines organic agriculture as "a system of agriculture that attempts to provide a balanced environment, in which the maintenance of soil fertility and the control of pests and diseases are achieved by the enhancement of natural processes and cycles, with only moderate inputs of energy and resources, while maintaining an optimum productivity." Whereas Lampkin (1990) defines organic agriculture as "a production system which avoids or largely excludes the use of synthetically compounded fertilisers, pesticides, growth regulators and livestock feed additives.
To the maximum extent feasible, organic farming systems rely on crop rotations, crop residues, animal manures, legumes, green manures, off-farm organic wastes and aspects of biological pest control to maintain soil productivity and tilth, to supply plant nutrients and to control insects, weeds and other pests." These two definitions both share the aim of growing crops and raising livestock with non-reliance on man-made, synthetic inputs and agree that the requirements for crops and livestock should be sourced 'naturally'.

However to successfully achieve this and expect the same results as conventional farmers is not possible.
Due to the restrictions organic farmers face on inputs, problems arise in combating disease and the following from Lampkin (1990) highlights other difficulties:

- Reduced yields (crops)- Reduced output/ slower growth (livestock)- Weeds- Pests- Prices of output products - Labour requirements- Fertiliser application and release
Following the restructuring of the farming subsidy scheme between 2002 - 2004 (entry level, organic entry level and higher level), the options for growers to go organic has been widened and the money being provided through the subsidy, for all stewardship schemes, is better targeted towards environmental farming practices.

The list of options available for farmers on the organic scheme (OELS) includes buffer zones, hedgerow management, beetle banks, under sowing cereals, and conservation headlands, Defra, (2005).
It is clear from the above that the potential for biocontrol is present, as beetle banks and conservation headlands are known to host natural predators which could be exploited for control of pests in the crop, and the correct choice of species sown in the headland could attract pests out of the crop and provide alternative food for predators (Root, 1973).

This method of biocontrol can typically be termed as 'conservation biocontrol', as this is the creation and improvement of habitat for hosting already existing pests or predators, contrasting with the actual application or addition of biocontrol organisms, which would be brave in broad acre agriculture as the effects would be unpredictable and questions would be raised regarding the economic viability and environmental safety.
Thus, the use of this technique in glass houses has more application due to a more confined, controlled and protected environment.

Therefore, a lack of information in the literature for biocontrol in broad acre agriculture can be understood, however some work regarding conservation biocontrol has been carried out, but not in the UK.
Work in east Africa investigating stemborers and striga in maize crops has shown remarkable reductions by planting Napier and Sudan grasses around the crop and intercropping plants such as Desmodium spp.

and Melinis minutiflora.
The stemborers are attracted by the surrounding grasses and repelled from the crop by Desmodium spp., resulting in a 'push - pull' system, which also provides suppression of Striga, which is a parasitic weed.

The results of this project speak for themselves, with increased maize yields of between 18-25%, Khan, (2005), demonstrating what can be achieved when enough is known about the ecology of the target pest.
One attempt at controlling slugs in broad acre agriculture by the application of nematodes has been tried in Holland.

The nematodes they used were Phasmarhabditis hermaphrodita, and the application was by walk through broadcasting in some fields and a helicopter boom over the crop in other fields.
The crop in this instance was Brussel sprouts and the findings of these trials found that after 2 applications of 500 million nematodes per hectare, combined with 4 applications of Ferramol at 25 kg/ha, (this combination saving 6 applications of Ferramol, a low environmental impact slug bait), sufficient slug control was achieved at little expense (Ester et al.

2006).
In a recent study carried out by Prasad and Snyder, (2006), it is mentioned that few studies have been undertaken to investigate whether or not the conservation of natural enemies, through the use of beetle banks, improves pest control.

Not only is this the case, but the ecology for some of the most common species of interest in an agricultural context is not known, Holland et al.
(2005).

The observations mentioned from the study of Prasad and Snyder, (2006), were that the presence of a beetle bank, on land managed organically, does increase the number of beetle species populations.
However, it was shown that although the number of beetles increased, the predation of fly eggs over the growing season wasn't affected and the reality was that larger beetles were predating smaller beetles.

A study carried out by Thomas et al.
(1991) showed that the emigration of Demetrias atricapilus from such beetle banks were successful, with beetles found 60 metres from the origin and the apparent control of pests was positive.

Another study carried out by MacLeod et al.
(2004) concluded that over the period of study, the investigated beetle bank maintained its role in providing over-wintering habitat for polyphagous predators and that carabid beetle diversity increased since the bank had been established.

This is all very well, but little is known about the affect of the presence of these enemies and whether or not they provide sufficient pest control.
Therefore from the above, an organic farmer may chose the option of a beetle bank for his OELS payment, thereby increasing the densities of beetle populations on which he was relying for the control of his pests.

However, fly egg numbers may not be reduced sufficiently to persuade the farmer of the benefit of a beetle bank.
For such a method to be employed as an option in the OELS seems questionable, not that it is a bad thing, but that the reason for it being there is more idealistic rather than quantifiable.

More specific studies therefore need to be funded to investigate the most efficient way of using these techniques and for information of the individual beetle species to be collated, so that the best, and desired results can be obtained.
Funding for such projects is not readily available however due to chemical companies wanting to maintain sales of their products and therefore retain their economic viability.

Not only do organic farmers have crop problems to deal with, but also livestock problems, a major one being parasitic worms in sheep (Nematodirus battus, Ostertagia, Trichostrongylus and Nematodirus species).
The techniques for overcoming this problem without the use of medication are:

Selection of an appropriate breed, some breeds show great resistance, eg; Merino & Finn Dorset Encouragement of the natural immunological defences of the animal by: high quality feed, exercise & access to pastureAppropriate stocking densities (10 ewes/ha )Host Immune Response - allowing some degree of exposure to promote resistance Biological Fungus - a recently developed fungus which can be incorporated into feed supplement & break down the larvae stages of Trichostrongylus, once the eggs have been deposited in the dung.
Adoption of a 'clean grazing' regime, which does not aim to eradicate roundworms but rather achieve an acceptable level of control based on the known ecology of the nematodes.

Figure 1 shows the life cycle of a nematode.
By rotating sheep between fields & ensuring that the 'new' field the sheep enter in the rotation has been free from grazing sheep, the likelihood of there being egg larvae present is small & therefore that field is deemed to be 'safe pasture'.

By carrying out this technique the life cycle of the nematode is broken & the threat posed by this parasite in a low input system is reduced, however, completely clean, uncontaminated land is impossible to achieve.
(University of Aberdeen, 2003)

Animals that become sick or injured and for whom none of the above methods are effective, must be treated by first using medicinal products that comply with organic farming principles; (plant extracts or plant, animal, mineral products).
If these products don't work & the animal is in distress or is suffering, chemically synthesised medication may be administered, a record made and there is a legal 'withdrawal' period (28 days).

Organic farming is a more responsible way of farming, however, the 'red tape' and the structure which has been developed seems to have been formulated from behind a desk, rather than quantified and thought through from farm level.
This lack of flexibility makes integration difficult.

What is integrated farming and can biocontrol help?
Integrated Farming Systems (IFS), Integrated Arable Farming Systems (IAFS), Integrated Crop Management (ICM) are all, in essence the same thing, where the focus is to combine traditional methods of farming with modern technology.

A "best of both worlds" scenario could be assigned to this method of agriculture, as it lies between conventional farming and the recognised organic approach to farming.
Farmer et al, 1994 defined this method of agriculture as: "a holistic pattern of land use which integrates natural resources and regulation mechanisms into farming practices to achieve maximum but stepwise replacement of farm inputs, to secure quality food and to sustain income." LEAF (Linking Environment and Farming) is one of the organisations at the forefront of this scheme and promotes various principles of IFS.

These include; a commitment to good husbandry and animal welfare, efficient soil management and appropriate cultivation techniques, the use of crop rotations, minimum reliance on crop protection chemicals and fertilisers, careful choice of seed varieties, maintenance of the landscape and rural communities, and an enhancement of wildlife habitats (LEAF, 2006).
The system is composed of 4 main areas; fertiliser usage, pesticide usage, cultivation methods and rotations.

Individually, these are familiar approaches to farming, however IFS aims to implement changes in all these areas and thereby join up the thinking behind each approach to form a well balanced and systematic method for agricultural production.
Figure 1 expands upon the 4 main areas stated above.

Fertilisers and pesticides are permitted and there are no restrictions as to which kind are used, (unlike the organic system), but the application is reduced.
Park et al 1997, outlines the concept of 'Threshold Spraying' as a technique for reducing the amount of fungicide and/or pesticide used.

Threshold spraying is based on close crop monitoring so that application takes place at the most appropriate time, when a disease or pest is present, thereby reducing the use of prophylactic spraying.
IFS are more complex than conventional systems, due to the greater variety of crops, related diseases and pests to contend with.

Threshold spraying relies on more crop walks, as a higher dose of spray will be required if a threshold is exceeded, more so than if a low dose prophylactic spray had been used (Park et al.
1997).

Therefore there is a higher degree of risk involved.
However, to hypothesise for a moment; if the options suggested in the OELS are adopted and integrated into this system of agricultural production, there could be potential benefits all round with the inclusion and conservation of natural enemies to aid crop pest protection, as stated in the above examples, therefore attaining to the threshold philosophy for pesticide inputs and using these chemicals in a manner which complements the biological control.

By spoken word of Helmut F. van Emden, (2006) the suggested ratio between biological control and the use of complementary chemical input was estimated to be 70:30, whereby sufficient protection through the use of biocontrol was insured through the use of some chemical input.
Conclusion :

In conclusion, it has become clear that the organic system of agriculture isn't wholly accommodating, and a more holistic method of integrated farming will prove a better suited system for the introduction of biocontrol measures.
The nemaslug example proved that biocontrol can be successful and economical, but to fully utilise the benefits of biological control, further funding is required so that the ecology of these biocontrol agents can be discovered and implemented in either conservation or broad acre agriculture application.

The groundnut (Arachis hypogaea) is the 13 th most important food crop in the world, the 4 th most important source of edible oil and the 3 rd most important source of vegetable protein, ICRISAT (2006).
Therefore, it is not only of local importance to the countries in which it is grown, but also global importance due to demand for the raw and derivative products.

Developing countries account for 96% of the groundnut production land area and 92% of the world production.
In the global context, Africa contains 38% of the land area cultivated and accounts for 25% of the total production of groundnuts, ICRISAT (2006).

With annual rainfall figures of between 600-1000mm, suitable climate and sandy Alfisols and Oxisols soils, (Virmani & Singh, 1986), West Africa provides ideal conditions for groundnuts to be sown in June/July and harvested in September/October.
Labour constraints pose a serious problem to production, as do the plethora of pest and disease problems which threaten yields.

The following points will be addressed:
Environmental, social and economic factors affecting groundnut production in West AfricaConstraints to growing groundnuts in West Africa Proposals for improving the groundnut agricultural system in West Africa

Environmental, social and economic factors affecting groundnut production in West Africa
Climate

Figure 1 highlights the geographic area of West Africa and also shows the location of the inter-tropical convergence zone (ITCZ), which is an important circulation of air that causes climatic variations around the equator, tropics and subtropics.
This circulating air pattern is known as the Hadley Cell, which is a series of rising and falling air masses which create areas of low and high pressure between 30° north and 30° south of the equator (Wikipedia 1, 2006).

The basic cause for this phenomenon is known to be linked with the variation in solar radiation receipt with latitude, which explains the seasonal variation of the ITCZ, due to the movement of the sun throughout the year.
However there is a lag time of 6-8 weeks.

In July, the ITCZ is at its most northerly point and at its most southerly point in January (Dennett, 1984).
Figure 1 shows the position of the ITCZ over Africa.

In West Africa the movement of the ITCZ results in seasonal rain patterns, which then affect the type of agriculture which can be carried out.
In figures 2, 3 and 4, annual rain patterns at three sites in West Africa are shown.

Although there are small differences, the general pattern is clear, which is that from November to March there is a dry season where there is little or no rainfall and potential evapotranspiration is also high.
For the other months of the year, there is more rainfall and rates of potential evapotranspiration are lower, allowing this period to be the growing season.

Potential evapotranspiration is a concept introduced by Thornthwaite (1948), where it is described as the amount of water that could be transpired or evaporated in conditions of unlimited soil moisture and complete vegetation cover.
This varies throughout the year with the change in climate, and is known to peak between March/April and October/November in West Africa.

ETo values of more than 7mm/d can cause plant water stress which can lead to stomatal closure and a reduction in growth rate (Kowal & Kassam, 1978).
Soils

There is a mixture of soils present ranging from vertisols to ferrisols and ferrallitic soils, but by far the most extensive soil type in West Africa is the ferruginous tropical soil, covering 60% of the region.
These ferruginous tropical soils are highly weathered soils that have developed under 500 to 1200mm of annual rainfall and suffer large losses of clay through the profile which results in a compact subsoil and a sandy surface soil, low in organic matter and base exchange capacity (Kowal & Kassam, 1978).

Social & economic factors
Figures for 2006 estimate the total population of West Africa to be 252 million (Wikipedia 2, 2006).

Figures for infant mortality and death rates, which have previously been high, have decreased, although not to what could be deemed an acceptable level, but this has aided population growth.
West Africa has an annual population growth of 2.7% and an increasing percentage of its population, currently 35%, is moving to live in urban areas and this is predicted to rise in the future (Evergreen, 2004).

The world's media sporadically reports on the HIV/AIDS situation in Africa and although there hasn't been mention of this is recent months, the problem facing an increasing number of people with this disease still remains.
In sub-Saharan Africa, which covers some West African countries, the number of people with HIV/AIDS accounts for 60% of the worldwide cases of this disease (25 million HIV/AIDS cases out of a worldwide total of 39 million, UNAIDS, 2004).

These numbers are rightly sobering, and even more so when it is appreciated that 53% of the economically active population are involved in agriculture (Kormawa, 2005).
The constraints this has on production will be highlighted in a later section.

In summary, the agricultural sector in this region provides employment to a large number of the population.
However, due to out migration of people from the rural areas to the urban areas, an ever increasing demand for food, declining soil fertility, overgrazing and soil erosion the supply of produce, in this case, groundnuts, is restricted (Kowal & Kassam, 1978).

The only way forward in this situation is to bring new areas of land into production, but there will come a point were no more land is available and it is questionable whether the land they are beginning to use is in the best condition to start with.
With regard to groundnut, it is a tropical crop well suited for this area, and although yields vary from season to season, the average is about 1000kg ha-1 (Ishag,1999).

The optimal growing conditions for groundnuts is 30°C (Ishag, 1999), and 600mm of rain for good pod filling performance (Boote & Ketring, 1990), and these conditions are seen to be present in the data for three countries in West Africa (Figures 2, 3, 4, for rainfall and 5, 6, 7 in annex 1 for temperature).
Constraints to growing groundnuts in West Africa

Climatic
As has been mentioned above, one serious constraint to groundnut plant growth is potential evapotranspiration.

However, water supply is also important, as it is required for the pod filling stage of growth.
Labour

Labour constraints are a major problem in this region, as this determines the amount of land that is cultivated.
At subsistence level, mechanical cultivation equipment is uncommon due to the capital investment required for this.

Therefore, hand labour is most commonly seen and this is usually from family members, as employed labour is expensive (Kowal & Kassam, 1978).
The afore mentioned problem with HIV/AIDS causes serious labour shortages, not only those suffering from this disease, but also family members whose time and valuable resources are spent on caring for their loved ones (FAO: focus on Gwanda, 1997).

In years of low productivity, seed quality and seed supply for planting can be a constraint to following growing seasons.
Groundnut seeds are also prone to reduced viability during storage, or complete loss if insect and pest damage is high (Nautiyal, 2002).

Disease
There is only one bacterial pathogen which affects grounds nuts, Pseudomonas solanacearum.

This causes wilting of stems and foliage and can progress to total death of the plant, which would then have adverse affects on yield (Middleton et al.
1994).

The pathogen has a wide range of hosts, making it hard to control and prevent.
However, crop rotations provide some control, especially when rice is included in the rotation (Kelman & Cook, 1977).

Rice though is not a crop common to West Africa, and therefore simple rotations with different crops and their individual management conditions imposed for each crop provide significant improvements to continuous groundnut production on the same area of land (Middleton et al.
1994).

Groundnut is subject to a number of viral diseases, such as peanut mottle virus (PMV), peanut stripe virus (PStV), peanut stunt virus (PSV), peanut clump virus (PCV), and groundnut rosette virus (GRV).
All these viruses are transmitted by aphids, infected seed or the soil and are not exclusive to groundnuts, therefore making control by intercropping difficult (Middleton et al.

1994).
There have been developments in producing seed that is resistant to the rosette virus, however only for late maturing varieties, which could prove a constraint in years with less rainfall.

Early maturing varieties are being developed, which will make this less of a problem.
In all of the above diseases, yield is affected either by stunted growth of the plant or reduced ability to produce high quality groundnuts.

For example, PMV causes a reduced seed size, but does not reduce seed number, whereas PStV can present with altered seed flavour characteristics following infection (Ross et al.
1989).

Other problems, such as early leaf spot, late leaf spot, rust, phoma blight and powdery mildew to name a few, are caused by fungi that damage the foliage of the groundnut plant (Middleton et al.
1994).

Obviously, any damage caused to the leaves interferes with photosynthesis and therefore plant growth and seed production.
However the presence of such fungi is dependant on environmental conditions, almost all requiring 25-30ºC and high relative humidity, all of which are unfortunately common in West Africa.

Middleton et al.
(1994) proceeds to mention more fungal problems which affect stems, roots and pods.

Lastly, nematodes can be problematic, causing pod and root-knot disease, pod and root-lesion disease and yellow patch disease, which commonly occurs in West Africa, where it causes reduced leaf size and canopy development.
Nematicides are applied to help control the problem as there is no groundnut variety which is resistant.

Studies carried out by Germani et al.
(1985) and Germani & Gautreau (1976) have shown that applications of nematicides have increased groundnut yields by as much as 100% and reduced nematode populations in the soil.

Pests
The information available regarding insect pests in groundnut stands is extensive, however this knowledge requires further investigation as to the exact ecology of the pest species and their interactions with the plant (Middleton et al.

1994).
For simplicity, insect pests fall into three categories, soil insects, insects that live on the leaves and flowers and the insects which cause problems post-harvest.

Relatively simple things can help aid groundnut farmers, such as rainfall (which can wash aphids off the plants), and the presence of natural enemies aids control naturally.
Insecticides are used, however some pest species have become resistant.

Proposals for improving the groundnut agricultural system in West Africa
Regarding the control of pests, integrated pest management (IPM) offers great potential to these developing country farmers.

IPM can be defined as "a pest management system that, in the context of the total environment and the population dynamics of the pest species, utilizes all suitable techniques and methods in as compatible a manner as possible and maintains pest populations at levels below those causing economic injury" (Glass, 1975).
This essentially means preventing the establishment and spread of insect pests, controlling established infestations, or maintaining already existing pest/disease outbreaks at a level which causes minimum damage Lynch et al.

(1986).
Educating farmers to the possible benefits of IPM a) protection of the environment b) reduction of costs c) residue-free products, would undoubtedly be a good thing.

With groundnut being a cash crop and a legume, its importance to the grower is great.
Its use as an intercrop with cowpea and other cereal crops is important in maximising the output from the small areas of cultivated land and also in spreading the risk of loss, if disease/pest damage occurs in one of the crops planted.

More extensive use of the intercrop system would clearly be advantageous and a government-funded grant/aid scheme for the purchase of low tech mechanical cultivation equipment would help on every level, ie: area of land cultivated, labour constraints and urban demand.
Rain harvesting and/or irrigation systems would help reduce the reliance on seasonal variation in rainfall.

The introduction of a locally distributed seed programme to supply affordable high quality seed to growers, which would, in turn, if successful, be self-sustaining.
Reliance on a centralised seed supplier and associated problems would be avoided.

This has been successfully achieved in Malawi with the help of UNICEF and ICRISAT and the distribution of CG 7 variety of groundnut (Nautiyal, 2002).
Post harvest problems could be partly addressed by improvement in drying procedures and low cost storage facilities at farm level.

Archaeologists seek to find remains of the dead not simply because it is fascinating but also because it provides an authentication to people's past lives.
The physical remnants of the dead, for example: bones, hair and skin provide more details about the life of the individual rather than their actual death.

In this essay, I will attempt to understand past funerary practises, and why they were enacted in the ways that they were.
I will also talk about the discoveries made by archaeologists, their interpretations of the evidence, and also draw a conclusion as to how all this helps us, the modern day humans, in developing and shaping our own societies.

In general, archaeologists study the material traces of the disposal of the dead to make inferences about ethnicity and identity, religion and ideology, status, wealth, social hierarchy, as well as gender relations, health and diet, location and movement.
When examining the dead, archaeologists make inferences through the evidence given to them by means of studying bones and tissue remains.

We can find out "how long they lived, what sex they were, what illnesses or diseases they suffered, how tall they grew, what genetic ancestry they had, what sorts of foods they ate, what injuries they sustained and whether they were deliberately deformed, bound, tattooed, body-painted or even scarified." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) Mortuary analysis teaches us that sex, gender, age, cultural or ethnic identity, and various roles in the society help determine the type of burial, its associated ritual and the selection of objects buried with the dead.

One of the main factors which tells us about the individual and the way in which s/he was buried, is the position in which the body is found.
"Bodily positioning is an important part of the body's manipulation.

Rigor Mortis sets in within 12 hours of death and the dead body is normally arranged in some way while it is still flexible." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) The body may be curled up, laid on its side or flat on its back.

In arranging the body in a particular way, the living might have had a special purpose in doing so.
It could have been to differentiate between social groups (mainly gender and age groups.) One of the ways in which they did this in the Early Bronze Age was to bury males on their right sides and females on their left.

Another important factor includes orientation.
The direction which the corpse faces highlights what body positioning symbolises.

" The body is not simply a biological entity but is a carefully crafted artefact, further worked and transformed after the moment of death.
It is used to convey representations of death and the afterlife, of society's boundaries, of the nature of humanness, and of the ordering of the social world.

" The archaeological remains of the body are the culmination rites of passage which serve to separate the dead from the living and install them within another dimension of human understanding." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) There is another type of social differentiation apart from age and sex which appears in the Mesolithic cemetery of Oleni'Ostrov in Karelia.

" 'Wealth' of grave goods (the number of grave goods in a grave) tended to correspond with the presence of tooth pendants: bear with the 'wealthiest' (mostly with adult men), then elk or beaver (mostly with mature men and women of all ages), and finally those burials with no pendants (mostly old men).
They interpreted these differences as markers of physical prowess perhaps linked to food procurement.

There were also special status positions." Generally, men's grave goods tended to have more bronze and gold than women's.
Klavs Randsborg, in his analysis of social differentiation during the period of the Bronze Age, concluded that graves which contained more metal gave an indication as to the individual's social status; thereby implying that men were on a higher level.

When examined, the skeleton remains of the dead will disclose the gender and age at death of each individual, and perhaps any dietary deficit or other pathological condition.
It is hard to distinguish which grave goods go with which dead corpse because it is difficult to interpret, given the fact that they will be from communal or collective burials.

This is where more than one individual is buried.
Thus, it makes it easier to learn things from an individual grave rather than a communal one.

"A close analysis of grave goods can reveal much about disparities in social status.
One must take into account that what is buried with the deceased person is not simply the exact equivalent either of status or of material goods owned or used during life.

Burials are made by them to express and influence their relationships with others still alive as much as to symbolise or serve the dead." (Renfrew C.& Bahn P. "Archaeology: Theories, Methods and Practises" - 8 th edition) By analysing the remains, it would show the differences in male and female burials and help us evaluate whether the differentiation between the two signify a division in terms of wealth and status.
One other aspect of rank and status involves the age of the individual.

We must therefore establish whether this factor changed the way in which the deceased was treated in terms of grave goods.
"The archaeologist must ask, from the evidence available, whether the case in question is one of achieved status, or involves instead status ascribed through birth.

One useful criterion is to investigate whether children are in some cases given rich burial goods and other indications of preferential treatment.
If so, there may have been a system of hereditary ranking, because at such an early age, the child is unlikely to have reached status through personal distinction." (Renfrew C.& Bahn P. "Archaeology: Theories, Methods and Practises" - 8 th edition) When analysing the remains of a body, we can find out the diet and general health of an individual by carrying out forensic tests.

There are several useful methods of investigation, one of which includes a focus on humans bones.
Isotopic analyses of the skeletal remains of a population can show the types of food in the individual's diet as well as indicate the differences in nutrition between the wealthy and the poor members of a society.

"Dietary and health studies from human remains have significant potential for opening up new avenues of research into differential social status, when conducted hand-in-hand with other methods of analysis." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) The placement of the dead is another complex factor when it comes to burying the deceased.

During the period from the fifth to the third millennia BC, agricultural communities in western Europe constructed a variety of timber and stone monuments that were associated with the disposal of the dead.
Hundreds of these monuments are preserved to varying degrees to the present day.

These monuments give us a little bit of information about the societies who built them.
"Where to put the remains of the dead is generally not a matter of functional expediency.

The place of the dead in any society will have significant and powerful connotations within people's perceived social geographies." "Placing the dead is one of the most visible activities through which human societies map out and express their relationships to ancestors, land and the living." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) "External burial structures and cemeterial organisation were arguably more effective than grave goods because of the lasting visibility of many of these features.

Mound or chamber graves or restricted cemeteries served as exclusive markers of status in this world and the afterlife and conveyed to their audience, information about the hierarchical principles of that community." (Effros B. "Merovingian Mortuary Archaeology and the Making of the Early Middle Ages" 1965) Effros argues that while mortuary deposits played an important role, monuments and tombs were more effective in terms of spatial communication.
"Awareness of death and the marking of its occurrence among our fellow humans is supposedly something which is specific to our species.

We might consider our awareness of this aspect of the human condition as a fundamental defining characteristic of what it is to be human, at the very core of our being and self-consciousness." (Pearson M.P.
"The Archaeology of Death and Burial" - 1999) "The human race is the only one that knows it must die, and it knows this only through its experience.

A child brought up alone and transported to a desert island would have no more idea of death than a cat or plant." - VOLTAIRE Taphonomy describes the complex processes that affect an organism after death and ultimately determine if and how it is preserved.
It is "the assessment of what has happened to a bone between its deposition and its discovery.

Although bones have a better chance of preservation than plant material in most soils, they nevertheless survive only under special conditions- for example, if they are buried quickly, or deposited in caves." (Renfrew C.& Bahn P. "Archaeology: Theories, Methods and Practises" - 8 th edition) For my case study, I will illustrate how bog bodies are an important part of burial preservations.
It is the most well-preserved body in the entire world.

Found in 1991 near the Similaun glacier by German hikers, at an altitude of 3200m.
The body was the first prehistoric human found with his clothes on and his equipment surrounding him.

The body was given to the Innsbruck Anatomy department for treatment, and after careful examination, it was frozen at a temperature of -6oc.
A lot of work was carried out on the materials which were found near the body of Otzi, including x-rays, scans and radiocarbon dating.

It was thought that Otzi had died due to exhaustion on the mountain, probably caught in a snowstorm.
The body was protected from the movement of the glacier until a storm from the Sahara laid a layer of dust on the ice which absorbed sunlight and finally thawed it out.

Tests determined the physical appearance of Otzi; he was a dark-skinned male aged about 40 at the time of death and approximately 5ft 2in tall.
When found, he was bald and it is possible that he had a beard.

A full body scan showed evidence that he was very healthy apart from his lungs which were blackened (probably due to the smoke from open fires.) Some tattoos were found all over his body.
His nails had come off but one was recovered in the excavation.

It has been suggested that Otzi had died a violent death, due to the fact that he had been found with an arrowhead lodged in his left shoulder.
There were also cuts on his hands, wrists and ribcage.

To conclude this essay, I will talk about how the study of burials and funerary practises have in my opinion, helped to build a social structure.
"Burials are difficult to analyse and have their own problems, but they can be used to augment texts and pictures to give us a fuller understanding of one type of ritual.

From this, we can develop models of ancient social structures that make far greater allowance than previous interpretations for changes through time and space, for differing perceptions of structure by different groups, and for conflicts over meaning." (Morris I. "Death-Ritual and Social Structure in Classical Antiquity" - 1992) We can't treat each aspect of burial practises individually, instead, we must look at the context as a whole.
By trying to see the rituals from different social perspectives, we are able to figure out the ways and traditions of our ancestors.

"It might seem that archaeology is a straightforward process of discovery followed by description, yet it is accompanied by interpretation at every step.
Interpretation draws on theory- our rationalisations of our experiences in the world- in order to make sense of how and why people of the past treated their dead, disposed of their remains, and provided ways for the dead to co-exist with the living." (Pearson M.P.

"The Archaeology of Death and Burial" - 1999)
TASK ONE

The purpose of this task is to explore the multimodality of a television advert for "Dr Pepper".
It is important that there is a focus on the effects produced as a result of different modes which are used throughout the advert.

As Cook confirms, "a multiplicity of voices is a feature of many literary works" (2003:64): my aim is to explore the extent to which this multiplicity is present.
The table I have designed (see Appendix: Task One) was an attempt to include all the modes which were present in the advert.

My main aim was to make the table comprehensive enough to allow the advert to be re-created merely from its use.
The advert opens with the scene of a man and woman; the man explaining that he works in a children's hospital, despite having won the lottery.

This sentence represents the use of a symbolic code; a children's hospital worker depicts a caring, selfless person who works in order to meet the needs of others.
The intentions of the man are obviously to create this imagery for the woman.

It is apparent that the element of paralanguage is consistent throughout the advertisement.
Firstly, the couple are sat at a small table and so we are able to deduce that, as the physical distance between them is minimal, they are perhaps romantically involved.

This is also portrayed through the use of body language during the advert; they consistently make eye contact with each other and are also facing each other.
The speech is dialogic and consistently revolves around these two people.

The room is light and airy, and a majority of the advert focuses merely on the faces of the two main characters.
This composition, known as the technical code, causes the viewer to feel personally involved in the conversation which occurs between them.

There is a combination of both synchronous and asynchronous communication; the speech is immediate and simultaneous and the writing which appears towards the end of the advert has a time-delay as it must be read by the voice-over after it has appeared on screen.
The purpose of the singing could be to draw the viewers' attention to the product.

As the couple are in a busy restaurant, it is somewhat unusual for the man to start singing.
This action may cause people to remember the advert.

It can be noted also that the prosody changes when the man breaks into song; the volume increases causing the overall delivery of speech to dramatically change.
There is also the introduction of music in the background which works in conjunction with the speech and song to create the desired effect.

The two main functions of language are significantly highlighted; the referential function conveys important information for the consumer such as the name of the brand and the flavour of the drink whereas the affective function conveys feelings and attitudes which are put to good use in order to create an overall effect for the consumer.
At the end of the advertisement, the text is written directly above the product.

The letters are large and bold and the lettering is also a mixture of brown and cream, possibly with the intention of being the same colour as the product.
This graphology of the written discourse can be seen as yet another intentional outcome of the advert.

It is significant that the only piece of text used in the advert appears at the end, along with the visual image of the product.
This could be to leave the viewer with the lasting image of the drink and its slogan.

As Bell and Garrett state, the use of media can "influence its viewers and listeners by causing them to modify their understandings" (1998:221).
In summary, the combination of both the paralinguistic features and the different modes used within the text has the ability of creating an overall effect for the consumer.

It has become clear that each feature is carefully designed with a given incentive.
As Cook confirms, there are "carefully crafted words seeking to attract our attention, win our support, change our behaviour" (2003:63), this being the precise aim of the advertisement.

TASK TWO
The purpose of this part of the investigation was to compare the opinions of people of different ages.

I was eager to see if opinions on the perceptions of politeness would change or stay consistent over time.
I created a series of topics which all related to the broad aspect of politeness and ran a "pilot" test.

As a result of the pilot I simplified a number of the questions.
I asked the questions in a quiet area where only the interviewee and I were present.

My questions were answered by 3 males, each interviewee being a different age.
I decided not to use anyone who was studying the same course, as this would mean that the individual would undeniably have a previous knowledge base on the topic.

I wanted all participants to be of the same gender so that this variable could not be used as a possible reason for differentiation.
I made all participants aware that their answers would be used only for the purposes of my investigation and that their names and data would remain anonymous.

All three participants were willing to partake.
FORMULA I chose to create a semi-structured interview as this way I was remaining objective and trying not to dictate the answers of the interviewees, yet I would still be able to maintain a certain level of structure.

The six interviewing areas used were:
Manner and audienceManner and situationUse of standard EnglishPoliteness and classReceived pronunciationHow levels of politeness have changed in today's society

It is difficult ensuring that the interview is consistent from one participant to the next.
Suchman and Jordan explain how "the interview is an essentially interactional event as well" (2003:191) and go on to refer to how "the interviewer is charged with the responsibility of conducting inquiry in something of the manner of conversation" (2003:193).

For instance, if the interviewee was conversational there was less of a need for me to probe the person and request more information.
I tried to keep the questions fairly open-ended, allowing the interviewee to either give a short and succinct answer or to expand wherever necessary.

Closed questions have been heavily criticised and Converse and Presser refer to closed questions as questions which "force people to choose among offered alternatives instead of answering in their own words" (2004:5).
All participants seemed to be in agreement in terms of their own levels of politeness; they changed their mannerisms according to the perceived formality of the situation they were in and according to their audience/listener.

Crowley talks of how Standard English is "the use of a specific, uniform and communally accepted code" (2003:78).
All three participants were of the view that this "code" was becoming increasingly less noticeable in today's society.

However, their reasons for this perceived digression in levels of Standard English varied from generation to generation.
Technological advances such as the increase in texting and the use of computers seemed to be the main point of blame by the youngest and oldest generation.

This can be seen in theories such as that of the "Great Divide" in which it is believed that such technological advances have created a serious division between those who have access to modern technology and those who do not; literate and non-literate members in society.
Interviewee B in the intermediate age category blamed cultural influences for the decrease in the use of Standard English, and talked about influences from the USA.

I would have predicted that the younger generation would be in support of recent advances in modern technology, but my findings seemed to suggest the opposite.
All three interviewees also characterised Received Pronunciation as being associated with royalty and the upper classes.

Trudgill (2000:6) refers to Standard English as "the variety which is normally spoken by educated people", therefore reinforcing this concept of perceived superiority.
They were all of the opinion that levels of politeness are changing in today's society and that politeness has become less important.

The differentiation between the generations was most obvious with this question as the older interviewees tended to refer to how "younger" generations were becoming less polite.
The interviewee from the youngest age category merely stated that "people in general" were becoming less polite.

This finding tends to support my prediction that there is in fact a marked difference between generations.
The fact that older generations are witnessing a change in the levels of politeness of younger generations suggests that they were previously polite.

Interviewee C in fact went as far as saying that impoliteness has become "an accepted norm" in today's society.
As Trudgill boldly states, "social change can produce a corresponding linguistic change" (2006:17), and this seems to be the origin of the interviewees concerns.

Having completed both tasks, I concluded that the role of language in today's society has dramatically changed, and seems to be continuing to do so.
From the advertisement, it became clear that the linguistic features which were put to use have become both more advanced and more prominent.

According to Bell and Garrett, "media texts have moved far away from the traditional view...to take on a far broader definition to include speech, music and sound effects, image, and so on" (1998:3).
This would seem to support my view and suggest that the features used in media have changed a great deal.

I also came to the conclusion that interviews are not wholly reliable.
For instance, Foddy highlights how "ranking devices clearly give information about the order in which each respondent would place items.

They do not, however, provide information about the subjective importance...each respondent would assign to each item" (2004:351).
This was a cause of concern for one of my questions in the questionnaire which asked, "do you think that levels of politeness have become more or less prominent?".

My results could have been affected as a consequence.
However, the interviews did seem to highlight the changes that have taken place with regards to language and its function.

A majority of those interviewed came to a consensus that the role of language has taken a completely different form in recent years.
Also evident from my transcription was the variety of functions that our language can yield.

In media discourse, the persuasive powers of language become highly apparent.
Cook summarises this by telling of how, "language manipulates our feelings and thoughts in ways...which we are willing, even eager, to accept" (2003:63).

Our information about the 5 th to 8 th centuries AD is sparse.
Objects from this time are rarely preserved, due to the decomposition of artefacts.

One way of gathering information is through the understanding of Burial rites.
These burials are open to much interpretation and are not able to give information on all aspects of life at this time.

Burials can show how the people of the time wished to express their society in the areas of hierarchy, ethnicity and gender distinctions.
We also get information about immigration and trade.

Burial rites are mostly shaped by the religious views of the society so the burials could give a representation of the theology.
Changes in burial rites at the end of this time could show the rise of Christianity.

Within Britain in both early and late cemeteries, cremation and inhumation were practiced simultaneously.
Inhumations were generally single, extended bodies with grave goods, however there was great variation.

These grave goods are clothing and objects buried with the body.
Cremations mainly took the form of an urn filled with the ashes and not fully cremated bone.

These bone fragments were, in most cases, placed in anatomical order.
Cremations and inhumation burial practices have been found in all styles of cemetery, flat cemeteries, barrows and mounds.

These are practised in both the early and late periods (Wilson 1960:38).
This leads to a theory that different societies and religions influenced burial practice.

Burial rites from the 5 th century were generally based in the Pagan faith but later changed to encompass Christianity.
Burial goods found with cremations and inhumations are usually everyday objects or representations of everyday items (Wilson 1960:40).

Clasps and other objects found in burials match the items found in inhabited areas.
This means that there was no or little distinction between clothing of life and for death.

Use of everyday objects could show a belief in an afterlife in which the same requirements were necessary.
Finds of food and drink concur with this idea (Wilson 1960:39).

Orientation of burials is noted by Hawkes (1983) the majority of 7 th and 8 th century inhumations with there head to the west.
Lucy writes that Hawkes interprets this as directing the body towards the sunrise.

This would account for the slight variation in westerly positioning of the body, as the sunrise changes during the year.
It would also imply the use of the sunrise in a burial practice which would mean that some of the ceremony took place in the early morning.

From the positioning of the graves one can estimate the time of year the burial took place.
This theory is known as the sunrise dating hypothesis.

This idea has been countered notably by Brown (1983).
Lucy states that Brown razes the point that this dating technique vastly under represents deaths between November and February.

His interpretation assumes that the feet are facing east rather than the head looking east.
This correlates with the Christian view that at the second coming of Jesus of Nazareth will originate in the East.

He explains the discrepancies in direction as not knowing true east (Lucy 2000:132).
Goods and positioning are assumed to be good indicators of theology however they are open to many interpretations.

Peter Ucho states that "there is no necessary connection between burial rite and specific belief about afterlife" (Halsall 1995:62).
This statement highlights the problem of assigning theology to objects.

There is however a distinct move from graves with goods to west-east graves with no or few goods which is generally interpreted as showing the rise of Christianity The rise of Christianity is most tangibly shown within the archaeological record.
Christian burial is distinct from pagan burial in number of ways.

Christian burials were normally full length inhumations; these would be in a west-east alignment (head to the west).
They also contain no grave goods.

These graves were similar pagan burials in the type of grave used; mostly taking the form of dug-graves (Thomas 1971:109).
The move from pagan graves to Christian is not simple.

Some Christian Germanic tribes still deposited grave goods for a time after conversion; this is a sign of the significance of grave goods.
One third of burials found from the 5 th and 6 th centuries AD are oriented west-east, with the head west.

This increases to one half in later centuries (Hodges 1989: 131).
This could show an increase in Christian thought.

The regularity of the graves in cemeteries necessitates a form of grave marking; wood is the most likely material as few have survived.
The commemoration of graves seems to be a Romano British and European Christian practice.

Stones have been found in the form of stone slabs and pillars, they record the name of the deceased, his father and other details (Thomas 1971:107).
There is evidence of a change over time from cremations and inhumations with grave goods to inhumations specifically of west-east orientation.

This would seem to be an increase in Christianity.
Burial rites reflect many different parts of society.

Anglo-Saxon cemeteries are mostly not found on Roman sites or even on Roman roads, but seem to follow rivers (Wilson 1960:41).
This could mean that these invaders did not use the Roman roads but moved along valleys.

In north east Kent however Anglo-Saxon cemeteries do closely relate to Roman roads.
This implies use of the road system.

The early medieval cremation site at York is located on a Roman Cemetery.
Hassocks and Ringmer argue that this is accidental re-usage of the site (Wilson 1960:39).

Grave goods can reveal social structure.
Early graves contain lots of grave goods.

Modern interpretation of value is flawed as the value of objects is relative.
Halsall interprets these objects as a sign of wealth without estimating value.

Their main function being to show wealth, being visible for only a short time meant that maximum impact was essential.
This lead him to the wider assumption that there was an unstable society, as the rich were reminding others of there wealth and power.

He has also found that the most lavish burials are those of adult males, so the burial would be showing off the person's wealth and that of the family.
This is also backed up with the decrease in use of burial goods which Halsall argues coincides with a more stable society (Halsall 1995:66) the ship burial at Sutton Hoo is seen as a rich burial even though it has less physical wealth than other grave sties i.e. Childric's grave.

When judged against other graves from its time it is much more extravagant and rich.
Another sign of change is the introduction of row-graves, these are standardised and found in smaller cemeteries.

This means that less people could be present for the funeral and grave goods were less appropriate.
Grave goods can help to plot trade routs, they are generally exotic and expensive objects.

African Ivory chatelaine rings and Indian Ocean cowrie shells have been found within Anglo-Saxon graves (Wilson 1960:45).
This is good evidence of trade routs.

These may not be direct trade from Africans to Anglo-Saxons but is more likely through indirect trade from Europe.
The society at this time seems to be unstable disconnected from the fallen Roman society but still able to sustain trade with other countries.

The movement of people is not easy to determine from an archaeological record, a recordable change could be that of burial rites.
The distinctive cremation rite of north-west Germania, was adopted in England by the 5 th Century AD.

This adoption is the clearest archaeological evidence of cultural contamination from non-Romanised Germany (Higham1992:173).
Immigration has also been interpreted using grave goods.

Wilson uses the example of Saucer brooches.
These are found in south east England and the Thames valley, but are similar those associated with the Saxons of the lower Rhineland.

There are however stylistic differences (Wilson 1960:42).
From documentation we know that three different tribes came over to Britain, the Saxons, the Angles and the Jutes.

They came from the Netherlands and Rhineland regions.
The similarity of burial rites in England and Germany seems to back up the idea of immigration; however the archaeological record could have been miss-interpreted.

Halsall sites the cemeteries in Dieve-sur-Meuse and Lavoye (Meuse) France which have very distinctive grave goods and could be easily attributed to different groups but they are found only 20 Kilometres from each other.
This is very close and would mean there was probably sharing of background and ideas (Halsall 1995:59-60).

The documented and archaeological records do reinforce the idea of immigration and the burial rites can show differences but as Halsall states it is just an interpretation and is limited in its use.
Traditionally ethnicity had been associated with grave goods.

These goods are significant because of there importance within society.
The styles of grave goods have been analyzed and the distribution plotted.

Documentation states that three tribes settled in various parts of Britain.
The Saxons were in Essex, Wessex and Sussex, the Jutes in Northumbria and Kent and the Angles in East Anglia.

Grave evidence has shown that the distribution of objects, specifically brooches is widely varied.
Anglican Cruciform Brooches and Saxon saucer brooches are found almost all over England.

Jutish jewellery on the other had, been specifically found in Kent, this could show a very defined and specific group (Lucy 2000:132-7).
There is also a separation of burials with different brooches within cemeteries.

In Stratton-on-Foss, Warwickshire, the burials containing saucer brooches are found in a different area of that of small-long brooches (Hodges1989:133).
This evidence can be interpreted as just showing the fashions of the time but the separation in some cemeteries means that they may have been asserting cultural differences.

The wide distribution of artefacts shows that there was a mixture of styles.
However Halsall argues that the use of grave goods could be inaccurate in determining ethnicity stating that we associate ourselves with objects that are not necessarily those from our ethnic type (Halsall 1995:60).

A person could be born in Russia but be brought up in Spain and associate themselves with Spanish objects.
This means that the use of grave goods in this case could be flawed and unreliable.

In the study of gender they may be more consistent.
The grave goods with both inhumations and cremations are gendered.

These items have been traditionally used to identify the sex of the skeleton.
This is unreliable as gendered objects can be found in the opposite sexes grave.

Gendered male objects include weapons and female objects include brooches and jewellery.
Gendered female objects seem to reflect the styles of specific areas.

The separation of genders is evident in the orientation of graves in some cemeteries.
In Sewby and Yorkshire the majority of gendered female, Jewellery burials, are oriented south and south-west and the male, Weapon burials, are orientated in most other directions (Lucy 2000:132).

Gendered grave goods are only found in adult graves.
This has lead to coming of age studies, finding the age in which skeletons are first buried with goods.

In North Gaul women were found to come of age younger than men (Halsall 1995:69).
Differences in grave goods and orientation show that the concept of gender was important within society.

The distinction of death rites implies separation in life.
Burial rites are a lasting form of society.

How a group processed there dead can show variation in ideas and difference of lifestyle.
Burial rites are able to give us an indicator of the perception of difference such as gender and hierarchy but also economic factors such as trade.

The separation or inclusion of different types of society does show use how they were viewed at the time.
However this is a limited resource.

Many have tried to outline religious practice and belief, but these become conjecture as the burial record is not sufficient.
The study of ethnicity is also problematic as ones ancestral ethnicity is not necessarily there practiced ethnicity.

This means that although burial rites are important as a source of information it is limited.
The definition of burial is important when demonstrating the presence of deliberate Neanderthal burials.

A burial can be seen as the act or ceremony of burying, or the places were a body has been buried in the past.
Pettitt (Pettitt 2002), in his book The Neanderthal Dead, states that the majority of scholars would agree that some of the Neanderthal skeletons that have been found went through a burial process.

He also states that generalisations are made that consist of sweeping statements such as 'they did bury their dead' (Pettitt 2002:1) which are not necessarily useful or completely accurate.
There is a range of convincing burial sites which have mainly been found across the European area of the Neanderthal world - most of these have been brought into questioned as to whether they are actual burial sites.

I am going to analyse three of the most convincing demonstrations of deliberate burial.
The first example I am going to look at is the Iraqi Shanidar cave, then the Uzbekistani Teshik Tash and finally the French La Ferrassie cave.

I will then outline some arguments against Neanderthal burial and the opposition to these arguments.
The Shanidar cave-excavated between 1953-60, by Ralph Solecki- has produced the largest number of Middle Eastern Neanderthals burials.

Remains of a few vertebras up to almost complete skeletons have been found from at least nine individuals (Mellars 1996:378), these bodies range in age from the very young to the old (Stringer and Gamble 1993:98).
Carbon dating has shown that this site was used for depositing the dead for over 15,000 years, suggesting this was a long term mortuary site and implies the transmission of mortuary tradition or an oral tradition.

Limestone rocks have been found over specific graves and may have acted as markers.
One almost complete skeleton, the Shanidar 4 man, has been found to have pollen in the sediments in and around him.

Archaeologists have interpreted this as a deliberate burial, which included the placement of flowers over the body (Stringer and Gamble 1993:89).
However this has been challenged by others such as Pettitt, (Pettitt 2002) who argue that the pollen is not conclusive evidence.

Scientific analysis of the micro flora (small deposits of plants) gave two conflicting answers.
The flora found was from plants with medicinal properties which would suggest deliberate placing.

But, the nature of the pollen suggests that it was deposited after death by burrowing animals (Stringer and Gamble 1993:158).
This is a natural process which should happen in other places however the other skeletons in the cave do not have signs of flora in there sediments.

There are other factors that also point to this being a burial.
The crouched position of the skeleton has been interpreted, as with other 'burials', as deliberate placement.

There is evidence of crouching burials from the upper Palaeolithic.
R. Gargett argues that articulation is only possible if the person is buried, as when bodies decay without burial the bones found are fragmented.

On the other hand another interpretation could be that the man died in his sleep and his body was covered with soil by others to keep away the carnivores.
In 'The Neanderthal Legacy' Paul Mellars concluded that the pollen found is controversial evidence and unproven (Mellars 1996:380).

Conversely he uses the physical facts, such as the number of burials and the remains' intact nature, as evidence for burial.
He argues that burial creates the correct environment for the preservation of bones and the high concentration of individuals is evidence that the site was returned to repeatedly over time (Mellars 1996:381).

The cave site of Teshik Tash is in the mountains of Uzbekistan, 1,500m above sea level.
Within this cave the partial skeleton of an 8 year old boy (Stringer and Gamble 1993:75) was found surrounded by an apparently deliberate circle of horn cores which had been driven into the ground around the boy (Stringer and Gamble 1993:158).

The findings of child bone are rare and for them to have been preserved it suggests a deliberate act of burial.
This cave's occupation was between 75,000 years ago and 30,000 years ago.

The long occupation of the site means that the horn cores could have been placed there many years after his death; the cave was littered with horn cores from the Siberian Mountain goat, which could have by chance formed a circle around the body of the boy, however the positioning of the horns is quite uniform which could suggest their deliberate placement at the time of the 'burial'.
At this time carnivores were a threat to all hominids and a dead body would easily be scavenged.

These horns could have been placed deliberately to stop the body being taken and dismembered (Stringer and Gamble 1993:158).The caves out of the way location would mean that effort was needed to get to it for both animals and humans.
This could indicate that the burial was in an area of low carnivore activity so the threat would be less and protection may not be necessary.

The formation of the horn cores seems deliberate and the preservation of the body is an indicator of deliberate placement of the body but there is no strong evidence for religion or symbolism.
The La Ferrassie cave site was excavated at the beginning of the 20th century.

Level 1 of the cave was found in 1909, 2 1910 and 3&4 were found in 1912 (Stringer and Gamble 1993:14).
This site is important not only because of the collection of individuals found here, seven in total, (Mellars 1996:378) but the seeming symbolism of the burials.

Two bodies, a male and female, were found head to head, though not place in a pit (Stringer and Gamble 1993:159).
There is also a collection of shallow circular depressions with mounds, some headed with limestone blocks (Tattersall 1999:16).

These blocks are from the Aurignacian level and have engravings on them; these engravings have been interpreted as vulvae (Stringer and Gamble 1993:202).
This evidence points to the deliberate placement of the Neanderthal bodies after death.

The children's bones that have been found at this site suggest burial.
Children's bones are soft, so child bone must have been processed in some way to allow the preservation (Mellars 1996:379).

Also at so at this site possible symbolism has been found in the form of a bone with fine incised parallel lines (Stringer and Gamble 1993:161).
This could be symbolism, but it could also be a sign of a stage of bone production, i.e. removal of flesh.

Stringer and Gamble also assert that the sites may not be Mousterian and may be dated to a later time which would change the importance of these finds (Stringer and Gamble 1993:183).
The evidence at La Ferrassie seems to be inconclusive as to whether this is a burial however, as with Shanidar the number of burials found suggests a form of tradition surrounding the treatment of the dead.

I shall now analyse the arguments against Neanderthal burials.
One Archaeologist who argues against the idea of Neanderthal burial is Robert Gargett.

Mellars sees Gargett's view as stating that although they may have buried there died this was not a symbolic or religious ceremony (Mellars 1996:375).
Pettitt has outlined Gargett's arguments in his paper The Neanderthal Dead.

Gargett's evidence is from studying sedimentology, taphonomy and stratigraphy.
He asserts that finding full remains does not necessarily mean burial.

His arguments take the form of four distinct points in answer to what he sets out as five key questions.
Firstly, pits are not necessary for burial.

Secondly, rock falls and natural death could account for the complete findings.
Thirdly, caves create a good preservation environment; he also defines good sits as out-of-the-way-places.

Lastly more burials in caves are a reflection of the increase in cave dwelling hominids (Pettitt 2002:3-5).
These are the arguments in favour of Neanderthal burials.

Pettitts response is that Gargetts ideas over simplify the evidence.
His first point relates to Gargetts view that pits are not necessarily burials uses the 'old man' of La Chapelle aux saint, which is found in a pit.

Gargett describes this pit as being a natural formation, but is has straight sides, is a regular depth and is small so the body would have had to have been placed into it.
His second point is a response to Gargett's he disagrees that full skeletons are the result of cave-ins, he sites the Kabara skeleton which is in a pit which has been cut through two hearths on lower occupation levels (Pettitt 2002:4) which would suggest that the pit was dug with a purpose and was not just natural.

Gargett's third point is based on the hypothesis that most burials are in the edges of the cave were sediments may have been deposited naturally over time to preserve the body, however many burials are found in the centre of caves, which would not be a good place for natural preservation.
Gargett's last point is based on the idea that we find more skeletons in caves from this time because they were inhabited more.

But evidence from modern day and ancient cultures shows that burials are not always in the living spaces (Pettitt 2002:5) and are generally outside of these areas, notably the Romans who generally buried outside the city walls.
Pettitt concludes that Neanderthals focused there lives and deaths on the body not objects.

He also asserts that these practises are very rare and widely spread which suggests they are just a 'brief epiphenomenon' (Pettitt 2002:18).
Other forms of burial precise have been found from the Middle Palaeolithic.

One such practise is the production of bones.
The Krapina cave site in Croatia is home to almost 900 fragments of bones, from at least 14 people, some of which show cut marks.

An interpretation of these cut marks is that they were caused by cannibals however others such as Richard Klein in The Human Career, argue that there are many interpretations to the cut marks on the bones.
He suggests that they could be from defleshing after decomposition, but before burial, or even signs of carnivore activity.

There have been hyena tooth marks found in some of the bones (Klein 1999:468).
This would agree with Stringer and Gambles idea on carnivore activity.

His first interpretation would show repetition of practises and imply ritual; conversely the carnivore hypothesis shows a harsher reality of animals ravaging the bodies after death.
Either of these ideas is almost equally as probable but the regularity of the cut marks do imply to deliberate production of the bones.

There have been few sites found which show without doubt that Neanderthals buried there dead.
These case studies seem to show that even convincing evidence of burial is not necessarily a sign of religious burial.

The floral remains Shanidar have been proven to have been deposited after burial and the Teshik Tash horn cores are not necessarily part of any religious ceremony.
The engravings of vulvae at La Ferrise are the strongest sign of symbolism, but are also inconclusive.

Neanderthal burials would be a reflection on there culture, which may be more focused on the body than any grave goods.
There is evidence of the treatment of the dead in other mortuary practises which focus on the body.

The treatments mentioned may be just body dumping however the repetition of the use of a site and the articulation of the bodies do show some sort of processing of the dead.
Absolute dating methods have revolutionised geology and archaeology.

Within the last 50 years developments in this field have allowed us to create a historical and geological context.
Before absolute adding methods chronologies were created based on findings firstly on individual sites then linked together over regions, this method shows the development of objects but does not show the time scale, within pottery one style could have been used for hundreds of years and another for only ten, this could not be deduced accurately from a chronology.

They are also open to vast human error and prejudice.
Scientific dating methods have allowed for a model showing the rate of change over time.

Two of these methods are Radiocarbon Dating and the Uranium Series.
While they are both based on radioactive decay, they cover different time spans and substances.

They are both popular methods in Europe and have advanced our knowledge of pre-historic people.
Radiocarbon dating analyses the level of decay of carbon fourteen (14C), a radioactive carbon isotope produced naturally in the atmosphere.

The 14C isotope is produced in the Earth's atmosphere when cosmic rays, originating from the sun and the Milky Way galaxy, react with 14N in a nuclear reaction.
This 14C with the stable isotopes 12C and 13C are absorbed by all living creatures, by photosynthesis for plants and respiration or absorption from food for animals.

This process ends at death at which point the 14C starts to decay, however the stable isotopes remain constant.
It is this ratio of isotopes which is measured; an older sample will contain a smaller ratio of 14C as it will have decayed.

The first radio carbon dates were published by W. Libby in 1949.
Libby's early work found the half-life of carbon, the time it takes for half the atoms to decay, as 5568; however later studies have shown it to be 5730.

Radiocarbon Dating assumes a constant half life.
It can be used on objects from any climate; this means it is useable in any region.

The dating range for this method is 100 years ago to 50,000 years ago; which is quite limited.
This method can only be used on substances which contain carbon; this however limiting, still allows for many different organic substances to be dated, these include wood, sediments, peat, water and ice, bone, shell and marsh gas.

There are also substances which although capable of being dated produce unreliable results such as iron objects, rock varnishes and substances containing trace amounts of carbon (Geyham and Schleiher 1990:162).
In the late 1970's and early 1980's the introduction of gas counters meant that smaller samples were analysable.

The original method used 10-20g of wood to get 5g of pure carbon, however with gas counters only a few hundred mg of charcoal was needed.
A new form of 14C dating, accelerator mass spectrometry (AMS) has allowed for the dating of even smaller samples.

This opens up a new range of objects able to be dated including the accurate dating of individual hairs and seeds.
There was conjecture that this technique would allow for the earlier dating of objects as it is able to pick up smaller levels of 14C, however this does not seem to be the case.

A well known use of this technique was The Turin shroud.
This shroud shows the imprint of a body which many people believe to be Jesus of Nazareth.

In the 1980's it was independently dated in Oxford, Tucson and Zurich, each laboratory finding dated of between 1260AD and 1390AD, (Renfew and Bahn 2006:148)thus disproving it's association with Jesus.
The similarity in answers shows the reliability of this method.

There were two main assumptions made when carbon dating was first used: firstly that the ratio of each carbon isotope in the atmosphere was constant and secondly that the ratio in the marine reservoir was the same as the terrestrial reservoir.
Both theses assumptions have been shown to be incorrect.

The levels of 14C on the atmosphere have been affected drastically but immeasurably over the last 150 years.
The 19th century industrial revolution meant that large amounts of carbon that had been locked way in plant matter many thousands of years ago were being discharged.

This organic matter lacked 14C because it had all decayed into non radioactive isotopes (12C), so the ratio of 14C decreased.
This was found by Suess who noted a 2% decrease in radioactive carbon activity in 20 th century trees compared to 19 th century trees, this decrease is known as the "Suess effect".

Dendrochronology (the study of tree rings) has shown that there have been different levels of 14C in the atmosphere over time, this could be caused by changes in the magnetic field of the earth, the sun or climate change.
Conversely the atomic revolution of the early 20th century has released undocumented amounts of 14C within the atmosphere.

This will mean an increase in the ratio of 14C.
Both these affects, if not accounted for, would create inaccurate dating results.

To counter this affect all dates are aligned to the reference year AD 1950 and the letters BP are used to indicate this, BP meaning before present.
Assumptions made about the marine reservoir have also been disproved.

Finds dated from the marine reservoir and even finds of animals or humans who have eaten marine animals will show a different level of 14C.
The sea is affected by many different factors including the oceanic circulation affect, which is caused by river runoff diluting the water.

In Mesolithic Oronsay, off the west coast of Scotland, the human remains found were dated to 400 years older because of this affect (Renfew and Bahn 2006:147).
There are many variations due to the wide verity of factors involved even in small areas.

As with all techniques Radiocarbon dating is open to human error.
On site problems can lead to inaccurate dating.

Contamination before sampling can be caused by ground water depositing 14C deficient minerals, and after sampling contamination can occur by the addition of paper or cardboard in a sample or even the addition of modern mud, these can be corrected at the laboratory.
Even without contamination problems can occur.

Wood may be reused over time which would wrongly date a site to an earlier time, to combat this, items which would have been used once such as seeds or twigs create a more accurate date for the site.
The archaeologists taking samples must minimise contamination and have knowledge of the processes of the site and the materials found or the sample will produce a date which does not reliably add to the knowledge of the site.

The Uranium series focuses on 238U, 235U and 232Th.
This method is popular in Europe because the Potassium-Argon technique is not applicable as there are few suitable volcanic rocks in this area.

The uranium series is able to date substances from 100 years ago to 1,000,000 years ago, so it covers a wide era.
This process exploits the decay series of two forms of Uranium, 238U and 235U, both forms decay into radioactive daughters until the end of the series which terminates with stable isotopes of lead.

During this decay varying levels of the parent to daughter ratio exist.
In an older sample more of the parent will have decayed into the daughter so more daughter isotope will be present and vice versa.

The amount of parent and daughter isotope can be measured because each isotope's alpha radiation is admitted at a different frequency.
The original method of measuring this frequency and deducing the abundance is relatively accurate giving an accuracy of ± 12,000 for a 150,000 year old sample and about ± 25,000 for a 400,000 year old sample, however the new method of Thermal Ionisation Mass Spectrometry (TIMS), which directly measures the quantities of the isotopes, can give readings as accurate as <1000 for a sample of 100,000 years old (Renfrew and Bahn 2006:150).

Es Skhul, Mount Carmel, south of Haifa contains burial sites of varying ages.
There has been question over the length of time these burials were used and the dates given to them.

Using the uranium series with electron spin resonance (ESR) this site has been dated to 100 and 135 ka (Grűn et al, 2005:316-334).
These dates are verified by each other, this indicate the accuracy of this method.

The Uranium series however does rely on a change in state of the isotope to create a beginning for this decay, as the ratio change is measured there must be a constant ratio until the change which is to be calculated.
This series relies on the solubility of the isotopes calculated.

The Uranium isotopes 238U and 230U are soluble in water; where as there daughters 230Th and 231Pa respectively, are insoluble.
This means that when water with a high level of calcium carbonate and Uranium condenses on cave walls or floors and looses H2O through evaporation it leaves the solid calcium carbonate and uranium which can decay into its insoluble daughters.

Deposits used for uranium dating are found in caves or near lime rich springs in the form of stalagmites and stalactites and layers of sediment.
This method is useful as the remains of human inhabitants within caves such as bone or material culture can be found between layers of sediments and are there for datable.

Is also reliable with Coral as the marine environment means it is formed within a closed system, both the uranium series and radio carbon dating are able to produce relatively accurate readings.
This method requires different elements for dating than radio carbon dating which has meant it is useful with a different type of material; the substances tested are carbonates, phosphates, some silicate phases such as clinopyroxe and volcanic glass, and water (Calsteren, 2006) As with Radio carbon dating the Uranium series requires a closed system in which no radio active isotopes could join or be removed from.

There are assumptions made when using the uranium series, firstly that the decay of each parent and daughter is constant and secondly that the creation of daughter elements is limited to the decay of a parent element or has been accounted for and thirdly that the daughter isotope was of a known amount or absent at the beginning.
The nuclides used have comparatively short half lives, compared with those used on other scales such as the Potassium-Argon Technique.

This means that short scale processes, those of less than 1 million years, can be shown.
Nevertheless they must also be long enough to have not decayed completely in this time, as it is the ratio of parent and daughter which is measured.

The abundance of an isotope is linked with its half life; a daughter isotope will have a smaller abundance and half life than its parent.
Different methods are used because of the variation in the sample size.

A shorter half-life as in 231Pa needs the more sensitive radioactive detectors such as alpha spectrometers where as longer half-lives with higher abundances, such as that of 238U are measured using mass spectrometers.
Neither of these methods is destructive.

This is unlike Radiocarbon dating where the method of the extraction of carbon destroys the object.
This method has been used both to verify others and be judged against other methods.

The Uranium series with other absolute dating methods has shown that radiocarbon dating must be calibrated because of fluctuations in the level of 14C in the atmosphere.
Radiocarbon dating was used at a Chinese Palaeolithic site by Chen and Yan (1988) on tooth dentine and bone to show that they were preserved in a closed system, this backed up the Uranium series findings.

Radiocarbon dating has revolutionised archaeology.
It has meant we can date objects quite accurately which we can cross check with other methods.

The uranium series is able to date objects further back than radio carbon dating; there are however other dating methods which do date to the same time span.
Since both methods conceptions many problems have arisen and been compensated for, the main assumptions have also been refuted and compensated for and with Radiocarbon dating the half life of 14C has been reassessed and changed by 162 years.

This means that all findings before these recalibrations will be wrong and must be reassessed and has created the need for calibration tables.
For both, scientific discoveries have widened horizons, AMS has allowed for the dating of objects previously un-dateable such as the Turin shroud and TIMS has allowed for higher accuracy within the Uranium series.

Both methods are important to the development our knowledge of the past.
The Women of Trachis (one of Sophocles' less revered plays) was first performed sometime around the third quarter of the fifth century BC.

It centres on Deianeira, a female character who though not weak, is one of the most feminine and virtuous tragic heroines.
Women were usually represented through Athenian drama either as strong and powerful characters, maddened by the conflict of their passions, and driven by revenge (for example Medea in Euripides' Medea, or Clytemnestra in Aeschylus' Agamemnon).

Or as more docile models of the traditional expectations of women (such as Chrysothemis in Sophocles' Electra) who are tragically flawed with the natural defect of femininity (Phaedra in Euripides' Hippolytus).
Both avenues seem set to highlight the shortcomings of the female gender, but as all sources written with regard to women are solely by men for a predominantly male audience, there is some sense in that women were regarded as inferior and the possibility of their strength was something to be feared.

Knowing exactly how fifth century Athenian women felt and behaved is impossible, as all sources alluding to women and the view towards them were written by men, and thus lacking a women's perspective on the reality of their world.
Greek drama, ancient pottery and legal archives can shed light to a certain extent the attitude held towards them by men.

Freeman puts forward the suggestion that although it is commonly believed the seclusion of women within the home was total, in comedy and tragedy women are portrayed outside the house.
Cohen suggests that despite the view that most women would never step out of their threshold, men were aware of women's daily activities outside the home.

Deianeira remains inside her household at all times however, giving the robe to Lichas to deliver to Heracles, and sending her son Hyllus to find him rather than to go herself.
She is portrayed as a virtuous and respectable woman in this sense, and fulfils the expectations of how a reputable woman should behave.

The expectations of a woman centred largely on the belief that women were the inferior gender.
Their primary function was to get married and bear sons, and they would ideally feel a strong sense of family loyalty.

Freeman details the nature of marriages, claiming that 'The most important moment of transition in a women's life was marriage." 4 Love played little part in the arrangement of marriages, however a strong loyalty to the husband should be present.
4 This is illustrated in Women of Trachis, as although Heracles had claimed Deianeira through violence, and the marriage had been made without love, Deianeira had developed a loyalty and in her own way a love for her husband, or at least a desire to keep him for herself.

In providing Heracles with many children she has well fulfilled her role as a wife, but despite her being portrayed as such an admirable example of womanhood, she still cannot ultimately prevent the downfall of her and her husband.
An ideal woman would be quiet, controlled and demure although women were always regarded as foolish and emotional, and thus were rarely expected to adhere to the ideal.

Women were generally viewed as very feminine, with stereotypical activities and mannerisms flanking this view of girlish femininity.
There were places within the household especially for sewing and weaving, activities which are distinctly feminine.

Deianeira exhibits femininity in this way, but paradoxically it culminates in Heracles' death, a contrastingly masculine action.
However, every aspect of the murder of her husband is done in a feminine way: it is accidental - a tragic, but very feminine mistake borne through her womanly desires and naivety (and perhaps in Athenian perspective typical female stupidity) in believing that the Centaur might try to help her despite her having been the reason for his demise; it is committed by means of a poisoned garment she has made for him (the making of it is in itself a feminine action) where both the use of poison and the deception involved in the attempt to trick him into wanting her are distinctive feminine actions; the motivation behind his death is that of love, or more importantly jealous desire, both emotions Athenians would have considered feminine.

It would have been expected that a women would be subject to jealousy and desire when her sexual life was under threat by another.
Freeman states that 'women's sexual desires were assumed (by men) to be strong." 4 A view supported by the dismayed reaction of the chorus women in Lysistrata when one proposes that they should deny the men sex in order to get their way.

Similarly it would appear that the Athenians expected women to be highly emotional, and were 'subject to particularly strong and threatening feelings and that suppression of their instincts by men was justified through fear of the emotional havoc they could wreak." 4 In all examples of Greek Tragedy it is evident that the Greeks saw women as prone to intense emotion, something which can be exploited in Greek drama to expose the fears of men.
4 The fact that the women featured in tragedy are not Athenian and indeed may not even be Greek, allows a playwright to convey a message that could easily apply to Athens within the safety of a context similar enough to relate to but distant enough to avoid public outcry.

Similarly characters of tragedy are mythical rather than historical figures.
Because of this the flaws associated with women could be exaggerated, and more outrageous behaviour can be expected from them; the Athenians would probably have felt superior to many other civilisations and as such would have expected more outrageous behaviour as an example of how dangerous women could become if left unchecked and uncontrolled.

Many female characters of tragedy subvert traditional expectations of women, exhibiting masculine characteristics and are strong, powerful forces of destruction (like Medea and Clytemnestra).
In this way, perhaps Greek tragedy is more a representation of how women could become, rather than how women were expected to be.

The fundamental belief that the purpose of women was to bear sons seems to hold fast through all societies.
There do, however, seem to be slight differences in the limitations on the women's lives from place to place, which could effect the expectations of the Athenian audience of the female characters.

Freeman indicates that Spartan women for example, enjoyed a greater freedom than Athenian women.
The men of Sparta, automatically drafted into military service and away from home would be leaving the women to their own devices.

This can be seen in Aristophanes' Lysistrata.
Setting tragedies in a distant country (like Sparta) could have changed the audience's expectations of female representation.

When watching drama set in a scenario where women are given more freedom (as in Sparta) the dangers of their nature can be allowed to be exploited.
Instead of the typical, traditional woman, the playwright can present his audience with a much stronger woman.

The audience would get used to seeing women represented in such a way.
In Women of Trachis however, (which is set a little closer to home, in Northern Greece) Deianeira is less of a calculated murderess, and more of a feminine character, and can be shown much more to be a representative fifth century female, despite her not being Athenian.

Thus those in the audience expecting to see a vengeance-crazed woman driven mad by jealousy would instead be presented with a gentle and submissive manifestation of womanhood.
Deianeira describes the piteous lives of women and relates it to her own torments, describing girlhood as a young plant living in sheltered regions, free to live an untainted and pleasurable life, and then the sudden leap into adulthood where marriage introduces troubles of the night.

Her monologue details her fears for her husband and children, a distinctly feminine train of thought, and her frightening loss of innocence.
She can relate the life of women back to her own story, which further highlights the misfortunes of females.

The fight between the river Achelous and Heracles is an intimidating display of masculinity which contrasts starkly with Deianeira's femininity and innocence as she crosses into adulthood.
She was fought over to be claimed as a bride, and was also molested by the centaur, Nessus, as a result of her beauty, and her terror reflects well on traditional Greek views of femininity.

Similarly, as she tells how men will always stray as soon as one beauty gets old and a new one comes along.
She relates this to her own situation, and yet she cannot find it in herself to be angry at Heracles.

She accepts it, which is the mark of a good woman.
However, instead of anger she is filled with a jealous desire, which causes just as much damage, albeit in innocence.

Neither is she angry at Iole, who is the cause of her anguish albeit, again, in innocence and unfortunate circumstances.
Deianeira's immediate suicide and deep shame as a response to her husband's death indicate a sense of honour which mirrors that of Athenian ideals.

Although the women of the play are not the powerful, blood-thirsty women representing women in most drama, these women still represent an expected view of Athenian women; one of noble strength and flawed idealism.
Desire is a theme which recurs in Greek drama.

Female desire is said to be the cause of many problems, and it is the strength of the female sexuality which is the cause for social damage within the plays.
Men would have to restrain the sexuality in order to protect society from destruction.

It is Deianeira's desire which causes many of the problems within the Women of Trachis.
She is struck with desire for her husband and cannot bare the thought of him sharing their bed with another woman.

Goldhill examined how desire is a sure way to doom in tragedy: 'Every woman who expresses sexual desire, even for their husband, causes the violent destruction of the household'.
Deianeira's excessive want for her husband reflects the common view of women's imperfections and as such shows that she fits the expectations of a fifth century Athenian woman.

Contrastingly Iole's lack of desire for sexual contact with Heracles, indeed her complete refusal to communicate with anyone, indicates her nobility, and casts her in a good light.
This portrayal of women is surprising and unexpected in comparison to other tragedies.

In conclusion, I believe that yes, the women in Women of Trachis are portrayed in a way that a fifth century Athenian audience would have expected, as Deianeira behaves predominantly in the way any respectable Athenian should, and is subject to the standard flaws that they would expect women to fall pray to, although admittedly she's not the typical powerful, sinning tragic heroine.
She exhibits behaviour classically associated to that of women in Greek society: she is a concerned mother, loyal and passionate wife, and frightened by the violence surrounding her marriage.

Her tragic error was a foolish, feminine mistake borne out of jealousy and desire, typical traits associated with women in Ancient Athens.
However, despite her femininity, she is not a weak character, as she has the strength and dignity to kill herself, she inverts the gender roles between her and Heracles as he weeps like a woman, and however unwittingly and unwillingly, she kills a man who could be killed by no other male.

Iole is silent and respectful, and the nurse (more surprisingly) is wise and helpful.
Despite erring, all the women are portrayed in quite a good light with gentle pathos, which is slightly unusual for a Greek Tragedy, but their faults are still very much that of women, and as such fit the profile of Athenian expectation.

Virgil uses the Aeneid to showcase an exemplary Roman character, whose qualities and characteristics were inherently considered ideal for a Roman citizen, and so act as a role model for both citizen and leader alike.
Williams sums this up in his précis of the heroic character in his book Aeneas and the Roman Hero: 'Virgil had to create in his hero a prototype of the Roman character, a person who showed by his behaviour the kind of qualities which had made Rome great and would make her greater still.

He had to be an ideal Roman'.
Virgil takes the story of the founding of Rome and writes it using the popular media of epic poetry, both embracing and modernising Homeric technique in order to create a new type of hero relevant to a Roman society.

This Roman hero would have to adhere to Roman virtues and reflect the Roman ideal; being pious and devout (subservient to the gods and his fate); bound by duty; devoted to family and country; and emotionally controlled.
These traditional values were appreciated by both Virgil and Augustus, who passed laws that "set limits to expenditure, encouraged family life, penalised sexual laxity." A most important feature is the idea of pietas, a Latin word used to encapsulate a devotion to all virtues; the gods, family, and country.

Aeneas fulfils these through being a devoted father and son, a devoutly pious man, putting the future of Rome above all personal desires and remaining committed to his destiny.
In this way, Virgil is able to portray the importance of loyalty to the Roman state, an importance of morality and virtuousness, and a sense of emotional peace and control that all Romans (including Augustus) could aspire to.

Comparing it to Greek models of heroism can help to enhance our view of Aeneas' Roman characteristics, but can also show similarities between them, occasionally bringing to the surface some of Aeneas' more Homeric tendencies.
On the whole, Aeneas can be regarded as a predominantly Roman hero, however I believe Virgil highlights certain elements of Aeneas' character that are more Homeric and as such, unsavoury to a Roman audience.

It could be that Virgil is trying to illustrate human weakness, and although maintains an ideal to aspire to, realises that we can never completely achieve it.
We can also examine what would define Roman heroism and compare it to that of Homeric (Greek), however Virgil's presentation of his Roman hero is modelled on Homer's idea of the epic, and so similarities are frequent.

Most often born of mortal parents, Roman heroes would be given a quest in which they must remain pious and devoted Roman citizens.
Obstacles would be presented in the form of other mortals rather than the mythical creatures of Greek epic and must remain loyal to their country and cause.

Greek heroes, by contrast, would be semi-divine, and fulfilling quests for personal glory and reputation.
Aeneas bridges the two definitions in this instance, as he is semi-divine (Venus is his mother) and although his main enemies are mortals, fights a mythical being as well.

He is presented as a rather more selfless leader than that of Greek epic, devoted to his quest for the greater good of the future of Rome.
Although in regards to traditional Roman heroes he shares characteristics with the presentation of Greek heroes, observing Aeneas' Roman characteristics within his heroism is rather more successful.

Virgil's Roman heroism is at odds with the Homeric heroism of the Odyssey and Iliad.
The ideals of Rome are reflected in the heroism that Virgil depicts in the Aeneid, emphasising family, piety and duty.

As a result Aeneas is a rather different hero to Odysseus; Virgil intends to reflect Roman ideals to portray a model Roman citizen and encourage national pride, whereas the Homeric heroes are rather less human, and are self-serving and glory-seeking, portraying supremacy.
Gransden summarises his heroism in describing him as 'willing and ready to subordinate his individual will to that of destiny, the commonwealth and the future, reluctant to fight and not really interested in victory.' Aeneas has some instincts of a Greek hero (these are most clearly illustrated in Books 10, 11 and 12 where he is fighting against Turnus and the Italians, upon hearing of Pallas' death and ruthlessly taking human sacrifices in anger, ((a reflection of Achilles in the Iliad when he is revenging Petroklos' death)) and upon killing Turnus when he should have showed mercy) but he has to countermand these in favour of Roman values and a selfless duty to the future state of Rome.

Aeneas has to consider the future of an entire race of people and retain his devotion to his quest to found Rome, a far less selfish hero than that of the Homeric kind, where victory is motivated by personal glory.
Aeneas cannot be an individualistic and selfish hero.

Williams explains that he has to be 'the social man', always concerned with the good of his people in order to achieve the destiny of Rome.
He has to be aware of his responsibility to his duty which includes looking out for the welfare of others without submitting to his own desires or becoming selfish and self-important.

Aeneas is often described as being 'pius', (pietas is the noun) a Latin word encompassing the qualities aforementioned.
It is a concept that was a vital Roman virtue, and is probably Aeneas' most treasured quality from an Augustan perspective.

Pietas incorporates an immovable devotion to the gods, ones family, ones friends and one's country (reflected in his unshakeable dedication to achieving his destiny of founding Rome).
Pietas can be coupled with the idea of self-sacrifice, another quality which Aeneas is in possession of that is notably Roman.

This could be said to be a reflection of Augustus' interest in Stoicism and its "strong emphasis on duty, and a stern, self-denying self control...ready to suffer and obey." His dedication to the quest, and his duty, forces him to sacrifice his personal life for the benefit of future generations and follow his divine duty to the best of his ability.
This is particularly illustrated in Book 4 when he abandons Dido to her fate after their love affair in Carthage: "But Aeneas was faithful to his duty.

Much as he longed to soothe her and console her sorrow...with a heart shaken by his great love...carried out the commands of the gods and went back to his ships." (Book 4, p. 80) A major aspect of Virgil's heroism is to show Aeneas as both heroic and human.
Aeneas is a good, strong leader, as is the case for Greek heroes, but also suffers from moments of self-doubt and depression: "But the disaster had made him despondent and uncertain, and he reaches here his lowest ebb, actually wondering whether to 'forget the fates'." He exhibits characteristics which humanise him, enabling Virgil to communicate to a contemporary audience the accessibility of Aeneas as a role model.

Homeric heroes are typically semi-divine, self-assured and single-mindedly determined to succeed.
In order to be perceived as great beings they need little human weakness, and this sets them apart from the general public.

In contrast, Virgil's Roman hero is shown to be inherently human, despite Aeneas having a divine parent, and as such can act as a vehicle for Augustus and the Roman government to portray the model Roman citizen.
The model Roman citizen however, requires Aeneas to become detached and prohibits emotional involvement, which could be said to dehumanise him.

During his visit to the underworld he exhibits a newfound stoicism: "Suffering cannot come to me in any new or unseen form.
I have already known it...

I have lived it all before".
(Book 6, p118) His subsequent self control and total devotion to his destiny are characteristically Roman; this is juxtaposed, however, with his human weaknesses early on and at the end of the poem, exemplifying how someone who is intrinsically human should master his emotions in order to fulfil the will of the gods and his destiny.

It is Aeneas' destiny to found Rome.
He is unable to follow his own will if he is to follow his destiny.

This is exemplified in book 4 where he has to leave Dido in order to pursue his destiny in Italy and explains that "it is not by my own will that I search for Italy." (Book 4, p79) It seems to be a characteristic Roman feature - to put duty to Rome above personal desires and to follow a supreme destiny as opposed to a personal quest.
Despite Aeneas' seeming desire to stay with Dido, he still proves his dedication to his greater cause by suggesting to her that he had no intention of lingering in Carthage and that his love lies with the future of his Trojan people.

He also backs his argument with the simple fact that leaving Carthage is beyond his control; the gods had demanded his devotion to the future of Rome.
Despite his claims, he has the choice as to whether or not he follows his destiny, and it is by his own will that he pursues it; it is the content of his destiny he has no control over: "Though Aeneas is commanded by a higher power, he is not compelled, and it is precisely the circumstance that his will is free and his decisions that distinguish his situation." The gods come to his assistance throughout the epic and help him to achieve settlement of the Trojan people in Rome, and always encourage him when the quest is neglected or being threatened with abandonment.

Aeneas' devotion to the gods is a Roman ideal and divine intervention is key to his success.
Turnus' furor juxtaposed against Aeneas' pietas exemplifies the different ideals of Homeric and Roman heroism.

Turnus represents the Greek mould of hero, exhibiting obvious physical prowess, energy and violence, a merciless individualist who fights for his own personal glory and gain.
Aeneas, on the other hand, is controlled, previously self-doubting but by the climax of the poem in possession of a quiet and assured strength, and fights in fulfilment of duty and destiny.

Aeneas does not choose to fight, but Turnus is more than willing to resort to violence to obtain grandeur.
Virgil engages the use of stock epithets to attribute particular characteristics to individuals and these can be used to identify the heroic differences between Aeneas and Turnus.

Aeneas is frequently referred to as 'just' and 'good', and of course 'pius', the Latin word which encapsulates his pietas - his devotion to duty.
The epithets prescribed to Turnus reflect the Homeric heroic model: 'proud', 'bold', 'violent', 'frenzied', and 'burning', with particular reference to his 'furor' - wild and passionate heroic anger.

Aeneas' moment of heroic furor in which he savagely kills Turnus is among his most Homeric action in the poem.
Virgil appears to attempt to depict Aeneas as displaying Roman ideals, but in places, such as Turnus's death, Aeneas shows signs of Greek heroism.

Although he becomes more of a Roman hero throughout the Aeneid, Turnus' death at the end of the work exhibits Aeneas at his most Homeric in terms of heroism.
When Aeneas gives in to his anger and kills Turnus, the fact that it has lead to the completion of his destiny can be seen as an event where the end justifies the means: "...this aggressive quality in Aeneas, which in another character be evidence of primitive, anachronistic emotions, seems to be redeemed by the end it serves." Aeneas is able to express emotion and take revenge upon Turnus because he knows it will not interfere with his destiny and the good of the future Rome, and so can submit to his humanity.

This is not to say however, that Aeneas' moment of emotional weakness can be excused, as it is so important for Aeneas as a reflection of a Roman ideal to control the passions of his emotions.
If he is to give in to his fury, as he does here, then he jeopardises the vision of a Roman future where the civilisation Aeneas brings will stamp out the archaic need to give in to violent emotion and chaos.

Perhaps Virgil intends to present this dilemma to a contemporary audience to reflect the nature of the human condition, and to illustrate that we can only suppress our impulses so far.
The Romans can bring civilisation, but not to a full and all-encompassing degree.

In conclusion, Aeneas is very much Roman hero, encapsulating Roman ideals of pietas, being presented as accessibly human and as a result subject to weakness and lapses into Homeric characteristics.
He remains dedicated to his destiny no matter what the cost, however, his attempts to check his emotions and control his furor are often compromised, and the vision of Rome as a civilising force where emotional control is key to peace is proved impossible to achieve completely.

Aeneas may be characteristically Roman by majority, but Virgil seems to prove it impossible to completely adhere to Augustan ideals, perhaps a reflection of his view of Augustus' visions for the future Roman Empire, or indeed human nature in general.
The hierarchical structure of ancient Greece with regards to sexuality between mortals and gods alike is lead by the elevated status of the gods.

The gods are not bound by mortal morality and as such can do as they please, and within the divine structure Zeus as the father of the gods, retains dominance over the other divinities.
We must analyse the myths involving sexual encounters and interpret them, and decide whether or not they are reflections of social concerns, taking for granted that the myths detailing rape and/or abduction will examine hierarchies of power and status by default.

Zeitlin certainly suggests that the former is possible: "Myths...explore and express the complexity of cultural norms, values and preoccupations...raise questions especially about nature, the universe and the gods...family, society and the 'human condition'".
When exploring social concerns, I am referring to the role of women in Greek society, and the attitude of men towards them.

The women's desire (or lack thereof in the majority of cases) to be involved in these sexual encounters are rarely considered; in such a patriarchal society the focus is on attempting to demonstrate masculine superiority, engaging the ideals of the male being the active subject, and the female the passive object.
One must bear in mind when examining the myths, however, that there is not one definitive interpretation of a myth, and that one must consider the media through which the myth is being communicated.

The interpretation of the sexual encounter in each particular instance will depend on the artist and the person responding to the media.
An example of rape within divinity is Hades abduction of the daughter of the goddess Demeter, Persephone.

Hades kidnaps her and takes her by force to the underworld despite her protestations, and as such is fairly simple to identify as a rape as opposed to another form of sexual encounter.
This would suggest that Zeus (who approved the match) and Hades where in possession of high status in this instance, leading the hierarchy because of their rank as divine kings, and as men.

Morford and Lenardon propose that "a religious artist or critic might maintain that god's will is god's will, and it was divinely ordained to have Hades and Persephone as king and queen of the Underworld." The rape also teaches about social concerns, in reflection to that of forced marriage, providing a "cultural archetype of marriage as a forcible abduction".
This rape reflects how a female can have no control over who she is married to, and highlights the development from maiden to woman, the death of childhood and the birth of adulthood at the loss of virginity.

This is forced upon Persephone but is taken as Hades' right, and she must just accept it.
Ovid describes the speed of the rape as "breathtaking", in his Metamorphoses, and Curren relates this to hierarchies of power: "At other times rape is instantaneous in order to demonstrate the helplessness of woman in the face of overwhelming male superiority." The account of this rape in the Homeric Hymn to Demeter clearly describes the nature of Hades' attack on Persephone; catching her unawares he "sprang upon her...caught hold of her, protesting...weeping...then she screamed in a shrill voice...no one heard".

This is not presented as Hades' divine right as is often accepted of divinity, but more so as a brutal rape, and as a contemporary source suggests a contemplation of the distressing nature of such an act.
This is largely un-sustained however, as most women (for example Semele, Dionysus' mother, who was 'raped' ((although even this is debatable)) by Zeus) are treated with disdain and often disowned after having been involved with sexual relations, desired or otherwise.

In contrast to sexual encounter amongst divinity, that of the two mortals Paris and Helen is portrayed in completely different light.
Helen may claim to have been taken by force in her defence of her actions in Euripides' Women of Troy, and sometimes their affair is referred to as 'The Rape of Helen', but Homer writes her in the Iliad as "her heart having quickened as she looked at him", clearly demonstrating a connection between them be it love or lust, which does not support her having not acquiesced to the affair.

She consistently refers to herself as "bitch that I am", "whore that I am", "slut that I am", throughout the Iliad, and it becomes clear that she views herself as being in the wrong, not suggestive of having been raped or taken against her will.
Suggestions of abduction arise in the instances in which she tries to defend herself, and can be seen as defensive manipulations of the truth.

This further supports the idea that interpretation of a situation is key to defining what is rape and what is not, and how it is more often than not merely a matter of opinion.
"The designation of the seduction/abduction of Helen by Paris as the 'Rape of Helen' was established at a time when the word 'rape' did not necessarily have the narrow, sole connotation it has today, that of a brutal, forceful sexual act against an unwilling partner." In this example, from a social point of view, we can observe the severe male bias against the female.

Helen shoulders much of the blame amongst the peoples of Troy and Greece; not Helen and Paris, and not the gods.
The most common form of sexual assault found in mythology is between that of male gods and human females.

Apollo's attempted rape of the nymph Daphne causes a metamorphosis; in her attempt to escape Apollo's attentions she asks to have her beauty destroyed to retain her virginity, and in answer of her prayer she is turned into a Laurel tree.
Apollo, still in love with her, takes its leaves and wreathes his hair with them.

In bitter reflection of the blame that is unfairly given to women because of their inferiority, "Daphne curses her beauty because it has made her too desirable." Women are raised on the principal that if something happens to them they must have done something to provoke it: "There are few from whom the victim can expect sympathy or comfort.
She and not the rapist is the one who must bear the injury, the guilt, society's blame, and the punishment." This can be supported by two similar example involving a metamorphosis.

One is that of Caenis and Poseidon.
She is raped by Poseidon and granted one wish; that she may be transformed into a man so that she may never again be penetrated and escape her fate as a woman, so becoming invulnerable.

She is still killed however; she is crushed beneath falling rocks.
Io suffers both under the rape of Zeus and a subsequent punishment from a jealous Hera, who turns her into a cow tormented forever by a gadfly: "Ovid subordinates the physical discomforts of Io's new life as a heifer to her psychological suffering...her ludicrous appearance expresses the humiliation and mortification to which the raped are subject...like a woman in a small town who must endure the stares of all in their knowledge that she has been raped." A rather unusual manifestation of rape is Aphrodite's seduction of the mortal Anchises in a Homeric Hymn.

She disguises herself as a "young virgin" and convinces him to sleep with her.
Afterwards she reveals her true identity, and he begs her to make him an immortal, for "no man retains his full strength who sleeps with an immortal goddess." It brings to the fore the issue of status; a man must never be in a passive position against a woman or else he becomes "enfeebled", and in this instance he has been involved in a submissive sexual encounter as a goddess is a superior being.

The gods and goddesses may behave as they wish, and mortals must submit to their will.
In conclusion, I believe the mythic accounts of various rape or sexual encounter highlight both hierarchies of power, reflecting the status of the gods and their ability to do as they please, and other social concerns, detailing the position of women and their unjust acceptance of the blame that should otherwise belong to the gods.

They teach of the punishments inflicted upon the victims, and how they must bear the guilt that should be reserved for the attacker.
The nature of interpretation of what is and what is not rape is also brought to the fore.

Hybrid creatures are a combination of human and animal, exposing a caricature of the base and bestial side of man.
Their violent and exaggerated behaviours are perhaps a representation of uncontrollability, allegories of chaos which must be overcome under the watchful eyes of the gods, in order to free society from their evils.

Both the hybrid and monstrous creatures appear to symbolise the forces that threaten civilisation, presented by the gods and conquerable only by heroes.
Hybrids can range from savage centaurs to beautiful mermaids, and in a reflection of the changing temperament of nature, those which might at first seem beautiful and enticing like the sirens and mermaids, can prove treacherous and fatal upon closer inspection.

In an exploration of roles played by monstrous and hybrid creatures we expose society's need for a manifestation of evil that can be defeated, a basis of control where in a Greek society the gods can bring order to chaos and make sense of the evils in the world, be in morality or in the violence of nature.
Without them there would be nothing for the heroes to conquer, and the function of the mythology would break down.

Hybrid and monstrous creatures are perhaps some of the most intriguing aspects of mythology, capturing the imaginations of many throughout time.
What makes them so fascinating is different from one individual to the next, but when theorising one might be able to attribute their allure to that which sets them apart from humans, the fantasy aspect of their existence.

Many characters in mythology are either human or human-like in appearance (the gods) and although did not exist, because of their physical manifestation being in human form could potentially have existed at some point.
Monsters and hybrids are very much creations of fantasy, products of the imagination that certainly do not exist, and so the possibilities and scope of their functions limitless.

They can materialise in the imagination of a person in any way that person desires, fulfilling the function that that person requires of it.
Humans are by their nature fascinated with things that are different to themselves, and the intrigue of something so physically alien embodies the oxymoron of an attraction to the exotic, a difference which simultaneously acts as a barrier.

The variety of different perceptions of these creatures mirrors the variety of roles that they can be said to play in Greek myths.
One of the roles of hybrid and monstrous creatures can be attributed, on a basic level, to the structure of a myth.

The hero, in order to prove his heroic strength and achieve his quest, must overcome a series of obstacles, more often than not in the form of some kind of monster.
We see this in Homer's epic The Odyssey, where he needs to pass by the monsters Scylla and Charybdis, and he must save him and his men from the Cyclops, the myth of Theseus and the minotaur (half man half bull), in Heracles' Twelve Labours particularly with the destruction of the Nemean Lion, a beast who had "made all the mountain district unsafe", and thus must restore the district's safety.

Without the obstacles the hero cannot prove himself to be heroic.
In a similar way, if we look at the monsters as slightly more than mere obstacles and regard them as a manifestation of evil or chaos, without them the heroes cannot triumph and bring order, and subsequently the myths would not work as allegoric tales.

The monsters and hybrids fulfil the roles of opposition to the good and virtuous forces in the world, and give the heroes cause to show their heroism.
Another role that can be attributed to hybrid and monstrous creatures is providing a direct antithesis of the hero of a myth.

They can be used in a representation of good versus evil in its most basic form.
The hero's virtue can be emphasised by juxtaposing him against a foil, a demonstration of the battle of one good force against one evil force.

This theory can be demonstrated through Odysseus' encounters with the monsters Scylla and Charybdis in Homer's Odyssey.
Scylla, the monster with a ring of dog's heads round her middle, and Charybdis, the whirlpool, are simplistic embodiments of malevolence, included within the story as obstacles to be defeated.

Their violence contrasted against the terror and innocence of Odysseus and his crew who have done nothing to incur their wrath than cross their paths, brings into relief Odysseus' comparative virtue.
If we consider Odysseus' episode with the Cyclops, Polyphemus, we can see an illustration of this theory.

Odysseus and his men are held captive in his cave, and the Cyclops plans to systematically eat everybody in an act of barbarity.
The Cyclops is being presented as evil and immoral and sharply contrasts Odysseus, bringing to light both his heroism and morality.

If we were to consider the situation in a little more depth it would become obvious that the Cyclops, despite being naturally immoral, is acting on anger stemming from Odysseus abusing xenia, a code of hospitality, where Odysseus has intruded upon his home and eaten his food in Polyphemus' absence.
This is an example of a hybrid exhibiting human characteristics, perhaps using such a creature to reflect the immoralities and vices of man.

Perhaps the monsters and hybrids can be seen as a personification of moral perversity through physical hideousness and abnormality.
Many of these beings are unsightly creatures, and are perhaps an ancient society's way of personifying the evils that surrounded them; a physical manifestation of immorality, disease and natural disasters.

When considering monsters and hybrids in conjunction with divinity, it becomes apparent that the gods can use them as vehicles of control over mortal society.
The gods create these creatures as manifestations of chaos, and are able to bring order and peace to the world in an act of control over the mortal realm.

The hero is typically aided through divine intervention thus showing the need for a higher power to civilise and bring order.
Paradoxically, the gods can be shown to be almost as bad as the creatures themselves in many instances, exhibiting acts of random barbarity and violence, and being exposed to human emotion and vices.

The gods are subject to jealousy and rage, for example Poseidon, who wreaks havoc upon Odysseus' fleet, conjuring up vicious storms and causing the death of members of his crew: "Poseidon...so violently at odds with you...
puts all these disasters in your path." (Book 5, p71) Similarly Zeus, father of the gods, is subject to immoral lust, and rapes many mortal women, for example Semele, the mother of Dionysus.

In realisation of the gods' own weaknesses and immorality, the paradox of their superiority over mortal society and their use of monsters and hybrids, of which they are often no better, become clear.
Some of the hybrid and monstrous creatures have been victims of the whims of the gods.

Scylla, the terrifying monster that Odysseus is faced with on his wanderings, is subject to the lusts and jealousies of the divine.
Once the beautiful daughter of Phorcys and Hecate, it is said that she was transformed by Amphitrite, the jealous wife of Poseidon, after he had made advances to her.

A slightly different version is detailed in Ovid's Metamorphoses, in which a jealous Circe transforms her, after Glaucus, the man she falls in love with, falls in love with Scylla.
Either way, Scylla becomes a monster as a result of the weaknesses of the gods, and is a victim of jealousy.

Similarly "fair cheeked" Medusa, the gorgon killed by the hero Perseus, whose head was wreathed in snakes and could turn a man to stone with one look, was a beautiful woman made that way by Athena as a punishment of having been raped by Poseidon.
She is made a monster as a result of the lust of the gods, and is perhaps even beheaded as a result of jealousy over beauty: "It is alleged by some that Medusa was beheaded for Athena's sake; and they say that the gorgon was fain to match herself with the goddess even in beauty." In these instances they appear as sympathetic characters who are at the disposal of the gods, reflecting the equal helplessness of mortals.

In contrast with the current theme of the essay, some of the hybrids exhibit signs of virtue, and play the role of bringing relief to mortals.
Take Pan for example, a hybrid god who is half man and half goat bearing resemblance to Satyrs.

He favours music and dancing, and in the Homeric Hymn to Pan is described as "delight[ing] the hearts of them all." Similarly there is Dionysus, the god of wine, music, and revelry, who is always accompanied by a band of Satyrs, hybrid creatures who are half human and half goat and horse.
The Satyrs encourage festivities by their very nature: "They dance and sing and love music; they make wine and drink it, and they are in a perpetual state of sexual excitement." They provide the basis for mortal enjoyment; promoting drinking, merriment and pleasure, alleviating the toils and troubles of everyday existence.

Conversely, all three embody immorality to a certain extent, and are given to wild mood swings which can involve terrible and devastating violence.
The duality of these characters is illustrated in Euripides' Bacchae, where there is imagery of joyous "devour[ing] of raw flesh", Satyrs in "their frenzy" and animals and humans being "torn to pieces...flesh stripped from their bodies." There is a dark side to these creatures who can so readily bring happiness.

In conclusion, I believe that the roles played by hybrid and monstrous creatures are those of portraying evil and immorality in physical form, enabling heroic display, and providing a channel through which the gods can be seen to bring order and control to chaos.
Despite, seeming superficially to be a mere paradigm of iniquity and violence, the creatures' duality in many cases indicate their reflection of human nature and the natural world, and even prove them to be forces for good on occasion.

Brought into being perhaps by society's need to explain and understand immorality and the threatening forces of the world through physical object, they are manifestations of human vice, woven into fables in which most often the virtue of the civilised conquers them.
Summary of the experimental conditions used

Dehydrated beans were provided.
They were soaked in water.

It was drained and weighed.
(Weight 1900 grams approx.) The beans were blanched in hot water for 10 minutes.

One litre of two types of basic sauces was prepared by using the given recipe.
The beans were filled into the cans by using two different weights 120g and 150g respectively.

About 10 cans of each were filled both recipes of sauces.
It was clinched first (half sealed) and exhausted for 5 mins.

The cans were sealed by suing the can seamers and checked for seal integrity.
The cans filled with peas and sauce was then transferred to a simple vertical batch retort for sterilising the cans.

The aim of sterilising is to achieve commercial sterility.
A temperature of 120˚C was carefully maintained for 60 minutes was achieved.

The two key features are that the temperature and pressure are independently controlled, and that a mechanical fan provides for continual mixing and circulation of the mixture of steam and air within the retort cycle.
The input of steam is used to control temperature and the input of compressed air controls pressure.

The action of the fans effectively draws the steam-air mixture through the crates of cans before re-circulation around the inner walls of the vessel.
One of the keys to reducing the thermal damage to products during manufacture is the shortening of the heating and for that matter cooling, times associated with the process.

It is clear that the current trend is to minimise the thermal impact of food processes.
The heat process is however still and essential element and likely to remain so.

This has led the growth in products that have been specially formulated to require a reduced, pasteurisation heat treatment to render them commercially sterile.
Raw material Quality

Raw material yields are reduced by the removal of defective and unacceptable product.
During the preparations, the raw materials are vulnerable to microbial contamination and growth, which results in spoilage and lowering process.

Raw materials should be protected from contamination by human, animal, domestic, industrial, and agricultural wastes that may be present at levels likely to be a hazard to health.
They should not be grown in areas where the water used for irrigation might constitute a health hazard to the consumer of the final product.

Control measures involving chemical treatment, physical or biological agents should only be undertaken by or under direct supervision of a personnel who has a direct understanding of the potential hazards to health, particularly those that my arise from residues in food.
Harvesting and production equipment and containers should be constructed and maintained so they do not constitute a hazard to health.

Raw materials that are unfit for human consumption or unacceptable for canning should be segregated at the harvesting and /or production sites and disposed of in a manner to avoid contamination of the raw materials or water supplies.
They should be stored under the conditions that provide protection against contamination or to the objectionable substances and minimize damage and deterioration.

All steps in the production process should be performed as rapidly as possible and under conditions that will prevent contamination, and deterioration, and minimise the growth of microorganisms in the food.
Effective measures should be taken to prevent contamination of by direct or indirect contact with material at an earlier stage of the process.

Product attributes
Size and shape

Shape, uniformity of shape, freedom from surface irregularities, and size are important processing attributes, especially in high speed mechanised processes.
Colour

Raw material colour is a prime attribute; however it is not always a suitable index of suitability for canning.
For e.g. some varieties of apples and pears develop a pink tinge on canning, rhubarb and some cherry varieties become bleached due to migration of colour into canning syrups, and chlorophyll is converted into brown-green phaeophytin during the heat processing of green vegetables

Texture and functional properties
The texture is very important.

The raw material should withstand the mechanical stress incurred during the preparatory and processing operations as well as yield a final product of the desired texture.
Maturity

The degree of maturity at harvest is the prime factor that influences the quality of the canned product.
Penatrometer tests and measurement of the dry matter-acid ratio are acceptable means for determining optimal maturity.

Maturity is important in controlling both the quality of the final product and effectiveness of processing.
Over maturation results in a high proportion of reject material, excessive product damage, and spoilage during storage.

Mechanical damage (e.g.; punctures, abrasions, etc)
Mechanisation can cause excessive product damage.

Damage increases the risk of mold and rot infections, infestation, and acceleration of enzymatic and chemical spoilage.
Insect, animal, fungal and microbial damages

All lots should be inspected for evidence of spoilage or insect infestation at the time of delivery.
Such problems should be dealt with before the product is brought into the plant to avoid contamination of the raw product.

Extraneous matter
The cleaning equipment should be sufficiently flexible to allow for the wide variability in the extent and types of contamination encountered in raw materials.

Mechanical harvesting has increased the extraneous matter in raw materials.
The role of syrups, brines and sauces

Water, sugar syrups, and juices are used as a packing media for fruits.
The determination of the concentration of sugars in the syrup should take into consideration the quantity and nature of sugars that will be contributed by the fruit and the proportion of fruit to syrup in the container.

(Orial, 1986).
Both syrup and fruit are generally consumed; it must have pleasing organoleptic properties.

The viscosity is an important factor in the thermal processing as well as organoleptically.
The syrup can also be the carrier and supplier of substances to improve the or alter the colour, flavour and texture of the product.

Glucose syrups which are products of starch hydrolysis have advantages over syrups made from sucrose only.
They are measured according to their dextrose equivalent.

(DE).
DE is a measure of the degree of the conversion of starch to reducing sugars.

There are advantages in using a combination of sucrose and glucose syrups over using sucrose alone.
The rheological properties can be maintained without excessive sweetness, especially in fruits that have low acid levels.

The physico- chemical and organoleptic qualities can be better maintained in glucose syrups then they can in sucrose as they cannot be hydrolyzed further even in the presence of very acidic fruits.
They have a favourable effect on the product, prevent the flow of aromatic compounds from the fruit to the syrup and moderate the acid taste better than does a sucrose at a given sweetness level.

A strong sucrose concentration in the syrup creates a higher gradient between the syrup and the fruit, and favours diffusion of fluids from the fruit to the syrup.
During storage, the dry extractable matter diffuses from the syrup into the tissues, which results in an increase drained weight up to the establishment of the final equilibrium.

Therefore the yield (drained weight) is dependent upon the packing medium composition, the particle size, and the shape of the fruit.
Blanching

Blanching is carried out in hot water or steam followed by rapid cooling to given to vegetables and some fruit.
Blanching removes gases from within the tissue and softens the product.

Blanching makes the product easier to fill into the can and obtain the correct weight.
The removal of the gas also reduces the oxidation of the product, maintains vacuum in the can, and prevents corrosion.

Blanching gives the product another washing treatment and inactivates enzymes which may cause deterioration of the food.
Enzyme inactivation is not as important to canned foods as it is for frozen foods, as canned foods receive a far greater heat treatment during thermal processing of the can.

Typical blanch times in near boiling water are 60 to 90 secs for small objects, such as green peas and diced carrot, and up to 3 min for larger peas.
Water blanching is of simple design, robust and least expensive to buy.

The water flows counter-current to the product flow and is continuously recycled.
The water heated by the hot blanched product in the cooling section is cooled in a heat exchanger which in turn is used to heat the water in the preheat section, providing economy in the use of water and energy.

Steam Blanching is quite common since almost all the canneries have an adequate supply of low-pressure steam.
The simplest design has a metal mesh conveyor that moves through a tunnel with steam jets located under the conveyor.

To minimise the e loss of steam the two ends are closed by curtains.
This method is inexpensive and easy to manufacture but they are prone to temperature variations due to the effect of air currents.

Blanching under higher steam pressures increase the temperature, improves steam convection, increases the blanching rate, decreases steam loss, and produces higher reduction in the microbial contamination.
Airlocks limit the loss of steam and further heat loss by radiation is minimised by appropriate insulation.

Filling
The primary role of the filling operation is to place a specified quantity (e.g. weight, number, or volume) of the product in the container.

The quantity that can be added is primarily dictated by the size of the container.
Precision and accuracy are dependent upon the type, state, shape, and size of the product.

For solid products, the particle size has a direct influence on the fill precision (i.e. reproducibility).For many products and container types; an appropriate head space is a critical factor in the attainment of the required vacuum in the sealed container.
Excessive head space can result in an excess of air in the final product, which can result in an excess of air in the final product, which can accelerate product oxidation and eventually cause internal container corrosion.

Insufficient head space can lead to permanent deformation of the container ends possible leakage due to the expansion of the product and contained gases, which would render the product unsaleable.
During thermal processing, the product is in continuous contact with the hot cover liquid osmotic exchanges take place.

Therefore, the solid components of some products during thermal treatment and to a lesser degree during storage are going to either take on or lose weight.
Fill weights must compensate for any gain or loss.

Exhausting
Oxygen remaining in the head space of cans accelerate of the tin plate in the head space area.

To prevent this occurring, the volume of the gas between the product and the lid of the can (known as head space) must contain a partial vacuum.
The conventional system of exhausting the can is to clinch the lid on the can.

Clinching is a partial, first operation seaming roll which holds the lid loosely on the can.
Exhausting carried out by passing the filled cans with clinched lids through a steam filled compartment for several mins to heat the can contents and displaces the air in the can with steam.

This is immediately followed by completion of the seaming operation.
Sealing

Double seam integrity is of paramount importance and the minimum levels of acceptability are essentially the same for both round and irregular shaped cans.
Two identifiable seals are provided within the double seams.

The primary seal is produced by embedding the body flange of the can (referred to as body hook) into the sealing compound or gasket within the end curl.
The secondary seal is created by overlapping of the body hook and end hook within the double seam.

The principal aspects of seam formation which provide a leak-free double seam are referred to as the critical parameters of acceptability.
These are

Body hook butting (Primary seal formation)Actual overlap (Secondary formation) Tightness rating Ensuring the seam is held under sufficient compression) and finally the seam must be free from visual defects.
Typical critical parameter dimensions for tinplate ends and bodies (1.0mm) minimum for actual overlap, 70% minimum for body hook butting, and 75% minimum tightness rating.

However, with the aluminium cans the tightness rating should be 90% minimum.
In the canning industry, visual external assessment is an ongoing process with cans from each seaming station every 15 minutes.

A full tear down of the double seam will be assessed for acceptable seams.
Retorting

It is important to distinguish between acid and low acid food.In microbial terms, foods may be readily classified into high acid foods, such as fruits which have pH values of less than 4.5, and low-acid foods which have pH values 4.5 or greater.
Low-acid foods will support the growth of heat resistant, spore-forming, pathogenic organisms and therefore require a relatively severe thermal process to ensure microbial destruction.

Process temperatures will be in the region of 115 -130˚ C, where as acid products may be simply pasteurised at temperatures of 100˚C or even lower.
The most heat resistant of the pathogenic organisms is Clostridium botulinum, and by convention thermal processes are defined in terms relative to the destruction of this organism.

Our product cans were loaded by putting in steel crate.
The lid was closed and the steam was turned on.

Steam is an excellent heat medium because of its ability to condense on container surfaces releasing large amounts of latent heat.
The main enemy of efficient heating in a closed vessel (eg.retort) using saturated steam is the presence of entrapped air especially that trapped in the small spaces between containers in the load.

Even a small quantity of air has a significant effect upon temperature.
For e.g. any given location 10% of air by volume will reduce the temperature by about 3˚C, which will have a dramatic effect on product sterilisation.

This means that in order to bring about efficient and uniform heating, air must be purged, or vented from the retort at the start of the process.
It was achieved at the start of the operating cycle by introducing high velocity steam into the retort.

This steam was allowed to pass through the vessels at the bottom and exit through a vent at the top since the retort was a vertical orientation type.
In practice, the efficiency of the venting process can be monitored by measurement of the temperatures in various locations throughout the retort, and the time required to remove all the air is determined experimentally.

At the end of the venting time the vent valve was closed and the retort pressurised until the desired temperature was reached.
During the hold phase there must be a means for the water generated as steam condenses to escape from the retort, because immersion of the lowest containers in the vessel could result in under sterilisation.

This is normally achieved either by a condensate bleed that is permanently open or having the drain cracked slightly open.
In either case the absence of water in the base of the retort can be detected by a free flow of steam.

On reaching the intended holding temperature the requirements are for a narrow spread of temperatures throughout the vessel and stable control at the intended temperature.
For a properly vented steam retort a temperature range of less than 0.5˚C is achievable, and this is a good target for all retort systems, though difficult to achieve in some types.

Cooling
Once the desired hold period, (in our case 60 mins) the retort was cooled.

The start of the cooling phase was most critical for ensuring the continued integrity of the processed containers.
Water used to cool containers after sterilisation or pasteurisation should be consistently is of low microbial content.

For e.g. with an aerobic mesophile count of less than 100c.f.u./ml.Records should be kept of cooling water treatment and its microbiological quality.
Although containers may normally be considered hermetically sealed, a small number of containers may allow intake of water during the cooling period mainly due to mechanical stress and pressure differential.

To ensure disinfection, chlorine or an alternative disinfectant must be thoroughly mixed with water to a level that minimise the risk of contamination of the contents during cooling.
Excessive chlorine level can accelerate corrosion of certain metal containers, so the detectable amounts of residual free chlorine should be within the range of 0.5 to 2 parts per million.

The effectiveness of the thermal process to achieve commercial sterility depends upon many factors, including the initial microbial population within the food, the time and temperature of the sterilising process, and a number of product related factors which effectively determine the heating processes which occurs within the can.
In microbial terms, food may be readily classified into high acid foods, such as fruits, which have pH values of less than 4.5, and low-acid foods which have pH values of 4.5 or greater.

Low-acid foods will support the growth of heat resistant, spore-forming, pathogenic organisms and therefore require a relatively severe thermal process to ensure microbial destruction.
Process temperatures will be in the region of 115-130˚C, whereas acid products may be simply pasteurised at temperatures of 100˚C or even lower.

The most heat resistant pathogen is Clostridium botulinum, and by convection thermal processes are defined in terms relative to the destruction of this organism.
When subjected to heat micro-organisms die in algorithmic manner.

The time taken at constant temperature to reduce microbial population to 10% of its former value is itself constant and is known as the decimal reduction time or D value.
The D value varies with temperature.

As temperature increases micro-organisms die more quickly and the D value decreases.
A further parameter, the z value, is used to describe the rate of change of D value with temperature.

A rise in temperature of z˚will, by definition, causes a tenfold reduction in the D value.
The z value for Clostridium botulinum is generally recognised to be 10˚C and the D value at 121.1˚C (250˚F), D 121.1, is 0.21min.

Factors affecting heat penetration and severity
Process-related

1. Retort temperature and profile: The higher the temperature the faster the heating rate.
Depends on sterilizer type: in static retorts and hydrostatic cookers, the temperature rises slowly and there is a lag in heating the cans; in rotary cookers the heating starts instantaneously.

2. Process time : The longer the process time the greater the heat penetration and the nearer to processing temperature the contents become.
3. Heat transfer medium: The external heat transfer coefficient, h governs the temperature at the surface of the container.

Steam has a very high h, but h for water or steam-air mixtures depends on the velocity and geometry factors.
4. Container agitation : Agitation and rotation improve the internal heat transfer, depending on the amount of headspace, and degree of agitation and rotation.

Product- related
5.Consistency: The composition, consistency and rheological behaviour control the rate of heat penetration.

For products of a thin nature or in thin covering liquid, convection heating occurs, whilst for thicker products heat transfer is mainly by conduction.
Some products (broken-heating) show both types of heating.

6. Initial temperature : The higher the initial temperature of the contents the shorter the processing.
The process is very sensitive tot the initial temperature, especially for conduction-heating products, which often do not reach process temperature by the end of the process.

7. Initial spore load: The severity of the process depends on the initial spore load; good factory hygiene keeps this low.
8.Thermophysical properties : The thermal diffusivity is the controlling factor.

This approximates to that of water for most products, but is lower for those which are oil-based and those of low water activity.
9. Acidity pH: The severity of the process depends on the pH of the product.

Products with pH>4.5 require to the severest processes, whilst products with pH<4.5 can be pasteurised; this includes acidified products.
10.

Additives : Certain additives.
E.g. nisin, nitrite, salt, and sugar, reduce the process time.

Packaging-related factors
11.

Container materials: These include; tin plate, aluminium, glass, and plastic and laminated materials.
The thermal conductivity and thickness of the material determine the rate of heat penetration.

The lower the conductivity and the thicker the material, the slower the heating 12.
Container shape : The external surface area and thickness of the container determine the heat penetration rate.

The most rapidly heating packages have the highest surface area and thinnest profile.
Heat transfer in canned foods

Thin liquid products which heat extremely rapidly due to internal convection.
E.g. fruit juices, beverages, milk, and thin soups.

The thicker the consistency becomes, the slower the hating rate.
Liquid products containing solid food.

The liquid portion heats rapidly by convection and transfers the heat into particulate by conduction.
For products such as peas in brine and strawberries in syrup, the process is determined to a large extent by the solid-liquid ratio and packing style.

For larger products such as whole potatoes or celery hearts in brine, heat penetration into the product centre of the product is necessary to ensure an adequate process and sufficient cooking.
Solid products in thick covering liquids, such as beans in tomato sauce and thick soups.

Depending on the formulation, heating is first by convection and then, after thickening by starch gelation, by a conductive mechanism.
These products show a heat penetration curve with tow different rates of heating, known as broken-heating curve.

Thick products which heat by conduction, and whose thermal diffusivities are about the same as water, there being insufficient or no covering liquid.
This covers a wide range of products thick sauces and gravies and solid pack products.

Products which start by heating with a conductive mechanism and then, because of thinning due to structures and rheological changes, heat by convection, e.g. some thickened puddings and some tomato juices.
Again this produces a broken - heating type of heat penetration curve.

Thick products which heat by conduction but have thermal diffusivities less than water, i.e. they have a high fat or sugar content.
e.g. fish oil Vacuum-packed products contain very little water.

Sufficient to produce enough steam inside the can to heat the product, e.g. corn on the cob, whole-grain corn and some vegetables.
Safety (microbiological) and quality issues involved in canning

Micro-organisms that contaminate the raw products used in canning directly affect the safety and preservation of the final product.
The prime microbiological concerns of canners are:

Micro-organisms that may survive the final thermal treatment and subsequently adversely affect the quality and /or safety of the final product;Micro-organisms that can cause adverse changes ( e.g. incipient spoilage) of the raw products prior to their use in the canning process;Micro-organisms that can contaminate the processing plant environment (eg.air, food contact surfaces, food handlers, finished product handling equipment, etc.) and may gain entry into the finished product (post process contamination), which affects the quality and/ or safety.
Effect of heat processing on sensory quality

The objective of this experiment is to determine the reducing sugar content of Jam.
Jam contains both reducing sugars (invert sugar) and non- reducing sugar (sucrose).

Method
A metal dish containing sand and glass were first weighed.

A minced beef sample were added and re-weighed accurately.
The sand and meat were mixed and kept (with the lid off) in oven at 150 C for 30 minutes.

It was allowed to cool and re-weighed.
The dried minced sample were transferred into an extraction thimble and place it in the Soxhlet extractor.

A dry 250ml extraction flask was weighed and a few anti-bumping granules were added.
The flask was connected and the extractor with petroleum ether (b.p 40 -60ºC ) until the solvent siphons over into the flask.

Sufficient solvent were added to half fill the extraction flask.
The heater was turned and were set, so that the solvent boils gently under reflux (2-3 drops per second).

It was kept for 1.5 hrs for extraction, due to time limitations of the practical, but the proper procedures is to extract for 4 hours.
Results

Soxhlet extraction
Calculations

According to the data obtained the net weight of the fat at the first run is equal to 0,792 g.
The weight of our meat sample equals to 3,0532 g.

By dividing the net weight of fat by the weight of our meat sample we can calculate the percent of the fat in our sample.
As a result, the percent content of the fat in our sample is equal to FORMULA When we did the similar calculations for the second run, we conclude that the percent content is equal to 25,3 %.

Discussions
Sinar et al (1976) have suggested limits for fat and moisture content of minced beef.

These authors advanced an opinion that the fat content of minced beef should not exceed 25%.
Minced meat is defined in "The Mince and Minced Meat Preparations(Hygiene) Regulations 1995 (which implements Council Directive 94/65/EC) as being "meat which has been minced into fragments or passed through a spiral screw mincer and includes such meat to which not more than 1 per cent salt has been added".

These Regulations set compositional requirements for total fat content of pre-packed minced meat of different species sold using specific designations (see Table A).
They only apply to products that use the exact wording of the Regulations.

The Regulations do not apply to minced meat sold loose in butchers' shops.
There are no statutory limits for the fat content of mince sold as "lean" or "extra/super lean".

Fat by acidic extraction
3g of meat were accurately weighed and transferred to a quick fit conical flask of 100ml.

20ml of water were added.
6 drops of ammonia solution were added in the fume cupboard and a few boiling chips were added.

The mixture was boiled gently until the meat dispersed.
20ml of hydrochloric acid were added and it was heated and boiled for 3 minutes and kept for cooling.

After that 20ml of ethanol were added and mixed well.
The liquid was transferred into a separating funnel.

The funnel was stoppered and shaken well for 15 seconds.
25ml of petroleum ether were added and shaken again for 30 seconds.

The organic (upper) and aqueous (lower) were separated.
The lower layer (aqueous) were ran off and the upper (organic) layer collected in a conical flask.

The aqueous layer were transferred back into the separating funnel and re-extracted with 15ml of petroleum ether.
Again the (aqueous) into a conical flask.

The two organic extracts were combined in the separating funnel and washed with 10 ml of water.
The water layer ran off and the organic layer was washed with 25ml of salt solution.

The lower (aqueous) layer ran off.
The organic solution were transferred into a conical flask ad it was shaken with 5g of anhydrous sodium sulphate to dry it.

The organic extract was filtered through a fluted filter paper into a weighed round-bottomed flask.
The solvent was evaporated by using a distillation unit with "tap-off" and finally the residual solvent was evaporated by using a rotary evaporator until it reached constant weight.

The flask was weighed and the percentage of the fat was calculated.
From the results we see that the net weight of fat in our meat sample is 0,668 g.

We have already mentioned that the sample we weighted is 3,006 g.
As a result, we can conclude that in 100 g of the sample the content of fat is 22 g.

So the fat content in our sample is 22 %.
Determination of fat content of milk (Gerber method)

10 ml of sulfuric acid were put into a milk butyrometer and 10.94 ml of milk were added and also 1ml of isoamyl alcohol.
The butyrometer was stoppered by using the keys.

It was placed in the protected stand and inverted.
The butyrometer was centrifuged in a Gerber centrifuge at speed 8( 1000rpm) for 5 minutes.

After that the butyrometer was placed in a water bath at 65˚C for 5 minutes.
The percentage of fat was directly read off from the scale.

Whole - fat milk
Semi-skimmed milk

We did not do any calculations since it the result was straightforward and clear perc by reading the percentage from the scale.
Discussions

Readings to an accuracy of 0.05percent are usually adequate for routine purposes.
As it is difficult to separate the small fat globules in homgenised (eg.sterilised ) milk, it is advisable to re-centrifuge after warming in the 65˚C bath until it reaches a maximum.

Method
A Sample of potassium iodate (approx, 1.0 g) which was dried over night at 120 ºC was provided.

The sample was weighed first and the empty container was re- weighed.
It was dissolved quantitatively and diluted with potassium iodate to volume and mixed well.

10ml standard potassium iodate was transferred by using a pipette into a conical flask and 10ml potassium iodide solution (10 percent m/v) and 5 ml 1 M Sulfuric acid were added.
10.1.1 Standardization of 0,02 M sodium thiosulfate solution

At the beginning of the experiment we have to calculate the molarity of KIO 3 solution.
The results obtained are shown in the table below (Table 1):

10.2 - Analysis of Milk
10 ml of milk was transferred into a 100ml Volumetric flask.

25 ml of water added and it was mixed gently.40 ml of tungstic acid reagent were added and gently mixed.
It was diluted to volume with water.

Stopper was put in and allowed the precipitate to settle.
It was later filtered through a dry pleated filter paper into a dry flask.

The first running was discarded and 50ml of filtrate was collected.
It was done in duplicate, together with duplicate blank determinations using 10ml of water instead of filtrate.

10ml of filtrate was transferred by a pipette into a glass stoppered flask.
Potassium iodide solution ( 10 percent, m/v) was added by a pipette mixed gently.

The flasks were left in the dark for 90 minutes.
The stopper was removed and rinsing with a little water, 5ml 2M of Hydrochloric acid was added.

The liberated iodine was titrated using standardized 0.02M sodium thiosulfate solution until the solution turned into pale yellow colour.
25 ml of water was added a few drops of starch indicator were added and the titration was completed until the blue colour just disappeared.

Calculations
The lactose content (percent m/m ) of the milks was calculated.

10.3.0 Determination of Chloride
10.3.1 Standardisation of 0.05M silver nitrate solution

The sample of Sodium chloride which was dried overnight at 120C, weighed with the container.
It was emptied carefully with the help of the filter funnel to a 500ml volumetric flask.

The container was re-weighed and the difference in weight of the Sodium chloride was noted.
The sodium chloride was dissolved and diluted to volume and it was mixed well.

Calculations and Results
At the beginning of the experiment we have to calculate the molarity of NaCl solution.

The results obtained are shown in the table below.
25 ml standard sodium chloride solution was transferred by a pipette into a 250ml conical flask and 1ml chrome indicator solution (4.2g potassium chromate plus 0.7g of potassium dichromate in 100ml water) were added.

It was titrated carefully by using 0.05M Silver nitrate solution until a faint red - brown persisted on shaking.
Two further titration were continued and the molarity of the silver nitrate solution was calculated.

10.3.1 Standardisation of 0.05M potassium thiocyanate solution
25 ml of standard silver nitrate solution into a 250ml conical flask.

5ml nitric acid (6M) and 1 ml Iron (3)Indicator solution (50g ammonium ferric sulphate dissolved into 100ml water, with concentrated nitric acid added afterwards).
The flask was kept on a white tile and it was titrated with 0.05M potassium thiocyanate solution until a faint red-brown colour persisted on vigorous shaking.

Further two titrations were undertaken and the molarity of the potassium thiocyanate solution was calculated.
Results

Analysis of Milk
The principle of this analysis is that silver nitrate reacts with chloride in the milk and excess reacts with thiocyanate.

Duplicate samples were prepared using milk.
10ml of milk was weighed into a 250ml conical flask.10 ml of standard 0.05M silver nitrate solution was added with a pipette.10 ml of conc.

Nitric acid was added.(70%m/m) It was mixed and the contents was boiled for a few minutes by using a steam water bath with occasional shaken, in a fume cupboard.
When the precipitate in the flask was granular and the liquid turned pale yellow, this flask was covered by a beaker and it was kept for cooling.

When it was cooled down, 20ml of water was added and 1 ml Iron (3) indicator solution was added.
It was titrated with standardised 0.05 thiocyanate solution until a faint red-brown colour persisted on vigorous shaking.

10.4.0 Realtionship between the lactose and chloride contents of cow's milk
a )Determination of lactose

At the beginning of the experiment we have to calculate the molarity of KIO 3 solution.
According to the results obtained we can do the following calculations: FORMULA Furthermore, we were asked to calculate the molarity of the sodium thiosulfate solution.

According to the data obtained from the titrations (table 2), we can do the following calculations: FORMULA From the results obtained from the titrations we know that we have used 63,5 mL of sodium thiosulphate solution at our milk samples and 78,4 mL at blank titrations.
The difference between the mean blank titre and the filtrate titre is equivalent to the lactose in 10 mL filtrate.

From this, we can conclude that the difference between 78,4 and 63,5 (14,9) is equivalent to the lactose in 10 mL filtrate.
Furthermore, we know that 1 mL of sodium thiosulphate solution is equivalent to 0,00342 g anhydrous lactose.

As a result 14,9 mL of sodium thiosulphate solution are equivalent to 0,00342·14,9 = 0,05096 g of anhydrous lactose.
The correction for the volume of coagulum in the graduated flask is given by multiplying by the factor 0,996 in the case of skim milk.

As a result, we have 0,05096·0,996 = 0,0507 g of anhydrous lactose.
Our goal is to calculate the percent content of lactose in our sample.

In 10,29 g of our sample we have calculated that there are 0,0507 g of anhydrous lactose.
As a result the percent content of lactose is 5 %.

Determination of chloride
At the beginning of the experiment we have to calculate the molarity of NaCl solution.

According to the results obtained (table) we can do the following calculations: FORMULA Furthermore, we were asked to calculate the molarity of the silver nitrate solution.
According to the data obtained from the titrations (table 4), we can do the following calculations for the first titration: FORMULA Doing the same calculations for the next two titrations we can conclude that the molarity of the silver nitrate solution is equal to 0,05 M. Moreover, we were asked to calculate the molarity of the potassium thiocyanate solution.

According to the data obtained from the titrations (table 5), we can do the following calculations for the first titration: FORMULA Doing the same calculations for the next two titrations we can conclude that the molarity of the potassium thiocyanate solution is equal to 0,05 M.
The principle of this analysis is that silver nitrate reacts with chloride in the milk and excess reacts with thiocyanate.

The reactions are the ones below: FORMULA The ratio of AgNO 3 and SCN- at the first reaction is 1:1.
So the number of moles of AgNO 3 is equal to the number of moles of SCN- (n 1).

We are aware that FORMULA For the first sample we can conclude that: FORMULA As a result the number of moles of AgNO 3 reacting equals to n 1= 1,95∙10 -4 mol.
The ratio of AgNO 3 and Cl- at the second reaction is 1:1.

So the number of moles of AgNO 3 is equal to the number of moles of Cl- (n 2).
The number of moles of excess AgNO 3 that react with Cl- is equal to: FORMULA So the number of moles of Cl- is equal to 3,05∙10-4.

But, FORMULA We have calculated that in 10,29 g of milk sample the content of chlorides is equal to 1,08∙10 -2 g.
So the percent content equals to 1 FORMULA.

Discussions
The chloride content of normal milk seldom exceeds 0.13 percent when calculated on the whole milk (national average 0.105 percent.

Milk derived from cows suffering from mastitis milk (mastitis milk), however when compared with normal milk shows increased chloride and soluble nitrogen figures, together with a reduction in the amounts of lactose and casein.
Refrigeration

Refrigeration is defined as the elimination of heat from a material at a temperature higher than the temperature of its surroundings.
The mechanism of refrigeration is a part of the freezing process and freezing storage involved in the thermodynamic aspects of freezing.

According to the second law of thermodynamics, heat only flows from higher to lower temperatures.
Therefore, in order to raise the heat from a lower to a higher temperature level, expenditure of work is needed.

The aim of industrial refrigeration processes is to eliminate heat from low temperature points towards points with higher temperature.
For this reason, either closed mechanical refrigeration cycles in which refrigeration fluids circulate, or open cryogenic systems with liquid nitrogen (LIN) or carbon dioxide (CO 2), are commonly used by the food industry.

The main elements in a closed mechanical refrigeration system are the condenser, compressor, evaporator, and the expansion valve.
The refrigerants hydrochlorofluorocarbon (HCFC) and ammonia are examples of the refrigerants circulated in these types of mechanical refrigeration systems.

A simple scheme for the closed mechanical refrigeration system is shown in the Figure below.
Freezing

Freezing is the unit operation in which temperature of a food is reduced below the freezing point, and a proportion of the water undergoes a change in state to form ice crystals.
The safety and nutrition quality of frozen products are emphasized when high quality raw materials are used, good manufacturing practices are employed in the preservation process, and the products are kept in accordance with specified temperatures.

The immobilisation of water to ice and the resulting concentration of dissolved solutes in unfrozen water lower the activity of the food.
Preservation is achieved by (a) a combination of low temperatures (b) reduced water activity (c) in some foods, pre-treatment by blanching.

During freezing, sensible heat is first removed to lower the temperature of a food to the freezing point.
Heat produced by respiration is also removed.

This is termed as the heat load.
It is important in determining the correct size of the freezing equipment.

Water has a high specific heat (4200kg -1K -1) and a high latent heat of fusion (335kJ kg -1).
A substantial amount of energy is needed to freeze foods.

The International Institute of Refrigeration (IIR) has provided definitions to establish a basis for the freezing process.
According to their definition, the freezing process is basically divided into three stages based on major temperature changes in a particular location in the product, as shown in Figures 1 and 2 for pure water and food respectively.

Beginning with the prefreezing stage, the food is subjected to the freezing process until the appearance of the first crystal.
If the material frozen is pure water, the freezing temperature will be 0 °C and, up to this temperature, there will be a sub cooling until the ice formation begins.

In the case of foods during this stage, the temperature decreases to below freezing temperature and, with the formation of the first ice crystal, increases to freezing temperature.
The second stage is the freezing period; a phase change occurs, transforming water into ice.

For pure water, temperature at this stage is constant; however, it decreases slightly in foods, due to the increasing concentration of solutes in the unfrozen water portion.
The last stage starts when the product temperature reaches the point where most freezable water has been converted to ice, and ends when the temperature is reduced to storage temperature (Persson and Lohndal, 1993).

Blanching
Blanching is the exposure of the vegetables to boiling water or steam for a brief period of time to inactivate enzymes.

The objective of blanching is to inactivate the enzymes causing detrimental changes in colour, odour, flavour, and nutritive value, but heat treatment causes loss of such characteristics in fruits (Gutschmidt, 1968).
Therefore, only a few types of fruits are blanched for inactivation of enzymes prior to freezing.

The loss of water-soluble minerals and vitamins during blanching should also be minimized by keeping blanching time and temperature at an optimum combination (Spiess, 1984).
Practically every vegetable (except herbs and green peppers) needs to be blanched and promptly cooled prior to freezing, since heating slows or stops the enzyme action, which causes vegetables to grow and mature.

After maturation, however, enzymes can cause loss in quality, flavour, colour, texture, and nutrients.
If vegetables are not heated sufficiently, the enzymes will continue to be active during frozen storage and may cause the vegetables to toughen or develop off-flavours and colours.

Blanching also causes wilting or softening of vegetables, making them easier to pack.
It destroys some bacteria and helps remove any surface dirt (Desrosier and Tressler, 1977).

Blanching in hot water at 70 to 105 °C has been associated with the destruction of enzyme activity.
Blanching is usually carried out between 75 and 95 °C for 1 to 10 minutes, depending on the size of individual vegetable pieces (Holdsworth, 1983).

Blanched vegetables should be promptly cooled down to control and minimize the degradation of soluble and heat-labile nutrients (Deitrich et al., 1977).
The enzymes used as indicators of effectiveness of the blanching treatment are peroxidase, catalase, and more recently lipoxygenase.

Peroxidase inactivation is commonly used in vegetable processing, since peroxidase is easily detected and is the most heat stable of these enzymes (Arthey, 1993).
Vegetables can be blanched in hot water, steam, and in the microwave.

Hot water blanching is the most common way of processing vegetables.
Blanching times for various vegetables can vary depending on the intended product use.

For water blanching, vegetables are put in a basket and then placed in a kettle of boiling water covered with a lid.
Steam blanching takes longer than the water method, but helps retain water-soluble nutrients such as water-soluble vitamins.

For steam blanching, a single layer of vegetables is placed on a rack or in a basket at 3-5 cm above water boiling in a kettle.
A tightly fitted lid is placed on the kettle and timing is started.

Microwave blanching is usually recommended for small quantities of vegetables prior to freezing.
Due to the non-uniform heating disadvantage of microwaves, research is still being conducted to obtain better results with microwave blanching.

Tests for catalase and polyphenol oxidase activity were tested after blanching by using Guaicol.
If it is active it goes brown.Some vegetable were even sulphited by using zero, 0.1% and 0.5% sodium metabisulphite solution for 5 minutes.

The results were summarised and tabulated as follows: In blanching we observed that, 2 minutes in soft water was better than hard water at the same time.
Hard water at 2 minutes better than 5 minutes in hard water.

This observation was not made depending on the quantitative data obtained.
Core temperatures of fruit and vegetables before freezing

3 tomatoes were splitted into halves and kept on an empty tray for vacuum freezing.
The weight of those 3 tomatoes were 251.60.

The temperatures of the six halves of tomatoes were as follows before freezing.
Tomatoes consist of 94% water.

After vacuum freezing the temperature ranges between -1 to 4˚C.
Vacuum freezing results

Some types of freezers used in freezing are as follows: Blast freezers -Air is recirculated over food at between -30˚ C and -40˚ C at velocity of 1.5- 6.0ms-1.
The higher velocity reduces the thickness of the boundary films surrounding the food and thus increases the heat transfer coefficient.

It is relatively economical and highly flexible in that foods of different shapes and sizes can be frozen.
However, the large volume of recycled air can cause freezer burn and oxidative changes to unpackaged or IQF foods.

A few fruits and vegetables were blanched and blast frozen during the experiment.
After 45 minutes of Blast freezing the temperature was as follows.

Plate Freezers - consists of a vertical or horizontal series of hollow plates, through which refrigerant is pumped at -40˚C.
They may be batch, semi continuous or continuous in operation.

Flat or relatively thin foods are placed in single layers between the plates.
A slight pressure is applied by moving he plates together.

This improves the contact between surfaces of food and the plates there by increases the rate of heat transfer.
This type of freezers has been used in the fishing industry to freeze "fish blocks" which had been used to make value- added fish products like fish fingers, nuggets etc.

Liquid Nitrogen freezers
Liquid nitrogen, with a boiling temperature of -196 °C at atmospheric pressure, is a by-product of oxygen manufacture.

The refrigerant is sprayed into the freezer and evaporates both on leaving the spray nozzles and on contact with the products.
The system is designed in a way that the refrigerant passes in counter current to the movement of the products on the belt giving high transfer efficiency.

The refrigerant consumption is in the range of 1.2-kg refrigerant per kg of the product.
This rapid surface freezing also offers advantages to the processor in terms of ease of handling (particularly for fragile or sensitive food products) or further processing.

Typical food products used in this system are fish fillets, seafood, fruits, berries (Persson and Lohndal, 1993).
Freezing time

The freezing time and freezing rate are the most important parameters in designing freezing systems.
The quality of the frozen product is mostly affected by the rate of freezing, while time of freezing is calculated according to the rate of freezing.

For industrial applications, they are the most essential parameters in the process when comparing different types of freezing systems and equipment (Persson and Lohndal, 1993).
Freezing time is one of the most important parameters in the freezing process, defined as time required to lower product temperature from its initial temperature to a given temperature at its thermal center.

Since the temperature distribution within the product varies during freezing process, the thermal center is generally taken as reference.
Thus, when the geometrical center of the product reaches the given final temperature, this ensures the average product temperature has been reduced to a storage value.

Freezing time depends on several factors, including the initial and final temperatures of the product and the quantity of heat removed, as well as dimensions (especially thickness) and shape of product, heat transfer process, and temperature.
The International Institute of Refrigeration (1986) defines various factors of freezing time in relation to both the product frozen and freezing equipment (Persson and Lohndal, 1993).

The most important are:
Dimensions and shape of product, particularly thicknessInitial and final temperaturesTemperature of refrigerating mediumSurface heat transfer coefficient of productChange in enthalpyThermal conductivity of product

Calculation of freezing time in food systems is difficult in comparison to pure systems since the freezing temperature changes continuously during the process.
Using a simplified approach, time elapsed between initial freezing until when the entire product is frozen can be regarded as the freezing time.

Plank's equation is commonly used to estimate freezing time, however due to assumptions involved in the calculation it is only useful for obtaining an approximation of freezing time.
The derivation of the equation starts with the assumption the product being frozen is initially at freezing temperature.

Therefore, the calculated freezing time represents only the freezing period.
The equation can be further modified for different geometries including slab, cylinder, and sphere, where for each geometry, the coefficients are arranged in relation to the dimensions (Plank, 1980).

Plank's equation was used to calculate the freezing time of an apple using liquid nitrogen.
FORMULA The maximum time to freeze the apple using liquid nitrogen was 2.51 minutes.

FORMULA The minimum time to freeze the apple using liquid nitrogen was 70.5 seconds.
Freezing rate

The freezing rate (°C/h) for a product or package is defined as the ratio of difference between initial and final temperature of product to freezing time.
At a particular location within the product, a local freezing rate can be defined as the ratio of the difference between the initial temperature and desired temperature to the time elapsed in reaching the given final temperature (Persson and Lohndal, 1993).

The quality of frozen products is largely dependent on the rate of freezing (Ramaswamy and Tung, 1984).
Generally, rapid freezing results in better quality frozen products when compared with slow freezing.

If freezing is instantaneous, there will be more locations within the food where crystallization begins.
In contrast, if freezing is slow, the crystal growth will be slower with few nucleation sites resulting in larger ice crystals.

Large ice crystals are known to cause mechanical damage to cell walls in addition to cell dehydration.
Thus, the rate of freezing for plant tissues is extremely important due to the effect of freezing rate on the size of ice crystals, cell hydration, and damage to cell walls (Rahman, 1999).

The figure 4 shows a general behavior of the dynamics curve of freezing preservation.
Rapid freezing is advantageous for freezing of many foods, however some products are susceptible to cracking when exposed to extremely low temperature for long periods.

Several mechanisms, including volume expansion, contraction and expansion, and building of internal pressure, are proposed in literature explaining the mechanisms of product damage during freezing (Hung and Kim, 1996).
In small samples, high freezing rate produces a large number of ice crystals, in large samples, nucleation is only produced in the zone that is in contact with the refrigerant.

The size of the ice crystals depend on freezing rate.
Freezing damage is associated with ice formation, either directly through the mechanical effects produced by ice crystals or indirectly by an increase in solute concentration in the unfrozen phase.

Physical changes in frozen foods include drip losses, moisture migration, freeze-cracking and ice crystallization.
The growth in size of ice crystal can influence the damage during frozen storage and therefore lead to loss in quality.

Recrysallisation at constant or fluctuating temperature occurs because systems tend to move toward a state of equilibrium where free energy is minimised.
Chemical changes that can be detected during freezing and frozen storage are protein denaturation, lipid oxidation, enzymic browning, flavour deterioration, and degradation of pigments and vitamins.

Formation of ice crystal can cause disruption in the frozen tissues, leading to the release of enzymes and chemical substances that affect food quality.
All these physicochemical changes contribute to affecting the quality of food.

The objective is to go through the procedures to produce UHT milk by direct processing and to evaluate the quality of a range of heat treated milk products.
Results

Observations/ Discussions :
Raw milk was yellow in colour and when it has undergone the UHT process the colour of the milk was more whiter.

When we compare the Freezing point depression values of Raw milk and UHT.
UHT milk has a higher value since, the milk has taken more water by the steam.

Commercial samples of Milk
Processing Aspects

Processing temperatures And the following pressures; (insert units) Flow rate: 129-130l/h Determine the average residence time (holding tube = 260cm long and 0.96 cm diameter: assume turbulent flow: Estimate the Fo (based on tmin).
FORMULA

Estimation of the F0 value
The UHT temperature was 145.2°C, thus the lethality the UHT sterilisation process, for T ref=121°C and z =10°C, of was: FORMULA Assuming that the flow is turbulent, the minimum residence time in the holding tube was: FORMULA Thus, the F 0 value of the process, based on the minimum residence time can be determined as follows: FORMULA

Observations of the turbidity and microbiological testing
The samples of the milk tested have been ranged for their intensity of turbidity and microbial growth on the agar slope.

The results are given in the table below with 1 indicating the lowest intensity of turbidity and 6 the highest.
Turbidity test:

The turbidity test is a measure of serum protein denaturation, which depends on the precipitation of denaturated serum proteins with casein in 20% ammonium sulphate solution.
The denaturated protein can be removed by filtration.

The filtrate, after reheated, if there is left no non-denaturated protein, will be clear and the turbidity test will be positive.
On the contrary, if non-denaturated protein is present, the filtrate will be turbid and the test will be positive.

(Burton, 1988) The degree of denaturation of the proteins is indicative of the severity of the heat treatment, combination of temperature and time, which the milk sample has undergone.
The ranking of the milk samples, regarding the intensity of turbidity, is in accordance with the above statement.

The heat treatment during pasteurisation is not severe, thus the pasteurised milk sample had the same turbidity as the raw milk.
The UHT direct milk sample was more turbid than the UHT indirect one, as the small time of heat treatment in the first case causes less denaturation than the indirect heating, although performed in much higher temperatures.

Finally, the sterilised and UHT shop milk samples were the most severely heat treated.
Microbiological test:

As it was expected, the sample of raw milk resulted to high microbial growth, as it has gone under no treatment so as the microbes present in the milk when it is collected from the cow to be eliminated.
Moreover, the samples of the commercial, packaged milk (pasteurised, sterilised and UHT shop sample) has shown no microbial growth, thus the process they have gone through had achieved the desirable sterility.

On the other hand, the two milk samples of direct and indirect UHT, which had been processed in the laboratory, has shown significant sings of microbial growth.
This could be mainly caused by post pasteurisation contamination, as there was not available aseptic packaging for the processed milk to be placed.

Other factors that are possible to have contributed to the low microbial quality of the two samples are poor sanitation of the equipment used (for the UHT direct sample the sterilisation of the plant was performed for only 20min) or insufficient heat treatment; as if the UHT processes have been performed correctly the commercial sterility of the milk would have been achieved.
Alcohol stability test

The alcohol test is used by the milk industry as reception test for milk in the plant, to measure the heat stability of milk.
The alcohol test, together with the acidity test, is used on fresh milk to indicate whether it will coagulate on processing.

Milk that contains more than 0.21 % acid, or calcium and magnesium compounds in greater than normal amounts, will coagulate when alcohol is added.
The alcohol test was carried out at three ethanol concentrations.

We noticed that the UHT milk is slightly stronger than raw milk.
It also depends ont the pH of milk.

Sensory evaluation
Our group of students did tasting of the commercial samples and our UHT milk sample.

The majority of members preferred, our direct UHT sample was much better than the three commercial samples.
The Waitrose organic milk had an oxidised off flavour.

There preference ranking for the samples were as follows:
Has the UHT milk been diluted?

By comparing the values of the freezing point depression, we could say that the UHT (direct) milk was slightly lower than the one of the raw milk.
Consequently, the water content was lower for the UHT direct milk, it has been concentrated and not diluted, comparing to the raw milk.

This was due to the fact that the control of the temperature of the before steam heating begins and the temperature of cooling in the flash cooler.
To avoid dilution or concentration of the milk these two temperatures should ideally the same, so as to allow the evaporation of the exact quantity of water that has been diluted during steam heating.

The lower the temperature of cooling, the more water evaporates from the milk, thus in the case of the UHT sample tested, the pressure of cooling was lower than the appropriate one.
The fact of the concentration of the milk can also be determined through the comparison of the concentration of protein, lactose and milk solids non fat between the UHT direct and raw milk sample, which are higher for the former.

Discuss the differences between direct and indirect processes
With direct heating systems, the product is heated to sterilisation temperature by mixing it with steam.

Some of the steam condenses giving up latent heat of vaporization to the product and giving much more rapid rate of heating than is available with any indirect system.
The steam may be injected into the product know as steam injection or steam infusion or product-into-steam.

These two methods give different characteristics and are considered separately.
This method of heating can cause considerable dilution of the product.

For example, a 60˚C temperature rise will involve about 11% addition of water as condensed steam.
The design of the holding section for a desired residence time must therefore take into account that the flow rate within it will be greater than the inlet product flow rate.

Cooling of the product may be by any indirect heat exchanger that is suitable to the product characteristics.
Therefore, the added water will remain in the product and allowance for the dilution must be made in the product formulation.

The steam quality is important, and contamination of the food product by steam pipe corrosion or boiler feed water additives must be eliminated.
Those food products where the added water is not acceptable, or where very rapid cooling is required, "flash cooling" may be used.

The thermal efficiency of a direct heating system may be increased by indirect regenerative heating before the product is mixed with steam; this may be achieved by heat transfer from the outgoing product is mixed with steam; this may be achieved by heat transfer from the outgoing product after the expansion cooling stage, by recovery of heat from the water vapour liberated in the expansion, or by using the condenser cooling water on the exit from the condenser, when it has been heated.
High rates of heating and cooling are obtainable in direct plants.

This means that the ideal of high temperatures held for short times, which is the basis of the continuous-flow thermal process, is more easily obtainable with direct than with indirect systems.
Practical advantage of the direct heating systems are its ability to process more viscous products, particularly those that cannot be satisfactorily handled in plate-type indirect plant, and comparative freedom from fouling.

For example, a direct plant may be able to operate producing UHT milk without cleaning for twice the length of time of a plate-type indirect plant.
A direct heating plant is relatively complex.

Much of it is non-standard and specially fabricated, unlike heat exchanger plates and tubes.
It requires additional equipment such as expansion vessels and condensers.

Water consumption for cooling in a high-regeneration indirect plant during operation is negligible, because the cooling requirements are met by the incoming cold product.
Even an indirect plant with lower regeneration levels, the cooling water requirements are relatively modest.

Direct plants require large amounts of water, particularly for the operation of the condenser.
Direct heating systems need more pumps than indirect systems.

It needs three product pumps of different types, as well as a homogeniser if the product requires it, and a vacuum pump for the removal of the non condensable gases from the condenser.
Only a single pump may be needed with an indirect system, together with a homogeniser.

At the stage of flash cooling, the temperature and the pressure of the process should adequately controlled to avoid dilution or concentration of the product due to removal of quantity of water less or more, respectively, than the one of the steam which has been condensed at the previous stage.
As mentioned above UHT direct systems can achieve the heating of the product at very high temperatures for short periods of times.

This combination is beneficial for the quality of the product, as although the desired microbial quality is attained the nutritional quality of the product is also preserved.
Finally, as far as the processing of milk is concerned, when milk is heated above 85°C there is production of low molecular weight compounds like H 2S, CH 3, SH which can give unacceptable taste.

The flash cooling process involved in the direct systems results in the loss of some components which are more volatile than water, like oxygen, the components mentioned above and, and other desirable or undesirable flavour components, which may lead to a further change of product quality.
This fact is the reason for which direct UHT milk is regarded to have less cooked flavour than the indirect.

Abstract
A platinum film resistance thermometer and a thermistor thermometer were calibrated over a range of temperature from 0 oC to 50 oC.

The resistance of the platinum film resistance thermometer was found to have an approximately linear response to temperature over the tested range, with an error in predicted resistance of 0.02% or less of actual resistance.
For this instrument the accuracy of the resistance meter used was found to be the limiting factor in the accuracy of the measurements taken.

The resistance of the thermistor thermometer was found to approximately obey an inverse exponential relationship to temperature, with an error in predicted resistance of 1% or less of actual resistance.
This uncertainty was proved to be the limiting factor on the accuracy of the instrument.

Introduction
Air temperature is a fundamental variable in meteorology, and so accurate measurement of temperature is of paramount importance.

In some situations a liquid in glass thermometer is used for this measurement, but the need for a human observer to take the reading makes them unsuitable for remote or automated sensors.
Resistance thermometers are therefore necessary for many applications, for example in radiosondes, having the advantage that their output can be logged electronically.

The aim of this experiment is to compare the characteristics of two types of resistance thermometers, a platinum film and a thermistor.
Pure metal thermometers have a resistance which increases with temperature.

Platinum is the most commonly used metal, due to its stability and relatively linear response to temperature change.
It has a temperature coefficient of 3.9 10 -3 K -1.

(1) The low resistance of platinum can lead to the contact resistance in plugs and sockets becoming a factor which must be accounted for.
Thermistor thermometers are semiconductors, whose resistance can either increase or decrease with temperature, depending on the materials used.

The temperature coefficient of a thermistor is typically an order of magnitude larger than that of a pure metal.
Thermistors are not as stable as platinum, and their temperature response is subject to a long term drift effect.

Both instruments in this experiment will be calibrated, and their sensitivity over a range of atmospheric temperatures will be investigated.
For the platinum film, the resistance increases with increasing temperature according to FORMULA where  and  are constants and R 0 is the sensor resistance at a reference temperature T 0 (measured in oC, and taken as zero here).

For atmospheric temperatures the quadratic term is very much smaller than the linear term, so Equation 1a can be approximated to FORMULA where b = R 0 The thermistor used in this experiment has a resistance that decreases with increase in temperature, approximated by FORMULA where T is the temperature (measured in Kelvin), A is the characteristic resistance and B is the characteristic temperature.
This equation is an approximation, and over the range -20 oC to 50 oC the error in the predicted resistance may be up to 5% of the actual resistance.

To calibrate the thermistor a linear relationship between two variables is the most convenient.
To achieve this, Equation 2a is rewritten by taking the logarithm of both sides, giving FORMULA A graph of ln(R) plotted against 1/T should therefore be a straight line.

Experimental method
The thermometers were both placed in an insulated water bath.

A precision platinum resistance thermometer was used to provide the reference measurement of temperature, with an absolute accuracy better than ± 0.05 oC.
Crushed ice was added to lower the water bath temperature to 0 oC.

The temperature was then increased in approximately 5 oC steps, up to 50 oC and at each step the reference temperature was recorded and simultaneous readings taken from the platinum film and thermistor thermometers.
Results

Table 1 in Appendix 1 shows the results recorded during the experiment.
A graph was plotted of resistance against temperature for the platinum film resistor (Graph 1).

For the thermistor, the logarithm of resistance was plotted against the inverse of temperature (Graph 2).
From Graph 1 it is possible to estimate the experimental values of R 0 and b in Equation 1b as FORMULA From Graph 2 the experimental values of B and ln(A) in Equation 2b can be estimated as FORMULA therefore FORMULA To compare the agreement of Equation 2a with the experimental results, a graph was plotted of calculated values of resistance against temperature, and then the measured values overlaid on the same axes (see Graph 3).

At all points the discrepancy is 1% or less between measured and calculated values.
Equation 1a was also fitted to the experimental results for the platinum film resistance thermometer, to determine if there is any detectable non-linear effect in the temperature response.

This gave values for the coefficients of FORMULA The accuracy of Equation 1b compared to that of Equation 1a was investigated by examining the resistance residuals in each case.
This data is shown in Table 2.

Graph 4 shows a plot of the calculated residuals.
It can be seen that the quadratic fit is slightly better than the linear fit.

This difference is most marked at lower temperatures, but over normal atmospheric temperatures the linear fit is a good approximation.
By rearranging and differentiating Equation 2b it can be found that FORMULA Equation 2c gives the sensitivity of the thermistor.

The sensitivity of the themistor can be calculated for a range of atmospheric temperatures, and from this the resolution can be found.
The resolution of the multimeter used at this range was 1 Ω.

It should be noted that the values for the thermistor resistance in Table 1 have only been quoted to an accuracy of 10 Ω, due to the rapid fluctuations in the reading at the time of measurement.
These fluctuations were attributed to small local changes in temperature due to turbulence in the water bath.

The thermistor was able to detect these fluctuations because of its small size and rapid response time.
Assuming the multimeter is the only limit on resolution gives the values shown in Table 3.

The resolution of the platinum film resistance thermometer can be calculated from the gradient of its calibration.
In this case the resolution of the multimeter was 1 × 10 -3 Ω.

Again assuming this to be the only limit on resolution gives a calculated resolution of 2.6 × 10 -3 K. Given that a multimeter has a typical accuracy of ± 0.1% on all resistance ranges, this uncertainty can be translated to an uncertainty in the temperature measurement of each instrument.
For example, at 20 oC, in the platinum film resistance thermometer an uncertainty of ± 0.1% on the resistance reading gives an uncertainty of ± 0.3 oC.

This is a much larger percentage error than that produced by the fitting the calibration.
For the thermistor thermometer at 20 oC, an uncertainty of ± 0.1% on the resistance reading gives an uncertainty of ± 0.025 oC.

This is a much smaller percentage error than the 1% uncertainty introduced by the calibration.
Conclusion

The instruments have both been shown to provide an accurate means of temperature measurement.
In the case of the platinum film resistance thermometer the error in the measurement of its resistance is the most important factor, while for the thermistor thermometer the calibration fit introduces the largest uncertainty.

The uncertainty caused by the experimental method was fairly small, resulting mainly from the different response times of the instruments, and slight fluctuations in the water bath temperature.
Overall the method was satisfactory for determining the calibration of the instruments, and the experiment proved that both are sufficiently accurate for temperature measurements over the tested range.

Further investigation would be required to determine how the accuracy of the calibration fits may be affected by temperatures outside the tested range.
Introduction

Global mean surface temperature has increased by approximately 0.7 oC since pre-industrial times.
This increase is thought to have been mostly caused by human activity, principally the release of carbon dioxide and other greenhouse gases.

There are, however, many other factors, both anthropogenic and natural which could have an effect on climate.
Attributing the observed climate change to each of these factors is a great challenge facing climate scientists.

In this experiment a simple, zero-dimensional, two-layer spreadsheet model was used to investigate the effects of different radiative forcings on global mean temperature.
The model allows climate feedback parameter Y to be varied, and estimates of the ten most important radiative forcings can be included, plus one user-defined forcing.

Each forcing can be applied with a scaling factor to modify its effect.
The model calculates the combined forcing for each year between 1850 and 2006, and from this the temperature anomaly relative to the 1961 - 1990 average and also relative to the 1850 - 1879 average (In this exercise the 1961 - 1990 average will be used for all comparisons).

The equilibrium temperature change for the relevant forcing at each year is also calculated.
The model produces plots of the combined radiative forcings, the global mean temperature anomaly relative to the two stated time periods, with each plotted alongside the observed temperature anomaly, and also the model vs equilibrium temperature change for each year.

A χ2 error value for the difference between model and observed temperature anomaly is also calculated.
Part A

Method
The effect of different radiative forcings were investigated by scaling each to 1 (switched on) or 0 (switched off), in different combinations.

For all these different combinations the climate feedback parameter Y was left constant at a mid-range value of 1.3 Wm -2K -1.
Results

With only the well-mixed greenhouse gas forcing switched on the temperature anomaly rises slowly at first then more rapidly from the 1960's onwards.
The temperature change is too great over the whole period, with the anomaly being slightly too high from 1970 onwards, and substantially too low for most of the preceding time.

The increase is steady, with none of the small scale fluctuations that are seen in the observed data.
The rate of increase after about 1960 is similar to that in the observed data.

Adding all other anthropogenic forcings except the indirect aerosol effect has only a small effect.
However, the fit is slightly improved, particularly from 1950 onwards.

If the indirect aerosol is also included the fit worsens again, with the temperature anomaly becoming too high until 1940, and too low in the years after 1995.
The well mixed GHG's have the most effect of all anthropogenic factors; removing them causes the temperature anomaly to fall steadily throughout the whole time period.

See Figure 1.
The natural forcings, from solar fluctuations and volcanic eruptions, provide more small-timescale effects, with no overall trend through the time period.

The smaller fluctuations do not seem to fit very closely with the fluctuations in the observed data.
The negative anomalies due to volcanic activity are too large in several cases.

Including all the forcings, the general pattern of increasing temperature anomaly is reproduced, although the model anomaly is above the observed value for most of the period up to 1965.
The smaller scale fluctuations are not well reproduced.

See Figure 2.
Discussion

From these results it seems that the most likely major source of the increase in temperature over the past fifty years is the increase in anthropogenic greenhouse gases.
The upward trend in the temperature over this period is captured quite well in the model when including this forcing, and not at all when the forcing is removed.

The shorter timescale variations in the observations are not well represented in the model.
The cooling effect of volcanic eruptions appears to be overestimated.

This could be partly explained by the model's lack of random internal variability.
In the real world, random climate variability could have acted to mask some of the effects of volcanic eruptions.

It still seems likely, however, that there is too much weight given to the volcanic forcing in the model.
Part B

Method
The uncertainty in the climate feedback parameter is given as 0.8 to 3.3 Wm -2K -1.

The effect of this uncertainty was investigated using some idealised radiative forcings.
First, the forcing was defined to increase linearly from zero in 1850 to 1 Wm -2 in 1900 and then remain constant.

A second case was examined with a step change forcing of 1 Wm -2 between 1880 and 1930, and zero before and after.
In each case, Y was varied within the stated uncertainty range and the effects on transient model temperature and equilibrium temperature were studied.

Results
With the first idealised forcing, different values of Y gave very different temperature responses, as shown in Table 1.

The temperature change is greatest with small values of Y (ie. large climate sensitivity).
This effect is non-linear, with increasingly large effects as Y becomes smaller.

The difference between the temperature change reached in the year 1900 and the equilibrium temperature change is the "hidden" warming in the climate system.
This value also increases more and more rapidly as Y decreases.

The step change forcing shows how variability in Y affects the climate's response to a transient forcing.
This forcing was chosen to act for 50 years to give an easily measurable effect.

Table 2 shows the maximum warming caused by the forcing, and also the lasting temperature change.
The effect on the climate can again be seen to increase more rapidly as Y becomes smaller.

There is a larger transient effect and a larger lasting effect.
Discussion

The uncertainty in Y is a major factor in the attribution problem.
If the sensitivity of the climate to a given forcing is not known, it is difficult to predict the effect that the forcing will have.

In this model Y is assumed to be equal for each forcing, which in itself makes it difficult to distinguish the effects of one forcing from another.
It can also be seen that as Y becomes closer to zero the linear approximation in the model breaks down, and the temperature change becomes very large.

This means that using very small values of Y does not give sensible results.
Part C

Method
The effect of a postulated radiative forcing due to stratospheric water vapour was investigated.

The magnitude of the forcing was taken to increase linearly from zero in 1960 to 0.6 Wm -2 in 2006.
A mid-range value of 1.3 Wm -2K -1 was used for the climate feedback parameter Y, and all other forcings were included with a scaling factor of 1.

Results
The additional forcing has the effect of increasing the temperature anomaly towards the end of the time period, resulting in the model being about 0.2 oC warmer than the observations by 2006.

The χ2 error is reduced to 726.5.
This is due to the 1961 - 1990 average temperature being increased, and therefore the temperature anomaly earlier in the time period becoming more negative, and closer to the observed values.

Discussion
The inclusion of this forcing reduced the error value in the model, however the overall fit in terms of the shape was not improved.

It is possible that there are other forcings or feedback mechanisms not yet included climate models, which has implications for the attribution problem.
It is not possible to correctly attribute all the effects of climate change until all the forcing mechanisms are understood.

Part D
Method

The climate feedback parameter Y and the scaling factors on all radiative forcings were altered to minimise the χ2 error in the model.
This was attempted first adhering to the published IPCC uncertainties on all values, then using any value.

No user defined forcings were included, and no other model parameters were changed.
The error was minimised by analysing the size of each radiative forcing, and the time over which it has an effect, then scaling each one accordingly.

The scaling factors were fine-tuned by a process of trial and error.
Results

The minimum error values obtained are shown in Table 3, along with the scaling factors used.
Discussion

It is possible to obtain a lower error by using values outside the IPCC uncertainty ranges.
However, the greatest improvement was caused by reducing the scaling of the natural forcings.

This suggests that one of the main sources of error in the model is in failing to match the smaller timescale variations in the climate.
As previously discussed, this natural climate variability could affect or mask to some degree the effects of solar and volcanic forcings, making it impossible to match all the fluctuations with a model which has no internal variability.

Conclusion
The model produces useful results, despite being a fairly simple approximation.

The fit of the model temperature anomaly to the observed values is good enough to provide evidence that the observed temperature change over the last century is largely attributable to anthropogenic sources, particularly the emission of greenhouse gases.
There are however several limitations to the model which could affect the reliability of the results.

The Climate Feedback Parameter Y is assumed to be the same for all different radiative forcing mechanisms, which is not the case in reality.
The model does not provide any information about spatial variability in temperature change.

Introduction
On the weekend of the 27 th to 29 th of October 2006 Meteorology MSc students from the University of Reading took part in a field course in an area of Dorset close to Swanage.

The aim of the course was to participate in taking meteorological measurements and observations, and experience the changes that can occur in the weather over the course of a few days.
This experience was gained first-hand by spending a large part of the time outdoors.

Thankfully the most hostile weather conditions encountered on the weekend were nothing more than a gusty wind!
Aim

The aim of this report is to provide an overview of the data recorded on the field trip.
I will compare some of the different types of measurements with each other, and try to relate the measurements to the overall weather situation during the weekend.

I will also describe how the weather changed over the weekend, and explain how these changes were represented in the meteorological data recorded.
Types of Data Recorded

A large amount of meteorological data was gathered on the field trip, from several different sources: An automated met mast was in place at Durlston Head from Tuesday 24 th to Tuesday 31 st.
The mast recorded measurements every five minutes of air temperature, relative humidity, wind speed and direction, solar and net radiation, ground heat flux, and potential gradient.

A total of six radiosonde balloons were launched from Durlston Head over the course of the weekend.
These sent back measurements of air temperature, pressure, and relative humidity during their ascent, allowing a vertical profile of the atmospheric conditions to be made.

Another automated weather station was at Leeson House from Friday 27 th to Sunday 29 th, making measurements every ten minutes of air temperature, relative humidity, and pressure.
In addition to this, measurements were taken by the students at intervals throughout each day, using a variety of handheld meteorological instruments.

The instruments included whirling psychrometers to measure dry bulb and wet bulb temperatures, digital barometers for pressure, cup anemometers for wind speed, and infra-red thermometers for cloud temperature.
Observations were also made of cloud cover and type, and wind direction.

These observations were made at Leeson House and Durlston Head, as well as during walks along the coastal path between the two sites.
Overview of weather for the weekend

Appendix 1 shows the Met Office synoptic charts for 00Z on Friday, 06Z on Saturday, and 00Z on Sunday.
Appendix 2 shows infra-red satellite images for 10:50 on Friday, 10:26 and 21:06 on Saturday, and 02:43 and 12:47 on Sunday.

Appendix 3 shows solar and net radiation fluxes recorded by the met mast at Durlston Head.
The chart for Friday 00Z shows a low pressure region centred over southern Norway.

A cold front trailing from this system has just passed over the south coast of England and out into the Channel.
This was a very weak front, bringing very little cloud with it, as seen from the solar and net radiation measurements (figure 18).

The front passed into northern France and soon faded away, as evident from figure 13.
Friday was a mostly fine day with some scattered cloud, but this built up through the evening as a new frontal system moved in.

This can be seen in the chart for 06Z on Saturday (figure 11), over Cornwall.
Throughout Saturday the fronts brought a lot of cloud, but no precipitation.

The advancing cloud can be seen in figure 14, as fairly continuous high or mid level cover, with some low level cloud beneath.
In figure 15 the cloud is mostly at mid or lower levels.

The chart for 00Z Sunday (figure 12) shows two cold fronts over southern England and a ridge of high pressure developing over Ireland.
The fronts brought a small amount of rain during the night.

Figure 16 shows this band of cloud covering southern Britain, with clear skies to the north.
The cloud began to clear during Sunday morning and early afternoon as the high pressure region moved across Britain.

This can be seen from Figure 17.
Cloud cover and wind speed observations

Figure 3 shows a table of observed wind speed and direction, and cloud cover observations, taken on Saturday and Sunday.
The observations agree well with the synoptic charts and satellite images for the same period.

The winds on Saturday were quite strong, as expected from the isobars on the chart, and generally came from a south westerly direction.
The direction is not quite parallel to the isobars, but this is due to the fact that the measurements were taken by a person standing at ground level, and the frictional drag of the ground exerted on the air causes a slight change in direction towards the centre of low pressure.

The variation in wind speed is partly due to the sheltering effects of the local topography, as measurements were made at a variety of locations during the walk.
Every effort was made to find suitably exposed places for taking measurements, but this was not always possible.

The cloud cover was an unbroken low level layer, which prevented any of the higher clouds shown on the satellite images from being observed.
On Sunday the wind speed was much less, due to the high pressure moving in, and the spreading out of the isobars.

The low cloud began to break up through the day, allowing the higher, patchy cloud to be observed.
This can also be seen on the corresponding satellite images.

NB - Wind speeds were not recorded for locations that were too sheltered for a reasonable measurement to be made.
Cloud temperature is only an approximate measurement, and it was not always possible to measure temperature for all visible cloud types.

Comparison of temperature measurements on Saturday 28th
Figure 4 shows a series of air temperature measurements made on Saturday 28 th.

The data recorded by the automatic stations at Durlston Head and Leeson House are plotted, as well as data recorded by two of the student groups, either at the same sites or during a walk from one to the other.
It can be seen that the readings from Durlston Head and Leeson House agree very closely with each other.

The pattern of increasing and decreasing temperature is very similar.
The temperature is highest during late morning and early afternoon, with a slight dip around midday.

This corresponds with the incoming solar radiation measured (see figure 18), which also peaked and dipped at these times.
The higher incoming solar radiation would have increased the air temperature near the surface, although the observed increase in temperature could also be partly due to the thermometer screens not being completely efficient in shielding the thermometers.

There is a trend of the temperature at Durlston Head being slightly warmer than Leeson House in the morning and evening, and slightly cooler in the middle of the day.
An explanation for this is Durlston Head's proximity to the sea.

The temperature of the sea experiences almost no diurnal variation, and this acts to regulate coastal temperatures, reducing the amount of cooling during the night and warming during the day.
The temperature readings taken during walk A and walk B show greater random fluctuation.

This is due to the greater uncertainty involved in taking the measurements.
Causes of error which may have affected these measurements include parallax error when reading the instrument, and not allowing sufficient time for the instrument to reach thermal equilibrium.

The readings were taken at various different places on the walks between Durlston Head and Leeson House, but having seen how closely the measurements from these two sites agree (to within 0.5 oC) I do not think that location is a factor that influenced the temperature very greatly in this case.
The measured temperatures are generally higher than those taken by either automatic station.

This may be due to a systematic error, such as calibration of the thermometers, or may be due to the thermometers not being in a screen, causing them to be warmed slightly by solar radiation.
Analysis of radiosonde data

A total of six radiosonde balloons were launched over the weekend.
To compare the changes that occurred in the weather between Saturday and Sunday I will look at the data received from two radiosonde launches on each of these days.

Figures 5 and 6 show the data from the 10:45 and 15:02 launches on Saturday.
Air temperature and calculated dew point temperature are both plotted against pressure.

These launches show fairly similar profiles.
Both have temperature and dew point quite close together at low altitude, and show a slight temperature inversion at around 950 hPa.

During both these launches there was an unbroken layer of stratus or stratocumulus at low level, below this inversion.
The earlier launch shows a region of moist air from about 580 hPa upwards.

This may correspond to the mid level altostratus cloud observed earlier in the morning between breaks in the stratus.
Figures 7 and 8 show the same parameters plotted from the 9:30 and 11:45 launches on Sunday.

Again these two figures show an overall similar pattern to each other.
Both have quite moist air at low levels with a temperature inversion at about 980 hPa for the 9:30 launch and about 960 hPa for the 11:45 launch.

However, the low level air is becoming less moist by 11:45, this agrees with humidity measurements taken by the met mast, which show the relative humidity falling from 95% to 85% between the two launches.
As with Saturday there was a layer of low cloud beneath the temperature inversion, but by 11:45 this layer was breaking up.

Above the inversion in each case is a region of drier air that was not evident in the Saturday launches.
This may be due to subsidence caused by the incoming high pressure ridge.

Summary
Observations and measurements made of the weekend's weather agree well with expected conditions from the synoptic charts and satellite images.

Changes were seen it the weather over the weekend, and these changes were reflected in the measurements.
These changes were not very severe, for example the frontal systems passing over were quite weak, but the accuracy of the data recorded was easily good enough to allow the meteorological features to be identified.

The measurements taken did show the limited accuracy that can be achieved with handheld instruments, when compared with the met mast instruments.
The field trip was a useful introduction to making meteorological measurements, and also allowed plenty of opportunity to take embarrassing photos (see figure 9).

Introduction
The England and Wales precipitation record contains data going back to 1766, and is compiled from approximately 60 rain gauges.

In this report the seasonal totals of precipitation will be analysed from 1767 to 2001.
Seasonal differences will be compared, probabilities calculated for some extreme rainfall events, and any evidence of long term trends will be analysed.

Section 1
Table 1 shows a summary of the data set of total annual precipitation.

Figure 1 shows a histogram of annual precipitation.
It can be seen that the distribution is slightly positively skewed, with more outliers at high values than at low values.

None of the outliers seem so extreme as to raise concerns about their accuracy.
There is not a sharp peak in the distribution, with all bins between 800mm and 1000mm having a similar frequency.

Figure 2 shows a normal Q-Q plot for the annual precipitation.
The fit is fairly good, but it can be seen that the tails of the observed distribution are too short, particularly at the lower end.

Figure 3 shows boxplots for each of the seasonal totals.
On average, the precipitation is highest in autumn and lowest in spring, with the summer and winter totals being similar.

Each season also shows some positive skew, with more outliers at high values.
The seasonal nature of the rainfall explains the slightly broader peak of the annual distribution seen in Figure 1.

Each individual season has a sharper peak in its distribution, but the different means lead to the combined distribution having the shape as seen above.
Section 2

There are 8 recorded years where the total rainfall was less than 700 mm.
The probability that a drought year (Pr(DY)) will occur can therefore be estimated as 8/235 = 0.034.

From Bayes theorem, the probability that a drought year will occur, given that a drought winter and spring has occurred (probability Pr(DW)) can be estimated.
FORMULA This is equal to 0.5 for this example.

The probability Pr(DW|DY) can be estimated as 0.5 from the data, however as there are only 8 drought years in the record, this probability is very uncertain.
Rearranging the equation now gives FORMULA This probability equates to a winter and spring with total precipitation of approximately 281 mm.

Figure 4 shows the total annual rainfall plotted against total winter and spring rainfall.
It can be seen that a December to May total of 281 mm or less will result in an annual total of less that 700mm approximately half the time, according to the data.

However, it is possible to have a December to May total of up to 350 mm that still result in a drought year.
Section 3

Total autumn rainfall can be modelled using a Normal distribution.
FORMULA The total rainfall recorded in autumn 2000 was 502.7 mm.

This is the highest seasonal total on record.
Using the Normal distribution as a probability model, the probability of exceeding this amount is 0.00025.

The Normal distribution is perhaps not the best model to use in this case.
It can be seen from Figure 5 that the fit in the tails of the distribution is quite poor.

A better fit to the data can be obtained by using a gamma distribution.
FORMULA Using the Gamma distribution as a probability model, the probability of exceeding 502.7 mm is 0.0020.

Section 4
It can be seen from Figure 3 that the distributions for summer and winter rainfall are quite similar.

From this a hypothesis can be formed that the summer rainfall has an equal population mean to the winter rainfall.
This hypothesis can be investigated by estimating the population mean of each set of data and comparing them.

A simple point estimate of the population mean is the sample mean, i.e. FORMULA For the summer, FORMULA and for winter FORMULA.
These values differ by 5.2 mm, but a point estimate gives no information about the significance of this difference.

The difference in the means could be due to the sampling uncertainty.
To examine how significant this difference in the means is, it is necessary to investigate a null hypothesis, for example that that both means are the same; FORMULA This will be tested to a significance level of 10%.

The alternative hypothesis is that the two means are different; FORMULA The hypotheses can be tested by an unpaired two sample t-test.
This yields a t-statistic of 0.88 and a p-value of 0.3765.

Therefore the null hypothesis cannot be rejected at the 10% confidence level.
The 90% confidence interval for the difference in the means is -4.47 to 14.83.

The likely difference in sample medians, rather than means, can be considered simply by the notches on each boxplot in Figure 3.
The notch gives a guide to the likely range of the median.

Each notch extends to 1.58 IQR/n.
This gives roughly a 95% confidence interval for the difference in the medians.

Section 5
Figure 6 shows the data for each season plotted as time series.

Fitting a simple linear regression shows evidence of a marked increase in winter precipitation and decrease in summer precipitation, and to a lesser extent an increase in spring precipitation and decrease in autumn precipitation.
A lowess filter has also been applied to each data set to give an indication of the time variance of the trend.

The trends in different seasons do not appear to be changing at the same time.
For example, any trend due to climate change would be expected to be visible only from about 1900 onwards.

The lowess filter does not clearly pick up an effect like this, with the possible exception of in the winter data.
It is probable that the random variations in the data are too great to allow the precise trends to be detected in this way.

Figure 7 shows the autocorrelation function for the winter precipitation time series, calculated up to a lag of 10 years.
It can be seen that there is no significant evidence of correlation between different years.

This result is the same for each season.
A simple way to estimate the significance of the trends in each season is to compare a smaller sample from the start of the period to a sample from the end of the period.

The distributions of these two data sets should have significantly different means if there has been and average change in precipitation over the period.
Figure 8 shows boxplots comparing the first 30 years (1767-1796) for each season to the last 30 years (1972-2001).

The significance of any difference between each pair of samples can be investigated using a similar process to that in Section 4.
Using the null hypothesis that the means are the same for each pair, a t-test can then be performed.

Again the significance level will be chosen as 10%.
The results are shown in Table 2.

This means that the null hypothesis is rejected for the winter, spring and summer data.
There is strong evidence of a change in mean precipitation between these time periods for these three seasons.

Abstract
A series of 10 minute average wind speed measurements for various heights were used to produce a vertical wind profile close to the surface.

A logarithmic wind profile was found to be a good approximation to the observed data.
Calculated values of roughness length and friction velocity were found to vary significantly between readings.

This was mainly attributed to changes in wind conditions and interference of surrounding terrain rather than atmospheric stability.
Analysis

Three sets of 10 minute average readings (A B and C) were taken from the pulse counter between 12:20 and 12:54.
The logpro spreadsheet was used to convert the counter readings to wind speed, then produce plots of the vertical wind profile and wind speed against the logarithm of height.

These plots are shown in Figure 1 for readings A, B and C.
A linear trend was fitted to the log plots, and from the coefficients of these straight line equations, values were found for u* and z using the log wind profile equation.

These values for each set of measurements are shown in Table 1.
It can be seen that the log wind profile is a good fit to the measurements.

Weather conditions were quite changeable while the measurements were being taken.
Initially there were about 7 oktas of stratocumulus.

This cloud became more broken during the course of the measurements, reducing to 3-4 oktas during the final 10 minutes.
There was direct sunlight on the observing site shortly before the measurements were started, briefly measurement B and for most of measurement C. Grass temperature was initially 15 oC, fell to 11 oC at the start of B, rose to 14 oC at the start of C and peaked at 18 oC, before falling back to 14 oC.

Wind speed, measured with a hand anemometer, was mostly in the range 1 to 3 ms -1, with occasional gusts up to 5ms -1.
The wind direction was mostly westerly, occasionally veering south westerly.

To the west of the observing site there was a distance of approximately 150 m of short grass, which was bounded by a number of trees.
Roughness length for cut grass would be expected to be approximately 6 x 10 -3 m (Stull 1988).

This is fairly consistent with the results obtained, with the roughness length calculated for A and B being slightly larger, and for C slightly smaller.
The fetch of 150 m would only allow reliable measurements of the wind up to approximately 1.5 m above the surface.

In this experiment the wind was measured up to 6.4 m above the surface, which may have caused the values for A and B to be larger.
It can also be seen that each set of readings appears to have one slightly anomalous result which does not fit the logarithmic curve.

In each case it is the measurement taken by the anemometer at 2.24 m height.
This could indicate that this instrument is not functioning properly, but an alternative explanation could be that the wind above this height is being affected by the trees surrounding the observing site.

This effect was tested by recalculating the log plot using first just the data up to 2.24 m, then just the data from 2.24 m upwards.
This gave two new values each for u* and z, one for the lower wind and one for the upper wind.

Data set C was used, and gave the values in Table 2.
There is an increase in z 0 for the upper wind which would be expected if the trees are causing extra turbulence at that level.

The height at which this effect is seen is within a reasonable range of what would be expected.
Repeating the experiment with a different anemometer at 2.24 m would confirm whether this effect is genuine.

It can be seen from Table 1 that the friction velocity of measurement A was larger than B and C, which were quite similar.
The value of u* is dependant on wind speed, so this would be expected to be largest for A, were the average wind was stronger, and similar for B and C.

The roughness length is largest for A and smallest for C.
The log wind profile is only strictly valid in neutral conditions, so the most obvious explanation of these changes is that conditions were unstable for measurement A then became increasingly less unstable for B and C.

However, this explanation is only partly supported by observation.
There was direct sunlight on the observation site until shortly before the observations took place, so conditions may have been quite unstable initially.

There was, however, much more direct sunlight during C than B, which should have led to more instability.
There are also other factors which could have affected the amount of turbulence and hence the values of u* and z.

A slight change in wind direction could have had an effect, by altering the fetch, or by causing the wind to blow over some of the other apparatus in the observing site.
A roughness length of less than a centimeter would be easily affected in such circumstances.

The teachpro spreadsheet was used to simulate vertical wind speed and temperature profiles, and investigate how these are affected by varying u* and T*.
A larger value of u* gives a more rapidly increasing and larger wind speed.

As wind speed increases, there is greater wind shear, and therefore more shear production of turbulence, and corresponding greater vertical momentum flux (u*).
The overall stability of the profile is determined by T*.

When T* is negative, sensible heat flux H is positive and temperature is highest at the surface, leading to unstable conditions.
The wind speed tends towards a constant value as z increases (Figure 2a).

When T* is very small, temperature is almost constant with height, meaning conditions are neutral, and H is very small.
The wind profile is logarithmic (Figure 2b).

As T* increases, H becomes negative and the temperature is reduced at the surface, giving stable conditions and a wind profile that becomes linear as z increases (Figure 2c).
Data from the Penman mast were used to calculate the average surface sensible heat flux during the three sets of measurements that were taken.

The Obukhov length L was then calculated.
Due to the changing conditions during the experiment these values vary considerably, as can be seen from the results in Table 3.

These calculations show that conditions were unstable during measurements A and C, and stable during B.
The stability clearly does not have a large effect on the wind profile this close to the ground, as then a larger roughness length would be expected from C than B. Small variation in other conditions, such as the wind direction, must have had a larger effect on the observed value of z 0.

Conclusion
The experiment shows that the log profile is a good approximation of the vertical wind profile close to the surface.

The values of u* and z 0 obtained were quite variable and dependent on local conditions of wind and terrain.
Stability of the boundary layer was not found to have a large effect at the heights measured.

This variability was much larger than any uncertainty in the measurements.
The variability in the calculated values could be reduced by using a more open observing site with a longer fetch.

The Lyon Tablet is a transcript-like inscription that contains Claudius' actual speech in contrast to Tacitus Annals, which was written nearly 70 years after by someone who was not there or even a contemporary.
It is important to note the purpose of each of these texts as the Lyon Tablet inscription is an informative record of the speech by Claudius whereas the Tacitus version is, although meant to be to an extent informative, intended to entertain the readers of the time i.e. the aristocracy.

Therefore the style and presentation is fundamentally differing.
This is evident in that the Tacitus version summarises the senator's early views clearly in speech whereas the Lyon Tablet has interruptions from individuals who appear to be senators.

From a dramatic and stylistic perspective this would obviously not have suited Tacitus, thus he uses narrative prose.
The Tacitus version as a whole is more summarised.

"Not to enquire too minutely into the past...new members have been brought into the senate from Etruria...and the whole of Italy...
so not only single persons but entire countries and tribes might be united under one name." The Lyon tablet on the other hand, does go into minute detail about the past, indeed the entire second paragraph talks about the ancient history of Rome.

This suggests that Tactitus may have read this and decided that it was too long winded and so chose to use his own examples including, as the origin of the Julii, Coruncaii and Porcii as well as his own seemingly made-up rhetorical questions: "Are we sorry that the Balbi came to use from Spain?" these inaccuracies are seen to be more dramatic but deviate from the facts.
The senator's speech to Claudius at the beginning of Tacitus is completely fabricated in its existence, according to the Lyons inscription, and in context it is mainly over-dramatised.

For example, in the Lyons inscription Claudius does explain that the senate may have concerns over the "novelty" of introducing new tribes into the senate, however the Tacitus version reads "that every place will be crowded with these millionaires [The Gallic chieftains who wish to join the senate]...whose second and third generation destroyed our armies with fire and sword!".
Against such strong opposition as this it is difficult to see why after only the short speech by Claudius, the Senate was so eager to pass a decree.

True the Lyon inscription does show that the senate was in opposition but it does not use the same analogies or opinions shown by the senator in Tacitus, for example there being no distinctions left for "our noble houses".
This therefore suggests that Tacitus shows a greater opposition to add more dramatic and romantic effect to Claudius speech.

This explanation is re-enforced by the fact that he does not state which senator says this he simply says "These and alike arguments failed to impress the emperor" which suggests that the views expressed were possibly a mixture between what he himself thought might have been said at the time and what he factually knew.
Claudius personality emerges to us as greatly improved in Tacitus, the lack of interruptions, the narration, the more heroic, dramatic sentences and the rousing examples all create a strong, articulate character.

There are numerous sentences that show this, that are totally or partially distorted from those shown in the Lyon Tablet, but create a more impressive character of the emperor.
Firstly the fact that the Tacitus version is shorter in comparison to the Lyon tablet, creates a notion that Claudius ability to persuade is very great, especially as has previously been discussed, because the senators speech at the beginning is directly polarised.

Tacitus also keeps the attention of the reader to a greater extent, suggesting that Claudius had great oratory skills whereas the Lyon inscription shows this to a lesser extent.
For example realistically, The Lyon tablets contain tangents and slight irrelevancies such as Claudius mention of himself seeming boastful about Rome's many conquests, to which he follows by saying "But I will return to the point" and the second interruption by a senator asking "where the speech is tending".

It also ends slowly without giving something for the listeners to hang on whereas Tacitus' version ends "What we are, this day justifying with precedents, will one day become one in itself." Indeed it is the authority with which Claudius speaks in Tacitus that seems so influential and important to the debate "My ancestors, the most ancient of whom...governed by the same policy of transferring to the city all conspicuous merit wherever it was found." Claudius begins in Tacitus and yet the beginning of his actual speech shown on the Lyon Tablet seems to speak with much less conviction and authority, he deprecates his main obstacle and asks the senate not to be alarmed but to remember the changes that the city has undergone throughout the ages, which is a luke-warm statement for his character.
The entire tone of Tacitus version is inspiring using such words as "patriotic" and using an example of Athens and Sparta's decline due to their inability to befriend and capitalise on their conquered foes, yet it is virtually all fabricated and greatly more concise if the Lyon Tablet is to be believed.

It is also probably no coincidence that Tacitus was a himself a great writer, being taught rhetoric from a young age it seems that the majority of the devices used were created by himself.
There is further evidence for this in that Tactitus does have several distortions of the truth, for example he says "if you review the Romans wars, never has one been shorter than the one with Gaul", this is vastly far from the truth as Caesars conquest of Gaul took eight years of intense fighting (other wars, such as the social war, lasted significantly less time).

The Tacitus version of Claudius' speech then is a significantly different one to the actual speech as shown by the Lyons inscriptions.
There are some distortions of the facts, over-dramatising of the situation and a great deal of what appears to be Tacitus's own opinions of what people may have thought.

Nevertheless it has historical use in that it accurately describes that the decree was passed, it gives some useful insights into what the senators may have thought (as admittedly the Lyon Tablet only gives two senators interruptions and the rest must be deducted from what Claudius says) and illustrates the issues of provincial relationships with Rome at that time.
The figures of Brutus, Cassius and Cato left a legacy in the early Principate, as they were some of the last true champions of the Republic and were revered by some as martyrs to a cause.

This cause was the defiance of Tyranny.
If this is coupled with the fact that this same period contained some of the most famous and terrible tyrants in history then it is unsurprising that some were ready to die to depose them.

The purpose of this essay is to explore the reasons why some men would willingly lose their lives to oppose such tyrants as Nero and the Flavian emperors Domitian and Vespasian.
This essay will show the philosophical and traditional reasoning as to why some defied them, as well as illustrating how some of these emperors facilitated this resistance to their own rule.

There are several cases of learned or aristocratic men opposing emperors for their beliefs.
However before I discuss those I will firstly look at the Senate in the early Principate.

After the fall of the Republic the politics changed, the emperor now had the power with the senate existing only in an advisory role.
The main source of conflict between senate and emperor was that they were not a diarchy.

The Senate wanted an honourable place as equal to the emperor (Wirszubski, 1950, 137), however never got this wish and in fact on numerous occasions was seen to be abused.
Under Nero it was threatened with abolition (Suet, Nero, 37) and it's treatment under Caligula lead the Senate to question restoring the Republic (Josephus, Antiquities of the Jews, 14.2).

This, in context to the period, was probably not achievable, nevertheless fundamentally it was the attitude of the emperors and ethos of the time that is so crucial to understanding why some would oppose the emperor.
The main rivalry came from nobles, senators and intellectuals and it came from within Rome (Wiszubski, 1950, 125).

It also nearly always had a philosophical edge, usually in connection to stoicism and the legacies of the "great philosopher, tyrant haters" Cato and Brutus.
One such opponent was Thrasea Paetus who was eventually forced to commit suicide by Nero for "not what he did but for what he did not do" (Macmullen, 1967, 21) his refusal of flattery to the emperor and his independent opinions, such as walking out on Nero when he was explaining to the Senate why he had murdered his mother earned him suspicion.

Moreover his close relatives had been involved in treason, his father-in-law Seneca being famously involved in the Pisonian conspiracy, and as the final nail in the coffin he was a stoic philosopher.
As Macmullen says: "If Nero wanted to murder his mother or amuse himself...he wanted no stoic philosopher scowling in the wings." That is not to say all stoics were opponents to the Principate, however the motives Brutus had were stoical and many up till this point had defied the Julio-Claudians in the name of ideas.

Indeed his dying words were worthy of Cato himself, "We can pay our debt to nature by freedom.
Nero can kill me, but he can do me no hurt" (Tac, Ann, 16.34) His was a typical example of those that died for this cause in a similar way.

The stoic philosophy and "Libertas", which to our modern society has many meanings but can best be interpreted as "freedom" and "freedom of speech", were important to the understanding of the motives behind much of the opposition, whether actively hostile or not.
Indeed Tacitus remaks that "Marcellus should be satisfied in prosecuting so many innocent victims for Nero" (Tact, Hist, 4.7).

This is intriguing as Marcellus prosecuted Thrasea Paetus with minimal evidence, and according to Tacitus Helvidius Priscus, Thrasea's son-in-law and "welcome to class himself with Cato and Brutus" (Tact, Hist, 4.8), attempted to prosecute him for this, unsuccessfully.
Ironically he met a similar end to Thrasea, being executed by Vespasian as did the author of his biographer Herennius Senecio in the reign of Domitian.

This leads to another reason for constant opposition, once an emperor, such as Nero, executed a stoic he was seen as a martyr and thrived even more on this persecution.
It is incidentally and not coincidental that it often ran in families as men with similar ideas took their daughters hand in marriage and so forth.

"It was their [men such as Thrasea] receptions and banquets the emperors feared where voices got lower and zeal got hotter...
Here too was where men praised...

the Republic and Brutus...
and martyred fathers remembered" (Macmullen, 1967, 41) Groups of such men were the heart of opposition to the emperors.

The pressure of this persecution itself however created more friction.
Voluntary censorship, found throughout ancient works at this time, was encoded into such works as Tacitus Annals, which could be seen as being conceived to tell the story of the struggle between Principate and Libertas (Wirszubski, 1950, 124).

The burning of books, tabooed in today's society reflecting the suppression of ideas and free-thought, was employed at various times, for example the senator Aulus Cremutius Cordus' Annals in 25 A.D.
were destroyed for speaking too highly of Cassius.

His defense however was memorable, his most important point being that other authors had been quite free to write about the past, such as Livy (MacMullen, 1967, 20).
Seneca's view was that Cordus' has stood for Eloquentia and Libertas (Sen, Ad Marciam, 1.2).

Tacitus also had a view on this action by the emperors "We have only to read that the panegyrics pronounced by Arulenus Rusticus on Paetus Thrasea, and by Herennius Senecio on Priscus Helvidius, were made capital crimes, that not only their persons but their very books were objects of rage, and that the triumvirs were commissioned to burn in the forum, those works of splendid genius" (Tact, Agric, 2) Indeed Tacitus goes even further "They fancied, forsooth, that in that fire the voice of the Roman people, the freedom of the Senate, and the conscience of the human race were perishing," (Tac, Agric, 2) If we are to take what Tacitus is saying here to bear then it can be construed that these acts by emperors such as Caligula, Nero and Domitian were seen as attacks on the Libertas of the Roman people, something which was against Rome's most ancient traditions.
However vitally it was the Princepts of the day that were to blame not the Principate itself (Wiszubski, 1950, 126) after all the entire reason for the establishment of the Principate in the first place by Augustus was, albeit supposedly, to protect the ancient institutions and traditions of Rome.

Whether emperor or not then it can be argued that they sometime simply went too far, and this as will shall see was a good cause for many to oppose such "tyrannical behavior".
The stoic philosophy of this period serves as a background to the reasoning for these conflicts and punishments.

However if we look at Tacitus "life of Agricola" we can see that it was not at this point however just stoicism that was the cause of the problem, but the emperors terrible actions.
In this Tacitus talks of the emperor Domitian putting people to death for "anything trivial...

and slaying Aelius Lamia for joking remarks, which were reflections on him, it is true, but made long before and harmless" (Tact, Agric, 2) Indeed Tacitus' opinion of the emperor when he was only a prince was far from complementary "While not prepared to give his full mind to his official responsibilities, he was already playing the part of emperors son so far as seducing girls and women went." (Tac, Ann, 4.2) When educated senators and deep intellectuals look on this type of behavior it is quite within reason that they should want him out of office, stoic or not.
Within the period of the Republic the notions of moral virtue and self-restraint were held in high-esteem which is why such figures of Cato and Brutus were popular and why such men as Cordus eulogized them in their works, only to be burnt by the emperor Tiberius.

Nevertheless the Participate was still seen as necessary and, apart from a rather futile thought in the interim between Caligula and Claudius, never thought to be brought back.
Therefore considering all this, it is vital that we understand that it was not the politics that motivated the opposition but the morality, (Wiszubski, 1950, 129) it was not the sovereignty these people despised but the conduct of its rulers that drove them into defiance.

Indeed at first glance what looks like a desire for the republicanism of the past is actually an appreciation and admiration of what has been (Wizubski, 1950, 127).
That after all had always been the Roman way, traditions and heroic history were integral to the entire culture of Roman culture, intellectualism and politics.

It is, like so many questions on the empire, a complex one to answer as to why people opposed the emperors.
The answer is as complex and circumstantial but some general truths and trends are prevalent.

The legacy of Cassius, Cato and Brutus was monumental.
Never had assassination gone hand in hand with philosophy in Roman history until the death of Julius Caesar.

Like so much of Roman history once a precedent had been established and passed into a "norm" it was impossible to reverse.
This is not dissimilar to the end of the Republic itself, once the prizes got bigger and the methods more brutal the whole ethos changed.

This particular ethos change coincided with the beginning of the Roman monarchy and as I have illustrated the results were opposition to rulers.
The reasons this essay have described and analyzed here are admittedly only some of the very many that exist; one of the reasons for this is that each case in its way is differing.

The case of Cordus and the case of Helvidius Priscus for example were different in they had different motivations, ideals and opinions based on the particular circumstances and history they had been exposed to.
Cordus primary goal was glorifying the names of Brutus and Cassius as "the true Romans", Prisucs's was in part revenge for his father-in-law's unjust (in is opinion) death and the circumstances to which he was put with Domitian.

They shared the same philosophy, true, but the entire notion of Libertas, which was in many ways the foundation of their beliefs, differed itself as the situation demanded (Wirszubski, 1950, 125).
Patriots of Rome themselves did not, paradoxically maybe on the surface, disagree with the Principate but saw it as a "necessity", it was more the personal problems of each emperor they hated, resented and saw as "wrong", the figures of Cailgula, Nero and Domitian perhaps are the best examples of this problem, and in a way it was their poor rule which caused strong reactions from the astute, competent and learned.

Sparticus, a name now synonymous with justified rebellion, was praised by such men as Karl Marx and Voltaire yet mainly condemned by writers of the time such as Appian and Florus.
Today, through this historical character and the fictitious one of Maximus, popular cinema recreates these gladiators as heroic and inspiring to those who watch them.

The purpose of this essay is to see how both these films present the gladiator as a hero and how this reflects modern views.
The two films in question are Gladiator (2000) and Sparticus (1960) with reference to ancient sources to analyse the significance and differences in comparison with modern concepts.

The secondary source material is generally used to help understand the meaning behind the two films.
It is obvious from reading the ancient texts that gladiators had a very low class in society, on a par with prostitutes.

Florus, a Roman historian who is believed to have lived over a hundred years after Spartacus wrote on the Spartican gladiators "for the soldiers in it were slaves, and the commanders gladiators; the former being persons of the meanest condition, and the latter men of the worst class." (Florus, Epitome 2.
8. 20) Gladiators in the ancient world were even beyond the law, as they were actually seen as below it.

The oath of a gladiator, swore on entering, summarises what they were going to encounter "I will endure to be burned, to be bound, to be beaten, and to be killed by the sword." (Petronius, Satyricon, 117) Their rights were then none existent as was their reputability.
The Romans were, nevertheless, curious of the games, "One of the animals put up a marvelous fight - its feet being disabled by wounds it crawled against the hordes of the enemy on its knees, snatching their shields from them and throwing them into the air, and these as they fell delighted the spectators" (Pliny, Naturalis historia, 7.20) The famous studious naturalist he was, Pliny was as interested as were many of his fellow Romans and could not help but betray a sliver of respect, true they were not revered or deified like Augustus or Caesar but Sparticus himself was no less famous.

The irony here then lay in that they were hated and repulsed yet intriguing and admired at the same time.
In the modern epics Gladiator and Sparticus, the cinematic gladiators are very much different from the ancient opinions.

The characters in both these films are heroes to their audience and by our modern standards.
The Roman patrician class saw liberty as something bestowed upon the elite and the war against the slaves was nothing but an embarrassment to Roman society, something which Florus, as seen earlier, condoned (Wyke, 1997, 35).

The cinematic Sparticus however is fighting for the oppressed slaves, something in the modern zeitgeist that is most respected and admirable.
We see him as a highly moral character that remains self-controlled and is consistently concerned in doing what is right.

For example, he stops the freed gladiators from forcing two Roman aristocrats into fighting in the arena telling them not to become like their masters, for if they do, they will be no better.
The figure of Maximus is also highly moral, he shows deep contempt for his animalistic status as a gladiator and the fact that his exceptional combat skills are now being used for frivolous entertainment.

In one scene Maximus whilst in the arena throws his gladius (sword) up at the "celebrity" box and yells "are you not entertained!
Is this not why you are here!" and spits on the ground in disgust.

He relates to the audience in that we also feel his contempt for the spectators and the barbaric sport he is being forced to participate in.
Their opposition to violence and celebrity is key to their characters as heroes and also the fact that they use it for their notion of "good".

In the case of Maximus it is his killing of Commodus.
In Sparticus it is his war against the immoral Rome.

Indeed it is through this violent struggle against "evil" that we come to sympathise and glorify these two heroes.
Commodus is the personification of "evil" of which Maximus is the rightful, conquering hero.

Although Crassus is the personification in Sparticus, the overall enemy is the institution of Rome; this differs in Gladiator as Maximus says "He will always serve Rome." This is true even when fighting as a gladiator as wishes to achieve the restoration of the Republic and follows Marcus Aurelius' wishes.
However they both die in a Christ-like manner.

Sparticus is crucified along with his followers and Maximus is chained in a similar manner as he waits to fight Commodus, before he is wounded fatally.
Most importantly however they both die for their ideals which are presented to the audience as the embodiment of good.

The heroes In Gladiator and Sparticus reach back to the idea of masculine bravery and goodness that is relevant in both ancient and modern societies (Cyrino and Winkler, 2004, 131).
This is best expressed through their determination in fighting corruption.

Ironically Seneca, speaks of gladiatorial games corrupting those who watch them (Seneca, Epistles 7, "The Gladiatorial Games").
However in Gladiator Maximus uses the games to fight the corrupt emperor Commodus.

His inappropriate and incestuous advances on his sister Lucilla and his immature, selfish ruling of the empire are testimony to his morally corrupt character.
Maximus is the liberator of this corrupt character, freeing both Lucilla from Commodus and the Roman people from the corruption of the Empire by restoring the Republic at the end of the film.

When Maximus dies from his fatal wound he is carried out and Commodus is symbolically left in the arena.
Their places are now metaphorically switched, Maximus has become honoured as a hero and savoir Commodus is left lying in the dirt, insignificant, just like an ancient gladiator was.

The figure of corruption and self-indulgence is Crassus (played by Laurence Olivier), in Sparticus he is shown as similar to Plutarch's description "In the case of Crassus many virtues were deeply obscured by one vice, avarice." (Plutarch, Life of Crassus, 1.2).
Crassus is also bisexual in Sparticus ("I like both snails and oysters") which is alike to that of Commodus' incestuous longing for his sister.

However Sparticus differs to Gladiator as he becomes a Gladiator first before becoming a general.
So as opposed to "The general who became a slave, the slave who became a gladiator, and the gladiator who defied an emperor" it is the opposite.

The significance is that Sparticus is about the reaction to the horror of being a gladiator, a struggle against an evil oppressor, i.e. the Romans.
Sparticus struggles for liberty as a rebel reacting to his treatment when he was a gladiator.

"When a free man dies he loses the pleasure of life, when a slave dies...
the pain...

Death is the only freedom a slave knows." Sparticus poignantly states in the film, suggesting that he'd rather die fighting a freeman than in the arena as a slave.
However Maximus is heroic in the sense that he is forced into becoming a gladiator, a fall from freedom, which he eventually overcomes by killing Commodus in the arena.

In Sparticus and Gladiator both heroes suffer terribly.
Yet they are given choices of an exit, a way out but never take them.

In the ancient world ancient gladiators were admired by many "What even mediocre gladiator ever groans ever alters the expression on his face?
Which one of them acts shamefully, either standing or falling?" (Cicero, Tusculanae disputationes.

2.41) even Cicero, an academic and a politician shows here he had a respect for them.
But when refigured in Gladiator and Sparticus they were more than brave but morale and noble in their actions.

Crassus after defeating Sparticus speaks of how he was a bandit and an outlaw, attempting to destroy all that Rome stood for.
His wife Varinia says "He began all alone, like an animal.

Yet on the day he died thousands upon thousands would have taken his place." Sparticus's character then is portrayed as one who will always do the right thing, even if it is the hardest choice.
This is typified when a rich merchant offers to smuggle him and Varinia out, "go away" he says simply.

Maximus is similarly as strong in his journey, when asked for his loyalty Maximus refuses knowing that he is doing the honourable (and heroic) thing, even if it means going through dangerous, possibly fatal, hardships.
One major difference between the cinematic characters of Maximus and Sparticus and ancient gladiators are that they are not objectified in a sexual manner.

The character of Maximus does go through a period of objectification after his family are killed he renounces his character by not speaking, "My name is gladiator" he tells the emperor turning his back on him, again showing his status as an object.
However even as an object Maximus does not show his body, there is only one scene in which he does this and its purpose is to highlight the servitude and humiliation of being a gladiator.

The scene is raw and brutal, depicting a half naked Maximus being coated in lime and various gladiators being punished and killed, Maximus himself is seen from a camera angle that is unflattering of his body.
Similarly in Sparticus when the trainer paints him to highlight killing, maiming and slow killing "zones" on the body.

The scene is over quickly and the effect on the audience is of disgust, Sparticus himself looks angry and vengeful, indeed it is poignant that his trainer is the one who sparks the rebellion by calling him "Slave", it is at this moment that Sparticus attacks the trainer and begins the revolt marking the end to his objectification.
This is also true in Gladiator when Maximus takes back his identity and proclaims himself to the emperor "My name is Maximus Decimus Meridius, Commander of the armies of the north and general of the Felix Legion, loyal servant to the true emperor Marcus Aurelius" and defies Commodus to the roar of public approval.

Both cinematic gladiators then break free of there object selves as gladiators and become the champions of the causes.
This lack of sexuality is key to heroes characters.

Often in ancient sources gladiators were objectified in this manner, they sparked eroticism and sexual excitement fuelled the imaginations of the Roman people "The rich throw into gladiator schools all the best looking and fittest of men" (Seneca, Controversiae, 10.4.18).
Seneca's description is condoned by the archaeological findings in Pompey "Caladus, the Thracian, makes all the girls sigh." Reads one inscription "Crescens, the net fighter, holds the hearts of all the girls" (C.I.L.

IV, 4397) says more Roman graffiti.
However Maximus remains loyal to his dead wife wishing "Only to see her again, for all else is dust and air".

Sparticus, even though he is in love with Varinia, would not reduce himself to an "animal" by making love to her when she is offered to him by his masters, his refusal of his animal instincts shows his heroic virtue.
This is further highlighted as both Commodus and Crassus are in love with these women and both forcibly try to make them love them unsuccessfully, much to the disgust of the audience.

The figures of Maximus and Sparticus then are not of lovers but of family men.
All Maximus wishes to do is return to his farm, "Dirt washes off better than blood Quintus," he half jokes to his officers.

Returning to his much loved family are his central concern shown by his prayers, in which Maximus kisses two figurines of his family wishing for their safety, the scene is beautiful and poetic, with the slow instrumental music and close ups of Maximus' emotional face.
It allows the audience to connect to Maximus honest and humble desires creating a feeling of righteousness to his actions in the arena as this one simple wish, to love his family, is taken from him.

The character of Sparticus is loving and affectionate to his wife he, in a similar way to Maximus, wishes only to be free with his family.
His excitement at the news of Virinia's pregnancy is testimony to this.

Both cinematic gladiators then show themselves as family orientated.
This is connected with old fashioned masculinity, which shows so much appeal to the audience.

Their being forced to fight as gladiators then is all the more unjustified to the audience and makes them all the more heroic for their honourable motives.
As we have seen the ancients saw the gladiator to be the worst type of slave.

Indeed to be a gladiator was often a punishment if the slave had displeased their masters, committed crimes or they were slaves brought back by wars to be killed in the arena (Kyle, 1998, 91).
In a book by Donald G. Kyle he quotes a text from Josephus saying that at the fall of Jerusalem hundreds of slaves were brought back to Rome by Titus to be put to the sword in the arena.

Indeed many that died in the arena were untrained brigands, vagabonds and highwayman.
The cinematic characters of Maximus and Sparticus however have virtues and nobility far beyond what any Roman could have expected from a "mere gladiator".

Sparticus was everything Crassus was not "Sparticus possessed strength and many other laudable qualities; Crassus on the other hand was despicable in his lust for money and his small mindedness with Pompey." (Urbainczyk, 2004, 102) Crassus then does nothing but scheme and lust for greater wealth and power whereas Sparticus, ironically, embodies all the attributes the Romans would have admired in their time.
Maximus's virtuous and rightful triumph over the emperor Commodus is similarly as poignant and admirable.

These two cinematic gladiators then are more than gladiators, they are heroes.
The ancient Romans had very different ideas on gladiators and slavery, Romans saw gladiators as the worst type of slaves, the modern audiences see them as the worst treated of human beings.

Throughout modern human history slavery is seen as greatly unjust, therefore the use of the medium of a gladiator to pursue high morale goals could not be more poignant, relevant and righteous.
Maximus and Sparticus are simply a representation of this idea and that makes them all the more heroic.

Greenhouse gases are in majority colourless, odourless and transparent gases that have the distinct property of being transparent to incoming, short-wave, solar radiation and yet effectively absorb the outgoing, long-wave, solar radiation.
They may occur in the atmosphere naturally, or they may be a product of human activities.

Perhaps the most widely spoken of greenhouse gas today is carbon dioxide (CO 2) however, the most important role is held by water vapour, a gas whose abundant concentration in the atmosphere is heavily dependent on the climate itself and it is not determined by human activities.
Other, less abundant, greenhouse gases are methane (CH 4), nitrous oxide (NO 2) and chlorofluorocarbons, aerosol propellants more commonly known as CFCs.

Paradoxically, these insulating gases only make up 1% of the Earth's atmospheric content.
In 1859 J. Tyndall realised that the accumulation of these gases in the upper atmosphere, despite their low concentrations, acts as a heat trap.

It results from the overall inhibition of long-wave radiation back into space that makes up the greenhouse effect.
It is a completely natural process, taking place for thousands of years now and without it the average surface temperature of the Earth would be 33C colder.

There is, therefore, a direct link between the Earth's climate and the greenhouse effect.
Greenhouse gas concentrations have maintained fairly constant throughout time but during the past century, the increasing population had higher industrial and agricultural demands, which ultimately disturbed this balance by amplifying the gases' concentrations.

Energy accounts for 80% of all greenhouse gas emission in the EU; it is at the root of climate change and most air pollution.
Should the current energy demand of the world continue at the same rate, it will double the concentration of CO 2 in the atmosphere.

This constant burning of fossil fuels is the driving force behind the rapid yearly build-up of CO 2, estimated to be about 1.5ppm (0.4%) over the past two decades.
Each ton of carbon emitted into the air, results in 3.7tons of carbon dioxide.

The image above, taken from the IPCC report, 2007, clearly shows the rapid increase of CO 2 in the atmosphere.
The estimates for the previous years are made through ice core measurements.

On the next page, temperature deviations from the average global values are plotted against time.
Comparisons of charts like these are what gave cause for investigation into the correlation between greenhouse gases and the changing global temperatures.

The accelerated climate change that the world faces during the next few decades is a direct result of past growth concentrations of carbon dioxide and other greenhouse gases in the atmosphere.
From 1950 to 1973 emissions grew at an extraordinarily steady of 4.5% annually with few annual fluctuations.

Between 1973 and 1983, carbon emission were not steady but on average increased at a rate of 1% annually.
From then on however, a more rapid growth resumed at a yearly rate of 2.8% with the largest increase in 1988, 3.7%.

The Third World is currently burning fossil fuels at much lower rates than the developed world.
However, the combustion of wood, straw and other biomass fuels, which also emits greenhouse gases, is not counted in international statistics.

An additional burden from these countries is deforestation and desertification.
Brazil for instance, is contributing an estimated 336million tons of carbon each year due to deforestation: six times as much as it contributes through fossil fuels.

Carbon dioxide has the largest contribution to global warming.
However, methane has over 20 times the effect of CO 2 and nitrous oxide has nearly 300 times the effect.

Their concentrations are mostly affected by the agriculture.
Today, the estimated concentration of methane in the atmosphere is 1774ppb, a dramatic increase from its pre-industrial value of 715ppb.

Nitrous oxide has been at a mostly constant concentration levels since the 1980s of 319ppb, but like methane, it has also dramatically increase since pre-industrial values of about 270ppb.
Models suggest that the major grain growing regions of North America and central China will become substantially hotter and drier.

Less rain is likely to fall in these areas, winter snow packs will be smaller and remaining moisture will evaporate quickly as a result of more intense summer heat.
A study at Utah State University suggests that the cropping area in the U.S.

Great Plains could decline by as much as a third.
Although climate will warm in currently cold areas, such as Siberia and Canada, the agricultural damage done in the mid-latitudes cannot be offset by this due to the bad quality of the soil in these regions.

During 1965 and 1987, the extent of warming was lower in the northern hemisphere than in the southern.
Climate research models explain this phenomenon with the cooling effect of aerosols, present in the northern hemisphere.

In the subsequent period of 1987 until 1996, the northern hemisphere warmed up more rapidly, presumably as a result of an increase in greenhouse gas emissions.
A noticeable increase in temperatures in the northern hemisphere is during winter nights.

The regional impact of climate change has, until recently, been modest.
Atmospheric models imply that high and mid-latitudes are the areas that will be most heavily affected, experiencing a warming greater than the global average.

Tropical regions on the other hand will be experiencing the least effect.
The warming in the polar regions, together with the potentially doubling CO 2 concentrations, shall be two or three times the average global warming and will amount to about 6-9C.

By 1940 the majority of scientists still thought the rising temperature was just an observed cyclical event.
G.S. Calendar had suggested that CO2 gas concentrations changed the height of the atmosphere.

Although he was no able at the time to prove himself right, he did set the stage for further enquiries.
In 1956 the physicist Gilbert N. Plass gave out a warning that global warming through increasing CO 2 levels could cause serious damage for future generations.

Nevertheless, up to a few decades ago there was considerable uncertainty surrounding a statistical estimate of anthropogenic global warming, IPCC scientists argue today that Global climate change is very likely to have a human cause.
Today's observations have closely matched 2001's model projections that were made.

For the end of this century, the projections predict a rise in temperature in the range of 1.1C to 6.4C Throughout the second half of the nineteenth century, there has been a considerable decline in the overall glacier volume in almost all mountains.
The recession rate has been most rapid between 1920 and 1960.

Oerlemans concluded that temperature changes were the only common factor between the complex relationships of glaciers and climate.
A sea level rise of 28-45cm is anticipated by the IPCC.

As the water in the oceans warms, it will expand.
Furthermore, the melting glaciers will add to the oceans' volume.

This has a rebound consequence on agriculture, mainly in Third World countries which are heavily dependant on harvests that are now at risk from the menace of rising sea levels.
Many arid and semi-arid regions are likely to confront water shortages as global temperatures increase.

Winter snowfalls will decline and spring runoff will come earlier, thus reducing the amount of runoff that can be trapped in aquifers and reservoirs.
People would have perhaps been able to deal with the changes in the atmosphere; however, it is the rate at which these changes have been taking place that has been alarming.

World agriculture is particularly vulnerable to the effects of climate change.
This will also have the negative effect of rising food prices, thus jeopardising millions of lives.

Perhaps the most disturbing effect of global warming is that the damage already done is irreversible.
No matter what happens to future greenhouse gas concentrations, we will always be committed to an additional warming as the climate system tends toward equilibrium.

The graphs above, taken from the IPCC 2007, Fourth Report, show observed changes in (a) global average surface temperatures, (b) global average sea level rise from tide gauge (blue) and satellite (red) data and (c) Northern Hemisphere snow cover for March-April.
It is the purpose of this essay to assess the dialogue between Okin and Rawls concerning the application of principles of justice to within the family unit.

I will start by looking at Rawls' conception of social justice and Okin's critical response of its failure to address gender inequalities.
I will go on to highlight the tensions between both arguments concerning the private/public dichotomy and the ways in which they deal with this distinction, concluding that it is this very divide which both arguments fail to account for in their conception of justice.

Rawls' primary focus within his 'Theory of Justice' is, through the use of a thought experiment, to arrive at principles of justice which are applicable to the basic structures of society.
Rawls identifies the basic structure of society as the way in which "the major social institutions distribute fundamental rights and duties and determine the division of advantages from social cooperation."[1] Within the text, Rawls explicitly identifies 3 examples of what he considers to be major institutions, one of which is the 'monogamous family'.

The application of his principles of justice to the family has caused much controversy within contemporary political debate, particularly with respect to Okin's views on the injustice's caused by the gender distinction, and the way in which society's view of the traditional family unit can perpetuate this injustice.
Susan Moller Okin is a liberal feminist philosopher who has concentrated much of her writing on the issue of gender in politics.

In 'The Family: Gender and Justice', Okin presents a liberal feminist critique of the gender structure in society.
She highlights her belief that the biological differences in sex have been used to legitimise a society filled with discriminative values.

Although mostly recognised as equal under the law, women everywhere suffer continuing injustices and inequalities as a result of tradition and socialization.
The primary objective of her text is to highlight the need for the forced removal of gender inequalities, which can be achieved through the eradication of gender differentiation.

Rawls' two principles of justice consist of the basic liberty principle and the second principle which consists of the difference principle along with fair equality of opportunity.
In terms of political rights it is clear that the basic liberty principle is the most relevant.

Rawls gives a list of the liberties with which he holds the basic liberty principle to concern; "Important among these [basic liberties] are political liberty...
freedom of speech and assembly; liberty of conscience and freedom of thought; freedom of the person..." [2] Under Rawls formulation of the basic structure of society, these basic liberties are to be applied to all the major institutions, including the family.

However, in his 'Theory of Justice', Rawls hints at the idea that it is the heads of traditional monogamous families, rather than the individuals within families (be they traditional or not), that are most likely to consent to these two principles of justice.
He generally assumes that the internal structure of the family is, by its nature, just.

For this reason, contemporary philosophers, in particular Okin, have taken issue with Rawls' original conception of justice.
Okin believes that, given Rawls' conception of the basic structure includes families, it makes no sense to deny the application of his two principles to within the family.

To an extent Okin is sympathetic to Rawls' liberal approach, but believes that a feminist approach is necessary in order to fully secure justice.
The application of a similar theory to that of Rawls to within the family will secure a more just society.

She believes that, like Rawls, the family is of major importance in the securing of a just society.
The way in which children are brought up is of fundamental importance to their moral development.

As a result, Okin sees it as necessary to ensure that children grow up in situations which are as just as possible, therefore it is necessary to remove gender inequalities.
She argues that due to the current gender structure in society, women are forced to accept marital conditions which severely restricts their freedom of opportunity.

The nature of traditional family values pressures women to be the primary care givers within the family, whilst the men are the primary bread winners.
As a result of this, women are likely to end up financially dependent on their husbands, and essentially being severely restricted in terms of future options.

This, Okin argues, is a result of the deeply ingrained gender divide that is so prevalent in society, and due to the ways in which children greatly rely on their parents guidance in their upbringing, is likely to remain prevalent until something is done in order to remove these inequalities.
In 'Justice as Fairness - A restatement', Rawls adapts his views of the family in order to accommodate for Okin's criticism.

He redefines the scope of his use of the term family so it no longer only includes the traditional family; "essential to the role of the family is the arrangement in a reasonable and effective way of the raising and caring for children, ensuring their moral development and education in to the wider culture."[3] He goes on to note that this function can be equally fulfilled by families of any structure.
However, Rawls maintains that justice does not need to be applied to within the family structure.

In the same way in which his principles of justice do not apply to the church due to the free and voluntary participation of its members, they only apply to the family to the extent that families are part of the basic structure; "political principles do not apply directly to its internal life but they do impose essential constraints on the family as an institution"[4] Rawls justifies this approach to justice within the family though appeal to the public/private dichotomy.
This distinction has an important effect on the nature of justice and its influence on the family unit.

In his 'Contemporary Political Philosophy - an introduction', Kymlicka highlights a distinction between two types of the public/private divide.
The traditional version of the divide centres on the division between state and civil society.

The state is analogous to the public, in which it is the domain of political activity.
Civil society is analogous to the private, which consists of the relations amongst individuals outside of the political sphere.

This approach faced feminist criticism due to the fact that it failed to embrace female participation in either the public or the private.
Women were placed in a completely separate sphere, that of domesticity.

A later approach to the divide attempted to address this issue and others by highlighting the right to privacy, this distinction between the public and the private equated the public with the social and the private with the personal.
Any activity outside of the family is considered to be in the public domain, whereas any activity within the family is considered to be private.

Okin's analysis of the gender structures of society clearly requires that principles of justice need to be applicable to within the family.
However, she also holds that there are basic liberties which need to be protected.

If two consenting adults willingly and freely agree to live their family life in the traditional style, then Okin believes that it is unreasonable to force them to otherwise.
When discussing the nature of the original position, and the ways in which it can be used to ensure the eradication of gender inequalities, she also notes the importance of ensuring that free and rational choice is not hindered; "There are those, at one extreme, for whom the different roles of the two sexes, especially as parents, are deeply held tenets of religious belief...

Public policies must respect people's views and choices."[5] It seems that here Okin is submitting to elements of Rawls' argument.
In attempting to apply principles of justice to the family, you must at the same time respect the ways in which people choose to live their lives.

Similarly, Rawls recognised that it is important to ensure that there is justice within the family, but the measures that are taken to enforce this justice cannot go against the protection that his basic liberty principle provides.
Okin's analysis of gender structures in society has successfully highlighted injustices that lie within the family structure.

Given the effect that parents have on their children's moral upbringing it is likely that these injustices will remain within society for as long as the gender inequalities which Okin highlighted are still prevalent in society.
Rawls' equal political rights can, to an extent, be applied to these injustices.

However, both Okin and Rawls recognise the importance of respecting peoples free choice in such matters.
Where Okin wants to see equal political rights applied internally to the family, Rawls maintains that this would intrude into the private domain.

But if we are to fully remove the gender inequalities then it will be necessary to intrude into the family life and force the eradication of gender distinction.
But even Okin maintains that this is unacceptable.

So it seems that equal political rights are not an adequate response to Okin's gender analysis.
Okin's gender analysis requires state intervention in ways which are unacceptable according to Rawls' conception of justice, and to an extent her own conception of justice.

It seems to me that the eradication of gender within society requires more affirmative action than just the equal distribution of political rights, to an extent which will intrude on our most basic liberties.
The nature of the mind and its relation to the body (or matter in general) is a subject which bares witness to much debate.

Since the writings of early philosophers such as Socrates and Plato it had been suggested that there are elements of the mind or soul which possess certain characteristics which are not shared with the extended world.
This has led to much disagreement over the subject and given rise to many competing theories, each contending with its own problems.

One such popular theory is that of Substance Dualism.
The fundamental claim of the Dualist is that the mind and matter are two separate substances that are intimately related.

This notion arises out of various claims about the different properties which can be attributed to each substance.
For instance, it is commonly accepted that matter is extended.

It occupies a place in space.
Dualists claim that as the mind is not extended, there is no place in space in which the mind can be located, it must be a different substance to that of matter.

Dualists also point out differences in the epistemological nature of the mind compared to those of the material, as well as differentiating between the qualitative aspects of the mind and matter.
Dualism has encountered various problems and criticisms in its development, which has led to many competing versions.

This essay shall focus on one particular version; Cartesian Dualism, which is considered by many to be the most influential form of substance dualism.
Rene Descartes (1596-1650) dualism is most clearly argued for in his 'Meditations on First Philosophy'.

Within the text he argues from several different initial premises in order to clearly illustrate his position.
He distinguishes between what he holds as two different substances, the extended material world is made up of one substance, extended matter, and the mind is made up of a different substance, thought.

Descartes goes on to argue that each of these substances mutually exclude one another.
There is no object which can hold extension as an attribute whilst also holding thought as an attribute, therefore minds are necessarily distinct from bodies and other material objects.

He begins his argument for dualism using what is commonly coined his 'method of doubt', in which he calls into question the existence of everything which he has previously taken for granted.
The result of this doubt leads him to dismiss the existence of his body and the external world using the idea of an evil deceiver.

It appears to Descartes that there is only one thing which he can attribute necessary existence to; "At last I have discovered it - thought; this alone is inseparable from me.
I am, I exist - that is certain." [1] Descartes maintains that it is impossible to call into doubt the existence of his thought, and it must therefore exist necessarily.

The existence of the body, and extended matter in general can however be called into doubt.
His conclusion is that, as it is logically possible for his mind to exist independently of the body, it follows that it is possible for the body to exist independently of the mind.

Therefore the mind and the body are entirely distinct.
This argument from doubt has been the subject of strong criticism.

Most notably from a contemporary of Descartes; Antoine Arnauld.
Arnauld pointed out that Descartes reasoning is invalid.

He uses the example of a particular man's understanding of a triangle, and illustrates that just because this man is able to call into doubt a particular property that this triangle may hold, does not entail that this property is non essential.
The conclusion drawn from this example is clearly drawn out by J. Cottingham; "I may be able to doubt that I have a body, I may think I can imagine myself still existing without a body, but I cannot validly infer from this that having a body is no part of my essential nature." [2] This points to an important distinction between logical possibilities and empirical possibilities.

Although Descartes has identified a logical possibility, the conclusion he has drawn does not represent, as he has shown it, an empirical necessity.
Descartes responds to this criticism by denying a similarity in the example, but accepting the point intended.

The distinction between the attributes of mind and body is further highlighted by investigating the 'oneness' of the mind as compared to the divisibility of the body.
This also is an argument directly forwarded by Descartes; "...there is a great difference between the mind and the body inasmuch as the body is by its very nature always divisible, while the mind is utterly indivisible." [3] Once again this argument is aimed at demonstrating the different properties which mind and body possess in order to enforce the dualist stance.

Many contemporary philosophers have dismissed this argument on the grounds that it no longer stands true in the light of modern psychology.
It is commonly accepted that the mind is made up of a variety of faculties; the understanding, the will, the sensory faculties etc.

However, Descartes denies that just because it is possible to identify different functions of the mind, it is also possible to divide the mind into parts; "...these cannot be termed parts of the mind, since it is one and the same mind that wills, and understands and has sensory perceptions." [4] Here Descartes seems to be implying that, although the mind is capable of different functions, it is always the same indivisible mind which performs and is the conscious subject of these activities.
Although the concept of a simple, unified and singular 'I' has a certain intuitive appeal, many scholars have taken issue with it.

Possibly the most devastating comes from the studying of empirical psychological phenomena.
Experiments using brain bisection operations have shown some interesting results which seem to contradict the dualists claim that the mind is not divisible; "Under certain laboratory conditions, two 'centers of consciousness' seem to appear in patients who have had this operation"[5] The results of procedures such as this have led philosophers such as Nagal (1971) to conclude that the concept of a unified consciousness is misleading, it is more probable that thought processes are in actual fact a result of "complex interactions of several distinct subsystems" [6].

However, arguments from a physiological perspective have so far failed to prove that, although there seems to be some form of external empirically verifiable division in the consciousness, the patients awareness of oneself as unified has been damaged in any way.
Even if we are to accept the Cartesian notion of an indivisible mind, it still fails to account for the claim that mind and body are completely independent.

If we are to argue from a monist perspective we could easily claim that, although consciousness is indivisible, it is still coherent to claim that it is an indivisible property of an extended material substance.
The indivisibility argument thus fails to prove that the mind is a separate substance from that of the body.

However successful the dualists attempts are at responding to the criticisms that monists throw at the Cartesian model, there is one argument against dualism which has received the majority of attention.
If we are to accept that the mind and body are truly distinct substances, then a big issue arises when we attempt to address the problem of interaction.

It is commonly accepted that the extended material world is causally closed.
Every event which occurs is caused by and can cause some other event within this causally closed universe.

The problem comes when trying to establish how the mind, as an immaterial substance, can causally interact with the extended world.
It is clear in the execution of an action which began as a simple thought, it is possible for me to interact with this causally closed off world.

This seems to directly contradict the entire thesis which the dualist is trying to present.
In order to address this question the dualist either has to concede to the monist, or try to argue that the mind and body don't in fact interact.

In questioning what the strongest argument for substance dualism is, the only response can be to ask what the strongest criticism is.
The problem of interaction has stifled all dualists, and never has there been a response which sufficiently addresses this issue.

But despite this, it seems there remains a strong element of intuitive appeal.
The initial analysis of the various attributes which mind and body possess does seem to lead to the conclusion that they are distinct substances and, despite the fact that Descartes rational approach to understanding the mind often brings about conclusions which aren't necessarily entailed by the premises, it is still possible that these conclusions are valid.

By questioning the various properties of the mind as compared to those of the material world, Descartes has highlighted an important distinction.
It follows that the strongest argument for substance dualism lies in our own intuitions about the way in which the mind differs from the body; the very fact that we can question what the essence of the mind is and then compare it to the material world illustrates that there are some fundamental questions about our very nature which need to be addressed.

In general terms, the study of morality concerns an enquiry into how we ought to conduct our lives in terms of right and wrong actions.
This is the domain of normative ethics.

Distinct from normative ethics, the field of meta-ethics concerns the nature and grounding of our moral belief systems, and essentially attempts to define our notion of morality by appealing to beliefs and attitudes.
One such much debated meta-ethical idea is that of error-theory.

Error theory can be directly contrasted with non-naturalist moral realism.
Both these meta-ethical theories hold that when we make a moral judgement, we are expressing a belief about non-natural properties.

The difference between the two lies in the fact that realists hold that these non-natural properties exist, whereas error theorists hold that they don't.
Before we get into a critical discussion it is helpful to understand a little about 'beliefs' and 'non-natural properties'.

A belief is an attempt at representing the world as it actually is, thus we can either hold true beliefs or false beliefs.
A non-natural property is a property of the world that cannot be discovered through the usual method one employs in order to discover a natural property, namely through scientific methods.

However these properties are claimed (by the realist) to still be very much part of the world.
The error theorist, contrary to the non-naturalist realist, claims that these properties which all moral judgements are based on don't actually exist.

This has been a very controversial claim to make, as it essentially states that any moral judgement is false.
For this reason, error theory has recently been the subject of heated debate, and its legitimacy has been very much at stake.

J.L Mackie in his 'Ethics: Inventing Right and Wrong' forcefully argues for moral judgements as beliefs about non-natural properties, but maintains that there are in fact no non natural properties for our beliefs to correspond to; "there do not exist...objective values or requirements, which many people have believed to exist"[1] Thus he concludes all our moral judgements must be false.
He explains the non existence of these properties using two arguments; the argument from relativity and the argument from queerness.

The argument from relativity appeals to the varying beliefs that different cultures and societies hold in terms of what is right and wrong; "radical differences between...moral judgements make it difficult to treat those judgements as apprehensions of objective truths."[2] Every individual tends to hold their moral judgements as necessarily true, thus appealing to some kind of objective morality, but if this were truly the case, we must surely claim that any contrasting moral judgement is false, but this could lead to ruling entire cultures as immoral, which seems completely counter intuitive.
Therefore, Mackie argues, the non-natural properties which we appeal to must in fact not exist.

The argument from queerness states that these non-natural properties are unlike any other property there is.
They have an odd action guiding force driving them.

If we are to accept that these properties exist then we need a special way of discovering what these properties are.
Mackie basically argues that this is untenable.

On face value, Mackie's argument seems perfectly reasonable, but it has aroused much attention from numerous philosophers, the reason being that its basic claim is that all moral judgements are false.
If this is the case, then it follows that there is no purpose in making any moral judgements or even holding any moral beliefs.

This is clearly not a state of affairs that we want to willingly accept.
Mackie identified this problem and attempted to address it whilst maintaining an error theorist stance.

A result of which was 'moderate' error theory.
Mackie claims that although all moral judgements are false, we can still distinguish between justifiable and unjustifiable moral beliefs by appealing to a subsidiary norm.

The example he gives is that of social cooperation.
For instance, the judgement that we ought to respect others rights to privacy is wrong in terms of there being a non-natural property which it corresponds to, but is justified in its relation to the subsidiary norm of securing social cooperation.

Thus we can still be reassured of the benefit by our making moral judgements.
Crispin Wright in his 'Truth in Ethics' believes that Mackie's moderate error theory is fundamentally flawed.

He accepts that there is a strong need to avoid radical error theory, but believes that the appeal to the subsidiary norm gives way to a fundamental instability.
Wright argues that the subsidiary norm effectively forces error theory into redundancy.

The subsidiary norm can replace the need for any non-natural properties in our making of moral judgements.
He introduces the notion of 'superassertibility' ; "a statement is superassertible if it is assertible in some state of information and then remains so no matter how that state of information is enlarged upon or improved." [3] Wright holds that superassertibility is a truth determinate in any form of discourse, a superassertible statement will be true in relation to the norm that a particular discourse provides.

If we can apply superassertibility to a subsidiary norm we can still talk of some moral judgements as being true or false.
If a moral judgement is superassertible then no matter what additional circumstances or information arise, it is going to remain the same, thus it can be called true, and error theory is avoided.

However, Alexander Miller believes that Wrights argument itself has some fundamental instabilities.
Miller argues that such an indirect attack on moderate error theory will only lead to radical error theory, which is exactly what Wright wants to avoid.

The appeal to using the subsidiary norm as a measure of truth and falsehood in our moral judgements is open to two main attacks; "The subsidiary norm will either be an irreducibly moral normor will be explicable in purely non-moral terms"[4] Miller argues that in order to apply a truth value to the subsidiary norm we will either have to be able to reduce it to a moral judgement or apply a reductive analysis which will explain it in purely non-moral terms.
If it is the case that the subsidiary norm is 'irreducibly moral' then it can in no way be used as an argument against the error theory, as the error theory has already dismissed the possibility of truth in positive moral judgements.

It seems to have entered itself inside a vicious circle.
On the other hand Miller recognises that Wright himself has dismissed the possibility of the subsidiary norm as reducible to non moral terminology, in which case we have to accept it as a moral judgement which, after the application of the error theory, is false.

Miller recognises that Wright might reply using moral judgements as superassertible.
But dismisses this on the same grounds as he dismissed the truth bearing weight of the subsidiary norm.

If we use superassertibility to determine truth in moral statements, then the truth will either be based on the moral weight of the statement, or will be reducible to non moral terms (which as noted before, Wright rejects).
Thus the error theory can once again be applied, and superassertible judgements will all come out as false.

Miller recognises a failure in the approach of Wrights argument which results in an unsuccessful refutation of the error theory.
The indirect nature of the argument only works as a dismissive of moderate error theory, but leaves radical error theory unscathed which is exactly what the majority of moral theorists, including Mackie, are trying to avoid.

Wrights indirect attack is aimed at a surplus principle that has been introduced to error theory in order to save it from the extreme of radical error theory.
Wrights insistence on the truth that this surplus principle provides in moral judgements immediately leads to its necessary falsity as a result of the principles of radical error theory; namely that all moral judgements are false.

Any response to the error theory, Miller argues, must be aimed directly at it.
In order to avoid a radical error theory, we must come up with an argument which dismisses the claim that moral judgements are about beliefs in non existing non natural properties, but Wrights argument does not succeed.

So we are now left with Mackie's error theory, but it seems that not all is well here either.
Miller's attack on any form of indirect argument against error theory has meant that we can not accept Mackie's justification for moral judgements.

Contrary to Wright's claim that we "ought...to forgo making any [moral] claims"[5], Mackie believes we can still be an error theorist and be able to justify our moral beliefs without appealing to truth.
But Miller's argument clearly states otherwise.

If we are to be an error theorist, any justification of a moral judgment will be reducible to some moral norm, which will be false under the error theory.
Obviously it is in no way possible to be an error theorist and maintain that not all moral judgements are false, as that would clearly go against the main tenet of error theory, but Mackie had provided some hope that it was possible to still be justified in our making of moral judgements despite their falsity.

However, it seems that Wright's attempt at saving us completely from the falsity of our moral beliefs has lead us only into further trouble; we can now not escape radical error theory through any form of indirect argument; "any indirect reply to the moderate error theory will succeed only at the expense of delivering us into the hands of the radical error theorist"[6] We either have to attack its premises directly or, despite the claims of many commentators of contemporary meta-ethics, find a strong argument to support a subsidiary norm which can be reducible to strictly non-moral terms.
Otherwise it seems the claim that moral judgements are meaningless holds true.

In his 'Contemporary Metaphysics', Michael Jubien identifies properties in the Platonic sense as "entities that exist apart from and independently of the things that have them"[1] Plato's belief that properties are universal stems from a long line of enquiry into the nature of justice, and his perception of the ideal state.
Through question and answer sessions with the people he surrounds himself with, he slowly unravels his 'Theory of Forms' Although Plato has never explicitly stated this theory, many contemporary commentators recognise it as an intrinsic part of Plato's philosophies.

Throughout his many texts are references to 'things-in-themselves', or 'ideas', which are used frequently to portray his beliefs about justice and other abstract entities.
But because of the nature in which Plato reveals his Forms, commentators have failed to agree on his argument for their existence, some maintaining that he gives none at all.

If this is right, then what reason do we have to adopt Plato's theory?
In order to understand Plato's reasons for his theory of forms, we must first understand a little about the nature of properties as universals.

Most Platonists believe their to be an innumerable amount of properties.
Such properties can range from the colour of a particular object to the location it has at a particular time.

For instance, a pen might hold the property of having been used at a certain time on a certain date, by a certain person.
It can also hold the property of redness.

When we say that these properties are universal, the claim we are making is that the pen does not cause or explain these properties, but instead they exist independently of the pen, and the pen 'instantiates' them.
Therefore to claim that 2 different items share the same property, is to claim that they both instantiate the same property.

The redness which they instantiate, is one and the same.
The main body of Plato's 'Republic' is concerned with finding the nature of justice, but in order to do so, digressions are made to illustrate his arguments.

In book V we see Socrates (the mouth piece of Plato) questioned about the practicality of his ideal state.
This is where the 'Theory of Forms' first appears.

In order to demonstrate the possibility of his ideal state, Plato appeals to the idea of 'the philosopher ruler'.
The most important aspect of which, is his ability to grasp 'Knowledge'.

Plato distinguishes between knowledge, opinion and ignorance, each corresponding to different levels of 'being'.
Knowledge corresponds to 'what is', ignorance to 'what is not' and opinion lies between the two.

He uses a useful illustration in order to express this view clearly.
He depicts a cave in which prisoners are tied down and forced to look at a screen which is showing shadows of puppets.

The prisoners, having no reason to think otherwise, believe that these shadows are how the world actually is.
But when a prisoner is released, he encounters a further world in which truth lies.

Having grown able to accustom himself with the light of the real world, the prisoner is eventually able to look directly at the sunlight, which Plato frequently uses as analogous to the highest form of them all; the form of the good.
It is the aim of the philosopher ruler to follow in the route of the prisoner by escaping the trappings of the sensible world and striving to achieve knowledge in its true sense, that is, knowledge of the intelligible realm.

This realm consists of timeless and unchanging properties of objects and entities which represent things in the material world in their purest form, an action that is considered just, partakes in the pure form of justice.
These are universal properties.

Plato claims that the majority of people can only hold opinions, which are changing beliefs that hold no intrinsic value, and are only approximations to the world as it is.
This is the world of the senses, the material world.

He illustrates his point by distinguishing between the lover of knowledge and the lover of spectacles.
The lover of knowledge is someone who, rather that appreciating, for example, a particular beautiful object, instead aims to understand beauty-in-itself; the form of beauty.

Rather than arguing for the existence of forms, Plato focuses his attention on arguing against the possibility of knowledge of particulars.
He claims that knowledge is only possible of things that remain unchanged, and given that particulars are in a constant state of flux, it cannot be possible to have knowledge of them.

Any particular can instantiate a property at any time; A car can instantiate the property of speed, a sculpture can instantiate the property of beauty, 2 apples instantiate the property of double one apple.
But none of these particulars instantiate these properties necessarily; the car can be crushed, the sculpture may appear to one as beautiful but ugly to another, and the property of double depends on what we are relating the two apples to.

We can not claim to 'know' that the car is fast, or the sculpture is beautiful, or that two apples are always double, because in the sensible world, this is liable to change.
Plato argues that the Forms are these properties in themselves, as they exist independently of the objects which instantiate them.

The philosopher ruler will have knowledge of these universal properties, and be able to apply this knowledge appropriately to the governance of the ideal state.
Although Plato claims that we need universal properties in order to attain knowledge, within the 'Republic' the Forms are very much taken for granted.

He provides analogies expanding on and illustrating the idea, but at no point does he provide a proof of their existence or a single reason for why they should exist.
This can be obviously seen in Julia Annas' writings; "he nowhere in the dialogues has an extended discussion of Forms in which he pulls together the different lines of thought about them and tries to assess the needs they meet and whether they succeed in meeting them"[2] In order to successfully assess the reasons we have for accepting properties as universals we need to look at modern Platonist views, in particular realism.

Metaphysical Realists have built on the foundations that Plato laid in order to generate further understanding of subject-predicate relationships.
The idea of properties as universals helps us to understand what it is to say that two separate things can be qualitatively similar or identical.

If we make a claim about a car being red, and then later express a separate claim about a house being red, by accepting properties as universal, we can admit that these two separate particulars have something in common, namely; the instantiation of the property of redness.
If we deny properties as universal, then it seems we must also deny that the house and the car can share the same property.

Nominalists argue contrary to realists in claiming that there are in fact no such entities as properties in the first place.
They start by asserting that the role of properties as universals in Platonic terms is to act as an intermediary between the world as we see it and the 'world-in-itself'.

But go on to claim that we do not need an intermediary, and are in fact connected to the real world directly.
If we say that a house is red, it is the utterance of this sentence, and no abstract entity, that bears truth.

By saying a house is red, we are announcing that the subject 'house' satisfies the predicate red.
However, this theory has been criticised on its failure to account for how a sentence in one language can mean the same in another language.

In answer to the original question, it seems that we have plenty of reason to agree with the doctrine of properties as universals.
By denying its plausibility we are also rejecting a coherent theory of basic communication.

Everyday use of language commits us to properties and propositions.
Although Plato has never explicitly stated these reasons for believing in universals, he has definitely done the groundwork for later philosophers to build on.

Realism has shown us that, when discussing separate entities with the same properties, we can clearly understand the relation that these separate entities can hold to each other.
Thus providing us with a good reason to ascribe to Plato's theory.

As Michael Jubien explains; "The fundamental metaphysical reason for believing in the existence of properties is that they provide [an] account of similarity and difference"[3] This account of similarity and difference is clearly attributable to Plato and the Platonists that followed.
It seems odd to analyse a concept which is so thoroughly engrained in common language such as that of knowledge.

However, since the early writings of Plato philosophers have questioned what it is to know something.
Distinctions have been made between capacity knowledge, knowledge by acquaintance and propositional knowledge, each with various difficulties associated with them.

The most fundamental debate surrounds the concept of propositional knowledge; what it means to say we know a particular proposition.
One such response has been to claim that knowledge is justified true belief.

This is known either as the tripartite definition or more generally, the standard analysis.
In attempting to define a concept like knowledge, theorists are looking for a set of conditions which cover all aspects of how we use that concept.

These conditions must be necessary and sufficient; each condition is essential to the concept and the combination of these conditions is enough to satisfy the demands of the concept.
In the case of knowledge as justified true belief, theorists have identified 3 conditions that are said to successfully fill the necessary and sufficient conditions of knowledge.

Predictably, these conditions are truth, belief and justification.
The truth condition is pretty much universally accepted.

Its basic premise is that it is impossible to claim to know something which is false.
If I claim to know that the earth is in fact square, whilst maintaining that it is true that the earth is round, then my claim to knowledge seems somewhat oxymoronic; I am claiming to have false knowledge.

The real controversy that surrounds the truth condition lies in our understanding of the concept of truth.
Although this is in itself a difficult subject to tackle.

The belief condition, although generally regarded as necessary, has more controversy surrounding it than does the truth condition.
The inclusion of the belief condition boils down to the requirement of a psychological connection to an agents claim to knowledge.

Broadly speaking, it is impossible to know something which you do not believe of have no belief about.
This condition acts to exclude the agent from knowing something which they are at the same time ignorant about.

However, although the belief condition is mostly accepted, some philosophers have doubted its necessity; "...we can accept, or assent to, a known proposition without actually believing it."[1] The underlying idea is that it is possible to accept some proposition without acquiring the tendency to accept this proposition in certain circumstances.
In accepting some proposition, an agent is said to know this proposition, but does not necessarily hold the relevant 'dispositional psychological state' in order for it to count as knowledge.

Whether or not this view holds as valid, it is important that the concept of knowledge has as a condition some form of relation between that object and the subject.
In the standard analysis, this condition is belief.

The third and final condition for the tripartite definition of knowledge is the justification condition.
It's apparent through the use of thought experiments that true belief would not suffice in a fully comprehensive account of knowledge.

By saying that knowledge is a belief that is true, there remains the possibility of accidental true beliefs.
For instance; if an agent believes that he will win the lottery next time he plays, and against all probability, he does, it cannot be said that he knew he would win the lottery.

The justification condition means that lucky guesswork does not count as knowledge.
However, some philosophers have discounted the need for justification in an account of knowledge; "Belief is a dispositional state, implying a certain degree of stability"[2] The suggestion is that the holding of a belief implies some form of justification which has led to an agents arrival at the belief.

Therefore the justification condition is surplus to requirement.
This line of thought has an intuitive element to it; it makes sense to say that the an agent has a reason for believing in some proposition, and that this reason is itself justification for the agents holding that belief.

However it does not capture fully what is trying to be said in adding the justification condition to the standard analysis.
Although the justification condition is used in order to clarify our concept of knowledge, it in turn is a concept which needs much clarification.

The traditionally held view is that justification requires a form of evidence to secure the truth of a proposition.
However, as we have seen, some consider belief to contain an element of justification, but this does not satisfy our intuitions about knowledge.

As a result, philosophers have distinguished between internal and external justification.
In holding a belief, it is said that there are two ways in which that belief can be justified; "Sometimes we focus on the person's entitlement to hold a certain view.

But sometimes we are interested in whether the grounds on the basis of which he holds it are objectively adequate..."[3] Internal justification focuses on the responsibility of the agent in assessing the reliability of the ways in which he formed his belief.
External justification instead looks at the reliability of the evidence which provided the information and lead to the formation of a belief.

The identification of this division in the condition helps to explain the notion that an element of justification is contained within belief.
The claim here is that, although almost all beliefs are in some sense justified, those which are only internally justified do not meet the conditions which are required to constitute knowledge.

It is possible for an agent to internally justify a belief using completely unreliable methods.
However, in order to claim knowledge, the agent must also hold some form of external justification.

As a result, theorists have identified an extended version of the standard analysis.
In this version, two criterion need to be fulfilled to account for justification; An agent must be personally justified in believing some proposition and the agent must believe the proposition on the basis of adequate grounds.

[4] Although clarifying the nature of the justification condition, it is still difficult to judge how strongly we should interpret the condition.
When faced with a sceptical argument, it will be impossible to justify any belief that an agent might hold.

Platonic knowledge assures us that, in a state of knowing, there will be no choice other than to accept that you know, but no amount of empirical evidence will lead to this knowledge, it can only be acquired through the 'forms'.
These examples of the problems surrounding what we can know have many implications for the standard analysis.

However there is a problem which has received much more attention from proponents of the tripartite definition; it comes in the form of an ingenious paper produced by Edmund Gettier (1963).
In a three page paper, Gettier provided a counter example which directly attacked the sufficiency of the three conditions which make up knowledge.

Gettier provided an example in which an agent, Smith, has strong evidence to suggest that Jones will get a job which they have both applied for.
Having seen Jones count some money in his pocket, Smith comes up with the proposition that the man who will get the job has 10 coins in his pocket.

As it turns out, Smith's reliable source of information has failed him and it is in fact Smith who has got the job.
However, as it happens Smith also has 10 coins in his pocket.

Thus the proposition 'the man who will get the job has 10 coins in his pocket' is true.
Here Gettier has provided an example of a man who holds a true belief, which is justified, but does not seem to correspond with our intuitions about knowledge; to claim that Smith knew that the man who would get the job has 10 coins in his pocket seems absurd.

However, as much as the Gettier counter example seems to illustrate a fundamental flaw with the tripartite definition of knowledge, it seems to me that it would be easy to counter the objection.
If we accept that the justification for the proposition was not strong enough for Smith to claim knowledge, then it seems that the counter example fails.

The problem with this is it leaves open the question of what exactly constitutes a strong enough justification?
Clearly, Smith had strong grounds for accepting that he would not be the one getting the job, but these grounds were built on non-reliable information.

Therefore, we could argue that Smith's forming his belief in the proposition was not in fact justified.
Unfortunately, by taking this path, we are likely to end up nowhere but delivered into the hands of the sceptic; we will find no belief which has strong enough justification.

The Gettier counter example has clearly dealt the standard analysis a harsh blow.
As a result many philosophers have now directed their attention at finding the conditions of knowledge that will not allow for such counter examples.

However, I feel that any attempt to clearly define the limits of such a widely used concept as knowledge will only end in failure.
Including concepts such as justification in the definition only ends in the need for further analysis.

And this analysis itself is unlikely to be definitive.
The fact that it is questioned whether we are really able to know anything at all clearly illustrates the implausibility of ever being able to find a complete set of necessary and sufficient conditions that fully satisfy our concept of knowledge.

So, it seems that justified true belief does not definitively illustrate our idea of knowledge, it maybe the case that in certain circumstances this will be a adequate account of what we are claiming to know, but it will not suffice for a universal description.
Given the esteem with which rights are regarded in the majority of liberal systems, it seems only natural to question the justification which lies behind what is generally regarded as a legitimate, yet can often be horrendous, violation of these rights.

The two prominent theories within the justification of state punishment rely on two very different justificatory methods; the utilitarian perspective is instrumental in its approach, whilst the retributive perspective seeks justification through an appeal to the intrinsic nature of punishment.
Jeremy Bentham, in his An Introduction to the Principles of Morals and Legislation, expounds what is generally considered to be the most influential account of the utilitarian justifications of punishment; "The general object which all laws have, in common, is...to exclude, as far as may be, every thing that tends to subtract from...happiness: in other words, to exclude mischief.

But all punishment is mischief: all punishment in itself is evil.
Upon the principle of utility, if it ought at all to be admitted, it ought only to be admitted in as far as it promises to exclude some greater evil." In itself, Bentham considers punishment an evil; the unhappiness which is a result of particular instances of punishment is on utilitarian grounds a sufficient reason for regarding it as immoral.

However, to accept this is to overlook the positive consequences of punishment; deterrence, incapacitation and rehabilitation are three empirically verified consequences of state enforced punishment.
Given that the positive utility which is a result of these consequences outweighs any negative utility that may result from punishment, then punishment is justified.

The most popular appeal to the good consequences of punishment comes from an appeal to it deterrent effects.
There are two forms of deterrence; special and general.

Special deterrence aims at deterring the individual who performed the criminal act from committing a crime again for fear of being punished again, general deterrence aims at deterring the general population from criminal acts for fear of suffering a similar punishment.
Another popular justification of punishment which utilitarians appeal to is the incapacitative effect which it has on offenders.

An offender is less likely to re-offend in an environment in which he is constantly under surveillance such as prison or some outside monitoring scheme.
Finally, there is the appeal to the reformative effects which punishment can have; "He is reformed in the sense that the effect of punishment is to change his values so that he will not commit similar offences in the future because he believes such offences to be wrong." Clearly this reformative account of justification relies on a penal system which is constructed in such a way as to facilitate the rehabilitative process.

Before considering the retributive alternative to the utilitarian account of the justification of punishment, it is useful to consider the objections to the approach as it stands.
The primary objection to the utilitarian account is its potential endorsement of particular instances of punishment which appear counter intuitive.

Opponents claim that utilitarianism would justify the punishment of an innocent person providing that it was inline with the utilitarian maxim of being conducive of overall happiness.
It is quite possible to conceive of a situation in which the nature of a crime is such that it requires immediate resolution.

A situation is quite easily imaginable in which the punishment of an innocent civilian would result in the production of greater utility than not punishing any individual, or maybe even than punishing the guilty individual.
All it takes is a little imagination.

However, there are ways of responding to this line of argument.
Firstly, we can appeal to the idea that the concept of punishment implies guilt, and thus the 'punishment' of an innocent civilian is not to be considered punishment but is in fact an illegitimate act of state coercion.

Thus the implication of an innocent civilian as guilty of a crime is not within the scope of the utilitarian justification of punishment (although the criticism would still apply to utilitarian moral theory in general).
However, the idea that the innocent cannot be punished is far from being universally accepted.

It seems perfectly coherent to claim that the innocent person being punished was being punished on behalf of the guilty person, and thus the criticism still stands.
A second response relies on the claim that the punishment of an innocent would in fact result in an overall disutility.

The result of arresting an innocent person would be a general distrust of the whole system which could have disastrous consequences.
The utilitarian would appeal to the possibility of these consequences on order to justify a strict policy which requires that only the guilty be punished.

But this is not sufficient to escape the criticism, for the utilitarian principle requires that, in a situation in which the punishment of an innocent would maximize happiness, the innocent be punished.
The primary failure within the utilitarian justification, the retributivist argues, is its neglect of the fundamental concept of desert.

At the heart of the retributivist theory of punishment lays the Kantian maxim that one should treat individuals as ends-in-themselves, rather than as a means to reach some further end.
To employ this principle within the field of punishment is to deny that the consequences of an act have any intrinsic value in the justification of punishment.

The retributivists focus on the notion of desert.
A criminal is deserving of punishment because of the very nature of the criminal act.

Even if the consequences would be disastrous, the pure retributivist would still hold that the inherent wrong involved in a criminal act demands that the act be punished.
Because of the disagreement as to the exact nature of desert, and the difficulty involved in identifying exactly what the connection is between a criminal act, punishment and desert, a variety of different retributivist accounts of justice have emerged.

The Hegelian approach to the theory of punishment has played an influential role within retributive justification.
The focus of the Hegelian theory lays in the claim that punishment annuls the crime.

Initially this claim seems ludicrous, the past cannot be altered; how is it possible to annul a crime in which a person has lost their life?
Surely punishment is not adequate to nullify a murder?

However, although this interpretation of Hegel's claim is generally considered flawed, modern interpretations have had more success.
One interpretation argues that a criminal act has inherent in its nature the denial of the victim's rights.

Thus the criminal is said to hold himself in higher esteem than the victim, which is unacceptable.
Punishment is necessary; "Although the victim does not necessarily intend to deny the rights of his victim, the failure to punish the criminal is tantamount to an admission that an explicit denial of rights would have been correct." Failure to punish a criminal is interpreted as an acknowledgment of the criminal's superior rights.

This is unacceptable, so punishment is executed in order to restore the balance.
A second important approach to the Retributivist account of justification comes from Nozick.

Nozick makes an important distinction between retribution and revenge, appealing to the fact that retribution is a more objective account of justice, it ignores the emotive aspects that often accompany a revengeful act.
The objective stance which retribution relies on allows for an approach which is inline with considerations of justice.

Having established this distinction between retribution and revenge, Nozick goes on to expound his view in which he argues the primary function of punishment is to serve a communicatory role.
Punishment is a way of letting the offender know that his act was wrong; "...retributive punishment reconnects the offender with the correct values from which his wrongdoing has disconnected him." The communicative aspect justifies the punishment irrespective of any positive consequences which that punishment may have.

So even if the reconnection with the offender is entirely unsuccessful, the punishment is still seen as justified.
Ignoring the specifics of these last two retributivist theories, retributivism in general faces some serious criticism.

There are three primary weaknesses with the retributivist account.
These weaknesses fall under three headings; the moral status problem, problems with intelligibility and finally problems of rationality.

The moral status problem focuses on instinctual nature of the theory.
Critics argue that the retributivist theory is based on no real justificatory grounding, but is rather based on "premoral instincts to retaliate and take revenge." The intuitive nature of retributive punishment is often seen as an aid to its justification, but this intuitive nature can also count against it; many see the retributive account as without a purpose, and certainly not sufficient to justify punishment.

The second issue comes down to the problems with the intelligibility of retributivism.
The central role of the concept of desert leads critics to question exactly how this concept is employed.

What does it mean to say that an offender 'deserves' to be punished?
And equally importantly, what exactly does an offender deserve?

This latter question has caused a great deal of controversy.
The use of the notion of desert in order to justify punishment assumes that it is possible to identify exactly what an offender is due as a result of his criminal act, yet provides no basis for deciding what it is that is due.

Finally, there are the problems of rationality inherent in the retributivist justification of punishment.
Assuming that the other two issues were resolvable, it is still quite unobvious what rational basis we have for accepting this justification over alternatives.

Criticisms to both the utilitarian and retributive approach to the justification of punishment have been generally accepted as fatally damaging.
Attempts have been made on part of both the utilitarian and retributivist accounts to adapt their theories in a way which accounts for the heavy criticism.

Nozick's account of punishment as communication was an example of such an attempt.
The communicative nature of punishment, although solely justified by the fact it attempts to show the offender why he is being punished, can appeal also to the consequences of this approach as further justification.

However, I feel a more successful attempt at compromise comes from the utilitarian camp.
The two primary proponents of this view are H. L. A. Hart (1959) and Rawls (1968).

Both rely on a distinction between the justification of an individual act and the general principle which governs the justification of the act.
They claim that punishment as a concept is justified with reference to the positive consequences which punishment gives rise to.

Judicial Systems are an instrument of the state.
The state is set up to govern the dealings of society.

It makes sense that the judicial system justifies the use of punishment through its aim of furthering the interests of society.
Individual applications of punishment on the other hand are justified by the guilt of the offender.

This approach sees punishment as justified essentially through appeal to utilitarian principles, but recognizes that the direct appeal to utilitarian principles in the case of individual criminal acts could have disastrous consequences.
It is thus in the utilitarians best interests (as seen in solely utilitarian terms) to endorse, at an individual level, the justification of punishment on the retributivist grounds.

This utilitarian attempt at justification is by no means watertight, however it does seem to present a more promising approach to the justification of punishment than the retributivist.
The lack of rational grounding and its failure to prove its moral status are devastating problems with the retributivist account.

Although retributivism has an element of intuitive appeal, retributivists have failed to account for the moral nature of this appeal.
However, utilitarianism is not without problems; the potential acts which this account would endorse leave an incredible amount to be desired.

However, it seems to me that through appeal to a historic account of our intuitions, as based on utilitarian primary principles we are able to justify individual acts of punishment with reference to retributive principles.
Attempts at this unification are far from being complete.

But look as though they are more likely to be successful than retributive attempts.
It is rare that you'll find a discussion group in which the morality of paedophilia is a topic which will divide the opinions of its members.

The media and social hype which surrounds the subject leaves little room for an unbiased and constructive debate.
This is perhaps justly deserved, for in most cases acts of paedophilia involve the subjection of minors to physical and psychological abuse which can have severely detrimental effects on the rest of that individuals life.

It is also true that the acts often occur in relationships which are dependant on an implicit trust, such as the relationship between a carer and child.
The performance of an act of paedophilia in such a relationship represents an extreme betrayal of this trust, and thus cannot be perceived as anything but immoral.

However, any objective moral account of paedophilia requires the separation of an act of paedophilia from the wrongdoing which often occurs simultaneously with that act.
Only upon an investigation into the nature of paedophilia in itself can we assess its morality.

Before discussing the morality of paedophilia, we must first identify what an act of paedophilia actually entails.
Primoratz (1999) distinguishes between pederasty, ephebophilia and paedophilia.

Pederasty refers to the homosexual attraction and intercourse between an adult male and a boy aged around 11-12 and pubescent.
Ephebophilia involves the homosexual attraction between an adult male and a post pubescent youth.

Primoratz contrasts the ephebophile with the pederast; "...unlike the pederasts, ephebophiles are attracted to post-pubertal, sexually mature youths.
An ephebophile finds sexually attractive the very thing that puts off a pederast: the fully developed, vigorous maleness of adolescence." Having identified this distinction, we are now in a better position to understand the nature of paedophilia and it's moral implications.

In many societies an act of paedophilia is to be determined with reference to the age of consent which that society has specified through law.
Therefore almost all societies will view an act of pederasty as paedophilia.

Ephebophilia however, is much more dependant on the societies determined age for homosexual consent.
If that society specifies an age of 15 years for consenting homosexual intercourse, then some acts of ephebophilia will not be considered to be an act of paedophilia.

In the wide sense then, paedophilia refers to sexual attraction or intercourse between an adult and a minor of any gender, whereby the classification of a minor with reference to sexual relationships is determined by the societies age of consent.
However, for the purpose of this discussion we will examine the morality surrounding paedophilia in the narrow sense.

This narrow sense excludes acts of ephebophilia and focuses instead on the sexual attraction of an adult male to a child of either gender who is pre-pubescent or pubescent.
So, ignoring the wrongdoing which is often associated but not necessarily an aspect of paedophilia in itself, on what grounds can we consider paedophilia in the narrow sense immoral?

There are two aspects to the arguments for the condemnation of paedophilia, both of which can be explored independently of the other.
Firstly, there is the argument from consent.

This argument focuses on the nature of children and there inability to give informed consent for any form of sexual relationship.
Secondly, there is the argument from harm.

This argument holds the potential psychological and physical damage that a child can be subjected to is warrant enough for the complete moral condemnation of paedophilia.
However, neither of these arguments are universally accepted.

There exists a number of theorists, albeit a very small number, who advocate a thorough re-examination of the moral and legal attitudes directed towards paedophilia in favour of relaxing those moral and legal attitudes.
Those engaged in the advancing a more liberal attitude towards paedophilia argue from several perspectives in order to show that the current standing of the law and morality on paedophilia is unjustified.

Key to their argument is showing that the assumption that paedophilia results in psychological harm is unfounded.
This argument draws upon research which seems to support this claim.

One highly controversial study is that of Rind et.
al.

which appeared in the American Psychological Association's prestigious Psychological Bulletin.
Having researched into the harm which results from childhood sexual abuse, the results seemed to show that there was a weak correlation between a child's exposure to sexual abuse and damage to that child's later psychological development.

Advocates of paedophilia argue that research of this type needs to be seriously taken into account.
Although there may be the occasional research paper which can be seen to support the claims of defenders of paedophilia, there is also a vast amount of research which seems to suggest otherwise.

According to the case notes kept by the NSPCC, 451 young people spoke to ChildLine about suicide as a direct result of sexual abuse.
The harm caused as a result of acts of paedophilia is evident.

Ignoring any physical harm which might occur, it is clear from a vast array of research that the psychological harm which results from sexual abuse that any act of paedophilia is immoral.
However, this research is not the end of the matter.

For it is quite obvious that through the use if terminology such as 'sexual abuse' that we have returned to a discussion of the wide sense of paedophilia in which social and moral norms weigh heavily on the objectivity if the discussion.
Advocators of paedophilia argue that the majority of research conducted into the effects of paedophilia on children is biased for a number of reasons.

Firstly, the very nature of studies into the effects of sexual abuse will yield results that show negative experiences, for it is the nature of 'abuse' that it will involve some form of harm.
Secondly, advocates of paedophilia argue that the political and societal pressures exerted on the researchers will force results which will lead to the condemning of paedophilia.

This was clearly demonstrated in the uproar surrounding the research of Rind et.
al. Following the publication of the results of this study, radio presenter Laura Shlessinger launched a media campaign claiming the research displayed an intention to normalize paedophilia.

Further to this campiagn, congressman Tom DeLay sought congressional action against the American Psychological Association for their publication paper.
According to advocates of paedophilia, it is this general attitude towards the subject which leads research to focus solely on the negative aspects.

So, as it stands, it seems we have yet to establish whether paedophilia in itself is to be considered morally reprehensible.
If the advocates of paedophilia are correct, then the argument from harm seems to depend on the results of studies which are unavoidably influenced by the moral and legal norms prevalent in the society in which they are published.

Further to this they are conducted in a fashion which will necessarily yield negative conclusions about the effects of 'child abuse'.
They therefore do not present an objective account of the immorality of paedophilia.

Groups such as 'NAMBLA' (North American Man-Boy Love Association), a paedophile activist or 'childlove movement' group believe that sexual relationships between adults and children can be conducted in a way which is both harmless and actually beneficial to the development of those children.
The arguments forwarded by groups and activists attempt to show that paedophilic activity is misrepresented by the media and common morality.

Firstly they question the assumption of harm by suggesting that any harm which does result from a child's sexual experience with an adult is due to the outside perception of those acts and the sense of wrongdoing which is forced upon them.
Activists argue that if the common morality perception of paedophilia were to change dramatically so as to accept paedophilic acts then the psychological harm which results from such act would be eradicated.

In order to advance this view, some paedophile activist groups propose a code of ethics for 'adult-child' sex.
This code stresses the importance of consent on the part of the child to engage in the relationship, the child's ability to withdraw from the relationship, and promoting the relationship in an open manner, rather than conducting it in a secret environment.

If these key factors are acknowledged, the paedophile activists that 'intergenerational' relationships can be conducted safely and without harm to the child involved.
However, this argument depends on an environment in which paedophilia is not viewed as a sickening and highly immoral psychological disorder, as many currently do see it.

For it still follows that in the current environment any child who consents to an act of paedophilia will not be aware of the moral implications of that act.
For if a child were fully informed on the matter, then presumably that child would never consent to such an act.

This brings us to the second argument against paedophilia, that of informed consent.
According to WikiPedia, informed consent is a "condition whereby a person can be said to have given consent based upon an appreciation and understanding of the facts and implications of an action.

The individual needs to be in possession of relevant facts and also of his reasoning faculties...".
Those who view paedophilia as inherently wrong argue that the fact that a child cannot give informed consent is sufficient to prohibit on moral grounds the engagement of any of any sexual interaction between a child and an adult.

For only an adult can give truly informed consent, whereas a child can only give partial consent.
Some activists respond to this claim by arguing that children can in fact give informed consent, and often actively seek sexual interaction with an adult.

Others argue that partial consent is sufficient to justify a child's engagement in sexual interaction with an adult, providing that that the child's inability to give full consent is not taken advantage of, and the consent that is given can be withdrawn at any time.
However, this argument misses the point.

The purpose of informed consent is to ensure that a person is fully aware of the ramifications of their actions.
Children are generally considered unable to give informed consent because of their psychological constitution.

The sexual development of a child compared to that of an adult, and the way in which the engagement in sexual activity is construed is so different that it is impossible for a child to be informed, for this presupposes a full understanding of the nature of adult intercourse.
The appeal to partial consent also fails on the grounds that it represents an imbalance of power in the relationship.

If a fully informed adult accepts partial consent to an engagement of sexual activity, by the very nature of the distinction between informed and partial consent, that adult is taking advantage of the child's lack of information, and has thus committed an immoral act.
It is clear from this discussion that the social and moral reprehension which surrounds paedophilia is not entirely justified.

Any objective account of paedophilia must avoid taking into account the harms which are a result of these acts, for these harms are often a product of the way in which society views these acts.
However, it is also true that, no matter how much a paedophile can safeguard a child against future harms, the engagement of sexual activity between an adult and a child, by its very nature, represents a relationship in which the position of power which that adult holds over the child is exploited.

The legal positivist tradition can be traced back to the work of Jeremy Bentham, who in turn drew upon the writings of Thomas Hobbes and Jean Bodin.
It traditionally opposes the theory set out by natural lawyers, who hold that law and morality are inseparable.

Positivism can be characterized by two major tenets; firstly that law and morality are two distinct entities, and secondly that the validity of any legal system is ultimately determined by reference to certain social facts.
This essay will focus on the positivist approach as developed by Austin using his 'Command Theory' of law, and the weaknesses associated with this theory.

I will go on to consider Hart's reaction to Austin, and the way in which Hart remedies the positivist approach in order to account for the problems he believed were inherent with his predecessors theories.
Austin developed his command theory of law off the back of his contemporary Jeremy Bentham.

Where Bentham had merely outlined the theory, Austin approached the subject in a much more analytical manner, aiming to capture the essential nature of the law as a science and thereby eliminating all the common uses of the word 'law' which are not relevant to jurisprudence.
Austin's starting point in determining the exact province of jurisprudence was to exclude all the meanings of law which were not translatable into a command.

For Austin, a command can be broken into three components; "[a command is] 1.
A wish or desire conceived by a rational being, that another rational being shall do or forbear.

2. An evil to proceed from the former, and to be incurred by the latter, in case the latter comply not with the wish.
3. An expression or intimation of the wish by words or other signs." Austin's classification of a command helps to illustrate the exact nature of the type of law he is seeking to define.

In 'The Province of Jurisprudence Determined', Austin uses his conception of a command in order to distinguish between different kinds of law.
His first step is to differentiate between what he calls 'laws properly so called' and 'laws not properly so called'.

Laws not properly so called are those which are often referred to as laws but are, in terms of legal science, inappropriate.
Examples of these laws are what Austin terms 'Laws by analogy', which include things like the rules of etiquette and international law, and 'Laws by metaphor', which covers things like the laws of nature.

Although both of these 'laws' can be construed as commands, they lack a variety of qualities necessary in order to fall into the category of proper law; "Widely accepted rules that are only vaguely or by analogy regarded as laws...are laws, not as matter of hard fact, but by virtue of mere opinion.
Those which are laws only metaphorically speaking...are eliminated on the grounds that there is an absence of will to be incited or controlled." These laws 'not properly so called' do not lie within the province of legal science.

Having drawn this distinction, Austin goes on to break down his classification of laws 'properly so called'.
Within this category Austin places two sub-groups; laws set by humans and those set by God.

Although the laws set by God are properly speaking laws, they do not, according to Austin, fit into the province of jurisprudence.
Laws set by humans are of two types; laws 'strictly so called' and laws 'not strictly so called'.

Laws 'strictly so called' "consists of (a) laws set by men as political superiors to political inferiors; and (b) laws set by men as private individuals in pursuance of legal rights".
Laws 'not strictly so called' are those which do not derive their authority from politically superior individuals.

It is the former that Austin believes to be the proper domain of jurisprudence.
Having identified the variety of commands that fall under the category of laws 'strictly so called', Austin proceeds to elaborate further the exact nature of these laws; "Austin in effect defines 'a law' as 'a general command of a sovereign addressed to his subjects'" As we saw earlier, Austin identifies a command as consisting of three elements; a wish, an expression of that wish, and a sanction if the wish is not complied with.

In order for a command to count as a law strictly so called, Austin introduces the further notions of sovereignty and generality; the law must of emanated from a sovereign individual or body, and it should not be applied to particular cases, but rather be general in its applicability.
Austin's use of the term 'sovereign' can be distinguished from the way it is used within legal rules; "[For Austin], the sovereign is a pre-legal political fact, in terms of which law and all legal concepts are definable." Austin's use of the term allows him to avoid criticism based on the fact that many independent states, which have a full legal system, lack a sovereign in the legal sense.

Austin explicitly states the way in which he holds an individual or body of individuals to be sovereign; "If a determinate human superior, not in a habit of obedience to a like superior, receive habitual obedience from the bulk of a given society, that determinate superior is sovereign in that society..." So a law, as Austin sees it, is a general command which originates from the sovereign of a given society.
Austin's conception of law has been highly criticised.

The most notable collection of these criticisms comes from H. L. A. Hart.
In 'The Concept Of Law' Hart presents Austin's command theory of law in what he believes is its strongest possible form, then systematically picks it apart.

Hart's approach to this destruction of the command theory takes the law as it exists in today's society and compares it with Austin's conception.
Hart sees the command theory as implying that laws are just orders from a sovereign backed by threats.

He feels that this is a insufficient account of the way in which laws really work.
Firstly, although there are many laws which do resemble this model, for instance a law which prohibits theft, there are many laws which are not like this.

Laws which oversee the successful implementation and operation of contracts do not appear to impose sanctions if the way in which contracts should (according to law) be administered is not adhered to.
The only result from an incorrectly administered contract is the contracts nullification.

It may be the case that Austin can defend himself from this criticism by arguing that this nullification is itself a sanction, but given that a sanction according to Austin consists in 'an evil', it seems a bit of an implausible response.
Even if it is the case that this response will suffice, Austin's command theory fails to account for laws which govern the administration of other laws.

Where Austin has gone wrong, Hart believes, is his failure to recognise that some laws "provide individuals with facilities for realizing their wishes, by conferring legal powers upon them to create...
structures of rights and duties within the coercive framework of the law." Hart's next criticism of Austin focuses on the origins of laws.

Under Austin's definition laws are orders which are backed by the threat of a sanction if the order is not complied with.
If this is true, then any law should be traceable to a particular act performed at a particular time.

However, Hart recognises that many laws have their origin in custom, which often have no traceable date of conception and thus cannot be attributed to the command of a sovereign; "some rules of law originate in custom and do not owe their legal status to any...
conscious law-creating act" Hart accepts that although the enactment of a statute may represent the transfer from custom to law, it also the case that many customs attain legal status prior to their enactment in a court.

Just as some statutes are law before being enacted in a court, so are many customs.
Command theorists may respond by introducing the notion of a tacit command from the sovereign.

When hearing of a certain custom, by not repealing it, the sovereign is in effect commanding that this be the law.
However, this fails to account for the fact that in most modern states it is impossible to identify the exact point at which a sovereign sees a custom being applied as law, and accepts it application.

Thus, Hart holds, the notion of tacit approval fails to account for custom as law.
Austin's insistence that in order to be considered sovereign, the person or body must receive 'habitual obedience' has led to further problems with his theory.

Hart points out that in a system where the sovereign is an individual, the command theory fails to account for succession; "the idea of habitual obedience fails...to account for the continuity to be observed in every normal legal system, when one legislator succeeds another." When one sovereign is succeeded by another, the new sovereign cannot be said to receive habitual obedience.
Since habitual obedience is one of the marks of a law maker, then it follows that the new sovereign cannot make laws.

However I feel that this criticism is somewhat lacking.
Austin held that in order to be sovereign, you must be habitually obeyed.

But it could also be the case that a society will habitually obey the sovereign.
So in succeeding one sovereign, the new one will, out of habit, be obeyed.

As for the problem concerning the origin of all the laws in that society, we can claim that on his succession, the new sovereign commands all the previous laws be obeyed.
The last problem to be considered comes from Austin's notion of sovereignty.

For Austin the sovereign of any state has illimitable powers and does not obey any other legislature.
However, it is quite apparent that in many modern states, where laws are obviously in existence, it is not the case that the sovereign has unchecked power; "...the conception of the legally unlimited sovereign misrepresents the character of law in many modern states where no one would question that there is law." In the passing of a law, most countries have a system of checks and balances that ensure that the law is legitimate.

For instance, in the United States of America, a written constitution ensures that the legislature only passes laws which are within its competence to pass.
If these restraints are not adhered to the law will not be considered legitimate and thus not be considered law.

This shows that Austin's belief that all laws stem from a sovereign law maker is fundamentally wrong.
We may try to appeal to the idea that it is still the sovereign who creates these checks and balances, but this would still imply that the sovereign is in the habit of obedience to itself, and would thus fall short of Austin's conception of a sovereign law maker.

Hart's heavy attack on Austin certainly seems to of left command theory redundant.
Austin's belief that a law is the general command of a sovereign seems to fall short when considering the way in which many modern systems of government are set up.

The fact that legislatures consist of a vast body of individuals means that it is a lot harder to identify what Austin would consider to make up the sovereign.
The way in which governments distribute power means that any legislation a sovereign may try to introduce will be subject to a variety of conflicting interests within that sovereign which limit its ability to do so.

This essentially makes the Austinian concept of sovereignty redundant.
So where Austin succeeded in bringing legal positivism to the forefront of debate within jurisprudence, he failed to give a satisfactory account of that positivism.

This essay will focus on the design version of the teleological argument.
Arguments from design purport to show that the way in which the natural world elicits purpose entails that it must have had a designer.

I will first highlight the problems associated with Paley's version of the design argument, and go on to focus on Swinburne's distinction between temporal and spatial order.
I will discuss how this distinction avoids some of the criticisms of earlier versions, but still has problems affecting it.

One of the most famous arguments from design came from 18 th century theologian and philosopher William Paley.
In his 'Natural Theology', Paley compares the universe to a watch.

From the intricate design and complex interaction of different parts within the watch, Paley infers that the watch must have had a designer; "This mechanism being observed..., the inference, we think, is inevitable, that the watch must have had a maker" Paley goes on to suggest that the natural world contains similar examples of such complex interplay between parts, and infers from this that the natural world too, must have had a maker.
Arguments of the kind Paley expounded, which utilize analogy, are subject to various rules.

In comparing two distinct kinds of phenomena B and B* (in the case of Paley's example; the watch and the natural world), we must look to the similarities between the phenomena's causes; A and A* (man and x, where x is the object of the discovery).
The similarities and differences between the two effects of the phenomena leads us to postulate proportionate similarities and differences between their causes; the design of the natural world could be said to be infinitely greater than the design of the watch, and from this we infer that the natural world's designer must in some respect be infinitely greater than that of the watch.

These are the basic rules of analogy.
In his 'Dialogues Concerning Natural Religion', David Hume launches a devastating attack against arguments from design.

Using the character of Cleanthes as the primary proponent of the teleological arguments, Hume illustrates the kind of argument he wishes to show false; "The curious adapting of means to ends, throughout all nature, resembles exactly, though it much exceeds, the productions of human contrivance...
Since therefore the effects resemble each other, we are led to infer, by all the rules of analogy, that the causes also resemble..." The similarities between Cleanthes argument and that which Paley had forwarded are very much apparent.

However, Hume believes that arguments which are based on this type of loose analogy fail, and gives various reasons which show this is the case.
His first criticism focuses on the strength of the analogy between human contrivances and the natural world.

Using Philo as his mouthpiece, he argues that the considerable differences between the two cases proportionately weakens the strength of the analogy and "is confessedly liable to error and uncertainty".
Further to this, he argues that even if the rules of analogy have been strictly upheld, it is not necessarily correct to posit a single Deity over a variety of other plausible hypothesis; it is perfectly conceivable that there are a multitude of agents who have created the universe, all of finite power sharing equal responsibility for their creation.

Not only is this plausible, but it is also more inline with an analogy from a product of human creation (e.g. a house, which usually has several people working to build it), and thus more realistic.
He goes on to argue that even if we can reasonably infer the existence of a single designer from the order shown in the world, there would still be the need for a great deal of further explanation as to the nature of the mind of the designer; since many of the products of human designed are explained in terms of their function in respect to their creator, we would ideally want to find similar motivations for this being to have created the natural world.

Many commentators believe Hume's criticisms have decisively refuted design arguments of the form presented by Paley and Cleanthes.
Even if it is the case that these arguments can escape Hume's criticism, the advances made in the field of biology in the 19 th century certainly seem to of dealt a devastating blow.

Darwin's theory of evolution and in particular his theory of natural selection gave a far more satisfactory explanation as to the apparent design and complexity of natural objects.
From this it seems correct to reject any argument for God's existence based on the apparent order found in nature.

However, the debate surrounding the argument from design does not end with Hume and Darwin.
More recent versions have focussed on the possibility of eluding the Darwinian explanation, whilst also evading the criticisms posed by Hume.

One such account comes from Richard Swinburne.
In his 'The Existence of God', Swinburne distinguishes between two types of order found in the natural world; spatial order and temporal order.

Spatial order is illustrated in the way in which various parts of an organism work along side each other to perform some function.
Temporal order is displayed by the simple patterns of behaviour found in objects, and the way in which one event is naturally followed by another.

He goes on to suggest that many of the early criticisms presented against the design argument were based on the fact that these arguments were formulated from observations of spatial order; "Paley's 'Natural Theology' dwells mainly on details of comparative anatomy, on eyes and ears and muscles and bones arranged with minute precision so as to operate with high efficiency, and in the 'Dialogues' Hume's Cleanthes produces the same kind of examples" Arguments based on this kind of spatial order suffer immensely in light of the Darwinian account of evolution.
Darwin has shown how complex animals and plants can develop from the generation of less complex species, which in turn can develop from the generation of even less complex animals and plants, thus eliminating the mystery behind the apparent order and interplay of parts found in these animals and plants.

However, Swinburne feels that despite Darwin's theory of evolution, if we are to adopt the temporal version of the design argument, we are able to evade the problems which have arisen with the evolution of biology.
He also thinks that although Hume's objections are still applicable, it is also possible to evade these objections.

He starts by identifying the temporal order which he is referring to; "The orderliness of the universe...
is its conformity to formula, to simple formulable, scientific laws...

The universe might so naturally have been chaotic, but it is not - it is very orderly." This order, he argues, is not solely a result of man imposing it.
We as humans observe correlations in cause and effect and from this are able to induce future events, the success of which depend on these events actually taking place, for it is the case that "men cannot make nature conform subsequently to an order which he has invented".

However, various critics have highlighted the hidden tautology in the notion that we live in an orderly universe; "Our presence in this special sort of universe...follows from the tautology that we can only exist in a universe consistent with our own existence" It is a requirement for our existence that the universe should be orderly.
In spite of this objection, Swinburne believes this orderliness still suggests a designer; the presence of order is undoubtedly a requirement for our existence, but this order only need be immediate to humans.

It is plausible that earth contain structure and order which allow for the flourishing of the human race, whilst outside of earth the universe contained utter chaos.
Swinburne believes that the amount of order found in the world greatly exceeds what is necessary for humans to exist.

Further to this, the fact that we could only exist if the world was ordered takes nothing away from the fact that the level of orderliness is astonishing.
Having established the nature of temporal order, Swinburne goes on to explain how this order leads us to postulate God.

We utilize scientific principles in order to explain occurrences within the natural world.
These 'low-level' scientific principles are themselves explainable in terms of 'higher-level' principles, which are again explainable with reference to even higher-level principles.

This chain can be traced back to the highest-level scientific principles, which themselves are unexplainable in terms of scientific principles, seeing as they are the principles which all others are explained with reference to; "...from the very nature of science it cannot explain the highest-level laws of all; for they are that by which it explains all other phenomena." It follows from this that the order found in the world is unexplainable in terms of scientific laws, since the order is essentially the reason that these laws exist.
The fact that we cannot use a scientific explanation to explain the order found throughout the world leaves us, according to Swinburne, with two options; we either abandon any further attempts to explain, or we appeal to some form of personal explanation which posits an agent with intentions beliefs and capacities.

Swinburne believes that the latter alternative is the only reasonable option, and that this option entails an agent who has the power to maintain this order.
Although it seems to avoid the heavy refutation that Darwinism presented, this temporal version of the design argument seems to be as equally subject to Hume's arguments as was the spatial version.

It is not necessarily the case that it is a single infinite deity which is the cause of all this order, it is equally as plausible that it is a multiplicity of agents, all of which are finite in power and knowledge and are responsible for different parts of the order.
This suggestion, according to Hume, may be even more plausible given that it is more analogous to what we are witness to in the actual world.

Even if we can establish a single infinite deity we are still left with the complaint concerning the nature and cause of this being.
It will have properties which further need explanation, which this argument fails to address.

However, Swinburne has solutions to this argument.
A key feature of Swinburne's argument is his appeal to the principle of simplicity.

This principle as, as recognise by Hume states that; "To multiply causes without necessity, is...contrary to true philosophy" He believes that the acceptance of a hypothesis like that proposed by Hume only serves to complicate matters further.
Any finite being has properties which could be more so or less so, and thus we are able to question why they have this quantity of each property.

A single infinite being on the other hand is much less complex.
We no longer have to concern ourselves with why it possesses such and such amount of each property, or why it is only responsible for whatever part of the order.

For this reason, we should accept the existence of a single infinite agent as the cause of order over the existence of a multitude of finite agents.
In order to deal with the argument that God needs further explanation, Swinburne appeals to the notion of 'complete explanation'; "the arguments to the existence of God with which we are concerned are arguments to a complete explanation of phenomena" Although it is possible to view God's actions as requiring further explanation, these explanations will themselves be explicable in terms of God.

However his use of the notion of simplicity in his design argument has not gone unquestioned; "Swinburne simplifies his task by excluding one of the possibilities which Hume mentioned as an alternative to theism, the activity of a plurality of agents with finite powers...
I do not see how he can do as much as this." Mackie believes that Swinburne's quick dismissal of a multiplicity of finite agents is unwarranted.

Given the multiplicity of natural laws to be found, it would make more sense to posit an agent for each law.
However, Swinburne believes that the explanatory power of this hypothesis would leave a lot to be desired, where as with a single infinite being, no further explanation is required and is thus the preferred option.

It seems that Swinburne's arguments have decisively evaded the arguments associated with the development of the biological sciences.
His appeal to the temporal order as an example of the design of the world rather than spatial order cleverly side-steps problems associated with Darwinism.

However the criticisms posed by Hume do not seem to of been as successfully avoided.
Although he has not relied so much on analogy in order to prove the design of nature, his insistence that the simpler hypothesis is the option we must accept lacks weight in terms of proving the existence of a deity, for it seems equally plausible and possibly more so, that the cause of nature lies in a multiplicity of agents.

So whilst the distinction between spatial order and temporal order has saved analogical arguments from complete nullification, there is a lot more work to do if design arguments are to succeed in their aim.
Within philosophy of religion there is a type of argument, known as pragmatic arguments, which utilise the possible benefits of belief in order to persuade the listener that it is rational to accept God's existence.

The most prominent version of a pragmatic argument comes from French philosopher Blaise Pascal.
This argument is known as 'Pascal's Wager'.

It is the purpose of this essay to explore the rationality behind this wager and consider the objections which it must confront.
Initially this essay will briefly explain the nature of the wager and consider a simple objection.

I will use this objection to reveal some of the subtleties involved in the wager and go on to fully explicate Pascal's argument.
Finally I will consider objections to the wager and responses, concluding that Pascal has indeed shown that informed agnosticism/atheism is irrational.

In his Pensees, Pascal argues that we have the choice of either believing in God, or not believing in God.
Given that there is an equal chance of either being the case then it is rational that we choose the path of belief; "Let us estimate the two chances; if you win, you win everything; if you lose, you lose nothing.

Do not hesitate, then; gamble on His existence." Given the potential reward involved in believing in God, Pascal argues it is rational that we gamble on God's existence.
This is Pascal's wager.

Initially this Wager may seem superficial.
Critics have pointed out that to choose the path of belief in God because it presents an opportunity to receive great gains is highly against the Christian ethic; "...applying betting rules, relevant to moneymaking ventures, to a supposedly infinitely more exalted subject to lure sceptics by appealing to their grasping instincts offends religious proprieties." Although Pascal does explicitly state that this initial choice is only the first step in acquiring a genuine belief, his critics still believe that religion is incompatible with such a greedy way of thinking; to start the path of belief in such a way is to debase the end which the path leads to.

However, on a closer inspection of Pascal's argument, we find that Pascal draws heavily on the investment necessary for a genuine religious belief.
It is not the case that, upon reading the wager, a rational atheist will decide to turn theist and automatically believe in God.

Religious belief involves a lot more than the simple assertion that 'God exists', it involves a change in your way of life.
Pascal recognised this fact, and used the wager not as bait for self indulgent non-believers, but as a rational reason for these non-believers to start on a quest for belief; "The wager is an argument for undertaking an action which will lead to belief.

Pascal shows that the formation of a belief in the articles of faith is practically rational." Belief cannot be forced, it must happen naturally through choice.
Pascal's wager provides a reason for investigating the path of religion further.

It does not pretend to claim that it is possible to simply decide to believe in God just so we can benefit from the rewards which such a belief results in.
The fact that Pascal suggests the wager is only the start of the path towards genuine belief brings out another important aspect of Pascal's argument.

Pascal believed that epistemic proofs such as the teleological argument for the existence of God, if successful, can only result in the conclusion that 'God exists'.
These proofs do nothing to establish the articles of faith as essential to belief; "The metaphysical proofs of God's existence are so remote from man's reasoning and so complicated that they make no great impression.

Even if some men are affected by them, the effect does not last beyond the moment in which they see the proof; an hour later they are afraid they have been taken in...
This is what comes of a knowledge of God obtained without Jesus Christ..." Pascal holds that any argument which aims to establish God's existence via epistemic methods will not only fail to capture the practical consequences of dedicated religious belief, but also ultimately fail in it's primary aim.

Pascal argues that, if there is an existing God, His nature is such that He is infinite and without limits, we are thus "incapable of knowing either what He is, or whether He exists".
This is why Pascal places so much emphasis on belief; it is only through faith and belief that we can come to know of His existence.

Since we can only come to know of God through a genuine and dedicated belief in him, and any epistemic proof of his existence is impossible, it follows that any non-theist is presented with a choice, with two options of equal probability; either accept that God exists and make a small sacrifice in order to embark on a path of religious belief, with the possibility of being infinitely rewarded in the afterlife, or remain a non-believer without making the small sacrifice but risking the possibility of an afterlife of infinite misery and pain.
Given the equal probability of the two options, it is rational to choose the path of belief.

If we are to bet that God does exist, the maximum we will loose is, say 10 units of utility (this is the cost of dedicating our life to religious belief), with a potential gain of infinite utility.
If we bet that God does not exist, we keep our 10 units of utility, but run the risk of paying an infinite amount of disutility in the event that God does exist.

Pascal thus argues that it is not only rational to aim at acquiring a genuine belief in God, it is completely irrational to not do so; "since you are compelled to play, you would be mad to cling to your life instead of risking it for an infinite gain, which is as liable to turn up as a loss - of nothing." It is even debatable whether the path of religion involves any sacrifice at all.
It may seem that there a lot of pleasures which a non-believer would have to give up, but Pascal argues that, once engaged in belief, these 'pleasures' no longer seem pleasurable.

Thus, to choose to believe in God allows for the possibility of infinite utility, without any cost.
It is therefore completely irrational not to choose to believe in God.

Despite the strength of it's rationale, Pascal's wager has not gone unchallenged.
The strongest objection stems from Pascal's commitment to a Christian God.

Critics have pointed out that we can postulate an infinite number of different Gods; not only the Gods of other religions, but also made up God's with arbitrary worshipping requirements; "Pascal has been charged with making the unwarranted assumption that the problem facing the agnostic is confined to the question of which two options to choose.
In reality, however, in addition to the God of the theist, there are any number of other possible ones as well." As long as the Gods postulated as alternatives to the God of Pascal's wager offer the reward of infinite utility in the afterlife, then it seems that these Gods are equally as worthy of worship.

It may be objected by proponents of the wager argument that the existence of a made up God is far less probable than the God of Christian theology, but as long as the rewards of worshipping this made up God are infinite, then the value of its outcome is infinite, the same value which worshipping the Christian God carries; "given a possible infinite utility and a positive probability, no matter how small, an infinite expected utility is generated.
Hence, the wager, instead of uniquely picking out one religious hypothesis to believe, is left with any number of incompatible religious hypotheses, each with an infinite expected utility, and no obvious way to choose among them." If we were to invent a God who's only worshipping requirement was that we refrain from eating sandwiches for one day a year, then although the probability of this God existing is tiny, it is still logically possible and therefore has a probability which is greater than zero.

Thus the potential gain of worshipping this God is still infinite.
Since this God demands a lot less from the worshipper than the traditional Christian deity, then the rational non-believer may rationally choose to worship the sandwich God instead.

There are a variety of responses to this objection.
George Schlesinger argues although the expected gain in utility from worshipping any possible God is infinite, it still makes sense to worship the God which has the greatest probability; "I submit a crucial point, one that is contrary to what numerous philosophers hold, namely, that when each possible outcome carries an infinite expected value, it is rational to bet on the outcome most probable to occur." Schlesinger draws on the general principles surrounding gambling in order to argue that Pascal's wager requires a different set of principles.

In general the cost of betting is proportionally lower than the potential reward, depending on the probability.
Fairness requires that a combination of low probability and low reward means a low initial bet.

If we apply this same principle to wagers where the potential prize is infinite then we see that it is an unavoidable consequence that the cost should also be infinite, since if x is infinite, then any proportion y, of x will also be infinite.
Schlesinger sees this as absurd and it is thus the case that different principles of gambling apply in the case of Pascal's wager; "Because neither expected utilities nor the magnitude of the prize can serve as one's criterion, by elimination it should be reasonable to be guided by the value of the probability: wager on the outcome that is most likely to materialize." If we apply this to the many-gods objection, we see that the rational wagerer should choose to believe in the God which is most probable.

Since a God who is infinitely supeme is more probable than an arbitrary God who prohibits sandwich eating on one day a year, a rational wagerer should choose to believe in the former.
From this we can draw a general principle; the more sense a God makes then the higher the probability we can ascribe to that God.

Thus the God which makes the most sense is also the most probable.
Drawing upon the numerous religious texts which have appeared throughout the ages, we find that the majority of representations of God are as an infinite supreme being, of which nothing greater can conceivably exist.

This is the God which makes the most sense, and is thus the God which is most probable.
The rational wagerer should thus choose to believe in this God.

If the notion that the more sense a God makes then the higher the probability of His existence fails to impress, then another option is to appeal to simplicity.
An appeal to simplicity requires that we accept the simplest hypothesis as the most probable.

Out of all the God's we can postulate the simplest one will have properties of infinite goodness, power, capabilities etc., for we do not have to question why they are in possession of only a certain amount of each property.
Such an appeal to simplicity will lead us to only one option; "It is simplest because it is the only hypothesis that may be expressed with the use of a single predicate: to describe the God of the theist all that is needed is that he is an absolutely perfect being." Having established that the absolutely perfect God is the God deserving of worship, it is up to the wagerer to take the first step down the path of faith and decide what form this worship takes.

The majority of religious arguments attempt to prove only the existence of a God.
Any which go on to try and attribute properties that this God may have, generally fail.

Where these others have failed, Pascal has taken an alternative route.
Not only has Pascal shown that it is rational to believe in God, he has also shown that it is rational to believe in a God with infinite perfections.

The objection that it promotes irreligious attitudes lack weight due to its failure to grasp the essential motivation behind the wager; to persuade an informed agnostic of the irrationality of atheism, not to lure the agnostic in with the promise of eternal life.
The 'many-gods' objection inflicts more of a blow to Pascal's argument.

By solely appealing to the expected utility of religious practice in order to justify belief, Pascal is confronted with a potentially infinite amount of Gods whom the wagerer could rationally have faith in.
However this objection is easily sidestepped by appeal to probability and the principle of simplicity.

A God with infinite and absolute powers makes more sense and is thus more probable.
He is also simpler.

Both of these facts combined should lead a rational wagerer to accept this God as the God worthy of worship.
Overall Pascal's argument shows not only that theism is rational, but that agnosticism/atheism is completely irrational.

All that remains for the wagerer is to decide how his life as a theist is to proceed.
Introduction:

The aim of these experiments was to determine the thermal conductivity of glass (experiment A), and in oil (experiment B).
Experiment A: Determination of the thermal conductivity of glass (k)

Objectives of experiment A
In experiment A two different equations for k were used.

The results for the first equation were compared to the second equation (the second calculation gave an approximation of the value of k).
An objective of the experiment was to conclude whether the second equation was a viable approximation to use.

To compare the calculated results to the given values of k in a text book.
And conclude whether the calculated results were comparable to the given text book values of k.

Procedure: Experiment A:
A glass apparatus was used which consisted of a glass tube through which heat was transferred by condensing steam to a stream of cold water for 60 seconds.

The rate of water flow passed via a constant head device so that it could be controlled at a constant value.
The value was varied 3 times, to give an appreciable temperature difference between T1 and T2.

Two measurements for each flow rate were recorded and an average flow rate was calculated (See table 1).
Calculations

Determination of the thermal conductivity of glass (k) for first flow rate
FORMULA Therefore FORMULA The thermal conductivity of glass = k FORMULA Therefore FORMULA Therefore FORMULA Therefore FORMULA

Calculations for the second flow rate of 1.21467 x 10‾³
FORMULA Therefore H for second flow rate FORMULA K can be found by FORMULA

Calculations for the third flow rate of 3.9084 x 10‾³
FORMULA Therefore H for the third flow rate FORMULA

Results table of the thermal conductivity of glass (k)
For thin walled tubes the heat transfer rate through the wall approximates to: FORMULA Therefore FORMULA So FORMULA Therefore k using the equation 3 FORMULA

Conclusion
The given values for the thermal conductivity of glass are 0.1-1.0 W m‾¹ K‾¹.

Hence this value can be compared to the equated values of k in this experiment and it can be seen that they fall into the correct parameters.
By comparing the results from equations 1 and 2, to the result from equation 3 (see table 3 below) From the results collected it can be confirmed that the approximation is a reasonable one to make in this case.

Experiment B: determination of the specific heat capacity of a liquid by method of cooling.
Objectives:

A calorimeter was used to collect cooling values of water and oil.(see table 2) These values were then used to produce cooling curves.(see graph 1) From these cooling curves the rates of cooling gradients were determined.
Using Newtons law of cooling which states that the rate of loss of heat from a body is directly proportional to the excess temperature of the body above that of its surroundings.

The value of the specific heat capacity C of the oil could be calculated.
Calculations of the rates of cooling

Using the information from graph 1 the cooling curve for water and oil determined from the same temperature of 65 ºC can be calculated For oil (dθ/dt)2 FORMULA Therefore FORMULA For water (dθ/dt)1 FORMULA Therefore FORMULA The specific heat capacity C can be calculated by the equation FORMULA So FORMULA Using the information from graph 1 the cooling curve for water and oil determined from the same temperature of 60 ºC can be calculated For oil (dθ/dt)2 FORMULA Therefore FORMULA For water (dθ/dt)1 FORMULA Therefore FORMULA The specific heat capacity C can be calculated by the equation FORMULA So FORMULA
Conclusion

From the graphical data: At 65 ºC the ratio of (dθ/dt)1 to (dθ/dt)2 is 8.63 x 10 ‾³ºC/s to 21.875 x 10 ‾³ºC/s or 1: 2.5 At 60 ºC the ratio of (dθ/dt)1 to (dθ/dt)2 is 0.010 ºC/s to 0.015ºC/s or 1: 1.5 The ratio of the rates of cooling is not the same implying some error in the method or data collection.
The calculated specific heat capacity of the oil is exactly twice the value calculated for 65 ºC at 60 ºC again implying errors have occurred.

In 2006 Nutrition students at undertook a food frequency questionnaire, designed to summarise their average weekly diet.
The food composition of these results was analysed and a summary of the student's diet compiled, to assess their nutritional status.

These results were then compared to the reference nutrient intake (RNI) based on recommendations by the Committee on Medical Aspects of Food Policy (COMA) and the National diet and Nutrition Surveys 2003.
The RNI is the quantity of essential nutrients and energy required by 98% of the population of the UK, to prevent deficiencies and reduce the risk of chronic disease.

According to the questionnaire female students lack the essential nutrient Iron, which is vital for energy production and the transfer of oxygen in the blood.
A deficiency can cause anaemia, the symptoms of which are tiredness, dizziness and possibly heart palpitations.

The RNI for women is 14.8 mg, the female student's average just 10.2 mg.
About 31% less than the RNI.

The RNI is only a guide as some women may actually need more depending on their menstrual losses.
The students have good amounts of calcium, magnesium, and phosphorus in their diets.

These minerals are found in bone and are vital to maintain good bone health and reduce the risk of osteoporosis.
The government recommends that we should aim for less than 2.5g of sodium in our diets (sodium is found in table salt).

The male students average 2.9g so they should aim to reduce this.
The female student's average consumption was a healthy 2g.

Potassium is essential for healthy nerves and muscles and fluid balance.
Female students fell short of the RNI.

The students also lacked fibre in their diets.
So an increase in fruit and vegetables would increase their fibre intake as well as the potassium in their diets.

Both the sexes had enough zinc, iodine and copper in their diets.
However they have less than the RNI for selenium.

Selenium intakes are difficult to assess, and levels are falling in the UK.
It is an essential nutrient and higher intakes may promote health, but in excess it is very toxic.

They also obtain good levels of the antioxidants vitamin C and E.
However according to the questionnaire they have less than the RNI for Vitamin A. Female students have about 6% less and males 12% than the RNI.

The RNI is only a guide line.
Some of the students will have enough in their diets even below the RNI.

Deficiency in this vitamin is unusual in the UK.
Around the world it is the main cause of non-accidental blindness.

The student's eyesight maybe at risk.
The students receive plenty of water-soluble B vitamins and Biotin, which help the body to convert the fuel it needs for energy.

Dietary energy comes from fat, carbohydrates, protein and alcohol.
The RNI for fat states that it shouldn't exceed more that 35% of the total energy intake, these students's have 31% in their diet.

Different fats release the same amount of energy but the effects that they have on our health vary.
The students obtain 14% of their total energy from saturated fats.

It is recommended that saturated fats should not exceed more than 10% of total energy.
The student's should aim to replace some of the saturated fat with polyunsaturated and monounsaturated fat.

The students gain too much energy from protein, and not enough from carbohydrates (sugars and starches).
They should increase the amount of carbohydrates in their diet from healthy sources.

The students surprisingly consume no more alcohol than the national average for adults.
Assuming all the student's are average in size and moderately active they are consuming just below the average energy requirements.

(However people often underestimate their calorie intake by up to 25%.!) The questionnaire results are as good as the information that was put in, and can only cover some foods.
There are more accurate methods of collecting data such as using diet diaries and photographic records.

Using the questionnaire results the students have a fairly typical UK diet.
Their diet is mostly balanced, but it is lacking in a few nutrients.

Introduction
The aim of this experiment was to analysis my diet over 3 days, by weighing all foods consumed and inputting the results into a computer data base (foodbase 3.1 standard Edition),and to assess this method of dietary analysis.

These results were then compared to known DRVs to highlight any deficiencies, or over consumption, and possible ways in which my diet could be improved.
Discussion

The DRVs for energy have been calculated to give the average energy requirements for any populations group, The EAR for my age and sex is 1940 kilocalories (DEPARTMENT OF HEALTH 1991).
Diary analysis showed that my intake falls short of the EAR for my age and sex by 314 kilocalories.

By increasing my energy consumption the shortfall in nutrients in my diet which are below the DRVs could be addressed.
However the EAR is only a guide.

My total energy expenditure will be my BMR, dietary induced thermogenesis, and the total amount of energy that I use in physical activity (DEPARTMENT OF HEALTH, 1991).
This will vary within the population dependant on factors such as sex, height, weight, age, physical activity.

A detailed analysis of my energy requirements were not assessed in this assignment.
My fibre intake for the 3 day period averaged 10.4 g per day this falls well below the DVR of 18g.

An increase in fibre could be achieved by eating pulses, wholegrain foods, and eating a greater amount of vegetables and fruit.
(WILLS, J. 2005) An increase in my energy intake with these foods will also increase the levels of iron and iodine, which fell below the DRVs.

Fortified wholegrain breakfast cereals would also provide a good source of iron and fibre.
My calcium intake was 547 mg an average of 153 mg less than the DRV.

To increase calcium in my diet I could include more dairy produce, which provides a good source of calcium, as well as dark green leafy vegetables and the soft bones of seafood if eaten whole, e.g. Sardines (BEAN, A.2003).
Advantages of the weighed method are that it is quantitative, portions can be measured accurately.

It is a widely used method and can be compared to other studies.
This method was found to have high reproducibility of results, which led to the assumption of the validity.

Measurements with different methods however gave different results.
With the use of doubly labelled water it was found that individuals often under report their energy intakes, especially if they have a high BMI.

Hence care should be taken to consider sources of error and how they may alter the results of using any diet assessments (GEISSLER & POWERS,2006)
Conclusion

The 3-day weighed dietary survey gave an insight into my diet; however it did not cover a long enough period to assess my diet fully.
It was inconvenient to weigh my food.

Some of the foods that I consumed were not on the data base and I had to add the nearest equivalent.
The data can only be assumed to be valid if my collection and processing of the contents of my diet was accurate.

Even with taking great care there is still room for margins of error.
T HEORY

It is thought that water masses in the oceans are transported around the World only by the currents driven by the wind.
This is, however, not the case.

Waves produced at the ocean's surface also help to transport water masses.
The effect of waves on the transport of water masses is comparatively subtle.

However, sometimes the wave driven component can be as large as that of the wave driven component.
The average effect of the wave driven component of transport is referred to as Stokes drift and the following project will investigate certain aspects of Stokes drift, in particular characteristics such as wave amplitude and phase speed.

T HEORETICAL S ETUP
A body of water can move three dimensionally (i.e. in the x-, y- and z- directions) but in this investigation, for simplicity, the motion of a mass of water will be considered in one-direction, the x-direction.

When a body of water is transported by a velocity field, u, the x-coordinate of the body is defined by: FORMULA The velocity field, u, is a function of two components.
These components are the wind driven component and the wave driven component which is periodic in space and time.

The velocity field is given by: FORMULA Where u0 is the amplitude of the wind driven component; u1 is the amplitude of the wave driven component; c is the phase speed; k is the horizontal wavenumber and t represents time.
The velocity field due to the waves is u1cos[k(x-ct)] and because of this, a body of water is moved by the wave field systematically ( Stokes drift) and it is this that this project will investigate.

T HE P ROJECT
The above equations cannot be solved analytically, therefore the motion of a body of water needs to be solved using a computer program.

The computer program used will be MATLAB.
An explicit scheme will be used to solve the equations.

This is also known Euler forward time-stepping scheme.
Using the two stated equations stated previously and the explicit scheme, the following equation was derived.

It calculates the next value of x (xn+1) by using the previous value of x (xn): FORMULA It is proposed that a body of water is released from x(0)=0, with parameters of u0 = 1.0, u1 = 0.3, c = 2.0 and k = 1.0.
The equation is integrated from t = 0 to t = 50 with the time-step set at 0.1.

The following is the MATLAB computer code created to solve the previous equation with the previously stated parameters: FORMULA This MATLAB computer program was executed and the following graph (Figure 1) is a plot of the x-coordinate as a function of time: The line on this graph represents the drift velocity, u. One would expect that the wind driven component u0 alone would give a drift of 1.0 or in terms of graphical representation, a straight line with a gradient of 1.
However, it is clear to see from Figure 1 that the actual drift of the body of water is greater than the drift expected from the wind driven component u0.

Therefore, this difference must arise from the velocity field due to the waves, u1cos[k(x-ct)] or Stokes drift.
To find the magnitude of u1cos[k(x-ct)] or Stokes drift, the gradient of the line (velocity field, u) in Figure 1 must be calculated.

The value of x at t = 50 was found by typing 'xarr(500)' in the MATLAB prompt screen: FORMULA 1.041382 is the value of the velocity field with parameters that are stated beforehand.
Since the wind driven component equals 1.0, then u1cos[k(x-ct)] or Stokes drift is the difference between these values.

Therefore u1cos[k(x-ct)] = 1.041382 - 1 = 0.041382.
This now tells us that the magnitude of Stokes drift is equal to 0.041382.

The computer program is then run again, but with the phase speed, c, set to zero.
The following graph (Figure 2) is the outcome: It can be seen that Figure 1 is very similar to Figure 2, however, when t = 50, u < 50, unlike Figure 1 when at t = 50, u = 50.

This change in u can be calculated.
The gradient of the line in Figure 2 is calculated in the same way as it was for the line in Figure 1.

Once again, the value of x at t = 50 was found by typing 'xarr(500)' in the MATLAB prompt screen: FORMULA A value of 0.950002 was calculated.
This now tells us that u1cos[k(x-ct)] or Stokes drift is now negative.

The value of Stokes drift is therefore u1cos[k(x-ct)] = 0.950002 - 1 = -0.049998.
This tells us that by setting the phase speed to zero, Stokes drift becomes negative.

Although the value of Stokes drift is negative, the magnitude is 0.049998 but in the opposite direction to the wind velocity.
In the previous example, the amplitude of the wave field, u1, is set to 0.3.

The following experiment will vary u1 and see how the drift changes.
The value of u1 will start at 0.0 and increase to 0.9 in steps of 0.05.

The integration interval will also be increased up to t = 500, as this should give us a better estimate of the drift.
It should also be noted that the phase speed, c, is still set to zero.

Table 1 represents the findings: The following graph (Figure 3) is representation of the MATLAB code when u1 = 0.0 and c = 0.0.
It is clear to see in Figure 3 that when u1 = 0.0 with the phase speed set to zero, the term u1cos[k(x-ct)] becomes zero and u = u0.

Therefore the d x/d t is 1 and Stokes drift is non-existent.
However, when u1 = 0.9, the plot of x as a function of time changes markedly (Figure 4): It is evident so see now, that the affect of u1 increasing has reduced the amount the body of water had traveled.

This tells us that Stokes drift increases in magnitude (but in the opposite direction to the wind) as the wave driven component increases.
This means that the waves restrict the transport of water if the phase speed is set to zero.

By plotting a graph (Figure 5) of Stokes drift against the wave driven component, u1, it is possible to see graphically how the drift varies with the amplitude of the wave field: It is obvious to see that as the wave field increases, Stokes drift decreases.
According to Figure 5, this change is not linear; as u1 increases, Stokes drift decreases exponentially.

A theory exists for Stokes drift.
It states that for small amplitudes of the wave field, u1, the total drift of the body of water relative to the drift due to the wind field, u0, is approximately equal to: FORMULA In other words: FORMULA In the computer model stated previously, this theory can be proved.

For example, when u1 = 0.05, c = 0 and u0 = 1: FORMULA This theory approximates a value of 0.99875.
The computer model calculated a value of 0.998583.

Therefore it is clear to see that this difference is very small and that the theory holds up.
However, when the amplitude of u1 increases, the theory begins to deteriorate.

For example, when u1 = 0.9, c = 0 and u0 = 1: FORMULA A value of 0.595 was approximated, whereas the computer model calculated a value of 0.4341838.
This proves that the theory only holds for small values of u1.

The velocity field, u, is given by the equation: FORMULA Therefore udrift varies according to u1cos[k(x-ct)] if u0 equals 1.
If the phase speed, c, is above zero, then u1cos[k(x-ct)] or Stokes drift will fluctuate between being positive and negative.

If d x/d t is greater than 1, then Stokes drift is positive by the amount of u1cos[k(x-ct)].
If d x/d t is less than 1, then Stokes drift is negative by the amount of u1cos[k(x-ct)].

This shows that the gradient of the velocity field is dependant on the phase speed.
If the phase speed is zero, the gradient will decrease therefore Stokes drift becomes negative.

Referring back to Figure 5, as u1 increases, Stokes drift decreases or Stokes drift increases, but negatively.
Physically this tells us that when Stokes drift is negative, it acts in such a way to pull the body of water back in the opposite direction of the wind field and that when Stokes drift is more negative, this effect is greater.

This means that the transport of water by the motion of waves is restricted by the larger values of u1.
The opposite is also true.

When Stokes drift is positive, the waves actually help to push the body of water along, increasing the net transport of water.
However, as previously discussed, for this to be the case, the phase speed must be above zero.

For waves to play a role in the transport of water, Stokes drift has to be positive.
For this to be the case, the amplitude of the wave field, u1, has to be relatively small and the phase speed, c, greater than zero.

It should also be noted that if over a certain time period, if Stoke drift is negative more than it is positive, the total drift will be less than the drift of the wind field alone.
The opposite is true in that if Stokes drift is more positive than it is negative, then the total drift will be greater than the drift of the wind field alone.

For efficient transport of water, u1 must be comparatively small because with larger values of u1, a body of water will not move as far as with a lower value of u1.
PART A: THE BRACKETING METHOD

T HEORY
The bracketing method is a simple, iterative method.

It is used to find the root or roots of a function, or in other words, when a particular function is equal to zero.
The method works by choosing two values, for example x1 and x2 in which the root of the function or zero must lie between these two values.

If zero does not lie between x1 and x2, then the method simply will not work.
Since x1 and x2 are either side of zero, then x1 must be less than zero (i.e. negative) and x2 must be greater than zero (i.e. positive).

The value of the function exactly half-way between x1 and x2 is then calculated, this point is called xmid.
If the value of the function at xmid is greater than zero then the function must have a root that lies somewhere between the interval x1 and xmid.

However, if the value of the function at xmid is less than zero then the function must have a root that lies between xmid and x2.
By doing this, the range or interval in which the root of the function lies is halved and the solution for when the function equals zero is a step closer to being found.

The next stage is simply a repetition of what has just been done.
Assuming the value of the function at xmid is greater than zero, then the root must lie between xh and x1.

The value of the function exactly half-way between xmid and x1 is then calculated.
The interval has now been halved again and is a quarter of its original size.

This process is repeated until the root or roots are found.
These iterations half the interval at each step and thus "close in" on the solution.

If the root is not an integer, then the process could go on indefinitely, unless a degree of accuracy, such as 5 decimal places is stated initially.
T HE P ROJECT

This project will use the bracketing method to solve the equation; FORMULA The starting interval or range for the first iteration will be 20, 10 either side of zero.
Therefore the lowest value of x or xlow will be -10 and the highest value of x or xhigh will be 10.

This starting interval has been chosen because it is quite large and includes both positive and negative values.
This will hopefully mean the existing root will be found within this interval.

Choosing a smaller interval could mean the solution may exist out of the interval whereas choosing a larger interval such as 100 (50 either side of zero) may simply not be needed as it is too large.
The values of xlow and xhigh are also of equal magnitude.

In order to solve the equation stated previously, the equation needs to be rearranged so it is equal to zero, this gives; FORMULA This equation is then put into the MATLAB computer program to be solved.
Two computer programs have been created to solve the equation.

The first program defines the equation equal to zero and the second program uses the result of the first program to find the solution to the equation using the bracketing method.
(Both of these computer programs can be found in the appendix).

The number of iterations used in the computer program needs to be specified too, a value of 30 will be used as this should a sufficient amount of iterations for a solution to be found.
When both computer programs were run, a solution was found.

Below is a graphical illustration of the rate of convergence produced by the program (Figure 1).
The red line above represents the xlow value and the blue line represents the xhigh value.

It is clear to see that initially xhigh and xlow are apart by an interval of 20, but as the number of iterations increases, they converge and the root of the equation is eventually found.
It is clear to see that the rate of convergence for xlow and xhigh is not the same.

xhigh appears to converge almost exponentially, whereas xlow seems to converge abruptly and in steps.
Although for both, convergence is somewhat rapid, especially considering that the solution was found to 4 decimal places and the starting interval was 20.

It is also evident that the solution was found in less than 30 iterations.
The solution to the initial equation using the bracketing method has been calculated at 0.5672 to 4 decimal places.

The minimum number of iterations required to obtain the solution from the starting interval is 22 iterations.
So after this number of iterations, the size of the interval was reduced by a factor of 4194304.

However, it should be noted that the xlow value converged on the correct solution after only 18 iterations, whereas the xhigh value converged on this value after 22 iterations, therefore on the 22 nd iteration, the xlow value was equal to the xhigh value.
C ONCLUSION

By using the bracketing method and creating a computer program using MATLAB, the root of the following equation has been found; FORMULA A solution of 0.5672 to 4 decimals places was found with the starting interval was set at 20, -10 to +10.
This shows that the chosen starting interval was adequate, if not too large and that a smaller starting interval, such as 10 or even 2 would have been suitable.

The solution was obtained after 22 iterations.
This shows that the value of 30 iterations specified in the computer program was sufficient to find the correct solution.

This rate of convergence is linear as at each step the size of the interval halves.
The accuracy also increases by a factor of 2 at each iteration.

Although the rate of convergence is graphically represented in Figure 1, due to the poor resolution it appears the solution was discovered at about the 13 th iteration rather than the 22 nd iteration.
Although the bracketing method found the solution after 22 iterations, some may argue that 22 iterations is too much and takes too long, however the advantage of using the bracketing method is that if the root of the function being investigated is within the starting interval, a correct solution will always be found.

The bracketing method is therefore unconditionally convergent.
A PPENDIX

FORMULA
PART B: ATMOSPHERIC TRAJECTORIES

T HEORY
The Earth's atmosphere is not uniform.

It is dynamic and is characterized by jet-streams and eddies.
In the atmosphere, when waves reach large amplitudes they break and eddies are formed.

Sometimes, this may occur at critical layers in the atmosphere where the phase speed of the wave matches the speed of the jet-stream.
If a small-amplitude Rossby wave is formed and hits its critical layer, the atmospheric flow that is formed can be described by a stream-function known as the Stewartson-Warn-Warn stream-function.

In two-dimensions, this is given by; FORMULA The flow generated by the stream-function gives rise to a pattern of streamlines known as the 'cat's eye pattern'.
It contains eddy regions in which the trajectories are closed and stream regions in which air parcels follow trajectories that are not closed.

The velocity of the two-dimensional flow within this Stewartson-Warn-Warn stream-function is given by (u component); FORMULA and (v component); FORMULA
T HE P ROJECT

Using the software MATLAB, a computer program has been created to calculate and illustrate the trajectory of an air parcel carried along by the flow of the Stewartson-Warn-Warn stream-function.
An explicit forward scheme has been used to calculate the first step of the flow.

For all subsequent steps a leapfrog scheme including a Robert-Asselin filter has been used in the program.
(The computer program for this can be found in the appendix).

The air parcel will start from position (x0,y0) at time t=0.
The computer program has been run to calculate the trajectory of the air parcel from time t=0 to time t=20 using a time-step of 0.01.

A value of 0.01 has been used for the Robert-Asselin filter coefficient.
The trajectory of an air parcel within the eddy region of the stream-function has been simulated first.

The starting positions for this is (x0,y0)=(1.0,1.0).
The following (Figure 1) is the plot of the trajectory of an air parcel in the eddy region in the (x,y) plane: It is clear to see that with the initial position of (x0,y0)=(1.0,1.0), the path an air parcel would follow in the eddy region is closed and is circular.

This therefore verifies the numerical stability of the schemes used as one would expect a closed, circular flow according to the exact solution.
The program was then run again, but with the starting positions (x0,y0)=(1.0,2.0).

This minor adjustment of the y-value initial position will now simulate the flow within the stream region.
The following (Figure 2) is the plot of the trajectory of an air parcel in the stream region in the (x,y) plane: It can be seen that by slightly altering the initial starting position for just the y-component, the flow an air parcel would follow if it were under these conditions in the stream region has dramatically changed.

The flow is no longer closed or circular.
Instead it is sinusoidal and periodic.

The stream function just simulated is time-independent resulting in the trajectories being non-chaotic.
In reality, however, the trajectories may become chaotic if a weak time-dependent perturbation is added to the Stewart-Warn-Warn stream function.

This time-dependent perturbation can be characterized by: FORMULA This therefore means the streamfunction now becomes: FORMULA The addition of time to the streamfunction will now affect the velocity fields.
It can be seen that the time-dependent perturbation is only dependent on the x-component and not the y-component.

This now means that the velocity fields of the flow are now given by (u FORMULA component); and (v component); FORMULA The v component was derived by differentiating FORMULA with respect to x.
The original program has now been modified to include the changes to the velocity fields due to the weak time-dependent perturbations.

(The modified program can be found in the appendix).
The air parcel will again start from position (x0,y0) at time t=0.

The computer program has been run to calculate the trajectory of the air parcel from time t=0 to time t=20 using a time-step of 0.01, the same as before.
The following values for the weak time-dependent perturbation have been used; ε=0.1, k=1.0 and c=3.0.

The trajectory of an air parcel will begin at the starting position (x0,y0)=(1.00,1.70).
The following (Figure 3) is the plot of the trajectory of an air parcel at this starting position in a Stewartson-Warn-Warn streamfunction with a weak time-dependent perturbation; Figure 3 illustrates that with the addition of a weak time-dependent perturbation to the streamfunction, the path followed by an air parcel is chaotic and not what would be expected.

The trajectory is simulated again, but with slight adjustment to the starting position.
The starting position is now (x0,y0)=(1.00,1.71).

The following (Figure 4) is the plot of the trajectory of an air parcel at this starting position in a Stewartson-Warn-Warn streamfunction with the same weak time-dependent perturbation; It is evident to see that although the starting positions illustrated in Figure 3 and Figure 4 are very close together, when the computer program with the weak time-dependent perturbation was executed, the outcomes are completely different.
A difference of only 0.1 for the y value has a huge effect.

This shows that by adding a weak time-dependent perturbation, a system can become chaotic if the initial conditions varies only slightly.
When the starting position is (x0,y0)=(1.00,1.70), the co-ordinates of x and y at time t=20 are x=9.8014 and y=0.7130, this can be seen in Figure 3.

When the starting position (x0,y0)=(1.00,1.71), the co-ordinates of x and y at time t=20 are x=-2.5713 and y=-0.4908, this can be seen in Figure 4.
These different values again emphasize that with the addition of a weak time-dependent perturbation to the streamfunction, a minimal variation in initial conditions can have a chaotic effect resulting in completely different values at some point in the future.

C ONCLUSION
The trajectory of an air parcel carried along by the flow of the Stewartson-Warn-Warn streamfunction has been illustrated with varying initial conditions and with the inclusion of a weak time-dependent perturbation.

If the streamfunction is considered independently from time, then depending on what the initial co-ordinates are, the flow or trajectory of an air parcel under these conditions will either be closed, i.e. the eddy region or periodic and not closed, i.e. the stream region.
This variation can occur with minor adjustments to the initial co-ordinates.

The pattern created by this streamfunction forms a 'cat's eye pattern'.
When a weak time-dependent perturbation is added to the streamfunction, the pattern formed is irregular and chaotic.

Figures 3 and 4 both illustrate that by fractionally adjusting the initial conditions, the co-ordinates of the flow at some future time point are different.
These simple simulations illustrate how unpredictable and chaotic the Earth's atmosphere behaves and how difficult weather prediction is.

Objectives:
Using measured wind speed data from different anemometers, fit a log-profile law approximation.From this data, obtain estimates of friction velocity, surface drag and the surface roughness length.By using the aerodynamic method, estimate the surface sensible heat flux using measurements of temperature at two heights.Finally, using the surface energy budget equation, calculate the surface latent heat flux.

Experimental Method:
The apparatus used in the experiment included;

eight pulse anemometers mounted on a mast at various heights, with each anemometer producing electrical pulses at a rate which is proportional to its speed of rotation.two mercury-in-glass thermometers that were mounted on the mast.stop-clock.
Before the IOP (Intensive Observation Period) began, the two thermometers mounted at the same height were calibrated by taking a number of simultaneous readings, from this, a systematic offset was calculated.

One of the thermometers was then placed at a height of 2.0m with the other one remaining at a height of 0.2m.
After waiting at least 3 minutes for the effect of thermometer lag, the IOP began.

During this period (20 minutes), readings on both thermometers were recorded at 2 minute intervals.
The tendency for any of the anemometers to stop rotating or stalling and changes in weather conditions was also noted.

At the end of the IOP, the total number of counts was recorded, along with the total elapsed time.
Data Obtained:

Date of Experiment: 23/06/06 (Mast 1 was used in the experiment.)
Initial Weather Conditions;

Calibration of the thermometers prior to the IOP:
A sample period of ten minutes was used to record the temperature of the two thermometers mounted at the same height.

FORMULA Therefore the mean measured offset: FORMULA
Data obtained during the IOP:

The total elapsed time recorded was 19:59:59 on the stop-clock, therefore FORMULA Due to the calm conditions experienced during the IOP, all the anemometers stalled at least once.
The following table shows how often each anemometer stalled during the twenty minutes of the IOP:

Calibration of the thermometers after the IOP:
A sample period of eight minutes was used to record the temperature of the two thermometers mounted at the same height after the IOP to check whether the thermometers were still calibrated and whether the offset had changed significantly.

FORMULA Therefore the mean measured offset: FORMULA The offset has only changed by 0.01 ˚C compared to before the IOP and therefore it can be assumed that the calibration of the thermometers has not changed.
The total number of counts recorded for each anemometer is given in the following table:

Analysis:
The mean wind speed, U is given by; FORMULA Anemometer 1; FORMULA Since the anemometers are accurate to within FORMULA and U is greater than 0.2 ms -1, the accuracy of U is FORMULA.

Therefore, FORMULA Anemometer 2; FORMULA Accuracy of U is FORMULA.
Therefore, FORMULA Anemometer 3; FORMULA Accuracy of U is FORMULA Therefore, FORMULA Anemometer 4; FORMULA Accuracy of U is FORMULA.

Therefore, FORMULA Anemometer 5; FORMULA Accuracy of U is FORMULA.
Therefore, FORMULA Anemometer 6; FORMULA Accuracy of U is FORMULA Therefore, FORMULA Anemometer 7; FORMULA Accuracy of U is FORMULA Therefore, FORMULA Anemometer 8; FORMULA Accuracy of U is FORMULA Therefore, FORMULA By plotting a graph of U against ln(z), a logarithmic profile of the wind can be obtained.

A graph has been constructed showing U against ln(z).
A straight line of best fit has been fitted.

Estimating the gradient of the line of best fit, β:
By constructing a line of best fit, it is possible to calculate the gradient of the slope.

From the line of best fit; FORMULA To estimate the uncertainty in β, a maximum gradient has been constructed on the graph.
From the maximum gradient; FORMULA Therefore the uncertainty in the slope is FORMULA Since FORMULA where; FORMULA then the frictional velocity, FORMULA.

Therefore FORMULA.
The accuracy of β is FORMULA, therefore the accuracy of u* is; FORMULA So, FORMULA.

Estimating the surface drag, τ:
The surface drag, τ, is given by; FORMULA where; FORMULA Therefore FORMULA The accuracy of τ is proportional to the accuracy of FORMULA 2, however FORMULA.

Therefore the accuracy of τ is; FORMULA Therefore; FORMULA So, FORMULA
Estimating the aerodynamic resistance, ra, of the layer between 0.2 and 2.0 metres:

In order to calculate the aerodynamic resistance, the difference in mean wind speed, FORMULA, between the two heights must be known first; FORMULA Where; FORMULA At 2.0m, FORMULA can be approximated by: FORMULA and similarly, at 0.2m, FORMULA can be approximated by: FORMULA where; FORMULA Combining these two equations cancels FORMULA and gives; FORMULA Therefore, FORMULA From this, r a, can be estimated; FORMULA The uncertainty in r a is given by; FORMULA Therefore, FORMULA So, FORMULA The mean of the measured difference in temperature between FORMULA, is; FORMULA Therefore, FORMULA Standard deviation is given by; FORMULA The standard deviation for FORMULA FORMULA Therefore, FORMULA Standard error is given by; FORMULA The standard deviation for T 2; FORMULA Standard error is given by; FORMULA
Calculating the sensible heat flux, H:

The surface sensible heat flux can be estimated using; FORMULA where; FORMULA Therefore, FORMULA By combining the errors in r a and mean temperature difference, it possible to estimated the measurement error in H. Combining the standard error for FORMULA and FORMULA gives 0.117ºC, multiplying this by ρ and c p gives 140.967.
This value is the uncertainty of FORMULA.

Whereas the actual value of FORMULA is (1.2)(1004)(1.26) = 1518.048.
The value of r a and its uncertainty is FORMULA.

Therefore the error in H is; FORMULA Therefore, FORMULA From the other group, a value of FORMULA was calculated.
This value was; FORMULA The surface energy balance equation is given by; FORMULA Where; FORMULA Therefore the FORMULA is given by; FORMULA The experimental accuracy of FORMULA is; FORMULA

Estimating the Bowen Ratio:
The Bowen Ratio, B 0 is given by; FORMULA Therefore, FORMULA The accuracy of B 0 is given by; FORMULA Therefore, FORMULA

Estimating the roughness length, z0:
From the measurements of the wind profile, it is possible to estimate the roughness length.

The roughness length is given by; FORMULA From the graph, FORMULA The accuracy of z 0 is given by; FORMULA Therefore, FORMULA So, FORMULA
Conclusions and Discussion:

It can be seen that when studying the graph of FORMULA against ln(z), that all the points are linear, except the two middle values that lie slightly above the line of best fit.
Therefore the measurements taken do fit the logarithmic wind profile law.

It appears that the two middle values are inaccurate and the other values are more accurate.
However, when all these values for each anemometer are compared with how often each anemometer stalled, it becomes clear that the middle values would be more accurate as these anemometers stalled less frequently, and that all the other points are in fact out of line with the middle two values as these points represent anemometers that stalled frequently during the IOP.

This means that if the anemometers stalled less frequently, the line of best fit would be somewhat higher than it is on the graph.
It appears that the scatter of measurements about the log-profile are not that significantly larger, except the middle two values, than what might be expected due to measurement error, especially considering that all of the anemometers stalled at least twice during the IOP.

A value for z 0 of 2.37 x 10 -3 m was determined from the IOP results.
This is 0.237cm and represents the roughness length for the grass length in the Atmospheric Observatory.

The Monteith and Unsworth value for short grass is 0.1cm, since the grass in the Atmospheric Observatory is not that short, it can be said that the value obtained from the experiment is comparatively accurate, even more so if the error in it's value is considered.
The observed lapse rate was 1.26 K per 1.8 m near the surface, which equates to 700 K per km.

The DALR is approximately 10 K per km, therefore the observed lapse rate near the surface was much greater than the DALR.
During the analysis of this experiment, it was assumed that K H =Kμ due to near-neutral conditions.

However, during the IOP, non-neutral conditions were experienced where a strong temperature gradient was found near the surface.
This was due to eddies of warm air existing near the surface and not mixing with the cooler air above.

This was enhanced by the light wind conditions.
The eddy correlation method estimated a value of 40.00 Wm -2 for H, whereas the profile method predicted a value of 23.05 Wm -2.

The value estimated by the profile method is approximately half the value estimated by the eddy correlation method.
Since H is inversely proportion to r a, if rh is roughly half of ra, then H will be double.

This is what was found during the IOP.
Global warming is simply defined as the steady increase in the Earth's surface temperature.

It is considered to be primarily the result of a phenomenon known as the 'greenhouse effect'.
The Earth's atmosphere is composed of gases, as shown in Table 1.

Some of these gases have fixed proportions, such as nitrogen and oxygen, whilst it is the greenhouse gases that vary in proportion.
The 'greenhouse effect' is where solar radiation becomes trapped in the Earth's atmosphere making the atmosphere warmer.

On the whole, the amount of solar radiation absorbed by the Earth and the amount reflected back into space balance each other out, keeping the Earth's surface temperature constant.
Naturally occurring greenhouse gases in the atmosphere such as carbon dioxide, methane and water vapour absorb long wave terrestrial radiation (reflected, short wave solar radiation), thus acting to keep the Earth warm at an average surface temperature of 15ºC.

It is only when concentrations of such gases increase that more radiation can be absorbed.
This increased capability of the Earth's atmosphere to absorb more radiation has led to a term known as the, 'enhanced greenhouse effect'.

This means that the Earth retains more heat than is being radiated back into space, resulting in global warming.
It is known that in the Earth's history, the average temperature of the Earth has fluctuated constantly, resulting in periods of ice ages and warmer interglacial periods.

It appears that the most recent ice age reached its maximum approximately 20,000 years ago, and that the Earth is now in a warmer interglacial period.
However in the past century or so, it has been apparent that the Earth is experiencing an unprecedented rate of warming (Graph 1), and evidence collected suggests that human society may be to blame.

Approximately 200 years ago, Great Britain became the first country to industrialise.
The primary cause for such rapid growth in industry was the utilisation of fossil fuels.

Fossil fuels include coal, natural gas and oil, and it was the burning of these, especially coal that literally powered the Industrial Revolution.
Since then, much of the world has industrialised, burning even more fossil fuels.

Nowadays, fossil fuels supply over 80% of the World's energy needs.
However, the burning of such fossil fuels is having serious consequences.

When a fossil fuel is burned it releases greenhouse gases.
The most important of these gases is carbon dioxide.

Carbon dioxide is a greenhouse gas that strongly absorbs infrared radiation.
On average it comprises only 0.0003% or 365ppmv (parts per million of volume) of the atmosphere, therefore it can be seen that not much carbon dioxide released into the atmosphere is required for this value to change significantly.

Levels of carbon dioxide have steadily risen in the atmosphere in recent times (Graph 2), levels that have not be seen on this planet for millions of years and this has had the consequence of warming the atmosphere, in particular the lower atmosphere.
A further consequence of this is that as the air near the surface warms, more water will evaporate into the air from the oceans increasing the water vapour content of the air.

The excess water vapour, which itself is the most abundant greenhouse gas, will hence increase the greenhouse effect and accelerate global warming even further.
This is known as the water vapour greenhouse effect feedback.

Methane is another greenhouse gas.
It currently has a concentration of 1.745 ppmv3, considerably less than carbon dioxide, but is rising at a rate of 0.6% per year.

For the last two thousand years up till 1800 however, this value was only 0.7 ppmv4.
It is a damaging greenhouse gas that is increasingly becoming more of a concern because the enhanced greenhouse effect caused by methane is about 7-8 times that of carbon dioxide4.

Methane can form naturally, such as in marshlands, but the man-made sources include oil wells, coal mining and agricultural activities.
Agricultural activities are the main source of methane and are from sources such as rice paddy fields, enteric fermentation from cattle and other livestock.

Rice forms the staple diet for almost 50% of the world's population and because the world's population is increasing exponentially the production of rice in paddy fields is increasing rapidly too.
This can therefore only mean that concentrations of methane will continue to increase, exacerbating the greenhouse effect.

One other, yet minor greenhouse gas is nitrous oxide (laughing gas).
It forms naturally, but can also be man-made as well.

Sources include fossil fuel combustion, nitrogenous fertilisers and animal waste.
On average it has a concentration of about 0.3 ppmv in the atmosphere.

When compared to carbon dioxide, this amount is minimal, but nitrous oxide is thought to be responsible for at least 6% of the greenhouse effect so far2, with levels of this gas rising about 0.25% per year1.
It is considered to be of great concern because the lifetime of nitrous oxide is about 120 years1, which means that this gas will remain effective for many years to come.

Another greenhouse gas is Chlorofluorocarbons (CFCs).
They are man made chemicals that because of its special properties was widely used in items such as refrigerators aerosol spray cans.

Although there has been a complete ban on using CFCs, the effect of this gas when it was in use on the atmosphere was enormous.
The average concentration of CFCs is currently about 1ppbv (parts per billion by volume)1, this is incredibly small, even compared to nitrous oxide, but a single CFC molecule is 10,000 times more effective at absorbing heat than a carbon dioxide molecule and is why CFCs are a significant enhancer or the greenhouse effect.

CFCs also have a long lifetime and destroy ozone, a gas found in the stratosphere.
Although this gas exists in very small concentrations, 0.02-0.07 ppmv, it protects the Earth from harmful radiation from the Sun.

Deforestation also plays a role in enhancing the greenhouse effect.
It's responsible for roughly 25% of all carbon dioxide emissions entering the atmosphere, by the burning and clearing of approximately 34 million acres of trees per year.

Trees are capable of storing vast amounts of carbon dioxide, an acre alone of forest can store up to 2.8 tonnes of carbon dioxide.
When forests are cleared, more carbon dioxide enters the atmosphere as less is absorbed, thus enhancing the greenhouse effect.

Over the past century, there is evidence to suggest that global warming is happening.
It has been calculated using historic records that the average surface temperature of the Earth has raised by 0.6 ºC in the last 100 years.

This is clearly seen in graph 1.
Although this increase seems negligible, it is a significant rise for such a small time.

It has also been recorded that 8 of the 10 warmest years since 1860 have occurred during the last decade.
This occurrence is remarkable and indicates that the Earth is warming at an alarming rate.

Warming of the atmosphere has equally resulted in the warming of the oceans.
As the oceans warm up, they expand, causing sea levels to gradually rise.

In the last 100 years, the global sea level has risen by about 10 to 25 cm (Graph 3).
The problem is intensified by melting ice caps at both the Arctic and Antarctic.

To date, Arctic sea ice has shrunk by about 250 million acres.
Ice melting on such a large scale adds vast amounts of water into the oceans, accelerating sea level rise that threaten many low lying coasts across the world from serious flooding.

More frequent, extreme weather events are another indicator that global warming is taking place.
The warming of the atmosphere and the oceans has in some regions of the world has enhanced the hydrological cycle.

This has produced more intense storms and hurricanes, extensive flooding and in some parts of the world, severe drought.
It appears that extreme events that normally occur rarely are becoming more common.

It has also been recorded that the majority of all the world's glaciers are retreating.
The rise in lower atmospheric temperature is melting the ice of the glaciers causing them to shrink.

The Grinnell Glacier, USA, has already retreated by 90%.
This is similar across the world.

Further evidence of global warming can also be found in ice cores.
Analysis of ice cores taken from the Dunde Ice Cap in the Qilian Mountains, northwest China indicate that the years since 1938 have been the warmest in the last 12,000 years.

The ocean's coral ecosystems are also being harmed.
Coral is being bleached by rising ocean temperatures as the water is simply too warm for the coral to survive.

Higher temperatures are also reducing the calcium content of the water, limiting coral growth.
It has been estimated that 25% of the world's coral reefs have already been destroyed by global warming and pollution3.

During the past century, concentrations of greenhouse gases such as carbon dioxide, methane and water vapour have risen sharply in the atmosphere enhancing the greenhouse effect.
As a result of greater concentrations of these gases in the atmosphere, more radiation from the sun becomes trapped thus acting to warm the earth.

The last 100 years has experienced a rate of warming never witnessed before and this global warming is proving to have consequences for both the environment and human society.
Sea levels are rising, the world's weather is changing and with the world's population rapidly increasing, this problem is set to continue.

SUMMARY
The following paper presents the results of an Intensive Observation Period in which components of the surface energy balance are measured within a meteorological enclosure.

Three experiments are used to quantify the surface energy balance.
From this, values for the sensible heat flux density, latent heat flux density and Bowen Ratio are derived.

The sensible heat flux density is estimated using two methods.
The eddy correlation method calculated values of 69.8 Wm -2 and 40.0 Wm -2, whereas the profile method calculated values of 26.7 Wm -2 and 24.3 Wm -2.

These values are used to calculate the latent heat flux density using the surface energy balance equation.
It is found that the results attained are influenced with errors from many sources and that these errors are difficult to identify.

The accuracy of the profile method is affected by strong temperature gradients existing near the Earth's surface.
The accuracy of the eddy correlation method is reliant on sampling frequency, instrument lag and calibration accuracy of the instruments.

The variations in atmospheric conditions and human error also introduce error into the calculated values.
INTRODUCTION

The Earth's surface and the overlying atmosphere are constantly exchanging energy between one another.
The understanding of this dynamic process between the surface and atmosphere is of great importance.

Energy is transferred via radiative components such as short-wave radiation and long-wave radiation, as well as non-radiative components, such as latent and sensible heat fluxes.
The exchange of energy is what drives many processes on Earth, for instance, the growth of vegetation through photosynthesis and weathering of rock through diurnal temperature change.

However, processes such as the heating of the air and ground and the changing state of water are crucial processes and need to be understood as they are key to understanding the physical mechanisms that directly affect the weather on a micro-scale and therefore weather forecasting.
The exchange of energy occurs through four main processes; the absorption and emission of electromagnetic radiation; thermal conduction of heat within the ground; transfer of thermal energy directed away or towards the surface, the condensation of moisture in the atmosphere onto the surface or evaporation of water stored in the soil.

These four processes collectively illustrate the surface energy exchange.
To gain quantitative understanding of the surface energy exchange by these four processes, a representation of the surface energy balance is required.

This will be made possible with the use of a variety of apparatus within a meteorological enclosure to measure radiative and non-radiative variables.
The key objectives in this analysis of the surface energy balance are to derive values of the sensible heat flux density (H), the latent heat flux density (λE), and the Bowen Ratio (B0).

The sensible heat flux is defined as the rate of heat absorbed or transmitted by a substance during a change of temperature which is not accompanied by a change of state ( URL ).
Whereas, the latent heat flux is the rate of heat released or absorbed by the change of state of water into the atmosphere.

The Bowen Ratio is simply defined as the ratio of sensible heating to latent heating.
H, λE and B0 are fundamental variables and are very useful in analysing the exchange of energy at the Earth's surface.

Values of H, λE and B0 were calculated using different methods and then a comparison is made of these values with respect to the method used.
From this, the surface energy balance estimated by the different methods can be compared with their sources of error.

In using different methods to calculate the same variable, it was found that different values were estimated, this was perhaps due to variations in the method, differing accuracies of the apparatus used, human error and possibly other sources of error.
Therefore, this paper will take into account all probable sources of error, as well as the accuracies of H, λE and B0 and if possible, where such uncertainties arose from.

The derived values for the sensible heat flux, latent heat flux and the Bowen Ration will then be related to the weather conditions present at the time of the experiments and whether the obtained results are what would be expected in such atmospheric conditions.
This short paper represents key aspects of the surface energy balance examined during the morning of 19 th June 2006 within a meteorological enclosure.

METHOD
To gain an actual representation of the surface energy balance, an Intensive Observation Period (IOP) was carried out on 19 th June 2006 at 10:30AM.

During this period, the necessary data was obtained.
The IOP consisted of three experiments carried out simultaneously, with each experiment being duplicated using identical apparatus, producing six sets of data.

(See IOP Results Summary Sheet in Appendix for complete results.) The first experiment measured the surface radiation budget.
The second experiment measured sensible heat flux (H) by the eddy correlation method.

The third experiment measured momentum and heat fluxes by the profile method.
The key measurements acquired during the IOP were used to derive a set of surface energy exchange parameters which were then averaged over a common sampling period (length of the IOP).

The sampling period was a duration of 20 minutes (1200s).
From this, important surface energy balance variables were derived, in particular the sensible heat flux density, H, the latent heat flux density, λE and the Bowen Ratio, B0.

The radiation balance is defined as the equilibrium between the amount of incident solar radiation received by the Earth, and the energy radiated into space (Dunlop.
2001).

The first experiment calculated the radiation balance by using two solarimeters, one facing upwards, the other facing downwards.
Additionally, a heat flux plate was buried beneath the surface.

The mean voltage outputs of these apparatus were recorded by a switch box connected to voltage integrators.
The values for down-welling short-wave radiation, S↓ (upward facing solarimeter) from the sun and up-welling short-wave radiation, S↑ (downward facing solarimeter) from the surface were calculated with respect to the sampling period of 20 minutes, along with the net long-wave irradiance, Ln and ground heat flux density G0.

From this the net total surface irradiance, Rn was estimated.
Where Rn = Sn + Ln and Sn = S↓- S↑ and Ln = L↓- L↑.

Here, L indicates long-wave or terrestrial radiation.
By using the values of H and λE, the Bowen Ratio B0, was calculated using B0 = (H/ λE ).

Using the accuracies of each piece of apparatus (See Sheet in Appendix: Instrument Details and Calibrations Summer 2005), the error of each parameter and derived variable were calculated using a range of formulae.
Although having completely different methods, the second and third experiments both obtain values for the sensible heat flux, H and the latent heat flux, λE in the meteorological enclosure.

The second experiment measured H by the eddy correlation method.
This method measures eddies near the surface.

Eddies are turbulent fluctuations of 'blobs' of air with properties that differ from the mean properties of the surrounding atmosphere.
By simultaneously measuring the vertical velocity by an upwards pointing Gill propeller anemometer and the temperature by a fine-wire platinum resistance thermometer at a height of 3m, at frequent intervals (every second) using the GENLOG computer program, an accurate value of H was estimated.

By using the surface energy balance equation Rn - G - H - λE = 0 and using values of Rn and G obtained from the first experiment, a value of the surface latent heat flux, λE, was estimated.
From this, the Bowen Ration was calculated using B0 = (H/ λE).

Again, the errors and uncertainty in these values were calculated.
The third and final experiment measured H and λE using the profile method.

This method is somewhat different from the eddy correlation method in that instead of taking frequent measurements simultaneously of vertical velocity and temperature at a single point, measurements of the wind speed at eight points on a vertical mast and temperature at two heights are taken.
The wind speed is recorded using eight anemometers mounted above each other on a mast, with the top anemometer at a height of 6.4m and the lowest anemometer at 0.56m.

Each anemometer produces an electrical pulse at a rate which is proportional to its rotational speed.
The total number of counts recorded for each anemometer is used to obtain a mean wind speed at that height over the length of the sampling period.

By plotting a graph of the mean wind speed for each anemometer against a log-profile law approximation of the height of each anemometer, a straight line of best fit can be plotted.
By using the gradient of the slope, a value for the friction velocity, u*, can be estimated.

The friction velocity is defined as the frictional drag from the surface acting on horizontal winds at the Earth's surface.
It acts in the opposite direction of the horizontal wind.

The aerodynamic resistance, ra was then estimated by calculating the difference in velocity between z=0.2m and z=2.0m and using the value obtained for the frictional velocity, u* by using the equation ra = (U2-U1)/u*2, where (U2-U1) is the difference in velocity between z=0.2m and z=2.0m.
The temperature is also measured at a height of z=0.2m and z=2.0m.

Calculating the difference in temperature between these two heights and using the value for aerodynamic resistance, ra, a value for the sensible heat flux can be estimated by using the equation; H≈-ρ(cp/ra)(T2-T1), where ρ= density of air, c p=specific heat at constant volume and (T2-T1)=temperature difference between z=0.2m and z=2.0m.
Since values of the ground heat flux and net surface irradiance were previously estimated in the first experiment, by using the surface energy balance equation, a value of the surface latent heat flux, λE, were estimated.

Again the Bowen Ratio was calculated.
Errors and uncertainties in the values derived by the third experiment will be stated and commented on.

RESULTS
The following section will present the results of the IOP carried out on 19 th June 2006.

The IOP began at 10:30AM and lasted until 10:50AM and took place within a meteorological enclosure.
The findings of the surface energy balance are summarized in table 1.

It can be seen that the derived values for the net surface irradiance and ground heat flux for each group are not the same, even though each group carried out the experiments at the time (from 10:30AM to 10:50AM), under the same atmospheric conditions, using the same type of apparatus and using the same method and equations.
When comparing the values for Rn and G0, not only are they not equal but when their uncertainties are considered, neither of the values are within range of the other values' uncertainty.

This tells us that there must be some error involved.
It is difficult to explain what the exact reasons are for such discrepancies, however there are some likely sources of error.

All the apparatus used in deriving the radiative components in Table 1 have a degree of instrumental error that is associated with uncertainties in the calibration characteristics of the instrument (See Sheet in Appendix: Instrument Details and Calibrations Summer 2005).
When calculating the values for Rn and G0, the instrumental accuracies were used to estimate the uncertainties of these values and these uncertainties are stated in Table 1.

However, it may have been possible that the accuracies stated for each apparatus may in fact be inaccurate.
Although, even if this was the case, it would not have a considerable affect since the original accuracy of each apparatus are only several per cent.

Therefore it is likely instrumental calibration error has not affected Rn and G0 significantly.
Since the actual difference between the values are comparatively small, the error involved is hard to identify, therefore it is likely to be a combination of smaller errors, such as variations in fluxes across the meteorological enclosure, human error and calibration error.

In this experiment, the infra-red thermometer (IRT) was not fixed as it was held by someone and therefore when the readings were taken, the position of the IRT was constantly changing and this will have influenced the accuracy of the readings from the IRT.
Some regions of the grass of the meteorological enclosure were greener than other parts and this variation would have changed the albedo of the surface and therefore the energy fluxes.

The key results from the eddy correlation method are expressed in Table 2.
It is clear to see that the two values estimated for H by each group are not the same and are quite far apart (Group 4's estimate is some 42% lower than Group 3's value), even though theoretically they should be the same.

As discussed previously, it is likely the accuracy of the instrument has little influence on the value calculated.
However, during the experiment, it was noticed that there was a problem with instrument lag and that this had the effect of sensible heat flux being under-estimated.

The calculated values for λE are also dissimilar, even if their uncertainties are considered.
The reason for this is because λE is calculated using the surface energy balance equation (Rn - G - H - λE = 0), therefore if H is inaccurate, then so will λE.

The key results from the profile method are shown in Table 3.
It can be seen that the values calculated for H by each group are very close to each other, and if their uncertainties are considered, both values do agree with each other.

This tells us that the apparatus used were accurate with respect to each other.
Although the values of λE are not the same, once again, if their uncertainties are included, both estimates of λE are valid as they agree with one another.

The Bowen Ratio values tend to agree too if their uncertainties are taken into account.
Values for the sensible heat flux, latent heat flux and Bowen Ratio have been calculated using two different methods (See Table 2 and Table 3); the eddy correlation method and the profile method.

Since values attained by Group 4 seem to be underestimated due to instrument lag, the values calculated by Group 3 will be used for comparison.
If values of H estimated by the eddy correlation method are compared to the profile method, it can be seen that they are quite different, even if their uncertainties are included.

This shows that either serious errors have crept in at some point or one method is more accurate than the other or both.
As previously stated, the affect of instrument accuracy is likely to be minimal.

Therefore, errors must exist elsewhere.
During the duration of the IOP, the cloud cover increased from about 4 oktas to 7 oktas.

This change will mean less solar radiation reaching the surface.
This will affect the radiation balance and values such as Rn and G0.

It may also alter the characteristics of phenomena such as eddies.
In turn, this may influence values of H, λE and B0, in particular for the values estimated in the second experiment as this used the eddy correlation method.

The eddy correlation method is derived from a measured value of the eddy-covariance between vertical velocity and temperature.
Since these parameters are used in an equation in conditions where there is a constant-flux layer near the surface, when fluxes are not constant, the accuracy of the equation deteriorates.

As cloud cover did change appreciably throughout the IOP, then so did the radiation balance and therefore the fluxes due to the reduced amount of short-wave radiation reaching the surface.
This will have affected the accuracy of H, and therefore λE and B0.

In this method, the accuracy of H is also dependant on how frequently values of vertical velocity and temperature are recorded.
It may be so that during the IOP, the computer program, GENLOG, did not sample enough.

However, this may be limited as the accuracy is then limited by the time response characteristics of the instrument.
Another meteorological factor is the change in temperature with height.

For the profile method, H was calculated by H≈-ρ(cp/ra)(T2-T1).
It is assumed for this equation that near-neutral conditions exist, where the potential temperature doesn't vary rapidly with height.

If, however, during the IOP non-neutral conditions existed near the surface, where temperature does vary with height rapidly, then using H≈-ρ(cp/ra)(T2-T1) will be less accurate, depending on how the temperature changes with height.
This is difficult to quantify, but the IOP was carried out at mid morning on a summer's day where the sun's intensity was considerably high and cloud cover was not extensive.

This would have heated the surface quite significantly and therefore a strong temperature gradient near the surface may likely to have existed.
It should also be noted that the eight anemometers on the mast are prone to stalling if the wind velocity drops.

This makes the value derived for the friction velocity susceptible to error.
CONCLUSIONS

In deriving values of the sensible heat flux, latent heat flux and Bowen Ratio, it has been found that none of the values estimated are exact and that error is involved with all the parameters and variables.
It is difficult to identify what method for calculating H is more accurate.

It is obvious that both methods are subject to different errors, some uncontrollable like the weather, others such as sampling error and instrument error.
But once H has been calculated, its error is then carried on to λE and finally to B0.

The accuracy of the eddy correlation method is influenced by the frequency of sampling, the instrument lag and calibration accuracy of the instrument.
The experiments carried out have found that the time response characteristics of the instrument introduce great error and that changes in atmospheric conditions do not help the accuracy of the experiment.

The accuracy of the profile method is also subject to errors.
This method and its theory have been derived assuming near-neutral conditions.

To achieve this, especially on a summer's day is difficult as there is often a strong temperature gradient near the surface.
This and other factors such the anemometers stalling all introduce reasonable error.

When comparing each method, it can be said that the eddy correlation method is perhaps a better method to obtain values of sensible heat flux, latent heat flux and the Bowen Ratio.
This is because the profile method is sensitive to measurement errors in wind velocity and temperature and what the temperature gradient is like near the surface.

The eddy correlation is perhaps the more accurate of the methods because its accuracy really only depends on the frequency of sampling, the instrument lag and calibration accuracy of the instrument.
The calibration error is considerably small.

The instrument lag seemed only to affect one of the experiments, therefore it can be considered a unique event and the sampling frequency which was every second proves to be adequate.
The accuracy of the experiments could be improved in several ways.

The entire IOP could be repeated at least once and then values attained could be average.
This would also have the benefit of excluding results that are not consistent with the rest.

The profile method could be carried out when there are near-neutral conditions.
This should allow the accuracy of H to be greater.

The sampling frequency for the eddy correlation method could be increased to slightly improve the accuracy of H.
By improving the accuracy of H, the accuracy of λE and B0 should also increase.

The film Dantes Peak is about a dormant volcano that suddenly erupts and threatens the nearby town.
Prior to the eruption are indications of such an event occurring, some of which are realistic, whilst others are not so realistic.

The first indicator of an eruption is greater than average seismic activity below the Cascades, at a depth of some 10-20 km.
This is what would be expected with most eruptions.

After this event, the volcano itself is monitored more closely.
Seismometers and tiltmeters are set up around the volcano to monitor any seismic activity or bulges appearing on the mountain.

This monitoring is what happens in the monitoring of 'real' volcanoes.
Dead animals and trees were also found, evidence of poisonous gases being released from the volcano.

Although in real life this can happen, it is not a good indicator of volcanic activity about to occur, as wildlife and vegetation can die from many other causes such as disease, famine etc.
The acidity of water up the mountain had increased to dangerous levels too.

This is unlikely to be due to the acidity of the 'country rock' but most likely increased volcanic activity.
This phenomena is sometimes found in real life situations.

Later on in the film the town's water supply turns brown and smells of sulphur dioxide.
This contamination is not what one would expect to find in real life and is therefore an unrealistic event.

As it gets closer to the eruption in the film, the amount of seismic activity increases, until a powerful earthquake occurs during the first eruption.
This increasing seismic activity prior to an eruption is very similar to real eruptions.

The eruptions of Mt. St. Helens (Washington State, US) and Mt. Pinatubo (Philippines) had both seen increasing seismic activity before their eruptions.
Whilst ash is falling during the film, the residents of the town are seen walking through the ash as it falls.

If this was for real, the people would not be able to breathe as breathing in the ash would damage the lungs and they would suffocate and die fairly quickly.
A car is also seen driving over a lava flow and manages to drive clear of it!

This would not be the case as the intense heat would melt the tyres and the engine would quickly catch on fire.
The eruption of Dantes Peak also occurs in two phases.

The first stage being the first eruption resulting in an earthquake, an ash cloud and much ash falling.
The second stage being the a much more explosive eruption followed by a pyroclastic flow.

Many eruptions have just one single eruption so the events in the film could just have been put in for cinematic effects rather than portraying real science.
Here is a list of all the volcanic phenomena occurring during the film;

Boiling waterUnusual animal activityIncreased pH levels in waterDead treesDead animalsGases coming up through the water and crater (carbon dioxide and sulphur dioxide)25-75 micro earthquakes dailyGeothermal activityGreater than average seismic activityBrown waterIncreasing emissions of gasesFalling ashLightning in ash cloudFalling trees and bouldersLava flowsDead fishSnow melting creating laharsPyroclastic flowExtensive ash column
Jules Verne wrote 'Journey to the Centre of the Earth' in 1864.

The main story is about Axel and his uncle, Professor Liedenbrock discussing about the interior of the Earth and whether it is possible to reach the Earth's centre.
Much of the geological science referred to in the novel was at that time considered fact, but nowadays, much more is known about Earth's interior.

Accurate Statements:
On page 24, it refers to the limit of the terrestrial crust being at a dept of 30 miles or 50 kilometres.

This is true since seismic waves show that the average thickness of continental crust is between 40 to 50 kilometres.
It also mentions the temperature at this depth will be greater than 2372 Fahrenheit or 1300 Celsius, this is similar to reality as it is estimated that the base of the crust is around 1300 to 1400 Celsius.

Page 26 contains this, "it is quite reasonable to suppose that the external crust cooled down first, whilst the heat took refuge down to the centre".
This is mostly the truth as when the Earth was formed, the crust did cool first with the interior retaining the heat.

Professor Liedenbrock also asks, if there were internal heat, then can it not be concluded that it is diminishing?
The Professor was correct as evidence shows that the Earth has cooled since it was created and that the heat within is very gradually decreasing as the number of radioactive elements in the interior that can decay also decreases.

Inaccurate Statements:
On page 24, it refers to the internal temperature rising 1F| for every 70 feet in depth and that this is constant throughout the Earth's interior.

This is not the case because the thermal gradient is not constant with depth, but in fact changes.
Within continental crust it is 20C to 30C per kilometre, but in the mantle below the crust the thermal gradient drops to 0.5C per kilometre.

The actual figure of 1F for every 70 feet (approximately 1C for every 20 metres) is not true either.
For continental crust it is on average 0.5C per 20 metres, but for the mantle it reduces to 0.01C per 20 metres.

It also states on this page that the core's temperature is 360032F or 200000C.
This is not true as scientists now believe the core's temperature is actually near 5000C.

Following on from this is claims that all substances within the Earth's core must exist as a gas due to very high temperatures.
This of course is not true as no evidence suggests that just gas is found within the interior of the Earth, gas is found within magmas for example, but not on its own!

On page 25, the Professor says that the interior of the Earth cannot be water, gas or any known mineral as the Earth would not weigh what it does.
He is right by saying it is not water or gas, but wrong by saying that it is composed of unknown minerals because the bulk of the Earth's interior is made up of well known minerals at that time, such as iron, nickel and magnesium.

Page 26 contains a sentence saying that the terrestrial nucleus could not be liquid as it would be effected by the gravitational pull of the moon and would subsequently cause massive, periodical worldwide earthquakes.
Due to S-Waves not passing through the outer core of the Earth, then it can be inferred that the outer core is liquid as S-Waves do not pass through liquids and therefore this sentence is false.

This page also contains a theory as to why the Earth's interior contains heat.
The Professor claims that the Earth has been heated by combustion on its surface and that reactive metals such as sodium and potassium found at the surface ignite easily, and with rain, penetrate down into the Earth where they combust again with explosions and eruptions.

This is what he claims creates volcanoes.
This is not so as it is known that volcanoes occur where flowing magma with the mantle reaches the Earth's surface via plate boundary movement, fractures in the crust or hot spots in oceanic crust and then erupts, thus creating a volcano!

It is also known that the heat of the interior is not from very reactive metals penetrating the crust and combusting generating heat, but from the decaying of radioactive elements such as uranium and thorium that drive convective currents transferring heat to the surface.
Professor Liedenbrock also quotes, "there is no proof at all for this internal heat; my opinion is that there is no such thing".

When looking at the Earth, there is much evidence to suggest the opposite.
Volcanoes, greater temperatures found in deep boreholes and mine and hot springs all prove that immense internal heat exists.

Creon: So, men our age, we're to be lectured, are we?
- schooled by a boy his age?

Haemon: Only in what is right.
But if I seem young, look less in my years and more to what I do.

Creon: Do?
Is admiring rebels an achievement?

Haemon: I'd never suggest that you admire treason.
Creon: Oh?

- isn't that just the sickness that's attacked her?
Haemon: The whole city of Thebes denies it, to a man.

Creon: And is Thebes about to tell me how to rule?
Haemon: Now, you see?

Who's talking like a child?
Creon: Am I to rule this land for others - or myself?

Haemon: It's no city at all, owned by one man alone.
Creon: What?

The city is the king's - that's the law!
Haemon: What a splendid king you'd make of a desert island - you and you alone.

Creon: This boy, I do believe, is fighting on her side, the woman's side.
Haemon: If you are a woman, yes - my concern is all for you.

Creon: Why, you degenerate - bandying accusations, threatening me with justice, your own father!
Haemon: I see my father offending justice - wrong.

Creon: Wrong?
To protect my royal rights?

Haemon: Protect your rights?
When you trample down the honour of the gods?

Creon: You, you soul of corruption, rotten through - woman's accomplice!
Haemon: That may be, but you will never find me accomplice to a criminal.

Creon: That's what she is, and every word you say is a blatant appeal for her - Haemon: And you, and me, and the gods beneath the earth.
Creon: You will never marry her, not while she's alive.

Haemon: Then she will die...but her death will kill another.
Creon: What, brazen threats?

You go too far!
Haemon: What threat?

Combating your empty, mindless judgments with a word?
Creon: You'll suffer for your sermons, you and your empty wisdom!

Haemon: If you weren't my father, I'd say you were insane.
Creon: Don't flatter me with Father - you woman's slave!

Haemon: You really expect to fling abuse at me and not receive the same?
Creon: Is that so!

Now, by heaven, I promise you, you'll pay - taunting, insulting me!
Bring her out, that hateful - she'll die now, here, in front of his eyes, beside her groom!

Haemon: No, no, she will never die beside me - don't delude yourself.
And you will never see me, never set eyes on my face again.

Rage your heart out, rage with friends who can stand the sight of you.
(Sophocles: Antigone, lines 813-859) This episode in Sophocles' 'Antigone', between Creon, King of Thebes, and his last surviving son, Haemon, gives a strong indication of one of the great causes of tragedy in the ancient world.

This is the idea of a lack of understanding between a father and a son, because of clashes in opinion or thought, and through a stubbornness of nature, which is what not only brings about this lack of relationship and understanding, but also a large proportion of the tragedy that springs from it.
In this case, the tragedy set in motion is a double tragedy, that of Haemon, who will die an unhappy death, hating his father and, ultimately, that of Creon, who will lose everything and everyone he has cared about.

On the surface, this passage is about a conflict of ideas about Antigone, who Creon believes is a traitor to the State as he describes the treason that is the act of her burying her brother as 'the sickness that's attacked her', but who Haemon believes has acted nobly and rightly.
However, reading underneath the words allows the reader and the audience to explore the deeper theme of how costly a lack of understanding between father and son can be, which is also a theme illustrated similarly in Euripides' 'Hippolytus', with the idea of the relationship between Theseus and Hippolytus, which is shown by the fact that Theseus unquestioningly believes Phaedra's accusation against Hippolytus, showing that he has never been entirely trusting of his son, and only realises his mistake at the end of the play when it is too late.

This is not to say that Creon is distrustful towards Haemon in the same way that Theseus is towards Hippolytus, but instead that he is so consumed by the desire to appear as a good king that he finds himself putting the affairs of the State above everything else, including his family, which is why he 'fails both as a father and a civic leader'.
Creon is an unusual figure in Greek tragedy, because he appears to simply be the 'typical tragic hero who collaborates in his own downfall', however it is clear that his character is not as simple as this.

He is a man under an immense amount of pressure because he wants to be a good king and he knows that everything he does on his first day in office will dictate what the people think of him and it is this which makes him ignore other viewpoints and brings about the clash with his son in something familiar in Greek tragedy, as 'two strong wills inevitably clash - the son eager and impassioned, the father hardened by duty' as is shown by his words to Haemon 'the city is the king's - that's the law!', a view sharply contrasted by Haemon, who is very much on Antigone's side, as he fights for justice and for what he believes is right, despite the fact that he 'declares that no marriage means more to him than his father', earlier in the play, with the words 'No marriage could ever mean more to me than you'.
This is not to suggest that Haemon is a hypocritical figure, but instead it shows Creon's inability to accept what he believes is an action against his own rules, or his own inflexibility.

It can be argued that Haemon is equally as inflexible as Creon in this passage, because he too refuses to back down on his beliefs that Antigone has done nothing wrong, and it is this which destroys any possibility of a father and son relationship between them.
While it is true to say that Haemon is clearly on Antigone's side, he does not always make this entirely obvious at the start, as he does not simply argue her case insultingly to Creon, but instead he tries to use rational and tactful rhetoric to make him be flexible rather than firing personal insults on his father's judgement, even if he believes it to be wrong.

This shows an element of flexibility on Haemon's part which is missing in his father, as he is capable of calm negotiation when he knows that hot-headed impetuosity will achieve nothing, but it also goes a long way to illustrating the emotional conflict with his father, as Creon is unable to see the situation from his son's perspective.
With his words 'Why, you degenerate - bandying accusations, threatening me with justice, your own father!', Creon is showing that he feels Haemon is not only being disloyal to him as a king, but also as a son, once again showing the inflexibility which makes a functional relationship with Haemon almost impossible, and which ultimately brings about the tragedy of the play.

This having been said, Haemon has faults too, which are exploited in his later words to Creon, as he tells him 'Then [Antigone] will die...but her death will kill another' and, even more powerfully, with his last words to him 'And you will never see me, never set eyes on my face...
Rage your heart out, rage with friends who can stand the sight of you', which shows that, for all his earlier tact and rational speech earlier on in the scene, Haemon can be impulsive and insulting as he rages at Creon before leaving, his final words to his father later ringing true.

What is really interesting about the characters of Creon and Haemon in this passage, apart from the evidence that a functional relationship as father and son is impossible between them, is that, despite the fact that Creon has the authority over Thebes as king, it is actually Haemon who comes off better and has a greater sense of authority here, as he has the ability to be rational in the face of adversity which Creon appears to lack, meaning that Haemon comes across in a more mature light, giving an idea that, if he were a ruler and a father, he would be able to balance the two roles much better than Creon himself does.
A figure of fatherhood is a crucial element in Greek drama, but especially in tragedy, as he can be used to illustrate the principles of tragedy according to Aristotle, which are that tragedy is 'an imitation of an action that is admirable, complete and possesses magnitude' in some cases by contradicting them.

I will be focusing on will be Euripides' 'Hippolytus', where I will be exploring the character of Theseus as a father figure towards Hippolytus, and Aeschylus' 'Agamemnon', where I will be exploring the characters of Agamemnon and of Thyestes, the father of Aegisthus, to show how a father figure can also have a strong influence by being absent and also to show the conflict between fatherhood and kingship, which is especially shown by Agamemnon.
The major father figure in 'Hippolytus' is the character of Theseus, whose son is Hippolytus.

The audience first learns this from Aphrodite's speech, as she describes Hippolytus with these words: 'Hippolytus, son of Theseus by the Amazon, pupil of holy Pittheus' Throughout the play, Euripides makes it very clear that Theseus is the dominant figure of fatherhood.
It is, however, Theseus' attitude towards fatherhood and towards Hippolytus, as well as his relationship with his son, which goes a long way towards the tragedy at the end of the play.

Hippolytus is an interesting figure in the play for several reasons, one of which is the contrast between himself and Theseus.
Hippolytus is very much an 'ephebic' figure, or a young man who has not quite reached adulthood.

It is clear that he passes his time with appropriate pursuits for such young men.
The first time he appears in the play, he has just been hunting and he has also made a garland for the statue of Artemis, a goddess of hunting.

This is extremely important because, like many of Hippolytus' traits, it shows how he is 'detached from human relations and unchanging', not only in that he is showing no signs of moving out of the hunting phase into any other phase - in other words showing his unwillingness to mature - but also in showing one of his other major character traits.
Hippolytus is chaste, in the same way that the goddess Artemis was chaste, and he intends to prolong his chastity, because he shuns marriage.

Hippolytus' chastity is one of the factors which brings about his tragedy, because he is punished by Aphrodite, 'not simply because his way of life has no place for sex, but because he rejects it and rejects the worship of the goddess'.
It also shows a huge contrast with the figure of his father, Theseus, who 'was known for a polygamous love life', even though, after Phaedra's death, he honours her memory when he says: 'There is no woman in the world who shall come to this house and sleep by my side.' It can be argued that this idea of Theseus renouncing his desire for women is a device used by Euripides to make him appear to have more in common with his chaste son Hippolytus.

However, I believe that this is not the case, since, as the play progresses, it becomes increasingly clear that Theseus and Hippolytus have few or no common features and Euripides is using this device to show that, despite Theseus renouncing his polygamy in the face of his wife's death does nothing to give him similarity with Hippolytus, who refuses marriage and sex completely.
Another facet to Theseus' character which shows him to be completely at odds with Hippolytus is his attitude towards the 'polis', or the city.

It has been argued that Theseus and Hippolytus are so at odds here because they both take their roles to the extreme.
The audience can see, in the 'agon' speeches after Theseus finds Phaedra's note after her death, how 'Hippolytus is practically an anti-Theseus and between them lies the world of the polis [as] Theseus exceeds the status of a citizen [whereas] Hippolytus never quite attains it', as Hippolytus actually says that he is: '...no man to speak with vapid, precious skill before a mob, although among my equals and in a narrow circle I am held not unaccomplished in the eloquent art.' In admitting that he does not make speeches, Hippolytus once again shows himself as an outsider who does not take part in democracy.

He also presents a strong contrast to his father, as Theseus is a public figure in the polis, as he speaks as if he is speaking to a public assembly, especially with the words: 'Look at this man!
He was my son and he dishonours my wife's bed!

By the dead's testimony he's clearly proved the vilest, foulest wretch...
show me your face; show it to me, your father.' Theseus' story in the 'Hippolytus' is a tragedy within a tragedy, as it shows the fall of a great figure in just one day.

However, the tragedy stems from 'Theseus being what Theseus was and a relationship (or rather a complete lack of relationship) between Theseus and Hippolytus', which is because the two characters are so different.
There are differences of opinion amongst critics about why Theseus immediately believes Phaedra.

Some critics have argued that 'by having Theseus instantly believe Phaedra, Euripides suggests that he has never entirely trusted his odd son', whereas others argue that 'Hippolytus [is] a man belonging to a world apart' from the 'polis' and from what constitutes the average Greek male, which is what Theseus represents, whereas Theseus is the opposite to his son and so has never understood him.
Personally, I am inclined to believe that the tragedy of Theseus, and also the tragedy of Hippolytus, stems from both these factors.

It is Theseus' inability to understand his son which leads to an inability to trust him the way a father should be expected to trust his son and this is what leads to their downfall.
The lack of relationship between the two is emphasised by Theseus' description of his confrontation with Sinis, a bandit he claims to have killed, and the idea of the rocks being ravaged by the sea, which 'suggests the whole realm of cruelty and bitter experience...

[is] in contrast to the innocence of [Theseus'] woods-and mountain-loving son', which again emphasises their contrasting backgrounds and the lack of a relationship between them.
The final scene of the play, when Hippolytus returns, bruised and battered, is an exceptionally poignant scene, as it shows the relationship between the father and the son as it should be.

There are two strong Aristotelian concepts in this final scene between the two.
The first is the idea of 'recognition', which comes when Theseus realises his 'hamartia' in cursing Hippolytus with the curse of Poseidon and also shows the tragedy of how 'the prayers that become reality are the deadly ones', although he realises his mistake when it is too late, which can also be seen as a sign that he is also realising the true role a father should have towards his son when it is too late.

The second is the concept of a 'peripeteia', or a reversal, which means that when something happens that should have one effect, it has the opposite effect.
In this case, Theseus was supposed to feel justice that his curse had worked, when in fact, when he realises the extremity of what has happened, he feels guilt and pain.

For his part, Hippolytus is able to absolve Theseus of blame before his death, as he tells his father: 'No, for I free you from all guilt in this.' Like Theseus, Hippolytus has also had a recognition, as he 'rediscovers Theseus as a father', in the same way that Theseus can now recognise Hippolytus as his son.
Their character difference which made their relationship so disastrous will still be there, but they have now finally learned to accept the other for who he is, even though it is too late.

One of the most intruiging aspects of the opening play of Aeschylus' 'Oresteia' trilogy is the aspect concerning a figure of fatherhood.
This is because, even without such a figure performing an active role in the play, the theme of fatherhood is extremely important and it comes across in several moments of the play in different ways.

What is most interesting about this is the idea that a father figure is someone so powerful and influential that he does not have to be onstage constantly to draw the attention of the audience and, in some ways, dominate the action of the play.
Indeed, the only male figure who could be regarded as being close to a father figure who is actually physically seen in the play is Agamemnon, yet even he does not appear until late into the action, yet he manages to dominate much of the action on the stage.

In the 'Agamemnon', one of the greatest conflicts for a father who, like Agamemnon, is also a king and a warrior comes to the forefront of the play.
This conflict is a conflict where the two different roles are set against each other and the character has different loyalties to the 'polis' and to the 'oikos' and is shown most poignantly by the choice that Agamemnon has to make before the Greek fleet can set sail for Troy where he must sacrifice his daughter Iphigenia to placate the goddess Artemis so that the troops can set sail for Troy.

The Chorus, when discussing this, initially show Agamemnon's unwillingness to commit such an act with these words: '[Agamemnon's] fate is angry if I disobey these, but angry if I slaughter this child, the beauty of my house, with maiden blood shed staining these father's hands beside the altar.
What of these things goes now without disaster?' Agamemnon's instinct as a father with love for his daughter is being clearly illustrated by the Chorus as they tell that he does not want to sacrifice his daughter, but that he also feels that he has to set sail for Troy to preserve his reputation, effectively putting him in a situation where he cannot win, because he has great love for his daughter, but he knows that if he does not sacrifice her, he will not be able to sail for Troy, and he will also be committing the sin of 'hubris' if he goes against the will of Artemis.

It is ironic, therefore, that, when he returns from Troy, he commits a similar hubris when he walks on the tapestries laid out for him by Clytemnestra.
This, arguably, negates many of his fatherly instincts, because the hubris he sought not to commit when he sacrificed Iphigenia has been committed anyway.

Agamemnon's fatherly feelings are, however, set aside and he does sacrifice his daughter, in a way that the Chorus call 'reckless in fresh cruelty', which shows him not as a figure of fatherhood, but instead as a king and a warrior who is doing all that must be done in order to go to war: '[Iphigenia's] supplications and her cries of father were nothing, nor the child's lamentation to kings passioned for battle.' Through the Chorus' words, the audience and the reader now see how Agamemnon's paternal instincts have been repressed by his own and the other king's passion for war.
This shows him no longer as a figure of fatherhood, but instead as a kingly figure who partakes in battles and wars and again shows how the role of fatherhood and of a king come into conflict with each other.

Some critics have argued that Agamemnon's situation was such that he could not be to blame for his actions.
In 'Problem and Spectacle - Studies in the 'Oresteia', William Whallon states a theory that 'Agamemnon is compliant, hesitant, vacillating before the altar' and that he and Iphigenia were mere pawns in the game of destiny, thus absolving Agamemnon of blame for his actions, as if he went against the will of the gods, he would be guilty of hubris.

However, other critics have argued to the contrary, arguing that 'there is no intimation that Agamemnon was compelled by any god or spell to choose as he did', a concept stated by N.G.L.
Hammond, and quoted by Whallon, which implies that Agamemnon acted of his own free will.

My opinion is that Agamemnon did not want to murder his daughter, but he also did not want to commit hubris, thereby angering the gods, so he put his paternal instincts aside and took on the mantle of a king and a warrior.
In some ways, the rejection of his paternal feelings is his 'hamartia', which brings about his downfall.

Another aspect of fatherhood that comes across in this play is the idea of a father as someone who should be avenged if they are killed unlawfully or if they were wronged when alive.
Aeschylus brings this aspect across through the character of Aegisthus, the cousin of Agamemnon.

Like Agamemnon, Aegisthus does not make his appearance until the late action of the play, but when he does appear, he immediately gives off an aura of a son wanting vengeance for his now dead father.
This idea is especially shown with his words: 'For Atreus [Agamemnon's] father, King of Argolis - I tell you the clear story - drove my father forth, Thyestes, his own brother, who had challenged him in his king's right - forth from his city and home.' From the start, even when telling the legend of how his father was driven out of his home, Aegisthus already comes across as a son wanting to avenge the wrong done his father.

Once again, a figure of fatherhood is very dominant and influential because of his absence and the effect it has on the characters and their actions.
Now that Agamemnon is dead, Aegisthus believes that justice has been done and that he can: '.....die in honour again, if die I must, having seen him caught in the cords of his just punishment.' Once again, Aegisthus is presented as the son wanting revenge on his father and he uses another figure of fatherhood, Agamemnon, to exact his revenge.

This gives an idea of justice and of one figure of fatherhood dying for another.
The interesting thing about this is that it is not Aegisthus who commits the murder, but instead it is Clytemnestra, meaning that 'the agent of punishment is an adulterous wife, but one whose daughter has been cruelly sacrificed', which adds to the idea of justice, and also presents Clytemnestra as an arguable figure of fatherhood, as she comes across in a very masculine and strong manner, which one could associate with a father figure.

Therefore, the father figure is an interesting element in Greek drama as he shows, even in absence, what a powerful effect he can have on the action, as is shown in the 'Agamemnon' as well as relationships, or lack thereof, with his child, and how the absence of such a relationship can lead to a tragic outcome, as is shown by the character of Theseus in the 'Hippolytus'.
In epic, the concept of heroism and heroes has been passed down the tradition for centuries.

It has been transformed from Greek heritage with Homer, which then becomes Roman heritage with Virgil, Ovid and eventually Lucan.
The changes in the nature of heroism and heroic figures have been substantial.

Homeric epic is based around an idea of a 'shame culture', in which heroes were men who did great deeds of valour, by killing other great heroes, the greater the better, in battle and duel, as is shown in the 'Iliad'.
This meant that, if a hero did a great deed, then, according to the 'heroic code', the hero would win 'timē' or respect from other heroes, and this was how heroes were judged; on results, rather than on lineage.

By Roman times, however, heroic nature had changed with Virgil's 'Aeneid', and the figure of Aeneas as a more dutiful, or 'pius' figure, as he embodies all the values of a Roman, especially that of 'pietas', which is absent in many Homeric heroes.
Also, there was less emphasis on the idea of a hero being judged because of deeds on the battlefield as, while this idea still existed, it was overshadowed by ideas of destiny and lineage.

While Homer's heroes such as Achilles and Hector were judged on exploits in war, Virgil's chief hero, Aeneas, is judged on his descendants.
This meant that a Roman hero would, theoretically, have a greater sense of duty towards his people and his country than a Homeric hero, although, by Lucan's time, there were signs that this idea was changing once again.

By the time that Lucan was writing, a new concept of what constitutes a Roman hero had become popular in epic.
This was the idea of heroes as symbols, an idea that had been referred to in Virgil's 'Aeneid', with Aeneas, who is known as 'the dutiful Aeneas', as a symbol for piety.

In Lucan's 'Civil War', the heroes are people who 'suggest in their different ways...the vast confusion and ineffable terror of the poem's materia', as the protagonists are all presented in contrasting styles, leading to questions over what, according to Lucan, constitutes heroism, as these ideas are sometimes very different to what constitutes heroism, according to Virgil.
The 'Civil War' has been cited as, having 'no hero, or too many', which is reminiscent of the 'Iliad', especially with the idea that Lucan does not necessarily present his heroes in a good light, leading to the question of whether his interpretation of heroism is closer to Homeric or Roman heroism.

In turn, there is also a question of whether the backdrop of the poem is in itself heroic.
It is a war, but it is a civil war, where 'a mighty people [are] attacking its own guts with victorious sword-hand...kin facing kin', so the Romans are essentially fighting themselves, until everything that has been built will collapse 'under too much weight, Rome's inability to bear herself' which takes some of the glory out of the war.

Similarly, the statement that 'every deity will yield to you, to your decision nature will leave which god you wish to be' does the same, because it implies that the only reason this war is happening is because the gods agree to it happening and not because the men are brave.
Lucan is condemning the idea of civil war by showing it in such a macabre light and also by showing characters in an unfavourable light.

There is the question of why Lucan shows his characters in such an unflattering manner, aside from criticising civil war.
At this point, one should remember that Lucan was one of a number of post-Augustan writers who 'tended to see their Rome and its history as terrifying and incomprehensible', showing that, according to these writers, war is not the heroic environment that earlier epic writers had portrayed it as.

One of the characters portrayed by Lucan in such an unflattering light is Caesar.
Before exploring Lucan's take on him, however, it is interesting to view how he is presented by Virgil, when Anchises says at once, 'What carnage when the father-in-law swoops from...the Alps...and his son-in-law leads against him the embattled armies of the East', and moments later, 'You who are sprung from Mount Olympus, you must be the first to show clemency', showing, in the space of a few lines, Caesar's fighting expertise, and his ability to show mercy, or clementia.

Lucan's Caesar is portrayed very differently as soon as he is introduced.
The first thing mentioned is how Caesar 'had not only a general's name and reputation, but never-resting energy...

[he] never shrank from defiling his sword', leading to a viewpoint that he revels in bloodshed, for the sake of his reputation, which is a very Homeric concept that portrays him in a similar light to Achilles, who, stating his intention to fight Hector, says 'But now, may I win heroic glory!'.
Lucan has portrayed Caesar in similar fashion, as, like Achilles, he is confident of victory, which is somewhat unusual, as he 'considers himself Aeneas' successor', and yet he displays characteristics unlike Aeneas and unlike those of a Roman hero.

Later, Caesar is visited by ghosts of the Pompeians he has killed, seeing 'the ghost of a murdered citizen [standing] there...he sees faces of old men, he the forms of younger men...in this breast is his father - all these shades are Caesar', however, he is not afraid, reinforcing the idea that he 'rejoices in mutilated corpses, loathe to have his masterpiece ruined by funerals', giving an impression that this bloodlust and this slaughter that he participates in is his crowning moment, or his 'aristeia'.
However, this is tainted, because civil war is less heroic than other wars as Caesar is effectively fighting people who should be allies, not enemies.

There is now a question of who is Rome's true enemy, as they are all Romans, which also emphasises Lucan's condemnation of civil war.
Within this, there is also an idea that 'the man who will be guilty is the man who made [Caesar] Rome's enemy', which further defiles Caesar's 'aristeia', because, in participating in this war, he is as guilty of being Rome's enemy as his adversaries are.

Interestingly, Caesar is the only character to invoke the gods, calling on the 'Phrygian house-gods of Iulus' clan', showing his awareness of his status as a descendant of Aeneas.
Ironically, when he does so, he asks them to 'favour his cause, which is, in effect, the subjugation of Rome to himself', giving his 'heroic' deeds a revolting twist, because, even though he does not mention Aeneas' name, he is playing a part in destroying that which Aeneas helped to build the foundations for.

Once again, there is the question of how heroic his actions are, because he is effectively destroying his own city.
One of the problems Lucan faced in his presentation of Caesar is that of his clementia, or mercy, which is a distinctive Roman trait, but, in the epic, is almost entirely obliterated, except for an episode detailing events in Spain, where he 'makes no effort to maintain the picture of ferocity and impatience' so far created in the impression of Caesar, although he tries to diminish his humanity.

Caesar had also written an account in which he recalls his exploits in Spain.
In his version of events, he had initiated the surrender of the Pompeians, promising to spare their lives, so that 'everyone recognised that Caesar was reaping the benefits of his original clemency'.

Lucan's version of how the surrender came about is rather different.
He uses a desire for 'the sacred love of the world' as the reason for it, and that Caesar was affected by this love, thereby omitting any mention of his clemency.

This device not only diminishes his Roman attributes, but also serves to make him out to be less of a hero because of it, as Caesar has lost the opportunity to do something great in showing mercy.
What is interesting about Lucan's portrayal of Caesar is that, after initially showing him as such a ferocious warrior, he then goes on to describe him as someone bringing peace as his troops 'abandon weapons to the victor and safe...with breast disarmed', showing how, in such a time of war, 'only Caesar seems to be able to make men free' and bring peace.

Once again, the issue of clementia is raised, as the troops disarm themselves, thereby showing mercy that is unusually absent in their 'heroic' leader.
There is a message from Lucan in this move of the troops forgoing their weapons, that civil war is unheroic, because everyone is fighting their own people and that, not only should it not occur, but it is an unfavourable way to remember a great warrior.

As is traditional with epic, a principal hero always has a character whose characteristics oppose his own, and this is true with Caesar, whose balancing character is Cato.
Cato is first mentioned in Book One, where Lucan reveals that 'each has on his side a great authority: the conquering cause the gods and the conquered Cato', showing how Cato is matched against the gods as someone who the soldiers and commanders desire protection from.

Despite this distinguished introduction, he is not properly introduced until Book Two, and then is 'to all intents and purposes abandoned by Lucan until Book Nine'.
The opening of Book Two is important to the presentation of Cato as it is made clear that 'the universe gave open signs of war', showing that, not only Caesar and Pompey, but also the entire universe, is responsible for the civil war that Cato is about to become embroiled in.

This becomes even more important when it is placed alongside Cato's own words to Brutus, who has come to him for help, when he tells him 'civil war is the greatest crime...to make guilty even me will be the god's reproach'.
Cato is aware of his guilty part in the war, but, unlike Caesar before him, he 'is not attempting to shrug off his guilt completely', only saying that the reason he is guilty is because the gods have set the course for the events that will follow.

When Brutus comes to see Cato, he reveals that, 'for Brutus, Cato will be the only leader'.
Such an appeal 'distinguishes Cato not only from the crowd, but from Brutus, setting him apart from even the most courageous men', making Cato an emblematic figure.

It also serves to enhance the Roman idea of heroism, as he is portrayed in a much more Virgilian light than Caesar had been portrayed.
The comparison of Cato with Caesar comes to the fore in Book Nine, where Cato finds himself in Africa at the point of his 'aristeia', firstly when he 'resolved to head for Libyan Juba's kingdom bordering the Moors,', but he fails to complete the crossing, because 'Nature blocked their journey by interposing the Syrtes'.

There is a similar incident in Book Five, when Caesar attempts to cross the Adriatic, 'and endures the perils of a great storm' in his own failure to complete the crossing.
Their reactions, however, are different, as, while the events of Caesar are drawn-out with his speech of accepting death by the sea as he believes he has done enough great deeds to win fame, Cato's failed crossing is much less exaggerated, as he 'voices no cry for a death beneath the billowing sands' and, as he will soon encounter greater dangers, he is arguably more heroic than Caesar as he does not beg for death.

He also appears more heroic as a leader, as he 'draws his men up to his own level', thereby making him out to be a more than competent leader as he will only lead after consultation with his soldiers and troops, whereas Caesar will make his men fear him, as well as reducing them to subordinates and making them inferior especially as he tells them 'There is no need of prayers...how great will Caesar be?
- in your hands it lies'.

The reason for this difference in treatment is because of the cause that the two leaders are fighting for; while Caesar fights to win glory for himself, in a manner that is very reminiscent of Homeric culture, Cato is fighting for the freedom of Rome, showing the piety that is associated with a Roman hero.
The two characters also differ in their treatment of their troops when they disobey orders.

While, on the one hand, Cato tells them that they are men who have 'decided on one source of safety - following [his] camp...prepare your minds for...the highest toils', showing that he prioritises their freedom and the freedom of Rome and its citizens above all things, Caesar views the term 'citizen' as an insult, and thereby 'spurns his rebellious troops as unworthy of him, "cowardly citizens" who do not deserve to be called soldiers, much less his soldiers', showing how he desires only glory, seemingly uncaring about how many enemies he makes of friends in order to gain this renown.
It is therefore true to an extent that the concept of heroism and the heroic does have a place in Lucan's epic, although it is not necessarily always the heroism that is traditionally associated with a Roman hero, but instead there is a dominant showing of the more Homeric traits of heroism.

Passage for Discussion
'...

Hector sent his voice ringing out to the whole Trojan army: "On with you, horse-taming Trojans!
Smash the Greek wall and fire the ships!" So he spoke, and there was no Trojan ear that did not catch his stirring call.

Massing together, they charged at the wall and began to scale the parapet with sharp spears in their hands.
But Hector seized and brought a rock that was lying in front of the gate.

Broad at the base and coming to a point, it would have taxed the strength of the two best men in any town of the present generation to lever it up from the ground onto a waggon.
But Hector handled it effortlessly on his own.

Zeus, son of sickle-wielding Cronus, had made it light for him.
As a shepherd easily picks up a ram's fleece in one hand, carries it off and scarcely feels the weight, so Hector lifted up the rock and brought it towards the planking that made up the high, strong, well-fitted double gates, which were held on the inside by two beams sliding in from either gate-post, locked by a single bolt.

Hector went right up to them and bracing himself, legs wide apart for maximum power, hurled the rock, hit the doors full in the middle and smashed it out of its pivots on either side.
The force of the throw propelled the rock through, and there was a great roar from the gate as the planks were smashed to splinters by the impact of the stone and the bars gave way.

In leapt glorious Hector, face dark as nightfall.
He held two spears in his hands and the bronze of his body-armour gleamed with a baleful light.

None but a god could have met and held him as he sprang through that gate.
And now, with fire flashing from his eyes, he wheeled round to the crowd behind him and called on the Trojans to cross the wall.

His men responded to his summons.
Some swarmed over the wall; others poured in through the gates itself.

The panic-stricken Greeks fled back to their hollow ships, and all hell broke loose.' (Iliad, XII, 439-471) In this passage, the Trojans, led by Hector, have stormed the Greek camp, driving the Greek warriors back to their ships by the end of the passage.
Initially, however, despite the bloodshed on both sides, the Trojans "were unable to set their enemies on the run", until a moment when "Zeus gave the upper hand to Hector son of Priam", who had been the first to enter the Greek camp.

At this point, Homer has Hector showing some exceptional leadership as he first rallies his troops calling them to "smash the Greek wall and fire the ships!" and then, as the Trojans rally together to attack, he proceeds to pick up a huge rock, possibly even a boulder, which he uses to smash the Greek gate.
By doing so, Hector is showing his men how to act in the face of battle and danger and in showing the courage that he does, he sets a good example, particularly to the younger men in the Trojan army, of what a hero or a warrior should do in times of war.

There is anther concept, as well, behind Homer showing Hector performing such an act.
As well as it being a means for Hector to give his men inspiration in fighting, Homer also uses it as a means to show Hector in a strong and heroic light, as he describes the effortlessness with which Hector first lifts, and then uses, the rock.

This allows the audience and the reader to understand the truly heroic figure as someone who has an "extreme level of male energy, a level which the lesser men of later times can never reach", which also gives an insight into the time that Homer looks back to in his poetry.
The 'Iliad' in particular looks back to a time of great men and great heroes who could do things that great men living in the time that Homer was composing his poetry could not do.

Indeed, when describing the great rock lifted by Hector, Homer says that "it would have taxed the strength of the two best men in any town", describing Hector as someone who must be a great hero to be able to carry out such a task, and again showing the heroic society in which his poetry is set.
This idea of incredible heroism is also echoed later in the poem, most notably when the hero Ajax, second only to Achilles in the Greek ranks, is seen defending the ships by "taking enormous strides [as he] kept moving from one ship's deck to another", showing his heroic speed and strength as he defends the ships, in the same way that, in this passage, Hector's heroic strength is shown by his ability to first life the enormous rock, but then to also use it to smash the Greek gate.

The use of language in this passage, even in translation, sets the tone for the situation very well indeed.
With the use of phrases such as "there was a great roar from the gate", Homer is almost able to make his audience visualise the shattering of the gate by Hector.

The vivid descriptions of Hector actually throwing the rock with such power that it "hit the door full in the middle and smashed it out of its pivots on either side" are told in such a way as to show the carnage that war brings, but also, once again, to echo a time of great heroes and great deeds.
These deeds are told in such a way as to be familiar to the audience to whom Homer was performing his poetry.

This is shown here especially by the description of how Hector handles the rock "as a shepherd easily picks up a ram's fleece...and scarcely feels the weight, so Hector lifted up the rock", which shows how the concept of a heroic warrior culture such as the 'Iliad' was very much foreign to Homer's audience, but other cultures, such as agricultural and farming cultures were absolutely normal to his audience.
Homer therefore uses similes that would allow him to explain one society to another, as is shown here.

He also uses a variety of epithets, not only in this passage, but also throughout the poem, such as referring to the Trojans as "horse-taming Trojans" frequently, as this, like the description of a shepherd picking up a sheep's fleece to describe the ease with which Hector picks up the rock, is another example of how Homer is able to show a heroic society by using agricultural similes in order to make the heroic society of the 'Iliad' more understandable to his audience by using language familiar to them.
The description of the Trojans is also interesting, however, because it brings about a small degree of foreshadowing about how this war will end, with the Greeks sacking Troy thanks to the plan of entering the city in the wooden horse.

It also foreshadows the end of the poem, as the final line of the poem is "Such were the funeral rites of horse-taming Hector" and the Trojans mourn for the loss of their greatest warrior.
The end of the passage is also the end of Book 12 of the 'Iliad'.

It ends with Hector calling the Trojans to cross into the Greek camp, which they do and with "the panic-stricken Greeks [fleeing] back to their hollow ships [as] all hell broke loose." Such an ending to the book is interesting, because it appears to be a very abrupt ending, as it has been leading up to what looks to be a huge, violent battle.
Once again, by leaving the potentially huge battle until the next book, Homer is foreshadowing the bloodshed that is to come, rather than showing it right at the end of this extremely intense passage of action after the Greek gate was broken, giving the audience a sense of great anticipation of what is to come later in the poem.

The main subject of epic poetry is heroism and war, as war was one of the most serious, and epic, ideas a poet could write about.
In writing the 'Iliad', Homer looks back to an age 'containing Mycenaean, Migration and Dark-Age ingredients...mixed freely to create a world appropriate to his heroes." Homer is looking back to an age of great men and heroic deeds, when he was composing his poetry at the end of the 'Dark Age' in Greece.

Ancient society is, by and large, what is known as a 'shame culture'.
The heroes in Homer's 'Iliad' worry what other people will think of them.

It is for this reason that, when Agamemnon is forced to give up Chryseis, he becomes unhappy, claiming that he will be "the only one...without a prize" and if he ends up the only man without a prize, he will look bad in front of his men.
It is his response to this, however, that prompts Achilles' actions, with disastrous consequences.

One of the main concepts of the 'Iliad' is the idea of a 'heroic code', now widely used by modern scholars as a basic set of heroic values.
It is, however, a common idea that part of the heroic code is that, if a man is a hero, then he has to be the best at what he does.

Being a hero, he may die young, killed by another great hero, so it is important that he proves himself in his short life by killing other heroes, as heroes are men who "affirm their greatness by the brilliance...with which they kill" as well as the heroes whom they kill, so they are judged on results in battle.
Elsewhere, except for in the 'Iliad', the concept of a hero is one of a man or a figure who is greatly worshipped in certain cults.

In the 'Iliad', however, to be a hero "signifies a warrior who lives and dies in the pursuit of honour and glory" and a great example of this type of hero is the fearsome leader of the Myrmidon warriors, Achilles, who had a choice of fate and when he could die.
Achilles' choice was that he could either fight gloriously, die young, but be remembered forever, or he could die old in his home, but his "heroic glory will be forfeit, but [his] life will be long and [he] will be spared an early death." By choosing to fight in the Trojan War, Achilles forfeited long life, but achieved everlasting fame.

It can be argued, however, that with regards to the concept of what constitutes a hero in Homeric society, Achilles wins his fame for the wrong reasons.
In Homeric society, heroes are remembered for their valiant deeds in battle and for killing other great heroes.

Achilles' fame, chiefly, is not for such deeds in battle, despite his killing of Hector, but he is instead remembered as the Greek warrior who defected from the Greek army following his quarrel with Agamemnon, which is not the general action one would associate with such a hero.
While these are the general principles of the heroic code, certain heroes in the 'Iliad' view the code and their heroic priorities differently.

Achilles, for example, is fighting to win 'kleos' and 'timē', in particular from Agamemnon, with whom he has a background of, he believes, not being given enough respect, as shown when Achilles speaks of how Agamemnon had treated him "like some refugee who counted for nothing" during his reply to Ajax when the embassy is sent to him, after being offered prizes by Agamemnon if he returns to the fighting.
Agamemnon, however, is not showing Achilles the 'timē' and honour he deserves by offering him these gifts, because Agamemnon "is offering to include Achilles within his own sphere as son-in-law and subordinate", meaning that, even by making such a grand gesture, he will still be above Achilles in rank.

Achilles' nearest Trojan equivalent, Hector, has some similar heroic values to Achilles.
Like Achilles, Hector goes out to fight because he wants to be heroic and he wants to win 'timē'.

In a conversation with his wife Andromache, she begs him not to fight, knowing that, if Hector died, she would lose everything as Hector is "father and mother and brother...as well as my strong husband" after Achilles had sacked her home town and killed her father and her seven brothers.
Hector, however, ignores her pleas not to fight, because he had "trained [himself] always to be a good warrior...and try and win glory for [his] father and [himself]".

Hector knows that, by going into the fighting, he will leave his family behind at his death, but if he stayed at Troy "like a cowards and slunk from the fighting", then he would receive only 'aidos', which means shame.
If, however, he went out to fight, and even if he died, he would still win the 'timē' and honour he desired, as, by dying bravely, he can win glory as he has died in an act that "puts a seal on a life lived in accordance with...standards of heroic excellence" as he will die being killed by a great hero.

Interestingly, it is this moment that Homer puts into his audience's mind the fact that Hector will eventually die.
After Hector leaves Troy, Andromache and her waiting women "mourned for Hector in his own house, though he was still alive", showing that they are sure that he will not survive the battle with the Greeks, and showing how "Hector's story begins, as it ends, with his funeral", an idea not only foreshadowing Hector's death, but also the death of other great heroes, as it is ultimately through doing heroic deeds that a hero will meet his death, which arguably raises questions about a hero's leadership, if they become so involved with their heroic deeds that they end up getting killed, therefore diminishing the defences of their allies and, in the case of Hector, leaving Troy without its best leader.

While Hector and Achilles are fighting for themselves, certain heroes in the 'Iliad' view heroism as something other than killing heroes.
One of these heroes is Sarpedon, who speaks to his comrade Glaucus, that the people of Lycia have them "singled out for honour...they all look up to [them] as gods" and that they are not fighting to defend Lycia and its people, but "on behalf of their status within it", as they know they have a duty to fight better than common people in battle, because they are better than common people.

Therefore, Sarpedon's view of a hero's privileges is that they must be earned by the hero doing his duty in battle, but they also show "the warrior's special status and role".
In his speech to Glaucus, Sarpedon's perspective shifts, as initially he praises the role of the warrior, saying that he "could be sure of becoming ageless and immortal" thanks to the glory of war, but he then goes on to say "a thousand demons of death hover...and nobody can escape or avoid them", so that, even though a hero may be metaphorically immortal, because of his fame, he cannot be literally immortal, because mortal men must eventually die.

The concept of heroism in the 'Iliad' clashes on several occasions with the concept of leadership, raising the question of whether, when having an army to consider, the heroic option is necessarily the best option.
One example of whether the heroic option is the best option is an exchange between Hector and his right-hand man, Polydamas, who advises Hector to "withdraw into the town now and not wait for daylight here in the open", because while Achilles was absent, there was a chance of victory for the Trojans.

Now that Achilles has returned, however, Polydamas advises Hector that Achilles "will never be content to stay in the plain", but he will target Troy and Hector himself.
Hector, however, rebukes Polydamas, telling him that "the man who tells [him] to retreat...no longer speaks [his] language", as he wants to fight and win 'timē', but, in deciding to fight Achilles, Hector makes his 'hamartia', or his fatal error, because he pays for his heroic desire with his life.

In fighting Achilles and dying, Hector leaves Troy without a leader, thus raising questions over his ability to balance leadership with his desire to be a hero.
That said, while at this point, Hector's judgement is fatally clouded by this, it would not be fair to say that he wholly fails in his duty as a leader.

Indeed, there are occasions where he shows exceptional leadership, most notably in his fight with Ajax.
When he sees his opponent, he feels fear, but he knows that he cannot "turn tail and slink back among his men", because, if he turns away from the fight, his men will lose hope.

By fighting Ajax, Hector is sending a message to his troops, which will boost their morale.
He also sends the same message when, after being hit by a spear, he continues to fight, showing that he will not give up.

Another hero in the poem whose leadership comes under scrutiny is Agamemnon, most particularly at the start of the poem.
In the 'Iliad', Homer echoes the values of poetry, where the heroes, are the 'aristos', or the best, and Agamemnon is the most powerful king.

However, it is not true to say that, just because he is the most powerful, everything that he says goes, because Achilles feels able to stand up to him.
In the assembly at the start of the poem, when the Greeks are discussing how to placate Apollo, Agamemnon's judgement is clouded, but not by his desire to do heroic deeds, but instead by his fear that, if he loses Chryseis, he will look bad in front of his men, which causes him to take the action which precedes Achilles' withdrawal from the fighting.

It is this which calls into question his capacity as a leader because it he often lets his pride get in the way of his judgement, with disastrous consequences following Achilles' actions.
Finally, Achilles' ideas of leadership also come under scrutiny as, like Hector, he fights for himself so he can win 'kleos' and be seen as a great hero and this desire for fame is what brings about his 'hamartia' when he leaves the fighting in the first place, and then allows Patroclus to fight instead.

However, when he decides to send his men into battle, Achilles makes "an encouraging speech, he takes his place at the head of the contingent...and starts the advance to battle".
Like Hector during the fight with Ajax, Achilles show good leadership by rallying his troops, showing that, at this point, a hero can be a good leader, by giving his troops the motivation and encouragement they need.

However, Achilles negates this following Patroclus' death, when he vows to bring back "the armour and head of Hector" so that he can avenge Patroclus' death and show that he has killed a great hero, thereby showing that, like Hector, Achilles can let his personal feelings and his heroic desires cloud his judgement, whereas a good leader should be focused primarily on his troops and not on his feelings and desires.
The question, therefore, of whether being a good hero is compatible with good leadership, comes down to how well the heroes in the poem balance the two qualities.

Throughout the 'Iliad', save for certain instances, it is generally the case that the major heroes of the poem, Achilles, Hector and, to an extent, Agamemnon, are unable to balance the two, because they are fighting this war for themselves and their desire for fame is their downfall, especially for Hector, who's tragic end could have been avoided had he not fought Achilles.
Medicine in the ancient world, and especially in ancient Greek medicine, has been dominated by the idea that what can be observed by the five senses can be used to identify whether a patient is sick or healthy.

This meant that ancient doctors would have taken their observations on what happened on the outside of the patient's body and they were expected to use these 'in order to determine what was happening inside' the body.
This idea, along with several others, became very important with the introduction of Hippocratic medicine.

There are several texts associated with Hippocrates, known to many as the 'Father of Medicine', which were written between the 5 th and 1 st century BC, meaning that they could not all have been written by one man.
These texts were known as the 'Hippocratic corpus' and were presented in a variety of different ways, as some texts are based around recipes and advice, whereas others 'give grand theories about the nature of the body and the origin of disease' instead.

With the advancing of the concept that what could be observed on the exterior gave doctors the ability to diagnose illness on the inside of the body, Hippocratic medicine was given the status of being a 'technê', as what can be observed shows what cannot be seen, in the same way as prophecy allows people to know things that they cannot see upon seeing something else.
This is an idea portrayed in Aeschylus' play 'Prometheus Bound' with Prometheus' words: 'If you hear the rest, you will marvel even more at what crafts and what resources I contrived.

Greatest was this: when one of mankind fell sick, there was no defense for him - neither healing food nor drink nor unguent; for lack of drugs they wasted, until I showed them blendings of mild simples with which they drive away all kinds of sickness.
The many ways of prophesying I charted...' This idea of medicine as a technê, or as something which gives knowledge, is also shown in Thucydides' account of what became known as the plague of Athens, although many scholars have speculated that the symptoms may be those of a different disease, or one which has been modified over a number of years.

Thucydides, with his description of the epidemic, which lasted from 430-426 BC, leaves 'a clear clinical picture of the pestilence [and] identifies its infectious nature'.
This implication that disease and infection was passed on through contact as, according to Thucydides, 'those with naturally strong constitutions were no better able than the weak to resist the disease' was an extremely radical idea, because at the time, people were not aware of germs.

According to Hippocratic medicine, disease was not caused by contact, but by having too much of something in your body.
Hippocratic medicine was very much centred on the concept of the 'four humours', which are named in the text 'The Nature of Man' as being 'blood, phlegm, yellow bile and black bile', but other Hippocratic texts have cited them as being four different fluids, such as phlegm, bile, water and blood.

Hippocratic medicine, therefore, by and large would cite an imbalance in the four humours as one of the major causes of illness.
However, the causes of epidemic disease are cited by the Hippocratic texts as being something more than a simple imbalance in the humours.

The Hippocratic texts have stated that 'some diseases are produced by the manner of life that is followed [but] others by the life-giving air', because, according to the author, the air was 'the only factor shared by young and old, men and women' and the theory was that, because the disease was affecting everybody, there had to be a common factor to tell why everybody was being affected, and the surrounding air was the common feature.
Thucydides' reasoning for why epidemic disease struck was rather different.

The plague had struck Athens during the invasion by Sparta.
This meant that more and more people were moving from the surrounding area of Attica into the more well-protected city of Athens and consequently, the city became overcrowded, which 'made matters much worse than they were already' as the newcomers were being worst affected by the epidemic, as there was little or no housing for them because the city was so crowded, so they were forced to live in huts in the heat.

These huts, according to Thucydides, were very poorly ventilated, causing them to become overheated and promoting the spread of disease.
It is important to recognise the fact that Thucydides does not explicitly mention whether the Spartans, who were besieging Athens, were affected.

It is possible that this was for reasons of propaganda, so that the Athenians could appear impressive by surviving something as dangerous and strange as the plague.
Medically, however, the idea of crowding is crucial, because the absence of any form of this disease being recorded in Sparta is a reminder 'that Athens, not Sparta, was under siege and severely overcrowded', which shows how the idea of overcrowding in a city was viewed as an important factor for the spread of the epidemic.

There is, however, another part of Thucydides' text which indicated that overcrowding was not the sole factor that caused the epidemic, as he mentions that the plague returned in the year 427-426 BC, when Athens was not under siege, and it is stated that 'it had never entirely stopped though there had been a considerable decline in its virulence'.
This concept of the plague recurring in a second, if less severe, outbreak, shows how Thucydides, despite having little or no medical training, had 'clearly recognised the infectious nature of the disease and that patients who survived the illness were immune to further attacks or developed only mild recurrences', although Thucydides had never 'possessed an understanding of contagion or immunity', once again giving an indication of how little known these ideas of disease being passed on by contact and also of partial immunity from disease once a person had contracted it previously were in classical Athens.

It also raises the question of what other factors would have contributed to the cause of the epidemic if it was not simply due to overcrowding.
One of the theories which had been very popular until the 1860s was the 'miasma' theory, which stated that the plague had spread because 'the air spread poisons produced by rotting materials or stagnant water'.

This was very much a Hippocratic theory and even survives to this day, through terms such as 'malaria', even though, in the 1860s, when germs were becoming recognised, it was dominated by the theory of contagion, a theory Thucydides himself had noted, but had not fully recognised it for what it was.
Thucydides shows that he was familiar with Hippocratic medicine, most notably when he describes the means used by the Athenians to attempt to cure themselves of the plague.

Firstly, he mentions how the plague was a disease that was so new that the doctors had no idea about how it should be treated.
Indeed, Thucydides notes that the mortality rate amongst the doctors was the highest, as they were the ones 'more frequently in contact with the sick', once again emphasising the idea of disease being passed on by contact with other people, so because they did not know how to treat their patients, the doctors also did not know how to treat themselves when they fell sick.

One of the reasons why doctors did not know how to cure the plague was, according to Thucydides, the fact that it brought out a variety of different, and often obscure, symptoms, which made the disease very difficult to identify.
Doctors were well familiarised with Hippocratic ideas for treatment, but because the concepts of Hippocratic medicine centred on the balance of the four humours and on the influence of outside features such as the weather, they would have been confused by the plague.

This is because, as Thucydides states 'human art or science [was not] of any help at all', and this was largely what Hippocratic medicine was based around.
In the 'Hippocratic Corpus', which is the work which is made up of between sixty and seventy medical texts, there are seven books called the 'Epidemics'.

These are books of case notes written by physicians.
Thucydides' description of the time of year is, in terms of language, very similar to some of the case notes in the 'Epidemics', as Thucydides 'starts his description of the symptoms with a general remark that the year was notably free from all other kinds of illness' and comments such as these were very common in the 'Epidemics'.

The comment also adds to the ideas of why curing and diagnosing the plague was so difficult for doctors and also reinforces the issue of balance which was so crucial to Hippocratic medicine.
However, when the plague struck, there was no problem with the balance of the humours in the body, as Thucydides has noted that even healthy people were affected, or with the air at the time, so the doctors found it hard to diagnose what was causing the plague in the first place, which would have made it even harder to cure.

Another way which the Athenians tried to cure themselves of the plague was by praying to the gods and consulting oracles.
Again, this is very much a Hippocratic idea, as the Hippocratic writers believed that the 'divine', or the gods, were what caused changes in the weather or the balance of the humours and it was these changes that made people ill.

This is an idea used by the Greek historian Diodorus Siculus in his 'World History' when he 'used Thucydides' description of the Athenian plague as a model...
[when he] describes an epidemic which afflicted the Carthaginians investing Syracuse in 397 BC'.

The difference between Diodorus and Thucydides, however, is that while Diodorus' style is in accordance with the gods causing sickness, Thucydides says that 'As for the gods, it seemed to be the same thing whether one worshipped them or not, when one saw the good and the bad dying indiscriminately', thereby indicating that, unlike Diodorus, Thucydides did not hold the gods responsible for the attack of the plague.
He also notes here, and with the words 'equally useless were prayers made in the temples [and] consultation of oracles' that, although the gods were not responsible for the plague, they did nothing to help cure it.

Again, the fact that, ultimately, when discussing the cause and cure of the epidemic, 'Thucydides makes no attempt to account for the onset of the plague in terms of the anger of affronted deities' is exceptionally rare and radical for the time.
This is because, instead of using the well-known Hippocratic concepts such as the humours and fluctuations in the air and atmosphere, Thucydides is giving a 'rational, careful [and] detailed description of a disease', as he describes the initial 'vomitings of every kind of bile that has been given a name by the medical profession and then the outbreaks of pustules and the burning sensations.

This is an indication of a change in medical thinking which was very advanced for a time when there was no knowledge of germs and, consequently, no knowledge of contagious infection.
In choosing to describe the symptoms of the plague in such detail, Thucydides, while not rejecting the Hippocratic theories outright, is bringing about a new idea that diseases can be treated by examining the symptoms of the population and finding common features in the symptoms, rather than putting illness down to a lack of balance in the humours, or fluctuations in the atmosphere, which were either seasonal or brought about by vengeful gods.

Thucydides' description of the plague of Athens is, therefore, extremely radical for the principal reason that the ancient Greeks did not know about germs passing infection from person to person through contact, which is what Thucydides is implying occurred during the epidemic in Athens.
The description, however, does not reject Hippocratic theories on the cause and cure of epidemic disease completely.

Instead, Thucydides uses these theories and expands on them, therefore showing that the methods for the spread of epidemic disease can be much greater than simply through change in the atmosphere and instead can be a result of 'an infectious agent associated with an animal or insect reservoir, or respiratory transmission with a 'reservoir-like' mechanism ensuring persistent re-infection', which would give some explanation as to why the plague resurfaced when Athens was not besieged, and also how these methods are much more complex than Hippocratic doctors recognised.
"O perish all the gold and every emerald in the world sooner than any girl weep as we march away!

It is right that you, Messalla, campaign by land and sea to adorn your town-house with the spoils of war.
But I am held a pris'ner, fettered by a lovely girl, and take my post as keeper by her cruel door.

Glory has no charms for me, my Delia.
They can call me Slack and ineffective, if only I'm with you.

O let me gaze at you, when my last hour comes - Hold you, as I die, in my failing grasp!
Delia, you will weep for me on a bed of burning And you will give me kisses mixed with bitter tears.

Yes, you will weep; your heart is not encased in iron Nor is there flint in your tender breast.
There will be no young man and no unmarried girl Going home dry-eyed from my funeral.

But do no violence, Delia, to my departed spirit: Spare your flowing tresses and spare your tender cheeks.
Meanwhile, with Fate's permission, let us unite and love.

Tomorrow Death will come, head hooded - in the dark, Or useless Age creep up, and it will not be seemly To make white-headed love or pretty speeches.
Light Venus is our duty now, while there is no disgrace In breaking down a door, while brawling brings delight.

Here I can lead and soldier well.
Eagles and trumpets, dismiss!

Bring wounds to greedy husbands, and bring them money too.
But let me live at peace myself, with produce heaped in store, Looking down on hunger as I look down on the rich." (Tibullus, 1.1, lines 51-78) The opening lines of this passage recapitulate the opening words of the poem, which run like this: "Wealthy let others gather for themselves in yellow gold and occupy great acres of cultivated land -...

But let my general poverty transfer me to inaction so long as fire glows always in my hearth." In doing this, the words also reiterate the love that Tibullus feels for this woman, even though, as yet, there has been no mention of her by name.
It becomes clear to the reader that Tibullus has placed his deep love for her above everything else, including: "...all the gold and every emerald in the world" which he wishes would perish before he sees the girl he loves weep as he goes away to war.

With these words, as well as the previous couplet where he passes riches which he may gain in battle to those who can endure such a life, coupled with his opening words, Tibullus is explicitly rejecting a traditional means for a Roman male to achieve wealth and power, as he places his love for a woman above any material gain, even if she will not requite his love.
It is implied, therefore, that he does not crave any form of power, including the power over a woman that he might hold over his beloved if they were married.

This is made even more remarkable by the fact that, in his poems, the reader never in fact sees Tibullus and this woman happily together, raising the question of whether she may have had other lovers whom he knew about and that their relationship was an unhappy one because of this knowledge, but he was powerless to do anything to her because he loved her so much.
When Tibullus then addresses Messalla, his army general and patron, he immediately tells him: "It is right that you, Messalla, campaign by land and sea to adorn your town-house with the spoils of war." According to Tibullus, therefore, nothing is "intrinsically wrong with war and the wealth that comes from it", however, this is not a life that he desires, as his calling is different in that he is being: "...held a pris'ner, fettered by a lovely girl, and takes [his] post outside as keeper at her cruel door." The reader now becomes aware of this poem's state as a 'paraklausithyron', or a poem where the lover is left locked outside his beloved's door.

However, the imagery used for this description is considerably out of context for the situation, being at once very much military-orientated imagery and at the same time, very domestic imagery, bringing to light two different metaphors for the lover.
The first metaphor is that of Tibullus as one of the 'spoils' of war, as the Roman soldiers would display their spoils outside their houses, so that others might take note of what they had achieved.

In this way, Tibullus is one of the 'spoils', not of war, but of love and it is the woman he loves who has made him this way.
The second metaphor is of a slave or a doorman who, like other Roman slaves of this nature, was kept chained outside the door of the master or mistress's house to welcome guests.

Although the reasoning for Tibullus being there is different, the image works because he is, metaphorically speaking, a slave of love.
There is also a comparison between the girl he loves and his general, Messalla, as she " is the domina in a double sense, he is the slave, prize of love's warfare"; as well as being treated like a slave, or a war-prize, as he is kept outside, Tibullus is also literally a slave to his unrequited love.

Tibullus then, for the first time, introduces his beloved by her name, Delia, whereas previously, there have only been few hints of their relationship, possibly in order to "hold back the 'facts' to prevent them from interfering with [his] desires" as all the lover wants is a simple life where he can admit that: "Glory has no charms for me, my Delia.
They can call me slack and ineffective, if only I'm with you." The image of the funeral, which follows his declaration of happiness in passivity, is not uncommon in love elegy, but he begs that Delia might: "...let me gaze at you, when my last hour comes - hold you, as I die, in my failing grasp!" Ostensibly, the reader now sees a shift in the balance of power between the lover and the beloved, as, at his death, Tibullus "believes he will have some power, at least emotionally, over Delia", as he is no longer chained at her door, but at the moment of the burning of his funeral pyre, she: "...will weep for me laid on the bed of burning and [she] will give me kisses mixed with bitter tears." However, this is not truly a shift in the balance of power, as, even though Delia is leaving offers to him with her tears and kisses, mirroring how Tibullus himself left offerings to the gods earlier in the elegy, he cannot use her offerings to his advantage now that he is dead.

He also presumes that, since: "[Delia's] heart is not encased in iron nor is there flint in [her] tender breast." she must at least slightly care for him, as he believes making her weep at his death will not be difficult.
However, the reader must still remember how Delia had previously "bound the poet before her hard doors", making the reader doubt that she truly has feelings of tenderness and love towards him.

Ironically, Tibullus later begs Delia to: "...do no violence...to my departed spirit: spare your flowing tresses and spare your tender cheeks." as he believes that this will hurt his ghost and he does not want her to be responsible for this.
This undermines the initial superficial power shift as it now becomes clear that the poet's concern for Delia means that, not only can he not exercise full emotional control over her, but "the logical extent of such power would actually harm the poet", meaning that he is forced to appeal to Delia to not do these things, rendering him as powerless in death as he was in life when he was left outside her door.

Once again, Tibullus uses military imagery in a much more subtle sense, with: "nor is there flint in [Delia's] tender breast." This resembles, arguably, a flint arrowhead embedded in Delia's breast, giving the reader a different metaphor; the traditional motif of the wound of love in a lady's breast, the lack of which indicating the lack of love that Delia has for Tibullus, rather than the fact that she feels any compassion for him.
The final section preceding the final couplet ostensibly opens with an abrupt shift in the mood of the poem, lightening the tone with the idea of uniting and loving - although this is undercut by such a concept working with the consent of Fate - only for Tibullus to add that: "Tomorrow, Death will come, head hooded, in the dark..." This not only returns the reader to the sombre state of affairs shown earlier, but also, with the implication that love will end, apparently, with death, discards or undercuts the visions of Tibullus with Delia on his deathbed.

That vision, which previously seemed so loving, is now viewed "as being empty and unfulfilling [with the lover preferring] the much more immediate and much more enjoyable prospect of love in the present", forcing the reader to reconsider the previous lines where he talks about his vision.
The next five lines suddenly see Tibullus writing in reverse chronology, where he is suddenly an old man and realising that: "...it will not be seemly to make white-headed love or pretty speeches." This makes his earlier desire to be inactive ironic, because now this leads to what can be read as sexual impotence which is something that is undesirable to him.

Immediately after describing himself as aged, however, Tibullus turns his attention to the present, where: "...there is no disgrace in breaking down a door, while brawling brings delight." This personifies love in the form of the goddess Venus, who must be controlled and also shows the poet portraying love as something of a struggle for power.
There is something almost ironic in this as his affair with Delia is in itself a power struggle and one that Tibullus looked to have accepted was lost, but now he can break down a door without shame, leading the reader to the question of why he appeared so weak when he was faced with Delia's door.

The final couplet of the elegy: "But let me live at peace myself, with produce heaped in store, looking down on hunger as I look down on the rich." is an abrupt reminder of the opening of the poem, where Tibullus regurgitates his idea that he does not desire wealth or activity, like a traditional Roman man, or soldier, but instead only desires peace.
To the reader, these final lines can encapsulate the entire poem, because, with his love for Delia, Tibullus is showing that not only does he not crave a traditional Roman lifestyle, but he also does not appear to crave the power that goes with a position of authority.

Love elegy, by the time it reached the Latin poets, was already a very popular genre dating back to Greece and Alexandria.
Because very little poetry from these times survives today, several critics have questioned the importance of these works to Latin love elegy.

With that said, however, it cannot be denied that certain elements of Greek culture have become strong influences in Latin elegy, especially considering that "it is a task of no great difficulty to compile a register of passages where the Latin poets appear to have been inspired by Greek models", which is especially true in the elegies of Propertius when he makes allusions to myths in his elegies, and also in some of the longer poems of Catullus.
It is similarly true to say that, regardless of past influences, Latin love elegists found themselves "justified [when writing love poetry]...in introducing pastoral themes".

This is especially true in the poetry of Tibullus, where he is shown rejecting ideas that would be befitting to a traditional Roman male in favour of an inactive, country life.
The issue of power relations between the 'lover' and the 'beloved' is also an interesting one, especially when comparing how it is treated in the elegies of Propertius and Tibullus, and this is what will be explored here.

One of the most interesting aspects of the relationship between Tibullus and his 'beloved', Delia, is that, despite the enamoured nature of his poetry, they are almost never seen together in happiness.
There is the impression that she is, in fact, a prostitute, as well as a married woman.

The reader understands that Tibullus is aware of this, although he is powerless to do anything except warn her husband, especially when telling him: "But you, sure, careless husband of a deceitful girl, keep watch on both of us to stop her playing false." Already, the reader sees how Tibullus can only advise, rather than challenge, Delia's husband, in the same way that he cannot challenge Delia herself, as he is so infatuated by her.
By writing these words, he shows how, as a lover, he is powerless against Delia and can do nothing to ensure that she stays with him.

An earlier hint of his unhappiness at his unrequited love for Delia is given when he likens her to: "...the Nereid of old who rode a bridled dolphin to Thessalian Peleus - Thetis the blue-eyed." in which he gives one of his otherwise rarely-used, in comparison with other elegists such as Propertius, mythical comparisons, here used to show how Delia has 'bewitched' the lover and then failed to reciprocate his love for her.
The balance of power between Tibullus the lover and Delia is also reflected in the balance between Tibullus as a poet and as a lover.

Compared to Greek love elegists who wrote in the third person, "Roman elegists [wrote] about erotic matters in the first person and claim they [were] writing about their own personal experiences", which reinforces the powerlessness of Tibullus the lover, which is being constructed by the author, giving the author a similar power over the lover to the power that Delia holds over him.
Several of Tibullus' poems about Delia take the form of a 'paraklausithyron', or a poem about being locked outside a door.

In his poetry, the door he stands outside "is strong and impervious...[and] the poet is powerless to overcome it", to the extent that he is reduced to begging Delia's slave to: "Discipline fresh misery with drink, letting sleep invade these tired defeated eyes." thereby revealing how he is in need of other means of suppressing his grief and bringing himself comfort, such as wine and sleep.
However, even in comfort, he is showing his powerlessness with Delia as he has been reduced to begging a young slave-boy for help.

Another way in which Tibullus illustrates his helplessness as a lover is by actually talking to the door and initially cursing it: "O door, stubborn as your master, may the rainstorm lash you and launched at Jove's command my flash of lightening blast you." and then, barely moments later, by pleading with the door: "Please door - open just for me, moved by my complaining....
Forgive me if I cursed you in my infatuation.

Let the curses light on my own head." As an author, Tibullus is using the door, not only to show the lover's powerlessness, but also almost as a mediator between the lover and Delia, as he struggles with his feelings that alternately make him curse and plead with the door, in a way that he might do so in the face of unrequited love.
However, it also shows the lover as being weak, since "victory in...the tipping of the power balance in favour of the poet [or lover]...would open the door", but as the door remains closed, he cannot gain this victory, or the power that would come with it.

The paraklausithyron also serves, in the fifth elegy, to remind the reader that, because Tibullus is now reduced to seeing that: "[Delia's] door [is] unmoved by words" and that he must watch from a distance, "it is with [Delia] that the ultimate power lies" as he is forced to appeal to her, not only from the door, but also from a distance, so it is evident that she will pay him little attention compared to her rich lovers.
One of the great tragedies in the relationship between Tibullus and Delia is brought to light by the use of military and pastoral images in his elegies and, to a lesser extent, images of being hunted like prey.

The military imagery is used, in conjunction with the pastoral imagery, to contrast the inactive life in the country that Tibullus openly desires with the arduous life of a soldier which will bring riches that his beloved desires.
He is aware that she has other lovers, but he would rather injure himself than her: "I'd never strike you, Delia, but should the mad fit come I'd pray to lose my hands." This idea of the lover being unable to hurt his beloved, while at the same time granting her permission to: "...if she thinks me false...pull me by the hair." ties in with the idea of Tibullus as both a slave and as 'spoils' of war as he is waiting outside the door.

It is therefore, ironic that he wants nothing to do with war, yet he is being forced to, albeit passively, take a part in it, s he is a victim of 'militia amores', or the war of love.
He is therefore forced to "reconcile the twin poles of private and public life, of leisure and active service...of duty and love, of war and farming" and the tragedy is, as far as war and farming go, this is not possible for him.

The images of being hinted also tie in with the military imagery, as Tibullus desperately invokes the god Love: "How hard on me you are!
Is it so glorious for an immortal God to set a man-trap?" once again raising the issue of the war of love, as well as placing Tibullus in the powerless role of the 'servitium amores', or the slave of love.

Tibullus, as a lover, also gains the reader's sympathies through the "attempts to assert the power [he] claims he does have", most notably: "Your poor man is prepared to offer service always, the first to come at need, inseparate from your side." when in fact he only succeeds in emphasising the contrast between himself and Delia's wealth, thereby accentuating that his power is inferior to hers.
The pastoral imagery is used to show "the old Roman ideals of simplicity, poverty...and all domestic virtues", which is exactly the life that Tibullus desires, rather than the wealthy, traditional Roman life of a soldier and the power that comes with it.

The lover's anguish stems from the rejection of this power and wealth, as it is clear that Delia desires rich men and all he can do is dream that: "She can rule us all, take charge of everything...
Delia will pick [Messalla] delicious apples from the choicest trees." while all the time he knows that his dreams can never be realised as he and Delia each have completely contrasting desires from life.

Tibullus' desire for poverty and simplicity is, tragically, incongruous to Delia's desire for wealth.
When Augustus ruled, he brought about a legislation that not only condemned adultery and promoted marriage, but also condemned homosexuality, unless men were having sex with young male prostitutes.

Tibullus, however, unlike Propertius, does experiment with homoerotic themes when he writes his pedarastic poetry about a young boy called Marathus.
The first of these poems, the fourth elegy of the first book, "releases the pressure [from the Delia poems] by turning to...a different...poem", although the theme of powerlessness is still ever present.

Tibullus invokes the god Priapus, for help in finding a 'puer delicatus', or a handsome young boy and he is told first: "O flee and never trust thee to the troop of tender boys." but then, immediately after this attempt to dissuade him, he tries to tempt him at the same time by extolling the virtues of several young boys.
Theoretically, Tibullus should now hold the power since "boys should [respect] poets for their power to bestow the immortality of the Muses", as he can have revenge, as a poet, on those who spurn poetry.

However, while he gains power as a poet, as a lover, he appears as powerless as he was with Delia as the reader learns: "Alas, how Marathus in love's slow fire torments me!
Science profits nothing - neither do arts." Clearly, Priapus' advice to give the boy what he desired until he succumbed was fruitless, because Tibullus has found a young boy, but the boy does not desire him.

He is tormented by love for Marathus, but he can do nothing to gain him.
In fact, he tells him in a later elegy: "It's useless to keep changing your tunics and your cloak and cramp your feet in tight-laced shoes." as he portrays himself as an expert in love, passing on advice to Marathus on how to seduce a woman.

Despite the departure from the serious tone of his Delia poems, Tibullus' pedarastic poems still tell the same story of a man in love, but powerless, for whatever reason, to gain the love of his beloved.
Propertius, like Tibullus, wrote his elegies under the Augustan regime, but his methods of portraying his relationship with his 'beloved', Cynthia, are sometimes very different to those of Tibullus as both elegists often use different devices.

There is a small hint of a shift in the balance of power when compared to Tibullus, which is most explicitly shown with the words: "Today I walk in heaven, among the stars.
Come day, come night, she's mine." which are spoken after Cynthia gives up her idea of a voyage in apparent favour of staying with Propertius.

This shows a marked contrast with the elegies of Tibullus, who is never shown as being happy or victorious in his pursuit of Delia.
However, the idea of Propertius holding power over Cynthia is vastly undermined by the idea of "the lover [Propertius...belonging] to the category of romantic lover", meaning that, as a lover, Propertius is equal, or inferior, to his mistress.

Cynthia, right from the outset, has caught him and made him: "Smitten before by no desires; Then, lowering my stare of arrogance, With feet imposed Love pressed my head." as she "prompts, forwards or obstructs the desires of the lover" forcing all his Roman masculine arrogance aside, and rendering him helpless.
In a later elegy, Propertius, as a lover, is rendered even more powerless when he tells a friend that he has no fear of going to war, but rather that he is: "...verbally estopped by a girl embracing [him] Who often lends her grave pleas cogent colour." Cynthia torments Propertius "with complaints and threats because his friend is urging him to the active life".

The reader is now presented with another marked contrast between Propertius and Tibullus, as it is Tibullus who craves a peaceful life and Delia who desires the wealth war could bring, whereas Propertius looks ready for battle, but Cynthia is holding him back.
This leads into the idea of Propertius the lover pitted against Propertius the poet, and in turn, against Cynthia.

As an author, "Propertius regarded his love poetry as...inspired by Cynthia", meaning that her beauty is famous because: "These no Apollo, no Calliope sings to me; My only inspiration is a girl." Because Cynthia's beauty is what inspires him, the power between her and Propertius is interchangeable.
On the one hand, Cynthia holds the power over him as an author, because had he not seen her, he would never have been able to write his poetry about her.

On the other hand, the fact that she is this well-known because of Propertius gives him power, not only over his alter-ego since he is the one writing the poetry, but also to a lesser extent, over Cynthia herself, as it is through his poetry that she is famous.
In this way, Propertius, as an author is able to, indirectly, give himself as a lover, a small amount of power over Cynthia.

The state of being in love is commonly described as both a sickness and a curse from the gods.
From his very first poem, "Propertius gives us the...image of love as a disease, a madness", as it is now that: "...for a whole year this mania has not left me, Though I am forced to suffer adverse gods" Because these adverse gods have made Propertius 'suffer' from his lovesickness, there is yet another indication of how powerless he is against Cynthia, especially considering that: "Bravely will I suffer knife and cautery, Given liberty to speak as anger bids." Despite the fact that Cynthia is in no way a god, the distance Propertius is prepared to go for her indicates that she has an almost divine, that is absolute, power over him.

This can also be linked to the words to his rivals when he speaks about how: "On me our Venus levies nights of bitterness And empty Love is present." to warn them, ostensibly against falling in love, but with the hidden undertones that he is much more possessive of her and jealously warning them to stay away from her, linking his lovesickness to jealousy of them.
It also links to his powerless state, as against Cynthia, against his rivals, to whom he can do nothing, in the same way that he is powerless against the gods.

According to Propertius: "Medicine can heal all merely human ailments; Love alone hates the pathologist" showing that, if love is indeed a sickness, it is by dismissing similar situations involving superficial wounds, such as blindness and battle wounds that Propertius shows how it is not a sickness that can be cured by a mere physician.
Instead, he uses mythical ideas, rather than mortal ones, such as: "Lifting the liquid load from tender necks, To liberate Prometheus' arms from the Caucasian Crag and drive the vulture from his heart," to give an idea of what the one who has the ability to cure such a sickness would have to be able to do and thereby showing how it is impossible for him to be cured.

In conclusion, both Propertius and Tibullus find themselves almost consumed by their love for their respective mistresses, to the extent that, especially for Tibullus, any power they, as lovers, may hold over them is rendered inferior to the power Delia and Cynthia hold over them as they give up what they desire to be with them.
In the case of Propertius, he gives up the thought of going to war to be with Cynthia, and Tibullus is prepared to give up everything to be with Delia.

Abstract
This report aims to identify and review the very relevant prose related to the field of molecular control by employing evolutionary algorithms (aided with a suitable short pulse laser technology).

Whether the experiment is feedback based or not the report will aim to achieve and compare strategies utilized by academia in order to reach a consensus and understanding.This includes (but is not limited to ) previous work in the field, theories, direction, concepts and techniques.
Owing to the nature of the report discussion relating to topics are as important as the source of that information.

Background & Introduction
In order to understand and manipulate very fast chemical and physical processes a better control of optical pulses needs to be achieved.

In other words, It is possible control atoms and molecules by very short laser pulses using so-called femtosecond lasers which is fast becoming a very energized and challenging research field.
Evolutionary algorithms are increasingly being utilized to optimize the shape of such laser pulses, based on reformulation of task as a high dimensional optimization problem.

In terms of optimal control, laser induced molecular orientation as an optimization problem involves a global minimum search on a multi-dimensional surface function of varying parameters characterizing the pulse itself (frequency, peak intensity, temporal shape ).
There is a high probability that any Genetic Algorithm aiming for optimization of different possible targets may temporarily be trapped in a local minimum.

Lasers have been around for more than 45 years and there has always been interest to generate ultra-short laser pulses in the picosecond(10 -12s) and femtosecond (10 -15s) range.
The first recorded instance of a pulse below the 100 fs duration occurred in 1981 with the use of colliding pulse mode-locked (CPM) ring dye laser.

This was further improved with subsequent nonlinear pulse compression culminating in a 6 fs short pulse,a record which stood till the beginning of the nineties.
Solid state laser media based femtosecond pulse generation brought many advantages including an improvement in output intensity(needed to get accurate and distinct results) and a revolutionary low cost method for generation of femtosecond lasers way less than 6 fs.

The field which is more interesting is pulse shaping rather than pulse generation as they have a strong impact as experimental tool providing unprecedented control over ultra-fast waveforms for spectroscopy, nonlinear fiber optics and high-field physics.
Weiner and the scientific community at large has recently shown particular affinity towards waveform synthesis by spatial masking of dispersed optical frequency spectrum.

SLMs (Spatial light modulators) allow reprogrammable waveform generation under computer control.
Weiner's detailed report mentions several methods for shaping pulses mainly Linear Filtering, Amplified Pulse Alignments, Programmable Femtosecond Pulse shaping using Liquid Crystal SLMs, Programmable Pulse Shaping using Acousto-Optic Modulators and Shaping using movable and deformable mirrors.

All based on the Pulse shaping theory.
Evolutionary Algorithms and related Control Problems

Evolutionary Algorithms are popular tools for search,optimization,machine learning and for solving design problems.
EA s use simulated evolution to search for solutions to complex problems.

Generally evolution strategies and genetic algorithms are the two basic types of EAs.
Genetic Algorithms were developed in the US under the leadership of John Holland and his students while Evolution Strategies were nurtured in Germany under Ingo Rechenberg and Hans-Paul Schwefel.

The main ways these differ are that GA s put a great deal of emphasis on selection, recombination and mutation acting on a genotype that is decoded and evaluated for fitness resulting in the Recombination to be emphasized over mutation on the other hand ES tend to use more direct representations with mutation being emphasized over recombination.
Although nowadays both of these freely borrow ideas from each other.

Evolutionary algorithms are easily parallelized and are what are known as weak methods in the AI community as they do not exploit domain specific knowledge and usually are a sort of blind search method.
The idea of Coherent Control was born out of mode-selective laser photochemistry, despite the initial buzz in the scientific world the dream of exciting specific chemical bonds dimmed for various reasons including the fact that controlling certain aspects were hampered by fast relaxation, complicated mode structure,imperfect knowledge of inter-nuclear potentials along arbitrary coordinates and the distorting influence of strong external light fields.

Recent successful experiments have led to control of molecular ionization and dissociation, atomic and molecular fluorescence and passage of currents in semiconductors.
Large condensed molecules are so complicated that it is nearly impossible to calculate optimal pulse shapes in advance but recent experiments 15 have used experimental feedback to determine the optimal optical pulse shape to achieve a particular goal.

In these cases,absence of detailed knowledge of the system lead to selection of new pulse shape inside feedback loop using a Genetic Algorithm in order to discover control pathways in complicated physical systems.
Judson and Rabitz 15 were also clearly able to demonstrate that an adaptive learning procedure could teach a laser to selectively excite chosen states of a molecule.

Constraints on the form or amplitude of the driving field could readily be included in the Learning Algorithm in accordance with laboratory capabilities (laying foundations to experiment such as : by Hadjiloucasal.
Conducted some fourteen years later - discussed later.

) also indicating that the introduction of intelligent physical bias in the control field could yield forms more readily generated in labs.
Coherent Control using adaptive learning algorithm

Pearson et al.
(2001) showed a detailed investigation of a learning strategy incorporating a modified genetic algorithm in order to discover control pathways in a complex physical system.

They carried out several experiments (in different phases or mediums) in a variety of molecular systems whose common feature were the existence of observable physical properties that depended on the incident (shaped) light.
The experiment deals with the construction of the algorithm and related adaptable operators and the further with a generic arrangement for measurement (as shown in Fig 1.).

The gaseous sample related experiment explores non linear control of ionization channels in diatomic sodium while many liquid phase experiments describe control of self phase modulation in CCl 4 and excitation of vibrational modes in methanol and benzene.
The experiment incorporated other concurrently running search methods in addition to traditional GA search strategies.

Simple Harmonic Generation principles are utilized to observe changes in characteristics of the samples as this is demonstrated by the simple control experiment.
Control of reactions by Laser Pulses

Assion et al.(1998) used tailored femtosecond laser pulses from a computer controlled pulse shaper to optimize branching ratios of different organomettalic photodissociation reaction channels.
The optimization process was based on the feedback from reaction product quantities in a learning evolutionary algorithm that iteratively improved the phase of the applied femtosecond laser pulse.

The experiments used CpFe(CO)2Cl showing two different bon-cleaving reactions could be selected, resulting in a chemically different product.
The method worked completely autonomously finding optimal solutions without previous knowledge of the molecular system and the experimental environment.

Initially, laser selectivity is lost (or is almost non-existent) because of rapid intermolecular energy redistribution.
A coherent control scheme (based on a set number of optimization parameters) is utilized to access the broad range of interference effects but May not have sufficient number of parameters for a complex system.

Shial. Proposed that in order to reach a specific reaction channel, the electric field of the laser pulse should be specifically designed and fitted to the molecule, such that the amplitudes of different interfering vibrational modes add up in a given bond some time after photo-absorption, causing its breakage.
The original experiment realized that by using a femtosecond laser system of pulse duration of 80 fs and an energy of 1 mJ at a centre wavelength of 800 nm and a repetition rate of 1 kHz, the goal of either maximizing or minimizing branching ratios of different organomettalic molecules is achieved without any knowledge of the specific molecular Hamiltonian (hence without requiring any knowledge about the experiment environment ).

The experiment then used an EA to optimize the spectral phase of femtosecond laser pulses.
The "genetic configuration" of a single individual corresponded to the SLM pixel voltage encoding.

An "individual fitness" was set equal to the measured branching ration of the competing reaction channels, that is, to the ratio of the respective ion signals.The "environment adaption" was found in the shaped laser pulse's capability of producing the desired experimental output.
In the course of the evolutionary process, the reaction product in the numerator is favored at the cost of a reaction product in the denominator, because in each generation only the fittest individuals were selected for reproduction.

Molecular Orientation and laser control
A highly significant French study by Atabek et al.

(2003) built upon the already existing knowledge base in the field by looking at local minima for the thorough interpretation of the orientation dynamics (of individual molecules - in terms of basic mechanics).
Two targets were retained : the first, simple, one searching for an angle between molecular and laser polarization axes as close as possible to zero at a given time ; the second one combined the efficient of orientation with its duration.

The experiment was illustrated by referring to two molecular systems HCN and LiF respectively.
A sudden and asymmetric laser pulse leads to a kick mechanism resulting in a fast angular momentum transfer to the molecule, that turns out to be responsible for an efficient orientation after the pulse is turned off.

What needs to be considered is the fact that the control of such orientations require physically sound targets that have to be carefully defined.
These are : Target 1: aiming at maximization of the amplitude (in turn efficiency) of the orientation (expected value of cos θ) without any reference to its duration.

Target 2 : A hybrid item achieving a compromise between efficiency and duration in such a way as to maximise a time average of (cos θ)2 After the targets are defined, the optimal control schemedeals with the minimization problem on a multi-dimensional surface.
Within these limitations of these sampling spaces, GAs turn out to be very efficient algorithms reaching a robust minimum after typically 500 generations.

Related work
A very recent interesting Korean (Republic of.) study by Hong et al.

(2007) Explains reconstruction of electric field of femtosecond laser pulses from interferometric autocorrelation using an EA.
In this study the GA is optimized for the intensity and phase characterization of several-cycle optical pulses.

Then the efficiency of GA is tested with numerically-generated (random in this case) pulses and then applying it to the experimental data (using EPIRIAC algorithm - a special algorithm suited for reconstruction of several cycle femtosecond pulses from their IAC traces).The application of EPIRIAC to the experimental IAC traces of a negatively chirped 31 fs pulse and a sub 10 fs pulse generated by an MDC Ti: Sapphire laser resulted in the reliable retrieval of their intensity and phase profiles.
In this way the EA able to work for the temporal characterization of femtosecond laser pulses.

Hadjiloucas et al.
at the University of Reading, UK performed pulse shaping of femtosecond duration laser pulse to optically control the evolution of a chemical (or bio-chemical reaction) using MATLAB.

A pump-probe spectroscopic device was used to map the potential energy(during non excitation state) surface of reactions and an evolutionary meta-algorithm in MATLAB (under LABVIEW environment) was applied.
The evolutionary landscape generated (keeping in mind different trajectories followed during evolution of reaction) was searched with the EMA for all possible pulse shape leading to optimal product formation.

When a global minima was reached a feedback loop in place was able to provide input to EMA for successful product formation.
The software module had two portions with the first application used the GA to match target waveform while the second solved molecular control problem for a four level system.

The experiment concluded that it was possible and feasible to observe at least one order of magnitude improvement in the convergence rate using the designed EMA in both waveform matching and molecular control problems.
Metro-logical Applications and future work (NLP Report)

The National Physical Laboratories recent detailed report into femtosecond optical pulse shaping together with methods for quantum control are demonstrated to have a significant impact on a diverse range of commercially important applications.
It is the culmination of work by several academics and scientist across the industry.

The report provided several recommendations covering areas such as high speed electric pulse generation, generating tailored terahertz sources, molecular reaction dynamics and development of femtosecond enhancement cavity technology for applications among others.
The report also compared and contrasted pulse shaping control strategies and explained their suitability for different kinds of experiments.

The relevant recommendations of the report are summarized below with the corresponding programmes they should become part of : NMS Electrical Programme : Electrical Pulse generation and measurement - To calibrate high sensitivity devices,NPL will be applying optical pulse shaping to its existing sub-picosecond electric pulse facility to investigate and develop electrical pulse shaping on a very small scale in order to meet such challenges.
THz Project : Terahertz Radiation : Pulse shapers will potentially be used for tailoring terahertz sources, this could be achieved by broadening the accessible spectral bandwidth or by enhancing the output at selected terahertz frequency bands of specific interest for end-user applications.

NMS Photonics Programme : Optical Communications : The existing ultra-short optical pulse measurement infrastructure could be extended to provide a robust optical pulse measurement system and pulse shaping capability in the 1550 nm band.
This would enable NPL to characterize the soliton transmission properties of conventional and novel communications fibers for long-haul data transfer.

NMS Programmes including Time and Frequency, Photonics and measurement for Biotechnology : Femtosecond Optical Pulse Delivery : There needs to be further experimentation and studies into to optimize the propagation of femtosecond pulse trains in hollow-core fiber.
Quantum Metrology Programme : Chemical Reaction Dynamics - UK University groups should collaborate and share their expertise for development of techniques for measurement of femtosecond time-resolved molecular dynamics using pulse shaping and manipulate to achieve quantum control.

Multiphoton microscopy : pulse shaping in conjunction with adaptive coherent control would play a significant role in the development of microscopy methods such as two photon microscopy and CARS for practical imaging, particularly for biological samples.
Conclusion

In conclusion the report has successfully reviewed and identified very relevant articles related to the field.
The field is very vast and requires collaboration on part of academics and industry.

The combination of lasers producing specifically desired and intense ultrashort pulse with developments in programmable pulse-shaping technology and learning algorithms control will continue to a rapid expansion in the numbers of fields where they could be fully applied.
These technologies have generated a huge amount of interest both in the scientific community and the media,as the exciting possibilities of this new region of spectrum gradually becomes reality.

Introduction
As part of the module CY3A2 System Identification we are required to create an ARX (AutoRegressive eXogenous) based recursive least squares program (using a template which has already been provided) to identify and work on a model.

The General definition of a dynamical system is accepted as the following: FORMULA with input signal {u(t)}, output signal as {y(t)} and where v(t) is some disturbance.
(q -1) is the backward shift operator, In other words (q -1) y(t) = y (t-1).

FORMULA Equation (1) can be expressed as a linear regression FORMULA where, FORMULA Hence the simplest ARX model can be written as FORMULA Without going into too much detail the formulae involved here in order to make the Recursive Least Square algorithm work can be summarized in four equations below where our objective is to get the estimated value of θ(from now on referred to as θhat).
FORMULA The RLS algorithm can be modified for tracking time varying parameters.

One approach is the RLS algorithm with a forgetting factor.
In this a term λ called forgetting factor is introduced to (9).

λ<1 and is usually 0.99 or near that value.
This means that as n increases, the measurements obtained previously are discounted.

Older data has less effect on the coefficient estimation and hence forgotten.
Hence (9) is modified so that FORMULA

Task A - Create a test system & Identify using recursive least squares (ARX)
In our given case we have to modify crib.m in such as way that it works as a recursive least square program.

As required by our Assignment instructions we have to choose a stable second order system as our model.
In other words FORMULA Here FORMULA A suitable stable second order model utilized for this purpose is FORMULA

step input
FORMULA What we have noticed in this case is that the recursive least squares program is predicting nearly the correct coefficients, the final value of the estimate it gets in the allowed 50 iterations is : FORMULA This usually starts to converge close to the true value around after 24 iterations.

But after further experimentation at 2000 iterations FORMULA and doesn't change after that.
Fig a1.

shows a step input fig a.2 shows y and yhat.
sampled sin wave as input

FORMULA As we can observe from fig 3 that this iterates more than 300 times and roughly begins to settle (i.e. coefficients) after 75 cycles.
But after more than 350 iterations a 1 = -1.5350 a 2=0.8598 b 1 = 0.418 b 2=0.0394, Although much better than the previous input is still not exactly equal to .

Random input
FORMULA Observations after 200 iterations are taken and a 1 = -1.5342 a 2=0.8587 b 1 = 0.416 b 2=0.0396.

Which are not poor estimates of the system.
In addition to that these are the best estimates of b 1 & b 2 so far.

Step Input with Gaussian noise added to output
FORMULA as we see in this case that the solution never converges to any solution and gives erratic answers.

value of  varies greatly and the last observation made is FORMULA.
which is totally inconsistent to any readings previously made.

Sin wave as input with Gaussian noise added to output
FORMULA Again this gives highly inaccurate results, which do not even come close to the true values of the coefficients.

The last recorded observation is FORMULA
Random input with Gaussian noise added to output

FORMULA last observation made FORMULA
Step input with forgetting factor

Including a forgetting factor and observing its effect.
When comparing this to step input without forgetting factor there are hardly any changes as lambda is too close to being 1 and hence doesn't produce really different plots.

had λ been not as close to 1 then the responses would have been starkly different.
so far all models we have done have only n a=n b=2.

Task B- Identification of the system parameters for the master of a head controlled telerobot
This part relates to a hear operated master/slave telerobot.

We are provided with two sets of data each with step responses (from a 60mm step input) with different conditions of K p and K d in the PID controller.
The input in the following cases will be u=60*ones(size(y)).

There are two sets of data sets we have been provided on which we have to do four tests each and comment on the results.
In addition to plotting the coefficients and y and yhat, we are also required to plot a z transform and find the location of the a coefficients on a unit circle.

If they ('a' coefficients ) all lie in the circle the given model is stable.
There are a total of eight experiments in this part four each for the two different sets of data.

They specify dimensions of θ with three figures the first plotting the coefficients, the second showing both y and yhat and the third showing a z-transform and finding locations of the final poles of 'a' coefficients.
Working with xp30d07.dat

Experimental run on the x axis where the proportional gain was set to 30 and derivative gain was set to 7 FORMULA In this case n a = n b = 2 and the last observed value of θ we get is FORMULA FORMULA In this case n a = 3 & n b = 2 and the last observed value of θ we get is FORMULA FORMULA In this case n a = n b = 3 and the last observed value of θ we get is FORMULA FORMULA In this case n a = 4 n b = 3 and the last observed value of θ we get is FORMULA It is concluded that All models are stable, some more than the other (although commenting on their degrees of stability is beyond the scope of this report).
ii) The best model to predict this particular mechanical system is the one having least deviation between the measured and the predicted value of the output.

In other words y - yhat should be minimum.
Which in this experiment is the model θ = [a 1 a 2 a 3 b 1 b 2 b 3].This is done by subtracting both vectors and finding an average for all values.

Working with xp40d03.dat
Experimental run on the x axis where the proportional gain was set to 40 and derivative gain was set to 3 FORMULA In this case n a = n b = 2 and the last observed value of θ we get is FORMULA FORMULA In this case n a = 3 & n b = 2 and the last observed value of θ we get is FORMULA FORMULA In this case n a = n b = 3 and the last observed value of θ we get is FORMULA FORMULA In this case n a = 4 n b = 3 and the last observed value of θ we get is FORMULA It is concluded that i)All models are stable, some more than the other (although commenting on their degrees of stability is beyond the scope of this report).

ii) The best model to predict this particular mechanical system is the one having least deviation between the measured and the predicted value of the output.
In other words y - yhat should be minimum.

Which in this experiment is the model θ = [a 1 a 2 a 3 a 4 b 1 b 2 b 3].This is done by subtracting both vectors and finding an average for all values.
Although θ = [a 1 a 2 a 3 b 1 b 2 b 3] is quiet close to the best model in this case.

Task C - ARMAX
Upgrade the basic RLS algorithm to either include instrument variable or a moving average noise model based on Pseudo Linear Regression, or both.

Illustrate these by comparing results with those obtained in B.
From equation (4) FORMULA In other words if the disturbance is described as a moving average of a white noise sequence e(t).

FORMULA Then the resultant model is FORMULA also known as ARMAX model (AutoRegressive Moving Average and eXogenous variable) The model which I am going to demonstrate here (for the sake of simplicity and to keep in check the length of this report) is the one where n a=n b=3 & n c=1.
Reasons for this being that these give the best way to compare the two different approaches we have covered.Hence our model can be summarized as FORMULA FORMULA The last observed value of θ we get is [-1.3286 0.0802 0.2631 0.0000 0.0034 0.0111 -0.4698] which is very interesting as the values of 'b's quite closely matches to xp30d07 with θ = [a 1 a 2 a 3 b 1 b 2 b 3 ] but not the values of 'a's.

Hence the only value of c we get here is -0.4698.
Not only does ARMAX add an moving average error term it also further reduced the difference between y and yhat (from the corresponding ARX based model)in this case and is therefore a better model for prediction.

This is also stable as all the values of a lie in the unit circle.
FORMULA The last observed value of θ we get is [-1.2965 -0.1792 0.4905 0.0001 0.0056 0.0090 -0.2233] which is very interesting as the values of 'b's quite closely matches to xp40d03 with θ = [a 1 a 2 a 3 b 1 b 2 b 3 ] but not the values of 'a's.

Hence the only value of c we get here is -0.2233.
Not only does ARMAX add an moving average error term it also further reduced the difference between y and yhat (from the corresponding ARX based model)in this case and is therefore a better model for prediction.

This is also stable as all the values of a lie in the unit circle.
Abstract

In order to understand and closely predict abnormal spontaneous electrical activity in brains of patients suffering from movement disorders (PD in our case) this report shows how Artificial Neural Networks are employed to achieve this goal.
In this assignment I have used MatLab's Neural Network Toolbox to extract relevant portions of data provided to us and work upon it by testing it on two different Artificial Neural Networks as required.

Introduction
PD is a degenerative disorder of the central nervous system that often impairs the sufferer's motor skills and speech.

This takes place as a result of degeneration of neurons in a region of the brain that controls movement, causing shortage of brain signaling chemical dopamine.
Although formally recognized and documented in the early nineteenth century by British Physician James Parkinson, The oldest description of it is found in the ancient Indian medical system of Ayurveda under the name Kampavata around 2500 BC.

The first and most common symptom of PD is tremor (trembling or shaking) of a limb, especially when the body is at rest.
Tremor often begins on one side of the body, frequently in one hand.

Other symptoms include slow movement (Bradykinesia), an inability to move (Akinesia), rigid limbs, a shuffling gait and a stooped posture.
People with PD often show reduced facial expressions and speak in a soft voice.

The disease also causes depression, personality changes, dementia, sleep disturbances, speech impairments and sexual difficulties.
Severity of PD related symptoms worsen over time.

There are many theories about the causes of PD ranging from Environmental factors to Inherited susceptibility.
There is no test that can clearly identify the disease Although Tests such as brain scans, can help doctors decide if a patient has true Parkinson's Disease or some other disorder that resembles it.

Some experts also believe that PD is something of an "iceberg; phenomenon" lurking in as many as 20 people for each known to have PD.
There is no cure for PD and hence can only be controlled (partially).

Background
In this assignment we have been provided with a data file (from hereon referred to as dat) containing actual statistics (split into two columns for convenience)from a patient with the

First column being the nucleus local field potentials(LFP) recorded from the Subthalamus(the ventral part of the thalamus regulating movements produced by skeletal muscles) & theSecond column being envelop signal of the EMG (ElectroMyoGram recording physiologic properties of muscles at rest and while contracting).
dat is recorded over 100 seconds (as it has a frequency of 800 Hz and has a total of 80000 observations) with both variable having a diverse range of amplitudes.

In other words the way we load any data into a Neural Network would be dependent on how we would want to Interpret it as.
This will not only help us in getting a good response but will also aid our understanding of the whole procedure.

Aim
Our Purpose here is to devise a neural network such that when trained with a certain occurrence(s) of input LFP with target EMG,it is able to correctly predict EMG if provided with just the LFP when running.

In other words we need to model an autonomous system in which only the input LFP is enough to predict when an EMG change will occur so that a countering action is generated to reduce the amplitude of EMG so that a large tremor doesn't take place.
Procedure

There are several ways by which we can bring the participating data up-to a consistent standard that is easy to manipulate, recognizable and is comparatively smaller.
The best way that would suit this purpose initially is to create a mean value for each participant for every certain number of occurrences.

The main factors needs to be taken into consideration are That it acknowledges that LFP has both positive and negative value and hence there is a strong possibility that means at some interval would be 0 leading to inconsistencies.
(That is a reason we would have to use a variance (σ2) based approach for LFP.

This ensures that there is a "visible accentuated difference" between the two participant.) EMG (green line in Fig 2) is something which would suit us best if it is 'fuzzified'.
In other words, rather that having an intensity based EMG model, It is far more simpler to have and implement it based on an 'on-off' approach There is a tiny 'blip' in our observation at the beginning where our EMG slightly increases at the 15 second mark (around the 13000th observation).This means that it would have to be appropriately thresholded.

We then plot the simplest way in which the data can be plotted with a total of 80,000/800 = 100 observations.
Visually we are able to observe in this case that as LFP goes down EMG activates.

Our main focus now is to have an appropriate approach to realize a solution.
Approach

Artificial Neural Networks can be summarized as connectionist paradigm based programming on interconnecting neurons (or information processing units) in order to arrive at an overall response.
An ANN is usually designed to change its structure based on external or internal information flowing through it to get a 'desirable' output.

Understanding ways in which ANNs work is beyond the scope of this assignment (as the report assumes reader to have prior knowledge) and the report focusses on how accurately is a tremor predicted.
Normalised dat is split into a ratio of 1:1:1 (Fig 3)with each being for training, validation and testing.

There are several ways in which the data can be split for this purpose but the reason I employed this approach was because the initial blip is capable of training the network well so that it is adjusted according to its error also the validation bit is used to measure network generalisation, and to halt training when generalisation stops improving.
Testing has no effect on training and so provide an independent measure of network performance during and after training.

The intention is to have final plots with the prediction for all the three portions (even if they have been trained with).
Conventional Back Propagation

Back propagation is a somewhat ad hoc process.
The initial step is to check that the output is correct.

If it is not correct, an algorithm goes back and modifies the weights of the inputs to the output layer, such that a correct response would have been received.
These inputs are, in turn, outputs of a middle layer, and the weighted inputs of the middle layer must now also change to reflect the new output.

This process continues successively until input weights in each of the layers have changed.
There are essentially two passes through a neural network.

This process continues until the input weights no longer change.
Back propagation is a common technique; however this is not a method that biological systems utilize.

The rough figure shows a simple BPN Feed-forward (with one hidden layer trained with gradient descent Backpropagation) used for this purpose.
Learning mode used was Gradient descent with momentum weight and bias learning function.

Fig 4 Shows that the Prediction(shown in blue) and the original value (red dash dot lines) are close.
The arrows marked on the Fig show that : i) Although not visible (due to printing constraints) the Prediction is very close to the desired target value.

ii) The twin peaks are a result of momentum and are a common problem when using simple BPNs.
iii) Due to no limits being present the momentum causes the plot to go down into a negative area.

iv) & v) are due to overfitting.
Multilayer Perceptron

A Multilayer Perceptron can be considered as a advanced Feedforward Backpropagation model but this is where the similarity ends as a MLP has high capacity of input-output mapping.
In addition to this MLPs can far outperform other neural networks (such as RBF) and also requires smaller number of parameters in case of nonlinear input-output mapping for the same degree of accuracy.

As a consequence MLPs tend to be far more accurate than other complex ANNs (although there is a tradeoff as far as training time concerned as MLPs take the longest to train).
Architecture of MLPs are not completely constrained by the problem in focus.This is because the number of inputs nodes is constrained by the problem and the number of neurons in the output layer is constrained by the number of outputs which is required by the problem, whereas the number of hidden layers and the number of neurons in the hidden layers depend on designer.

As we know that there will be one input node in the input layer one output neuron corresponding to a target vector.
There isn't a set procedure to arrive at a suitable number of hidden layers but after experimenting this was set at two.

The activation function selected for this purpose is a logistic activation function.
A modified conjugate gradient algorithm based on the Levenberg-Marquart approach is utilized as the preferred learning algorithm.

Fig 5 shows the result observed in this case.
In this two interesting observations are made as (i) is very close to the predicted error but (ii) makes for an erratic viewing this is a trend we observed in the previous ANN.

The peaks are generated due to overfitting.
Comparison

When comparing the errors in the networks (Fig 6.) we get an expected result.
An MLP implementation of the problem is roughly 22 times more accurate than a conventional BPN one.

Conclusion
The conclusion can be summarized in the following points : A lot actually depends on the way we manipulate and work with the given data.

This means what we decide to partition and where has a major impact on the overall decision (in this case at least).
The solution discussed in this report is highly context specific and cannot be generalized whatsoever unless we have a training data from either several patients or different plots from the same patient.

ANNs at this stage are not very reliable, In addition to that any response needs to be thresholded properly.
Although we got a MLP to work relatively well, Its use in a real situation depends upon several factors such as criticality of the system or whether an ANN is used as the sole method used for prediction or an aid.

There is a strong possibility that such a system can be put into use in the future as it is fairly reliable at the moment.
Introduction:

The analyzed remains of this individual were recovered on a medieval monastic cemetery of Hulton Abbey in Staffordshire.
Burials took place for about 300 years from the foundation of the monastery in 1219 (officially in 1223) till its Dissolution in 1538.

This inevitably caused overlying of later burials on the earlier ones; some damage was also made after the Dissolution.
The abbey was of a Cistercian order, which indicates certain life style of the monks.

Palaeodemography:
Sue Browne's (2000) report states, that there were 91 skeletons excavated during the periods of 1972 to 1983 and 1987 to 1994.

The earlier excavations provided 22 individuals and 6 disarticulated crania excavated by Pape in 1930 and described in Staffordshire Archaeological Studies, 2.
Further 69 individuals were recovered in the second period of excavations.

50 of these (72%) were fairly complete, 12 of them (19%) were well-preserved, 8 (12%) represented a quarter of less of the skeleton.
In the whole sample of 92 skeletons, 78% are adult and 22% are immature, 20 years old or less at death.

60% of individuals of the adult and non-adult sample (n=76) are male and 22% female.
In the sexed sample (n=63) 73% is male and 27% female.

The greatest number of deaths occurred at the age of 25-35, although a wide range of age is represented.
More males than females died at older age, 20 males (57% of the aged sample) and 2 females of 35 years or older (18% of the aged sample).

The organisation of the cemetery does not show any sex or age divisions, apart from the south transept, where only males were found (and two skeletons of indeterinate sex).
Preservation of the material:

The preservation of skeleton 50012 can be classified as partial (25% - 75%).
Most of the bones are present, but they are fragmentary with bad surface preservation.

Root activity had major impact of the surface.
Water in the soil resulted in partial dissolving of the remains.

Substantial damage has been done post-mortem, as a result of post-depositional conditions, which was the continuous use of the remains as laboratory aids for university students.
The amount of data that can be collected from the skeleton depends on its preservation.

In this case the crucial diagnostic features necessary for determining the sex, age, stature and pathologies of the individual are absent, therefore affecting the accuracy of the results or even enabling necessary measurements for estimation of sex, age, stature and identification of pathologies.
There are 8 labial teeth of maxilla (incisors, canines and first premolars) and 13 of mandible preserved intact.

Other 4 are disarticulated, two of which are maxillar first molars and 2 maxillar second molars, lost postmortem.
There is also one disarticulated root, probably of maxillar second right pre-molar.

Four teeth were lost antemortem, the first and the second right mandibular molars, second left maxillar premolar and third left molar lost just before death, as there are little signs of socket reformation.
Methods:

The diagnostic features of skull (Krogman and Iscan 1986), pelvis (Krogman and Iscan 1986 and Phenice 1969) and the glenoid cavity length were used to assign sex, because of the high accuracy of the methods and presence of necessary bones.
Long bones and sternum could not be used due to their poor preservation or absence of both proximal and distal ends.

The estimation of age was based on cranial suture closure (Meindl and Lovejoy 1985), pubic symphysis (Brooke and Suchey 1990), dental wear (Brothwell 1981) and ossification of cartilage.
Each of these methods on its own does not provide accurate estimation, so they were combined to give better results.

Trotter's method using the length of the right ulna was applied for the gestimation of stature (Trotter 1970).
This bone had to be reconstructed therefore it is a gestimation.

The possible error in measurements ranges from one to four millimetres, which does not affect the results in any major way.
Poor preservation of other bones did not allow using them in the process.

Results:
Sex:

This individual has been recognized as a male.
The traits used and their description has been presented in Tables 1.

and 2.
Measurement of the length of the right glenoid cavity also indicated male features.

The length is 44.1 mm, which falls into the male interval of above 37 mm.
There is a slight surface abrasion, which might have affected the length by maximum of two millimetres, which cannot affect the estimation.

Age:
The age of the individual has been assigned as 35 to 45.

The comparison of the results has been demonstrated in Table 3.
According to the method of Meindl and Lovejoy (1985) for lateral-anterior suture closure the mean age is 56.2, with standard deviation of 8.5, mean deviation of 6.3 and the age range of 34-68.

This method is regarded as highly controversial, due to our poor knowledge of the relation between suture closures and age and various age of closure.
Therefore, pubic symphysis surface was also used.

Phase 4 of the development process was assigned to the one preserved pubic symphysis.
According to the Brooks and Suchey (1985) diagram this indicates the mean age of 35.2 years, with standard deviation of 9.4 and the range of 23-57.

Also in case of this method the results can be affected by the poor surface preservation.
It is grained and traces of ridge can be noted, it is hard to say though if the oval outline is complete.

Brothwell's (1981) method based on dental wear was applied, showing the age range from 25 to 35.
This result lowered the average age and the range.

The progress of dental attrition is relatively slight.
Although there is light attrition on each present tooth and some more developed on one disarticulated one, the general condition od the teeth in terms of wear is good.

This could be due to a good diet of the individual, who could be a monk or a lay brother at Hulton Abbey.
Stature:

The stature of individuals depends strongly on the environmental conditions, geographical location, socio-economic status and genetic features.
Lack of sufficient nutrients in diet during the childhood determines the characteristics of the skeleton in adulthood.

The studies of Korean War and World War II victims allowed Trotter and Glaser (1958) and later Trotter (1970) to develop formulae for estimating stature.
We can compare these data with the statures of post-medieval samples from other British sites showed in Table 4 (Lewis 2001).

Basing on the measurements of the right ulna (Trotter 1970) this individual's stature was estimated for the mean height of 171 cm (5'6") which falls within the range of 167 and 175 cm.
Assessment of Pathology:

Dental pathology:
Serious caries was noted, mainly on the left side.

On the present teeth, caries was recorded on the first left maxillar premolar and third left mandibular molar.
The second left upper premolar was lost antemortem possibly due to the caries infection.

Also lower left first and second molars were lost antemortem, probably for the same reason.
Three of the teeth disarticulated postmortem have caries; the two first molars and one second molar.

Because caries was strong on the left side, this migh help side the teeth, but there are no clear evidence that the ones with caries were on the left.
Medium dental hypoplasias in a form of pits was recorded on the left mandibular premolars and the first and second molars.

Also some slight traces on second left maxillar premolar.
Heavy to slight calculus present on all teeth.

Strongest towards the lingual side of the insisors; also strong on the disarticulated teeth.
Mainly supragingival type of calculus is present, although subgingival type also noted on the labial upper incisors.

There is little dental attrition developed, which has been mentioned above.
The labial teeth (especially evident on the right canine, which is rotated) of mandible are slightly overcrovded, which could be a result of the presence of third molars.

There was also medium alveolar destruction noted both on maxilla and mandible.
Cranial and post-cranial pathologies:

There were no cranial or post-cranial pathologies recorded.
This is due to the poor preservation of the diagnistic areas and surface erosion.

Especially the joints are damaged, the spine is very fragmentary which made the identification of potential pathologies not possible.
Conclusions and Discussion:

This skeleton is one of the most fragmentary of the whole collection from Hulton Abbey.
The state of preservation makes it difficult to apply all the anthropological methods.

The individual has been assigned as a male, as all the methods applied indicated.
However, aging the skeleton showed two different ranges of age, varying from 25-35 to 34-68.

This is due to the fact, that from th eosteological point of view this individual appears to be in his forties, however the dentition indicates lower age, due to the good condition is the teeth.
The stature of average 171cm is comparable to the other mean statures of the individuals from medieval cemeteries.

Lack of bones preserved well enough makes the stature estimation uncertain, as it was not possible to take more than one measurement.
Poor and fragmentary preservation also affected the identification of pathologies.

On the basis of present material no pathological changes could be observed in the post-cranial area.
Due to a great number of teeth preserved, intact mandible, partial maxilla with disarticulated teeth the dental pathologies could be studied in more detail.

Presence of caries, calculus, dental hypoplasias, alveolar destruction give us some information about the level of dental higiene in a monastery and medieval society as a whole.
Cistersian diet based mainly on bread, vegetables, beans, herbs, on occasion fish or eggs ( URL ).

The dentition of this individual reflects this diet, as his teeth were rarely exposed to hard surfaces.
In cistercian monasteries main labor force were the lay brothers.

If the joint surfaces were better preserved, it might have been possible to guess if the individual was a monk or a lay brother from th elevel of joint erosion.
Lack of any signs of trauma suggests also a peaceful community.

References:
Brooks, S and Suchey, JM (1990) Skeletal age determination based on the os pubis: A comparison of the Acsádi-Nemeskéri and Suchey-Brooks methods. Human Evolution 5:227-238.

Brothwell, D (1981) Digging up Bones. The excavation, treatment and study of human skeletal remains. Third Edition. Cornell University Press: Ithaca.
Krogman, WM and Iscan, MY (1986) The Human Skeleton in Forensic Medicine. Second Edition. C.C. Thomas: Springfield, Illinois.

Lewis, M (2001) St. Bartholomew's Church 2000. Great Barrow, Chester. Report on the Human Skeletal Remains. Department of Archaeology, University of Durham, Durham.
Meindl, RS and Lovejoy, CO (1985) Ectocranial suture closure: A revised method for the determonation of skeletal age at death based on the lateral-anterior sutures. American Journal of Physical Anthropology 68:57-66.

Phenice, TW (1969) A newly developed method of sexing in the Os pubis. American Journal of Physical Anthropology 30:297-301.
Trotter, M (1970) Estimation of stature from intact long bones. In: T.D. Stewart (Ed.) Personal Identification in Mass Disasters. pp. 71-83. Smithsonian Institution Press: Washington D.C.

Handlling human remains seems to be a great responsibility and moral duty of making sure they are used for a purpose and they actually do provide agreat deal of important information.
If we do have to 'disturb the dead' we should do that for a reason.

Personally I had doubts, whether I will be able to deal with this pressure.
But I asked myself a question if I had anything against if someone did the same with my remains in the future.

And the answer is 'no'.
But another question is if we study human remains just to learn about the past or could our knowledge also influence the future?

In which case these studies would be far more justified.
Despite all the theoretical knowledge we were provided with, I think the most important thing is experience, as osteology is based hugely on personal judgements, which require experience.

Especially identifying pathologies seems difficult, as when someone sees a skeleton for the first time, everything looks perfectly normal, if it is not an obvious pathological change.
To improve my performance the best thing to do would be looking at some more samples, to gain a base for comparison.

Archaeology as a discipline has been evolving extremely rapidly for the last few decades.
Considering its sudden appearance after many centuries of only sporadic archaeological interest of the Near Eastern rulers, the development we are witnessing is very amusing and understandable bearing in mind the recent technological progress.

Different areas of the subject were developing constantly and the direction of this development depended on the technologies as much as ideologies.
The study of countryside developed already in the 19 th century, when the scholars or rather antiquarians such as Dr. J. Wilson working at Woodperry in 1840s, Mr. S. Stone or Mr. Leeds carried their first excavations on deserted medieval villages.

The development of medieval archaeology was inevitably limited by certain factors.
It concentrated mainly on churches, castles or manor houses, so their view of medieval life was biased by towards the higher class of the society.

Also the works were often carried out by architects, interested in the structure of a feature at one level, instead of digging down to find out more about the chronological sequences (Beresford and Hurst 1971).
In the 14 th - 15 th centuries many medieval villages became deserted, forming perfect sites for future archaeologists due to the high level of preservation.

This is why most of the evidence about the villages from the 1840s onwards up till 1960s comes from this period of abandonment.
The excavations were held at such sites like Upton by Profs.

Hilton and Rahzt, Hound Tor by Mrs. Minter and most of all at Wharram Percy by Prof. Beresford (Beresford and Hurst 1971).
However, in the 1980s and 90s the interest of the scholars seem to turn more towards the study of the origins of villages and the surounding landscape.

Instead of focusing on individual settlements the new approaches emphasize the importance of the attached fields and the buildings within them.
This forms a "block of countryside" under examination, creating the opportunity to analyze the "inter-relationship between the human and animal populations, settlements, fields and the environment and how that relationship changes through time" (Astill and Grant 1988:36).

In other words, we are now interested in answering the question why people chose to live in villages, not why they deserted it.
Therefore the next arising question is why did we change our interest?

Why does the knowledge change?
The factors determining this might have been various, including new technologies available that allowed archaeologists to reach lower levels of settlement, thus enabling the study of its earlier periods in more details and with more accuracy.

For instance, Beresford and Hurst mention the limits of phosphate analysis to determine the location of the house, due to "the constant sweeping of the floor by the housewife".
Nowadays they would meet some strong protests against such view from feminists or some insulted housewives, but nevertheless the areas with high phosphate content, (which would mean the presence of animals) were in fact the spaces between the houses.

With present geophysical techniques the chance for obtaining accurate results are much bigger.
In addition, the administrative side of archaeology has changed significantly, taking as an example the development of rescue archaeology in the 1970s and 1980s and later the introduction of PPG16 which revolutionized the sources of funding the excavations.

Hence a greater amount of evidence became available to analyze as more excavations were held.
New ideological approaches such as New Archaeology or post-procesual Marxist archaeology undoubtedly influenced the way of thinking of the scholars.

The arguments for the change from timber to stone (which I will analyze in more detail in due course) clearly bear the traces of traditional and the New Archaeology.
We also should consider different cultural backgrounds of the researchers and the regional variations and different cultures, not only within Britain but also the whole Europe.

Ian Hodder claims that there are as many interpretations as there are archaeologists and that archaeology can never be fully objective, so that subjectivity and modern approach can affect the process of interpretation (Renfrew and Bahn 2002).
In this work I will analyze our changing knowledge about peasant houses from 1960s till 1990s, focusing mainly on the debates of different archaeologists and historians about the durability change of building materials and plans of buildings.

Voices on the durability of medieval peasant houses:
Since 1960s our idea about the construction of a peasant house has changed considerably.

If the data about the construction changed, this undoubtedly led to the reassessment of the opinions about their durability.
The study of deserted medieval villages that evolved throughout 19 th and 20 th century up till 1960s allowed the archaeologists to draw some conclusions about the construction of the houses.

This was based on however limited number of excavations and contemporary excavation techniques.
The most popular objects of investigation appearing in the publications seemed to be Anglo-Saxon sunken-huts like those at Upton or West Stow.

This was not due to the common interest of contemporary archaeologists in these structures, but to the fact, that sunken-huts can be relatively easy recognized during excavation process thanks to the darker fill of the huts (Beresford and Hurst 1971).
Also, the stone foundations or walls could be easier recorded, because of the lack of sufficient techniques to record the remains of timber.

Beresford and Hurst in their work "Deserted Medieval Villages" provided only very limited amount of information about timber buildings, claiming that the later stone disturbances did not allow them to investigate the timber phase (Beresford and Hurst 1971).
On the other hand, on the basis of the study of turf and cob walls (not very detailed either) from probably 9 th till 13 th century excavated at Hound Tor, they made a conclusion, that the buildings that had such walls could not be made to last more than five, maybe up till twenty years (Beresford and Hurst 1971).

Therefore, the surviving vernacular buildings, dated not earlier than 16 th century, had to be the results of the Great Rebuilding, which took place at different locations and in different periods of time and was a general replacement of those impermanent structures by permanent houses.
However, even those earliest and of poorest construction were evaluated as supreme to those from the excavations, which did not demonstrate any evidence for substantial timbers, able to withstand the weight of a massive roof structure (Wrathmell 1989).

After twenty years of research the new methods and resources such as written documents provided the archaeologists with evidence for use of cruck, forks or siles, which would be the "missing" major timbers supporting the roof.
Such evidence include the accounts of the vicar of Kirkby Malham, who in 1454 paid carpenters for placing stones under the crucks to stabilize the building.

The use of these elements is also often recorded in schedules of repairs, like the fifteenth-century one from Northallerton or Durham.
Furthermore, there is evidence for timber "tenants' buildings being moved and re-erected".

This indicates the houses had to be of good quality, enough to make it worth moving them to another location (Wrathmell 1989).
Already Beresford and Hurst (Beresford and Hurst 1971) noted a series of stake-holes around the buildings, which they thought indicated numerous rebuilding of the houses.

They could not, however, date them precisely, so they were not able to set the time interval during which the rebuilding took place.
It might well have been a few hundred years, hence being a clear evidence for the more substantial construction of these houses than they originally thought.

Continuous works on their houses would be rather problematic for the peasants who occupied them.
Therefore, a new approach states, that the Great Rebuilding took place because of the change from low-cost buildings, which required a great amount of maintenance works to high-cost stone ones, that did not need a constant care (Wrathmell 1989).

We need to remember, that the medieval peasantry was a hierarchical social group, within which we can distinguish those more and less wealthy.
Hence it is possible to incline, that their buildings varied considerably as for their quality, as William Harrison wrote in 1587 from those 'commonly strong and well timbered', to those 'slightly set up with a few posts...

in the fenny countries and northern parts...
where for lack of wood they are enforced to continue this ancient manner of building' (Morriss ).

So there was already a new manner of building, but because of the lack of resources people were forced to build in 'old way', and those who could afford it could buy wood on markets.
Approaches to the change of building materials of medieval peasant houses:

The changing evidence and techniques also affected our knowledge about the building materials that were used for peasant houses.
The general types of material were agreed already in the 1960s and that did not change.

What did change is the estimation of periods and regions the materials were used.
Thus, a fact is that there was a trend to proceed from turf or timber framed buildings to more substantial stone walls.

In their work on deserted medieval villages Beresford and Hurst (.....) describe the turf walls as being dominant in the south-west region of England, with not much evidence for their presence in any other part of the country.
As an example they put already mentioned Hound Tor, dated for about 9 th till 13 th century.

Some cob walls were recorded during excavations in East Anglia, also traces of timber constructions in many parts of the country.
Timber was said to be difficult to investigate 'because of disturbances from later stone buildings' and in most cases there is nothing but 'the odd post-hole or slot been found' (b&h 1971).

In addition, the debate on the use of substantial timbers in the construction has already been mentioned above.
The change from timber to stone occurred in late twelfth and thirteenth centuries all over the country.

According to the idea of the early medievalists about early medieval houses, they were not built to last more than twenty years, sometimes less.
They also found evidence for frequent rebuilding and although re-erecting new buildings from the same timbers also took place, there had to be undoubtedly enormous amount of wood used to build these houses over the centuries.

Therefore, the archaeologists came to a conclusion, that due to the development in agriculture and a greater level of land consumption the wood supplies in most parts of the country became over explored.
This was the main reason for the emergence of stone buildings.

However, nowadays as well as twenty years ago, archaeologists reevaluated this view, suggesting a new solution for this phenomenon.
Namely, the new factor causing the occurrence of stone buildings (or timber buildings on stone foundations) is said to be the availability of money.

As the idea of durability of the houses changed, which I analyzed above, the use of materials had to be also reconsidered.
Dyer presents his opinion on high standard of medieval peasant houses, from the thirteenth century being constructed by professional carpenters.

This brings us to the reflections on the medieval economy in the countryside.
As the archaeologists of let me call it the 'old school' thought, the materials used for building a house were those available in the closest vicinity.

Therefore, we see turf buildings in the south-west, cob walls in the east and timber all over the country.
However, even in the areas where stone was easily accessible we do not note the occurrence of stone buildings before late 12 th century.

This puts that idea of 'regionalism' into question.
In addition, in the 12 th to 13 th centuries, as I mentioned, the change from timber (or wattle-and-daub method) to stone took place, and this was in most parts of England, regardless of the immediate availability of stone.

Astill claims, that this was a proof for the medieval economy relying more on accessibility of market, that the resources themselves.
If people could afford hiring a skilled carpenter, this was an indication of their material status, which leads us to the conclusion, that people started to build in stone not because the wood resources were over explored, but because of the economical growth and availability of money (Dyer in Astill ).

Here we can notice the influence of the New Archaeology.
While the 'old school' explanation was simply lack of wood resources, in 1980s archaeologists came to a conclusion, that maybe it was something less prosaic than that and found a connection to the contemporary economical and social processes.

The change of plans and typology of medieval peasant houses:
In 1971 Hurst presented his typology of medieval peasant buildings.

He included three groups, a peasant cot, a long-house and a farm.
They were generally associated with the material status of the peasants, placing the cottagers, who had little arable land, if any, on the lower part of social status.

They lived in peasant cots, which were small houses with one room of around 5 by 3.5m or two rooms 10 by 4m (Beresford and Hurst 1971).
The next level was inhabited by the villeins, who lived in long-houses, which by definition is a long building, of a varied size, with living part at one end and a byre with for agricultural use, separated from the living part by a cross-passage.

It has been found in many parts of England, mainly the south, but also Yorkshire and Northamptonshire.
Hurst claims, that long-houses were the development from earlier forms of timber buildings, occurring from the 12 th century (Beresford and Hurst 1971).

As we know, this was the time of change from timber to stone.
In addition, the different types of houses are said to be the result of varied prosperity of the farmers, thus we can observe the change from timber to a cot or to a long-house.

This was not a chronological development then.
The next group consists of farms, in case of which the byre was separated from the living part in order to stress the material status of the people living there, the new-forming group of yeomen.

They imitated the manorial houses, forming a courtyard by placing the byre or barn perpendicularly to the house.
This type of settlements occurred in 13 th century and was often simultaneous with long-houses and a change from long-house to a farm was also recorded.

In their work, Beresford and Hurst admit, however, that the evidence they possess, on which they base their observations is very limited due to a small amount of excavations and the existing ones being brief and incomplete (Beresford and Hurst 1971).
Therefore, archaeologists must continuously reevaluate their knowledge according to the new data.

Regarding the cots, we now know, that they were rather located on the outskirts of villages or within the village green (Wrathmell).
There is relatively more evidence about long-houses.

However, the difference between long-house and a farm has been recently put into question.
While the known sites have been excavated again, there is apparently no evidence either for presence of absence of the cattle in the 'cattle part' of the long house, which forms part of its definition.

There has been a considerable confusion over the term "long-house", since over the last few decades archaeologists have been assigning this term to buildings with opposite entrances.
It has been thought, that in the late medieval and post-medieval period long-houses were converted into farms.

Hurst suggests, that this was a development of a type of dwelling.
Due to the above circumstances as well as the fact, that the three examples used to prove the transition and as Gardiner claims in his work, the three of them are 'suspect' (Gardiner 2000).

Furthermore, the occurrence of long-houses has recently been associated with the type and state of local agriculture.
Chapelot and Fossier (in Astill) noted, that this type of houses "disappeared" from most parts of the country apart from the Highlands and British Isles.

They explain, that houses in which people live under the same roof with animals represent an undeveloped agricultural system, 'based on grain production and a backward form of stock raising'.
On the other hand, according to the evidence from Goltho, Beresford claims, that the reorientation of houses in a form of a yard was directly connected to the new, stock-based form of agriculture (Beresford 1975).

This is clearly a sign of regionalism, neglected earlier by Beresford himself and Hurst (Beresford and Hurst 1971).
Nowadays we know from the documentary evidence however, that the long houses were much more popular also in the parts of the country that seemed not to have any traces of them (Wrathmell ).

This can either undermine the whole theory about regionalism, or simply mean that we do not know those other regions well enough yet.
Conclusions:

Over the last forty years archaeology once again proved to be a very dynamic discipline.
Our knowledge can change surprisingly quicky due to the changes in technology, new evidence, influence of a certain ideology and approach.

Archaeologists, as well as their students and the public should know and accept this fact, remembering at the same time that no solution is the only correct one.
Very true are the words of Chrostofer Dyer, who argues that 'the size, quality and complexity of the late medieval peasant buildings have frequently been underestimated' (Dyer 1986).

The studies of the deserted medieval villages gave the archaeologists an impression, that the earlier medieval houses were not built to last long, they were mostly made of local, bad quality materials and that at different time and locations there was a rapid change to more permanent dwellings, which they called the Great Rebuilding.
This marked also the barrier between medieval and post-medieval farmhouses (Wrathmell 19..).

These ideas were then undermined by the new approaches, based on more careful excavations and better techniques.
Now we think that the durability of the medieval houses depended on the status of the peasant and the accesibility of resources and money.

Those more wealthy peasant hired skilled carpenters to build their houses.
As well teh Great Rebuilbing seems not to be a rapid change any more, but rather a whole process.

The changes in durability as well as materials used occurred in different regions, which is an evidence of trhe process being more varied and complex than it was previously assummed by Beresford and Hurst.
The plans of houses were thought to develop chronologically, one form from another, from a peasant cot, through a long-house to a farm.

Now it has been proved, that some forms developed independently and did not derive from the 'previous' one.
There are still many unanswered questions or those which apparently have an answer, but it needs further reconsideration.

Therefore, we should not accept the knowledge we have as the one providing all the answers.
Introduction:

"The problem with archaeologists, it appears, is that they are always too late..." (Tim Ingold 1999: ix) This witty comment of Tim Ingold brilliantly introduces to the main problem, which I will discuss in this piece of work.
Archaeology is undoubtedly an important discipline which gives us a 'visual' insight into the history of ancient societies.

However, its very nature, being based merely on perishable material remains, causes a serious interpretative dilemma and a relatively narrow perspective, focusing mainly on material culture.
Here anthropology comes to aid, filling the gap between the data and their significance for the structures of ancient societies.

The relation between these two disciplines is perceived differently in Britain and in America, which needs to be taken into account, since this can affect the perception of some scholars, depending on their academic background.
In America archaeology is but a counterpart of anthropology, broadly understood as the study of human activity, in which material culture (the focus of archaeology) is inevitably incorporated as a product of human activity.

In Britain on the other hand, these two disciplines evolved separately, the main distinction between them being in terms of data they use for their analysis.
Bradley and Edmonds (1993) picture this difference by making an analogy between American and European mode of quest respectively, and the famous novel by Alexander Dumas, "The Count of Monte Cristo".

The two main characters, Dantes and Faria, have two different ways to escape from prison.
Dantes worked out the elaborated system of tunnels so that he could effectively dig one of his own, whereas Faria was simply digging blindly in a desperate attempt to find a way out.

Thus, pure evaluation of material culture will lead us only to a blind, desperate act to find out the truth.
But if we work out the whole system hiding behind the objects, we will be able to find our way out from a dark cell of ignorance.

As the two tunnels of our characters eventually connect, also anthropologists recently combined forces with archaeologists by introducing new anthropological approaches to previously purely archaeological investigations.
This is especially evident in case of the study of exchange, because the scope of evidence available for archaeological study is limited largely to non-local imports and cargos found in shipwrecks.

Therefore, in either archaeological or anthropological research we should use both of these disciplines in order to obtain an ultimate picture of an analyzed issue.
As a result of their cooperation, anthropologists have been called theoreticians of archaeology, and in this work I will attempt to analyze how exactly that happened, by analyzing the new anthropological approaches in archaeology.

Nothing gives a better picture than the study of exchange systems, therefore I will use it as an example to illustrate my thesis.
Archaeological study of exchange:

Exchange of material goods, whether in a form of gift exchange, redistribution or trade, has been extensively studied for the recent years.
New scientific methods allow archaeologists to realize that the materials an object is made of can reveal much more about its origins that the form or shape.

Therefore, the systems of exchange and the movements of goods have been examined.
The basic task of such analysis is to determine the process of production and distribution of goods.

The more complex reconstruction would extend to the organization of exchange systems.
This inevitably made the scholars notice that the prestige goods also had symbolic value within a society (Neolithic axes were thought to be exchanged between elite groups as a symbol of prestige), which pushed the study of exchange in a different direction.

As a dataset, an archaeologist uses the material remains, however the flow of information and ideas that accompanied them was equally, if not more important.
So far the similarities in material culture were regarded as evidence for contacts and diffusion between two societies.

This approach emphasized the origins of goods.
Instead, recently the study of the contacts themselves became more influential.

A theory has been suggested, that the social structure mirrors the model of repeating patterns in exchange relations between societies (Renfrew and Bahn 2002).
This is undoubtedly the influence of anthropology on the typically archaeological study of exchange.

We are not interested any more in merely the distribution of goods, but we want to know that was the motivation of people who exchanged this object?
What value did it have for them and what did they get in exchange?

There is an indefinite range of questions one could ask regarding the social aspect of the study of exchange.
It is crucial, however to remember that without anthropological approach these questions would have not even existed.

Anthropological approaches:
In order to understand the origins of these new trends let us go back to the 1960s.

and 70., when together with the gradual popularization of New or processual Archaeology, the social context of material culture became the subject of extensive research.
As an excellent example we can use the study of exchange of stone axes in Neolithic Britain, which was affected by the search for social meaning.

Extensive petrological analysis of artefacts and raw materials was no longer enough, hence the attempt to identify the processes behind different distribution patterns (Bradley and Edmonds 1993).
As a result of the introduction of the cultural evolutionary theory within processual archaeology, three basic modes of exchange have been defined, which are reciprocity, redistribution and market.

The first one is an exchange of goods between equal representatives of society or equal societies in pre-capitalist economies, the second involves some sort of bureaucratic central organization that redistributes goods that it acquired mostly through tribute and finally the last one represents the exchange of goods for other products of the same value.
According to substantivist school of economic anthropology, embedding economy in social relations, societies are though to be concerned mainly with self-subsistence and static striving for equilibrium with the local "environment" (Hodder 1982).

There is, however, a lot of criticism to this way of thinking.
Firstly, Polanyi (1957) claims, that exchange is "a moral act following social obligations".

Therefore, as it has also been stated by Dalton (1969), individuals are strictly bound to certain rules existing in society.
Hodder is concerned about this lack of space that is left there for the individuals to act as a generating force for the strategies of exchange.

Secondly, substantivists operate within what they term "primitive" societies, however, the boundaries and distinctions between "primitive" and advanced are highly subjective and misleading.
Moreover, on the wave of neoevolutionarism, Sahlins ascribes these three types of exchange to different levels of social organization, thus reciprocity occurred segmentary societies, and redistribution in chiefdoms and sates (Hodder 1982).

Also Service divides societies into bands, tribes, chiefdoms and states on the basis of the form of exchange they represent.
Fried, however, uses the social status for a basis of such division (egalitarian, ranked and stratified), which is rather a political approach.

The absence of redistribution mode in case of Hawaii chiefdoms stressed by Earle (1977) significantly undermines Service's theory.
The formalist approach on the other hand, is concerned with finding analytical techniques to identify particular modes of production and distribution.

Formalists assume, moreover, that such factors as scarcity, maximization and surplus have already existed in prehistory.
The main limitation of this theory is that it is absolutely deductive and applies abstract quantities, which lack explanation backed by facts.

Also, in its mathematical calculations it does not take into account the variability of social processes that might have led to the same outcome.
The basic limitation of both of these schools is that they do not consider the role of symbolism of objects in the relations of exchange.

Exchange can be used as a source of information about social structures only when we take into account the symbolism and contextual significance of goods exchanged, whether they are luxuries or ordinary food products, which have a certain cultural value (Hodder 1982).
We need to determine the relation of the object to its regional context, bearing in mind that it could have different meaning depending on the location.

These differences can even manipulate the boundaries between ethnic or social groups.
Also the shapes and forms of objects could determine their fate as symbols, since some forms associated with elites tend to spread with the greater availability of resource materials to copy these forms.

Their value might not be the same, but the symbolic meaning of an object associated with elite is preserved, although might be changed from the original one.
Therefore the study of exchange should be put into social context of the regions in which the exchange is carried out.

However, according to Hodder (1982), substantivist approach is insufficient because it treats societies as bound to certain rules and lacks consideration for the role of an individual in the process of formation of symbolism and meaning of commodities and therefore in the valid reconstruction of social systems.
Also the formalist approach seems to be very limited, purely because using merely mathematical methods have limits our scope of understanding of the wider social processes leading to the formation of value of a certain object and its social significance.

Hierarchical societies give different meaning and symbolism to objects that can be taken into account neither by substantivist nor by formalist methodology.
For this reason we shall look for alternative ways of analyzing social relations and consider how different socio-political formations interacted with each other, which have been dealt with by Preucel and Hodder (1996) in The Production of Value.

They refer to Renfrew and his "peer polity interaction", which states, that relations between societies (competition, "symbolic entertainment" and increased trade) form social structures, as for example, occurred in Greece where the city-states evolved as a result of interaction with each other rather than with Egypt (as the cultural evolutionary model states).
The prestige goods theory says that social status depends on the access to prestige goods, therefore an increased demand and trace relations between core and peripheries.

World system theory of Wallerstein is based on the interaction between societies from "the First and Third worlds such that development in one area causes underdevelopment in another" (Preucel and Hodder 1996: 103).
These theories indeed offer an alternative approach, but is also has many limitations.

The "peer polity interaction" seems to undermine the economic motivations of people, focusing mainly on political aspects even when explaining the increase in trade.
Also prestige goods theory stresses mainly the importance of luxuries as the main driving force in the development of state economies, diminishing the significance of food products that were also part or economy.

Finally the world systems theory is limited to the modern world, which archaeologists tend to forget about and impose it in archaeological data from pre-capitalist societies.
Because of so many limitations, it is impossible for an archaeologist to assign himself to one particular theory.

It is crucial to take into account all three of them while analyzing social relations, because each is contributing from a different angle to the debate.
Another discussion emerges, about the relation between the different types of exchanged commodities and the societies that perform the exchange.

Preucel and Hodder make an in-depth analysis of the nature of the commodities themselves.
They present the economic view, according to which "commodities and things which circulate throughout an economic system and can be exchanged for other things, usually money" (Preucel and Hodder 1996: 106).

As an example, O'Shea (1981) presents the relation between the production of commodities and the emergence of social ranking in the Aegean, where the commodities are produced in a process of social storage as food surpluses, which are then exchanged for tokens value.
The social-reproduction view uses the Marxist perspective of the commodities being a product of labor within a particular mode of production, which is then passed to another social group in form of exchange (Preucel and Hodder 1996).

The politics of value consider the interrelation between the movement of goods within and between spheres of circulation and the impact which this movement has on the societies involved in exchange.
Commodities and their meaning instill certain symbolism in an individual or in a group, therefore it is not possible to ascribe only one meaning or one value to an object.

We need to bear in mind the route it had to go through, its biography.
According to Kopytoff (1986: 68), "a commodity is a thing that has use value and that can be exchanged in a discrete transaction for a counterpart, the very fact of exchange indicating that the counterpart has, in the immediate context, an equivalent value".

The author does looks at commodities from a more social than economic perspective, analyzing their meaning to different societies rather than economic, political or technological values.
Thus, there are different types of goods exchanged: commodities, valuables and people.

These three can change their meanings and function depending on their social context and economic process they went thought to reach their current location.
Therefore, commodities can become valuables, people can decline to the position of a commodity (mainly through a slave market) etc.

But each of these three groups develops a biography with time.
Therefore, it is crucial for an archaeologist or anthropologist to create a biography of an object they scrutinize, in order to provide a wider perspective on the history of its value, which then helps to understand the social process involved in the journey of this object.

The argument that allows us to treat objects like people and create their biographies is that in fact it is the society that creates individuals and individuality, whether of a single person or a whole group of people (a tribe, a chiefdom etc.) and society also creates objects, therefore this puts people and objects in the same position and gives them inter-exchangeable meaning, which gives us right to build a biography also for objects (Kopytoff 1986).
This, however, makes me think whether it is not actually the opposite?

At least in some cases the objects create society, being imported from another social group it has a different meaning in the one it currently exists, but because of its former history and symbolism it can affect its new owners.
The massive amount of Coke bottles in our European society is but the one factor that slowly transforms us into an even more consumption-focused American society.

It might have not created the European Western society, but it certainly has an impact on it.
Conclusion:

It seems that Tim Ingold's problem with archaeology being "always too late" can be easily solved by applying anthropological approaches to archaeological investigations.
As it has been proved in this work, they not only provide us with a wider social context of studies material culture but actually put that aspect into being in the first place.

This process was, however, initiated by the introduction of processual archaeology and its quest for more contextual interpretations.
Therefore, it might be more appropriate to state, that first archaeological approaches were applied to anthropology and then used in archaeology itself.

Considering this, it is necessary to take into account regional differences in approaching these two disciplines on both sides of the Atlantic.
American anthropology consists of archaeology, therefore it is difficult to talk about anthropological approaches there.

In Europe, on the other hand, these two disciplines tremendously affect one another, the example of which is best seen in the study of exchange systems.
In terms of the relation between the type of exchange and social structure our understanding of the matter has improver significantly after the works of such scholars like Polanyi, Dalton or Service, later also Hodder and Preucel.

The latter two reevaluated the theories of the former ones, pointing to their main limitations and suggesting an alternative solution.
Thus, instead of using merely substantivist and formalist approach to study of exchange, they promote considering also symbolic value of material culture and their political implications.

In a way they did not achieve much more than their predecessors, as they did not offer an ultimate way of approaching the subject, but rather a series of another very specifically focused theories, which are also by no means flawless.
Therefore, it is necessary to bear in mind the limitations that all these theories bring and preferably try and apply all of them at once (to a certain extent, of course) in order to obtain an accurate picture of the past exchange systems and their social implications.

The nature of the artefacts themselves seems to be well-scrutinized by Preucel, Hodder and Kopytoff.
They emphasized the complexity of the nature of exchanged artifacts and the shifts in their value due to various social processes.

This draws the attention of scholars to the social aspect of material culture instead of merely examining the typology and distribution.
Kopytoff, moreover, stresses the individual character of goods, which can become individualized throughout the history of their use, which corresponds to the theory of Hodder, that we need to remember about the role of individuals in the process of formation of symbolism of objects, which fills in the gap in the substantivist approach represented by Polanyi.

Therefore, it is evident, that anthropology contributed a lot to the understanding of the material culture and together with archaeology can form a coherent and decent view of the past exchange systems.
For the past few decades archaeologists criticized the amount of attention that the typological approach was gaining within archaeological study.

With the development of anthropological approaches, we need to beware of the danger of pushing the line in the other direction, not to be faced with the problem of insufficient archaeological data in the future.
Introduction:

In order to investigate the urban development of Late Bronze Age Levant it is necessary to explain its territorial and chronological frameworks.
The argument starts already about what is actually the land of Levant.

This is a geographical rather than political term, referring to the historical lands of the Middle East.
Levant lies between hammer and anvil, being located in the proximity of such powers as Egypt, Mesopotamia or Syria.

This strategic position has always been causing many conflicts.
This work will concentrate mainly on the southern Levant, as this is the territory of Egyptian influence, which left the strongest mark on the Levantine urban landscape.

The period of Late Bronze Age has been chosen for analysis in this work.
There are several reasons for that.

The tripartite division of urban development of the Near East, the Early, Middle and Late Bronze Age, is rooted in the history of Egypt (Falconer 1994).
The conclusions are mainly based on pottery data and written sources.

The Thirteenth Dynasty marks the end of Middle Bronze II A period, Middle Bronze II C occurred parallel to the Fifteenth Dynasty of 'Asiatic' or 'Hyksos' kings who ruled in Lower Egypt.
The end of Middle Bronze Age is characterized by the military activity of the collapsing Hyksos kings.

The Late Bronze Age begins, with many towns being destroyed by the conflicts.
Again Late Bronze Age II A corresponds to the 'Amarna Age' and Late Bronze Age II B to the Nineteenth Dynasty and its military campaigns of Ramses I and Seti I.

In the Late Bronze Age Levant adopted almost a role of an Egyptian colony (Falconer 1994).
On the breakthrough of the Middle and Late Bronze Ages the major settlement shifts in the Levant took place.

However, is that correct to call these shifts a 'decline'?
And if so, a decline merely in terms of population, settlement or material culture?

In this work I will try to prove that the processes that took place in the Late Bronze Age Levant cannot be simplistically called a decline, but rather a shift.
The investigation is based on a number of excavated sites, the percentage of which is relatively low in comparison to the number of existing sites that have not yet been treated with a trowel.

To begin with, some archaeological evidence needs to be presented in order to give a base for a worthy interpretation.
Types of settlement:

There were changes in types of settlement in the LB.
They do not, however, give sufficient evidence for the decline scenario.

Settlements differed in terms of their origin and it was the economic-political situation in a given region that determined their location and character.
Hence, the fluctuating circumstances in the Late Bronze period decided about the change of existing settlements or complete decline of the 'outdated' ones.

Thus we can distinguish also satellite settlements (Tel Ma'aravim), villages (Tel Kittan), and solitary temples (Tel Mevorakh).
The expansionist activities of Egypt resulted in the creation of fortresses Har'uvit, Bir el-Abed or Deir el-Balah) on the routes linking Canaan with Syria and safeguarding Egyptian control of the region (Gonen 1984).

Also the copper-mining settlements (Timna) were established in order to provide the supply of copper for trade (Baumgarten 1992).
Garrisons placed in some unfortified tows played a rather symbolic or administrative role for the region during the 18 th Dynasty and with 19 th Dynasty their number increased with the intensification of imperial policy.

Thus we can see that the types of settlement introduced by the Egyptians were new to the region and they served imperial purposes, mainly safeguarding the payment of tribute and overall Egyptian dominance.
This justifies the wealth of Levant so emphasized in the written sources, but does not mean the prosperity of the region as a whole, but also does not prove a decline.

Spatial distribution of towns shows that people not simply abandoned the old settlements, but rather turned to another type of subsistence, which cannot be treated as an indicator of decline but of various socio-economic processes.
Number of settlements in the LB was greater in coastal zone than in mountain area.

They were located mostly on the edges of cultivated areas, along the roads or at natural harbors (Baumgarten 1992).
The areas in the inland, more densely inhabited in MB seemed to suffer a desertion during the later period.

The LB period is the time of enormous development of trade with Cyprus, the Aegean as well as lands reaching far beyond the Near East such as India or China.
Therefore, the new settlements that were established were in fact along the coast, especially northern, of the Mediterranean, indicating the importance of the trade with Cyprus.

Therefore the spatial distribution of the towns indicates the movement of people towards the source of income, which is a sign of some sort of stagnation, but also indicates that some people could find their way in times of crisis, which led to the development of these towns.
The trade routes were also safeguarded, the evidence for which we can find in the Amarna correspondence between the rulers about the safety along the trade routes.

This implies a certain level of unrest in the region in which earlier the roads did not have to be protected, therefore could indicate decline in economic wealth of the population.
Size, number and new settlements:

These three characteristics of the LB Levantine towns overlap each other, hence it is crucial that we discuss them in relation to one another.
From these aspects also the main evidence for decline has been derived, therefore it is crucial to analyze them in order to prove the more complex nature of the phenomenon.

Even though the data from excavated sites is rather scarce, a classification of sites in terms of their size has been made.
Thus we can distinguish between tiny or small, medium-sized, large and very large towns or metropolises.

There was a substantial decrease in size of the existing settlements stressed by Gonen (1984) and Baumgarten (1992), in the transition from the Middle to Late Bronze Age.
The authors combined the study of size with the changing numbers of settlements.

The problem is more complex than that, because indeed there was a decline in number of large settlements, but in the course of Late Bronze Age the amount of the small and tiny settlements actually increased significantly (Gonen 1984).
This could either imply that the size of the existing settlements actually decreased or there were new small settlements established.

That results in the general increase in number of settlement towards the end of LB, after the decline of 16 th and 15 th centuries.
We need to bear in mind the character of these new and reused settlements, most of which were comparatively small.

That means that the urban life did not pick up on such a big scale as in the Middle Bronze Age.
Falconer calls this phenomenon 'ruralisation'.

This, however, again indicates a shift and decline in inhabited area, but not necessarily decline in general.
After the MB II destructions some sites were reinhabited in LB and there is little evidence for existence of new towns.

Baumgarten (1992: 144) claims that "in size (...) LB cities display the same tendency toward preserving the achievements of the preceding periods".
The area of town was limited to the upper part of the tell and no attempt of expansion was noted as it took place in the previous period (Baumgarten 1992).

Gonen, on the other hand, calculated that in the LB the occupied urban area was only 45 percent of the one in MB.
If we do not regard the re-inhabited sites as new, their points of view seem to agree because there was a bigger number of small settlements, they were not regarded as new and due to their location they "display the same tendency toward preserving the achievements of the preceding periods".

The opinions of scholars can be affected by the character of sites they excavate.
After works have been undertaken in the bigger settlements, which show signs of abandonment, it is easy to slip into a conclusion that there was a decline in number of settlements.

But we need to remember how much unexcavated areas it still is to examine.
Gonen (1984) also claims that there were not many new towns established in LB, however along the northern coast some new ports were found around older MB harbors developed for trade with Cyprus and Mycenae.

In the southern, Egypt-dominated part of Levant the already mentioned fortresses establishing took place, which served utterly the imperialistic purposes.
The scarcity of information that we posses is caused by lack of attention to small sites.

It seems, however, that the new settlements were either those small ones established by people moving from big towns or the new harbors serving the expanding trade.
This does not necessarily indicate decline, but a search for alternative ways of subsistence.

Plans and defenses:
In order to announce the general decline in LB Levant we need to analyze the shifts in the towns themselves and see what they might have meant to the lifestyle and status of the inhabitants.

The plans of towns were generally of the same type, but there were different variations depending mostly on size of town.
Decrease in density of structures was recorded, accompanying the demographic decrease, which is another factor supporting the thesis of general decline of LB settlement.

The facts, however, are not so obvious.
In each bigger town or city there were both temples and palaces or governor's residencies.

In Late Bronze Age palace and temple tend to be separated, with the palace being moved to the area near the city-gate.
This may be an indication of the roles of these two authorities in the city and the changing relations between them through time.

Social changes are also evident in the architecture of the buildings themselves.
The LB period is the time of the development of the courtyard dwellings in the Near East.

Most houses were now equipped with a second storey and the movement of the residential rooms from the ground floor can be seen, which is a proof for improvement of the standard of living and maybe raising the status of the household.
Certainly we cannot talk about any decline in terms of housing then.

Decreased density was a response to decreased population, but no decline in material culture can be seen and in fact quite opposite, the houses improved with time.
Ben-Dov (1992) distinguishes two types of settlements, walled and unfortified, in the MB period.

He claimed that latter almost entirely disappeared during the Late Bronze Age, whereas the fortified settlements tend to have survived through the course of the following, politically unstable period (Ben-Dov 1992).
According to Gonen (1984) the majority of settlements, however, were unfortified at all, which in the times of frequent Egyptian invasions seems to be a great puzzle.

This implies that these unfortified settlements must have been those new small villages, as Ben-Dov claims that the MB unfortified settlements disappeared after MB period.
Many towns reused old defenses, as this was a common policy of Egypt, not to let the Levantine towns to build their walls in order to have a better control of them, weaken the power of the single city-states and to prevent revolts (Gonen 1984).

The crisis in defense system does not have to mean decline, but rather change in political situation and imposing Egyptian policy.
Prosperity or decline?

The presented data lead us to the debate on weather prosperity or decline characterized the Late Bronze Age towns.
The urban life during the period under investigation certainly undergoes a significant recession.

Many towns, destroyed during the military campaigns are not inhabited for a long time, others do not recover at all.
The fate of Levant in LB period is disputable.

Albright and Ahituv prove that Canaan could not have any other significance for Egypt as only the passage to other regions beyond it, therefore due to many internal and external factors its economy, thus city-states, declined (Gonen 1984).
On the contrary, Kenoyn claims that "by the last years of the Eighteenth Dynasty almost every town for which there is evidence in the Middle Bronze Age was once more flourishing and some had been newly established" (Kenyon 1971, quoted by Gonen 1984).

Na'aman also stresses that Canaan paid Egypt large tribute, from which pharaohs would not resign, furthermore they intensified their activity in this region, which resulted in flourishing of Canaanite towns (Na'aman 1981, quoted by Gonen 1984).
These opinions largely depend on what sort of material the scholar is drawing upon.

Those analyzing the Amarna Letters, like Na'aman will surely note a significant increase in wealth, because the letters do not mention decline, as they were mainly records of Egyptian exploitation of the region.
Archaeological data indicate decline, which was based on the evidence of destruction and movement of population that indicated decline.

However, we need to remember about the non-sedentary population which is hard to quantify, but still a part of the ancient society.
Therefore, although we inevitably deal with crisis after a major conflict, there is no evidence for decline in material culture.

More recently Knapp (1987) restates the traditional view of Albright, that LB Canaan underwent a period of serious cultural crisis, but Liebowitz (1987) argues that the increase of ivories in Palestinian sites in LB II is a proof for a cultural zenith rather than decline.
Bienkowski (1989), however, criticizes Knapp's opinion, using Jericho, Hazor, Tell Deir 'Alla as examples of degeneration in terms of architecture and pottery (towns outside Egyptian rules), but indicates also that in Beth-shan, Lachish and Tell el-'Ajjul there is evidence for more elaborate architecture and pottery.

Moreover he accuses Liebowitz for taking into account only the evidence from elite palaces (ironically, doing the same thing in his studies) in the towns influenced by the Egyptians, such as Megiddo, South Tell el-Far'ah or Beth-shan and therefore argues that these ones indeed developed, but the ones outside of Egyptian control declined during LB period.
This variation resulted from the position of a city-state being either strategic or marginal, therefore outside of Egyptian interest.

Archaeological evidence from the major tells (or even more precise their elite areas) reveal a great wealth of palaces, temples and luxurious goods.
Settlement and demographic studies, on the other hand, prove a serious decline in settlement and population.

Bunimovitz (1994) claims the paradox of prosperity and decline is caused by the elites desperately fighting for power in politically and economically unstable city-states.
The decline in human resources is the main root of the general stagnation, therefore looking at the settlement pattern we can examine the shifts of population causing socio-economic implications.

However, we need to take into account the regional political and economic circumstances while making such generalization.
To sum up it seems like there was a general increase in wealth in terms of architecture in the towns under Egyptian influence, but the ones outside of it gradually declined.

Indeed the interest of Egyptians proved to be crucial for those towns under their rules, since they were their source of income.
Like in case of all major demographic crisises, the population that is left mostly becomes wealthier because of reduced competition, more living space and resources.

Although there is a wide scope of written evidence as well as archaeological data for the urban life in Levant in the Late Bronze Age, the character of this data is often misleading, unclear and incoherent (Gonen 1984).
The written sources include the list of Thutmosis III (79 settlements in Canaan) and el-Amarna Letters (24 or 25 entries) from which we can conclude that there was a number of strong fortified urban settlements in Levant at that time.

Archaeological sources, however, show abandonment, destruction and decline.
These potentially contradictory views, however, have a reasonable explanation.

It is evident, that there was some level of destruction and abandonment in the LB Levantine towns, whether caused by the Egyptians or any other power.
If we assume this was the Egyptian influence, which is recorded in written sources, it becomes clear that by obtaining regular tribute from the region they create an illusion of a wealthy country.

They obviously did not know or care much about what was happening there as long as they received the tribute.
Therefore any destruction would not be noted in the Amarna letters as long as they were not significant for the amount of money Egypt acquired.

Continuity of occupation:
Gaps in continuity of occupation occur in the LB period in Canaan and the general view ascribes them to the activity of the Egyptian Empire in this territory.

There are, however, different opinions regarding this event.
One scenario suggests the "rapid destruction", others however claim, that the decline in settlement was a gradual process.

Kenyon states that such sites like Tell ed-Duweir (Lachish), Tell el-Fār'ah and maybe Sharuhen captured by Amosis were abandoned, which could have taken a year or a century (Kenyon 1973).
In addition, according to Gonen (1984) only 17 out of 77 analyzed settlements were continuously occupied since MB.

In the coastal area some of the tiny ports were still occupied and in some other cases new ones were established in the same place.
There is no evidence for occupation at Beth-shemesh and Gezer in the northern coastal plain and in the hill country Gibeon, Bethel and Shiloh were abandoned (Kenyon 1973).

By abandonment, scholars mean the destruction resulting from Egyptian attack and the outward movement of people that followed it.
Some of the abandoned sites, however, do not necessarily show evidence for destruction.

In addition, some sites do not follow the pattern of a rapid destruction.
Tell el-Far'ah (North and South) do not show evidence for destruction in the discussed period.

Furthermore, Beth-shan seems to be continually occupied which is evident in tombs and pottery.
The royal scarabs are as well a good indication of the potential continuity of occupation.

13 of the excavated sites show the evidence of royal-name scarabs.
Their presence is supposed to prove the Egyptian activity in Levant, but only six of the sites where the scarabs were found show sign of abandonment and destruction.

However, Weinstein argues that 81.5% of the scarabs come from these sites, which makes them even a better proof for the Hyksos-Egyptian conflict.
Two main questions arise: were the destructions in Levant devastating enough to drive the Hyksos from the territory?

And were the Egyptians responsible for them at all?
It seems as if the patterns of continuity were more complex and more difficult to establish.

Surely Egyptian invasion left a clear sign on the urban system of Levant, the changes being more evident in the southern and coastal parts of the territory.
Conclusion:

In the course of this investigation it has been proved that the urban landscape of Late Bronze Age Levant underwent some substantial changes from its Middle Bronze Age character.
Egyptian and Asiatic attacks had tremendous impact on life in Levant during 18 th Dynasty and the region never recovered fully from this blow.

General decline in both population and urban life can be noticed, where the latter resulted from the former.
Egyptian control imposed on the region after the military campaigns enabled the Pharaohs to send people from Levant to Egypt in order to obtain a cheap labor force.

Moreover, the invasions themselves undoubtedly motivated some of the inhabitants of towns to escape from the enemy.
In addition, the tribute that the Egyptian imposed on the native people also makes some of them move to satellite towns or villages in order to flee from the taxes.

Therefore the decline in number and size of the Levantine towns has been recorded in archaeological record, but it is difficult to trace the exact movement of people.
The question that arises is that of the distinction between ruralisation and decline.

Here again the answer is not straightforward and regionally unified.
The political and economic situation in this area of the Mediterranean contributed to the development of trade, therefore some coastal sites and harbors were established or survived, if not flourished.

On the other hand, more inland people seemed to turn to local production of goods mainly for subsistence (and to pay tribute in some places), therefore smaller settlement were favored to the big cities.
Within the towns themselves some changes also inevitably occurred, as there were fewer people, hence not so many buildings were necessary and due to Egyptian control fortifications were often just slightly repaired or left to decay, in all probability because the Egyptians wanted to prevent the rebellions in the towns.

For this reason also the established administration centers, garrisons within towns and separate fortresses.
Paradoxically, some signs of prosperity have been recorded in archaeological evidence, however this comes from wealthier towns and in addition from elite residences and palaces, which are not representative of the population as a whole.

Therefore, the argument is that the elites facing the Egyptian influence and unsteady political situation within their city-states had to struggle more to maintain their power, hence showing more material wealth as sign of prosperity.
Therefore, it might be more appropriate to use the term "change in lifestyle and subsistence methods" than decline.

Indeed we can speak about a decline in population and most definitely in the occupied area, however there is no clear evidence for the decline in material wealth.
In fact, there is sign of prosperity in elite groups as well as some significant improvements in the quality of dwellings of regular inhabitants.

However, in order to perform a valid investigation of the topic in the future, it is crucial that more sites are archaeologically examined and excavated, especially in the hill country, as there is little evidence from that region.
Introduction to the project:

Viking-age shipbuilding has always been an important part of Danish cultural heritage.
The techniques of this craft as well as its cultural implication has long been known to the people, however only with the discovery of Skuldelev ships, such a thorough analysis became possible.

In the second quarter of 11 th century AD the inhabitants of the Roskilde Fjord loaded five ships with stones and deliberately sunk them in a narrow channel in a place, where the sea bed was naturally raised.
These ships then formed a barrier, part of a whole system of barriers protecting the southern part of Roskilde Fjord and the town of Roskilde.

The Roskilde Fjord project is one of the most detailed and planned investigations of ships and shipbuilding techniques in the world.
It is also outstanding in terms of its duration, from the early investigations in the late 1950s.

up till now, when more works on the publications and further interpretation and exhibition are still being carried out.
The Danes are a very marine nation, hence the salvage and conservation of the ships is hugely significant for them, as being a part of their national tradition and cultural heritage.

The Skuldelev ships are interesting both because of the unconventional purpose they have been used for in the past and of the innovative techniques that have been used to excavate them.
By far the most detailed and recent report from the excavations that has ever been published is the volume written by the two site directors, Ole Crumlin-Pedersen and Olaf Olsen, The Skuldelev ships I : topography, archaeology, history, conservation and display published in 2002 by the Viking Ship Museum in Roskilde in cooperation with Centre for Maritime Archaeology of the National Museum of Denmark.

This is the main work that this evaluation will be based on, although it is important to remember about numerous smaller publications that appeared since the 1960s.
Strengths:

The approaches and methodology:
Undoubtedly, a strong point of this project is the approach with which the archaeologists started their works.

The investigation has been carefully targeted since its very beginning.
Firstly, its aim was to document and confirm the date of what has previously been regarded as a Queen's Margreth's Ship of 14 th - 15 th century forming a barrier defending Roskilde against the Swedes.

In addition, the newly developing techniques of scuba diving for archaeological excavations could have been applied on this site, since it was only 0.5 - 3 meters under the water surface.
There was a need to practice these new techniques and people preferred to practice them on the less significant sites.

However, shortly after the first surveys and initial excavations, the site revealed unexpectedly a lot more evidence than it was ever hoped for.
Bearing that in mind, the aims of the project altered to the salvation, conservation and exhibition of finds.

Undoubtedly an important advantage was the extensive funding received from various organizations interested in promoting the Danish cultural heritage.
In 1962, in order to raise the ships safely it was necessary to drain the whole area of the site (ca.

2,500m 2) by building a cofferdam.
It could not be drained completely straight away, because the timbers needed to remain moist, otherwise they would corrode and become virtually impossible to lift.

Additionally, the stones from ships' cargo had to be prevented from sinking into the soft wood of the construction elements, which would inevitably cause further damage.
Publication:

The volume published in 2002 is an excellent example of a detailed, in-depth and well structured publication of a project report.
Crumlin-Pedersen and Olsen have skilfully gathered the results of over forty years of excavation and interpretation and presented it in a form of a modern, well-thought and illustrated publication.

The work starts with presenting the environmental context of the find, namely the topography, hydrography and geology of the area, which of so important in underwater archaeology and understanding of the site.
Next, they report the excavation process, documentation, analyses, dating, conservation and display methods.

This is followed by the description and interpretation of each ship and historical background for the ships and the barrier.
The summary is in English, Danish and German.

Excellent exhibition and research:
The finds from the site have been displayed in a purpose-built museum in the port of Roskilde.

Since the first excavations, the museum has expanded both in physical and theoretical sense.
It includes the exhibition hall itself, but also a boatyard, where many other activities are taking place.

A lot of attention has been put into the conservation of the ships.
The original planks and other construction elements were exposed as part of a metal skeleton, with those elements incorporated where they would have been originally.

This visualization is surely very effective and helpful for visitors without archaeological background to interpret the elements, which otherwise would remain largely inexplicable.
The museum became also a centre for maritime studies in Denmark and one of the most important in Europe.

The extensive research conducted by its staff is a base for many other publications and projects.
Experimental archaeology is also an important component of this research.

Numerous journeys made on replicas of Skuldelev ships and other vessels contribute enormously to our understanding of both the technicalities of early medieval ships as well as the navigation methods.
Historical and social context:

The ships have been put into a much wider historical context.
The authors did not concentrate only on the techniques of shipbuilding, but also attempted to place their finds in a more social context.

Although it might be obvious to do so, shipwrecks have not been extensively used as evidence for social contacts between communities.
The aspects of technological advances of a given society and its relation to other groups have often been two separate areas of study.

In case of Skuldelev project, however, society plays an important role in the interpretation of finds.
This is evident, for instance, in the reconstruction of the history of use of the ships, the functions they had and how does that fit into a larger picture of the region.

Undoubtedly, the development of processual and post-processual archaeology in the 1970s and 1980s significantly influenced the methods of interpretation of the finds.
Weaknesses:

It is comparatively demanding to evaluate the weaknesses of the project, considering its scale and duration.
Significant drawback of the project is a limited number of thorough, detailed and summarized publications of the results until recently, when the above mentioned volume was at last finalized.

It is possible, that due to the lack of proper technology, mane data could not be analyzed in a sufficient way to present it in such a form.
Additionally, it is fair to suspect, that Crumlin-Peredsen and Olsen were largely preoccupied with the conservation and presentation of the finds to the local people, as that was their initial priority.

There have been, indeed, many technological developments since the ships have been raised.
New techniques of archaeological investigation were invented, such as geophysics or chemical analysis.

Most of the data from the Skuldelev ships, however, come from the time of excavations in 1960s.
Therefore, these new techniques have not been applied to them to the full extent yet.

Although new dating methods have been already used and the species of wood analyzed, not much else has been done in terms of other potential source of information, such as the more detailed analysis of organic remains or the chemical analysis of traces of food or plants.
Although there have not been many other stray finds discovered with the ships, the ones that have been found have not received enough attention.

They have been studied in their own context, but have neither been put into a wider context of the region nor of the relations with other regions.
Many inferences could have been made about the social aspect of the finds, had they been analyzed from a slightly more anthropological point of view.

Conclusions:
Roskilde project is a unique example of marine archaeology research, both in terms of its significance for Danish people and of the understanding of the development of Viking-age shipping.

The wrecks themselves and the barrier as a whole can tell us a lot about the technology and society of contemporary people.
The scale of the project, its importance and the amount of funding it received, all contributed to the establishment of such a successful centre for marine archaeology, which is a leading institution in Europe.

Also because of the innovative techniques of excavation, the project became known to the wider public already at an early stage, which is evident in the number of people that visited the site when the works were still in progress.
28,000 people came to Roskilde to marvel the cofferdam that has been erected around the site and the emerging Viking ships, such an important part of their cultural heritage.

The project has numerous strong points, bearing in mind its extent, duration and scale it has almost reached perfection.
Since the very beginning the methodology applied to the analysis of the site has been innovative and effective.

During the first phase of excavations in the 1950s the techniques of scuba diving excavations have been put into practice and during the second phase in 1960s archaeologists showed even more technological creativeness and advancement by constructing a cofferdam to drain the site.
In the course of research in the post-excavation phase, the interpretation of finds has been strongly influenced by the development of processual archaeology, which resulted in adoption of a more contextual approach.

This allowed putting the site into the historical and social context of the Viking-age Europe.
Ship wrecks can be a useful source of information about the different types of contacts that occurred between European societies.

This, however, has not been much explored yet, but Roskilde project is surely a pioneer in this field.
The results of the investigation have been very well published in the analyzed volume.

The presentation of data is well-thought, clear, organized and clever.
The authors began with the environmental background of the site, described the finds, their interpretation and the methods of conservation and dating.

The volume is valuable both for professional archaeologists as well as simply people interested in the finds.
Furthermore, the results have been excellently displayed in a purpose-built museum building.

The wooden elements have not just been put on display as they were found, but there is indication which part of ship they came from.
The museum also made a reconstruction of the whole way of living associated with marine activities.

Many attractions for kids have been organized, which draws even more attention to this already popular tourist destination in Denmark.
The drawback of the project is the enormous amount of time it took to produce a decent publication summarizing the obtained data.

This, however, could be explained by the continuous technological developments that the museum team encountered in the way, which provided them with more and more new material, unavailable before.
Bearing that in mind, the volume already seems slightly outdated, and many other things could have been analyzed, weather physically or chemically.

Also the stray finds might have potentially been put into slightly wider context.
Therefore, there are still a few points the authors can improve in their next publication, which in progress and will cover the experimental side of the project.

Let us hope that the technology will not be too much ahead the authors at the time of this next publication.
Introduction:

The first things that come to mind when we talk about the Viking Age are plunder, siege and violence.
However, this period in European history has undoubtedly left a much more significant mark on its cultural development, than merely burned monasteries and plundered villages.

After the fall of the Western Roman Empire, Europe faced an entirely different type of political, economic and social organization.
While the south was dominated by the Gothic kingdoms, the north-west Europe was much less centralized and the system of chiefdoms was predominant in the early stage of post-Roman times.

The growth of Frankish power and the spread of Christianity began the process of political and social changes, leading to early state formation.
Changes in the character of production, the increasing social stratification, the intensification of trade, and the formation of urban-type centers and the early-feudal states were accompanied by the development of shipping of inland, coastal, and sea-going types, emphasizing the differentiation of tasks in the sphere of transport.

Therefore, with the development of more advanced shipbuilding technology and the establishment of more centralized rules in Scandinavia, the so called Vikings initiated their sea voyages in search for new lands.
Till that time military conflicts or alliances dominated in the relations between the regions of north-west Europe.

Their aim of the Viking voyages, however, was not entirely limited to invasion and conquest.
They were as skilled tradesmen as warriors, therefore their expansion resulted in the rebirth of what was first the trade settlements around the Baltic and later proper towns or cities such as Dublin or York in Britain.

By that time, the seas around the north-west Europe were regularly crossed by Frisian merchants, Anglo-Saxons, Slavs and growing in strength Scandinavians.
The Viking voyages have been thoroughly studied for many years.

Much ink has been spilt on the Viking warfare and trade.
The vast amount of evidence available both in Britain and on the Continent encouraged the studies of shipbuilding.

Also the contacts between north-western societies received a lot of attention.
However, these two aspects of life in the Viking Age have often been two separate areas of study.

This topic has not been much explored yet, but shipwrecks undoubtedly carry a lot of information, not only about the shipbuilding techniques, but also about the contacts between early medieval societies and regions of north-west Europe.
The aim of this work is to examine the role played by the ship finds in our understanding of the cultural contacts between societies in north-west Europe under Viking hegemony.

Although trade and war were part and parcel of the social relations in the studied period, they will not be the subjects of analysis in this work.
Instead, it will concentrate on the evidence for the spread of culture, art, technology and religion.

Early Medieval wreck finds:
Most of the ship finds known also inevitably comes from the Viking Age from that region, resulting in the Viking ships being one of the best documented and studied development of ships.

This is largely due to the fact that Scandinavians treat that period as a very important part of their cultural heritage.
Therefore, the archaeology of the Viking Age, weather settlements or ships, is highly developed in Denmark, Norway and Sweden.

In addition, at least till the 10 th century the Viking dominated the navigations at all seas around north-west Europe.
Their early voyages were mostly targeted at war and conquest, therefore the safety at sea was very limited and the danger from the pirates caused decrease in sea traffic.

The Vikings did not fight their battles on their ships, but rather preferred to attack on land from sandy landing places, which was the reason why we do not find many remains of ships sunken during sea battles at that time.
There are, however, some wreck finds from other regions of north-west Europe, as all in all the Vikings did not manage to seize all the traffic at sea.

In the Viking Age the predominant type of ships used in north-west Europe was the clinker-built, round-hulled vessel, both oared and under sail.
They further divided into the keel plank boats associated with shallow waters of the rivers, creeks and coastal seas between Denmark and France and the T-shaped keel boats, associated especially with Scandinavia (Greenhill 1995).

In order to analyze the significance of shipwrecks for understanding the cultural contacts in Viking-age Europe, it is necessary to present in short some most important finds representing these types in different regions of north-west Europe.
It is impossible to present all the finds, however the ones shown below are surely representative for their seamanship traditions.

Scandinavian finds:
Scandinavian shipbuilding, as it was mentioned above, is very well studied and documented, mainly because of the extensive research directed by the Danish National Museum at Roskilde.

Numerous ship finds coming or related directly to that region contributed enormously to our understanding of early Medieval shipbuilding technology.
The best known examples of these finds include the Skuldelev or Ladby ships from Denmark, but also Norwegian Oseberg, Gokstadt and Tune or Swedish Äskekärr or Galtabäck to name but a few (Delgado 1997).

They represent a wide range of types, including warships (Oseberg, Gokstadt and Ladby), cargo ships (Äskekärr) and small fishing units (Skuldelev 6).
British contribution:

Archaeological evidence suggests that by the time of the Viking invasions, the shipbuilding traditions in Britain were also already well-developed.
Since we still know relatively little about it, the discovery of Graveney boat contributed largely to the understanding of the craft of shipbuilding.

In terms of its function it can be paralleled with Skuldelev cargo ships.
Greenhill (1995) suggests that it has been used in early tenth century AD to sail between creeks and rivers of Sothern England and probably to the Low Countries.

Another interesting, although not that obvious example is the Skuldelev 2 warship, which has been built in AD 1042 in Dublin and sailed possible as part of a diplomatic voyage to Denmark (Crumlin-Pedersen and Olsen 2002).
Germanic and Slavonic ships:

North part of what is today Germany has been inhabited by both Germanic and Slavonic tribes in the Viking Age.
Therefore, these two ethnic groups developed somewhat similar shipbuilding tradition.

By the end of the analyzed period, their achievements were only slightly behind those of Scandinavia.
Their ships were also clinker-built, predominantly cargo or multi-function vessels.

Extensive research has been done at the trading centre of Ralswiek on the island of Rugen, where ships have been excavated dated to the 9 th to 11 th century.
Although the lands inhabited by the Western Slavs lay more in central Europe, their inhabitants had also their input in the formation of the Baltic trade zone.

The ship finds discovered on the southern shore of the Baltic contain wrecks of cog and clinker-built boats, as well as many artifacts associated with the ships' function, weather it was war or more commonly trade.
The dugout and raft would appear to have been the prototype of the Slavonic boat.

The second stage covered the evolution of the oar-sail and sail-oar boats fitted with a side-rudder.
Those finds include ships such as Orunia I (I, II, III), Mechlinki, Charbrów, Czarnowsko, Szczecin, Frombork or Dzierzgon (Smolarek).

Wrecks as evidence for cultural contacts:
Spread of culture and art:

The large extent of influence of the Viking culture is evident even in such remote places like America or Constantinople.
In north-west Europe the traces of their artistic styles are visible in almost every place they visited.

Such elements as rune stones or sagas are well-known examples of the spread of Viking culture.
However, the shipwrecks on which these people traveled also inevitably contribute to our understanding of the spread of their culture as well as cultures of other societies that interacted with each other at that time in one way or another.

Towards the end of Viking Age, the societies in Europe were undergoing a transition to feudal society.
Because of its remote location Scandinavia was always slightly behind Europe in terms of social and religious changes such as feudalism, spread of Christianity or European trade.

These changes, however, reached Scandinavia at last and the way by which they 'arrived' is visible in shipwreck evidence.
Skuldelev 5 represents an interesting example of adoption by the Danish society of a system of relation between the king and local communities.

It is thought to have been built as part of the laidang system, where local community had the duty to keep one ship for the king's use.
The elements of construction of Skuldelev 5 suggest, that it was built from very cheap local materials and repaired many times to prolong its seaworthiness (Gardiner 1996).

Ship finds are often used to support the vast amount of written evidence that we have from the Viking period.
Numerous sagas, as well as chronicles reported about the everyday life of contemporary people and their contacts with foreigners.

It has been suggested, that the two women buried in Oseberg ship were Queen Åsa and her slave girl, a person known historically from Snorri Sturlusson's saga (Delgado 1997).
Also the Bayeux Tapestry is a source of many information about the Viking-age shipbuilding as well as everyday life of Norman people.

It presents activities from early stage of construction of a ship, to a full representation, together with their cargo and crew.
This brings the debate to the issue of spread of shipbuilding techniques throughout north-west Europe.

The remains of the Slavonic boats allowed reconstruction of number of early medieval vessels used by Western Slavs.
A good example might be the replica of Ralswiek 2 ship, Bialy Kon, which helped to shed some light upon the technicalities of Slavonic boatbuilding as well as the navigation methods.

Although less directly, this contributed to the wide debate about the contacts of Western Slavs with other Baltic regions, since during the trial voyage from Ralswiek to Wolin, the ship proved its sea going capability (Indruszewski 2000).
Also the art of the subsequent regions have been largely spread throughout north-west Europe thanks to the use of ships.

Most Slavonic ships were discovered in the place where they were discarded for one reason or another, which explains lack of major other finds associated with for instance northern ship burials.
However, numerous Slavonic finds in Scandinavia and vice versa proves that these ships once must have carried some vast amounts of objects, contributing this way to the cultural exchange between Scandinavian and Slavic societies (Chudziak 2003).

This thesis seems to be proved by the presence of late Slavic pottery on board of Skuldelev 5 ship, found also often in Scania (Crumlin-Pedersen and Olsen 2002).
Other finds related to Viking-age boats and harbors include a set of total 106 toys found by divers in the Gdańsk bay in 1948, which were models of boats from the same period.

Weather these models imitate Slavonic or other boats, this inevitably contributes to our understanding of medieval shipping and if these toys imitated ships from other regions, also the character of local contacts with different groups.
Therefore, it is evident that shipwrecks provided archaeologists with numerous valuable information about the technicalities of the spread of European cultures, which otherwise might have been unavailable from the finds inland.

Because of the limited space on a ship, the warriors, merchants or travelers took with them only what was essential for their life.
Warriors, who set off for a long journey would take 'a piece of home' with them on board, which is useful evidence for reconstruction of their ethnic identity (Hardt, Lübke and Schorkowitz 2003).

Merchants were traveling with the cargo that they hoped to sell on coastal markets.
Therefore, the finds from shipwrecks are very important in studying the cultural context of a ship and its crew.

Unfortunately, many such finds do not survive well in wet environment.
However, the wooden construction elements of ships survive surprisingly well.

This brings us to the analysis of the spread of technology in Viking-age north-west Europe.
Technology of shipbuilding :

There are several theories regarding the development of European shipbuilding.
Greenhill (1995) suggests, that many European shipbuilding traditions originate from one root, from which they sprang simultaneously in slightly different and independent directions.

While the Viking influence on the shipbuilding in the British Isles is quite obvious because of the scale of Viking settlement and presence in Britain, the lands beyond Germany have been comparably little investigated.
The site of the ship from Ladby, with its rich archaeological material proving constructions and repairs of local ships, is an evidence for the influence of Slavic shipbuilding techniques within contemporary Danish society (Delgado 1997).

Also, the wood analyzes of many Slavic boats have suggested that they carry some marks of repairs conducted on Danish islands.
The presence of Irish ship in Denmark (Skuldelev 2), which in addition has been built largely according to Scandinavian tradition, is an interesting indication of the flow of shipbuilding techniques across Europe.

McGrail (1993: 98, in Greenhill 1995) claims, that 'during the Viking period and up to the mid-twelfth century....
the evidence is thus overwhelmingly that the Dublin ships and boats were in the mainstream of the Viking tradition, in their sequence of building, in their form and structure, and in their method of propulsion (and probably also steering)...

The Dublin wood working techniques are in the mainstream, with little, if any, sign of regional variations'.
The Graveney boat also provides the first archaeological evidence for what is called barde in Danish, an angular projection at the junction of the stem and stern posts with the bottom structure of the ship.

Crumlin-Peredsen believes that in Scandinavian shipbuilding this feature was introduced with the use of sail, as it improved the stability of the ship (Greenhill 1995).
Large number of drawings representing barde appears in many north European contexts like the images on coins from Birka or Hedeby.

Although an exchange of technologies between societies which are in frequent contact might seem an obvious thing, archaeological material from shipwrecks themselves can reveal some unknown aspects or regions of such exchange, as the Slavic example has shown.
Therefore, we should not regard shipbuilding as a fully exploited area of research, but apply new technologies on known data in order to draw conclusions that could not be reached before.

Religion and burial rites:
Well before and during the Viking period, Christianity was spreading widely across Europe.

While the Franks or Anglo-Saxons was already Christians, in the central parts of Europe, as well as in Scandinavia the local pagan belief systems were still predominant.
For this reason, it is fair to suspect, that the Vikings having reached and settled in so many locations would have had a frequent contact with Christianity.

And indeed, towards the end of the Viking period we have evidence for wide adoption of Christianity within the Scandinavian society, together with more centralized power and political organization.
Weather pagan or Christian traditions, they surely must have spread by means of ships, similarly to other cultural aspects such as artistic styles.

Significant evidence proving this thesis is the spread of Scandinavian burial rites.
Ironically, many of the available ship finds today come from inland contexts, such as burials.

The well-known examples include the pre-Viking Sutton Hoo in Britain or Oseberg and Gokstadt ships from Norway.
This indicates the secondary use of the ships and may bias slightly their interpretation, since it cannot be entirely certain to what extent are they representative of the contemporary ships used at sea.

However, these finds are a valuable source of information about the burial rites in north-west Europe.
The tradition of ship burials seems to have originated in Scandinavia, from where it spread to the Germanic and Frankish people, as well as Britain.

On the territory of north-western Slavic tribe of Vielets in Menzlin, a number of burials have been discovered, which resemble the Scandinavian fashion of ship burial.
Although the graves did not contain ships themselves, the grave cut was shaped into boat-type cuts laid with stones.

This is an evidence for either Scandinavian settlement in that region or strong cultural influence (Duczko 2000).
It is justified to suggest, therefore, that the ship finds can reveal a lot of information about burial rites and their spread throughout Europe.

The settlers that practiced those burial rites must have sailed to the Vielet territory on ships, which they then tried to take with them to the afterlife.
This also provides interesting insight into the ideology and the system of beliefs of the contemporary people, weather the natives or the new-comers.

Conclusions:
The studies of Viking-age shipwrecks as evidence for cultural contacts are still relatively little developed in Europe, although vast amount of shipwreck materials have been used for investigation of shipbuilding techniques.

Many ship discoveries have been made either in late 19 th or the first half of the 20 th century.
The technologies of archaeological investigation were far less advanced than today, therefore many evidence that today would have been studied in detail, have been omitted and inevitably lost.

On the other hand, the new opportunities that experimental archaeology has to offer today open new perspectives and approaches to the study of boats.
A more social approach has recently been applied in this field, resulting in more advanced examination of the relation between shipbuilding and cultural contacts.

In the future experimental archaeology as well as innovative display methods will surely contribute to the understanding of the organization and relation between societies in north-west Europe in the Viking Age.
The Viking Ship Museum is an iconic example of the use of new technologies and theoretical approaches to the study of ships.

The research conducted there proved that shipwrecks can provide precious evidence for the mechanisms by means of which cultures and art spread in early Medieval Europe.
Such phenomena as the transition to feudal society or adoption of Christianity on European scale are certainly better understood if we know how they occurred.

Also archaeological materials might prove useful in verifying the often subjective historical evidence.
The shipbuilding techniques have been shared by many societies, which contributed to the general development of the region, resulting in the establishment of extensive trade connections.

The types of technologies adopted from other societies might as well tell us a lot about their mutual relations.
If the Irish adopted the craft of warship building, that might suggest, that the predominant relations that they had with the Scandinavians were military.

Undoubtedly, the technological advancement of one society put it in a superior position towards the other, which drove the technological development and competition.
The spread of Christianity has been studied in detail, however the pagan traditions that many medieval societies shared were equally important.

A more in-depth analysis of the finds accompanying the crew on their voyages might shed some new light on the spread of religious practices.
Many European burials contain ships, the tradition which spread from Scandinavia.

That might have meant that the Scandinavians were somehow treated as more advanced and adoption of their rites became a sign of higher status within other societies.
However, not only the shipwrecks themselves can be useful source of information about the Viking-age cultural contacts between the societies in north-west Europe.

Also other finds associated either with boat technology like the toys from Gdańsk or with boat burial rites, like Menzlin cemetery can reveal other aspects of these contacts, not visible otherwise in shipwrecks.
Therefore, it is fair to suspect, that with the development with even new techniques of archaeological investigation and surely with new approaches, even the finds that we have today can surprise us with new information.

Some require re-interpretation, the others just more in-depth analysis.
In the future, the emphasis should definitely be put on the new way of reconstructing and displaying the finds, to move them from the pages of archaeological reports to the museums, where they can come back to life as education tools for the present and future generations of visitors.

This flask belonged to an army sergeant William Belcher from Abingdon (Berkshire).
It was used in the late 19 th century to carry gun powder either for hunting or military purposes.

You would think that this is made of horn, but it is in fact an imitation.
What does that tell us about the owner?

Analysis:
Levels of information:

The label has been design in a way to provide the reader with different level of information, bearing in mind that there are various kinds of museum visitors, with different expectations and levels of engagement.
Thus the first line contains only the basic knowledge about the object, informing that it is a powder flask.

This piece of information is for those who only want to know what the object is and move on with their museum visit.
The second, informative part of the label is designed to provide the more interested visitor with some more in-depth information about the flask.

This is, however, not too detailed, mainly because of the limited availability of the information about this particular object.
This would not be a crucial part of an exhibition, therefore a thorough analysis is not so much essential at this stage.

The third, interpretive part contains more specialised information, especially of use for people connected with museums or students.
I have decided to use both informative and interpretive kinds of label and combine them to create one coherent piece of information.

Using only one type would be over simplistic and aimed at one particular group of visitors, whereas combining two types provide a reasonable amount of information for everybody.
Font:

I have used different sizes and styles of font emphasising different levels of information.
Thus the basic information about the object is written in bold and using larger font.

The main body of text was written using font size 20, because the object is small enough so hat people would look at it from a relatively small distance, therefore font 20 would be enough.
In terms of type of font, I chose a fairly simple form, to make the text easier to read, but a different from usual Times New Roman.

Contrast of text:
Adequate contrast should be maintained, especially in dim lightning, meaning that the closer the colours keep to black and white the better.

Black type on white background is easier to read than light type on dark background.
Moreover it is recommended to line up the text to the left (ranged left), as it is easier to read than type lined up on both the left and right (justified type).

Since the label would be placed in Museum of English Rural Life its design should match the style of the already used labels.
They are all fairly simple and mostly in black and white, therefore I kept my design in similar form.

Character of the text:
The text itself aims at answering the questions that the viewer might ask looking at an object he/she does not know.

The content of the text depends on the kind of museum the object would be displayed in.
As the potential place of display for the flask would be the Museum of English Rural Life, the text should emphasise the social context of the object instead of for example focusing on the materials used to produce it, which would be stresses in a museum of science, for instance.

The text contains information about the owner of the flask and its possible uses and significance as a social indicator.
I tried to keep the text concise and quick to read, using short sentences, avoiding any elaborate vocabulary.

The latter of which I find a little ironic, since we are creating an educational piece of work, the purpose of which is educate people and is aimed rather at the educated part of society.
However, taking into consideration the number of children that visit the museum it seems to be reasonable to use simple language.

The text is also written in a tone appropriate for the artefact.
Namely, the language is not too authoritative and not too light either.

The information I did not include in the text is the one about the manufacturer, as there is insufficient amount of information to put on an official label.
The flask is marked with a trademark of Sykes Patent, which is probably the manufacturer, but we do not have enough data.

What I did manage to find was the information about W. Belcher from Abingdon.
To find this I used the Kelly's Directory of Berkshire from 1891 where William Belcher was listed under private residents.

He was a member of Volunteers, which indicate the military use of the flask, but also does not exclude the use for hunting.
I deliberately used the word 'you' to address the visitor personally and create an opportunity for him to engage with the object on a more individual basis.

I also ask a question in the last part of my label, in order to make the reader think about the use of this object and its biography.
Catalogue entry design:

Analysis:
Design:

I based the design of this catalogue entry on the other entries from the database of the Museum of English Rural Life.
The reason for this is that the object would have been displayed probably in this museum, therefore the entry should match the pre-existing ones.

This design gives already at the very beginning a clear division of levels of provided information.
It starts with basic data about the object, then going to a physical description and mentioning also the names that are known and connected to the object.

It also supplies the more detailed information for those who are more interested in the subject and maybe will want to come to the place where the object is located for academic of other professional purposes.
The layout is as clear as possible, so it is easy to read and easy to find the information ones needs without going through the unnecessary bits.

Originally the points were presented in a form of a table, but this could be adjusted when necessary.
Font:

I used fairly simple font in order to make the entry easy to read.
It does not have to stand out in any way, as the database is mainly for professional use and the aesthetic values do not play such an important role, opposite to the functional role, which predominantly determines the outlook of the entry.

I used different styles of font as well in order to emphasise the distinct information and make it easier for the viewer to find what he is looking for.
The database is entirely for computer use, therefore we do not need to worry too much about the size of the font, as long as we obviously do not make it extremely small or extremely large.

The usual size of 12 will probably do.
The contents:

The contents of the particular parts of the entry were determined by the contents of the other entries on the database.
This is again to keep them possibly similar, since this entry would have been placed in the same database where all of them should be similar in terms of the amount of information provided.

The first thing visitor should find is obviously what the object of that entry is.
Then I have put the names associated with the object, but personally I think they should be placed further down as this information is pretty detailed and rather not meant for a general visitor, but for the one who searches for further information about the object and its history.

The physical description should in my opinion appear next, providing the viewer with introductory knowledge of how the object looks like before they open the like with a picture, to which I will come back in due course.
The next information that a reader comes across in this entry is the mysterious classmark.

I presume this is the negative number that appears in the files (the name of which is equally enigmatic), but the term seems to be too hard to understand for a general audience.
The entry also directs to other related objects from similar categories, namely Hunting, Shooting and Containers.

This is very useful for someone who undertakes a research of several objects of a type of object and wants to quickly find different ones in related categories.
The next part is purely designed for people who intend to physically find the object within the collection, as it provides the information about its exact location of the flask, which museum is it currently displayed or in this case stored in, exact location within that museum, with the number of shelf and the status.

The object could have been under conservation, in which case this should be changed to "Unavailable" or "Under conservation".
A very useful tool is the link to a new window with a picture of the object, which of course should have been made prior to the publication of the entry.

A small image of the flask could have been placed in this entry itself, so that a viewer could simply click on it to enlarge it, but since this is not a common practice in the Museum of English Rural Life I deliberately decided not to do this.
Introduction to the object:

The saddle shown above will be the object of this short account.
It will be based mainly on the interpretation of the author, although some major pieces of information from written sources will also be incorporated, as will be the materials from the files of the object held in the Museum of English Rural Life, by which the object is currently owned.

The scarcity of these materials, however, do not allow using them to a larger extent.
The lack of archival evidence might have been caused by the fact, that the saddle was acquired directly from its previous owner and user, which did not allow the whole elaborated pile of paperwork to be compiled.

The existing files included the correspondence with the Godmans' farm from Horsham who donated the saddle to the museum in 1958 (information derived from the object file).
This particular saddle is now held by the Museum of English Rural Life, which in obvious way determines the way of interpreting it.

The museum has divided its collections according to the materials the artefacts are made of, hence the metal, wood, straw or leather section, a part of which the examined saddle would have formed had it been in the main exhibition area.
Instead a similar saddle is presented as a part of a larger revealing the aspects of using leather on a farm, mainly to do with horses and their harness.

The analyzed one is meanwhile kept in a store, maybe because of the incomplete form.
There is probably a cushion missing, as there is one on the rest of such saddles and as one needed to be used while putting the saddle on a horseback not to cause any damage to the health of the animal.

At this stage it is crucial, therefore to explain the construction of such type of saddle and the way it was used in the past and is still used nowadays in some parts of the world, which will be analyzed further in this work.
A cart saddle consists of a pad or a panel, which in other words are two cushions, lay parallel to form a long gap between them and above the spine to lay the burden of the cart on the muscles on the sides of a horse instead of its spine and withers to protect the animal from injuries.

A wooden frame or a tree is attached to the pad, which is then covered with leather housing.
Leather part actually plays mainly decorative and protective role here, but it is undoubtedly dominating the outlook of the artefact which is possibly why it is allocated to the leather section.

A bridge is a part of the wooden frame and mostly a metal protection is incorporated in it, in a shape of a channel, in which the chain is placed.
A girth strap passes under the belly of the horse to keep the saddle in position.

Straps along the back of the horse, the meeter strap attached to the collar and harness, and the crupper strap along the horse's rump, kept the saddle from slipping backwards and forward.
A breech band (breechings) was fitted around the horse's hind end, attached to the crupper by hip and loin straps, acting as a buffer when bringing the wagon to a halt or pushing it backwards (Brown 1991).

The basic function of a saddle like this one is to provide a support for the chain connecting the two shafts of a cart, which are also attached to the collar through the fittings in a saddle.
The use of saddle allows taking a small proportion of the weight of a cart, to balance the weight and decrease the amount of energy the horse needs to move the cart (Keegan 1973).

A saddle like this would have been used by some major agricultural activities, as well as to pull some carts not necessarily in the countryside, but for example vehicles distributing barrels of beer etc.
For the lighter carts a collar was usually enough.

Sometimes cart saddles are also used for while ploughing the field, when a saddle with a 'redger' iron is put on a horse nearest the plough to take the weight of the downward thrust caused by the change in the line of draught (Keegan 1973).
There are signs of use on the leather housing on the sides in the places where chain would have contacted with its surface.

Also the straps seem to be well-worn, which is evidence for rather agricultural than representative use and/or for a long term utilization of the object.
Possible ways of interpretation:

There are several ways of interpreting the object.
Although now this particular saddle is owned by the Museum of Rural Life it is not impossible, that it changes the owner in the future but also, and more obviously, can be displayed in its current location.

Therefore, it is essential to think of other contexts in which a cart saddle can be presented.
For instance it could be a part of an exhibition showing the technological progress on a farm in the field of saddlery or in a more general perspective the means of transportation.

It could be considered more generally in terms of the development of the use of horsepower and chariots or ploughs to work the fields.
Or in terms of the technology of making the saddles themselves.

It is important to notice, what materials is the object made of and what is the workload involved in the production of such saddle and if it was the same or different in the past.
Given that the saddle consists of wood, metal and leather and the parts are fairly elaborated this might mean that different people were involved in the process of production of this object.

Also there could have been an exhibition about the designs of saddle used on farms or not only on farms.
This would involve displaying different types of saddles.

This particular one is a cart saddle but the also are riding saddles or racing saddles, the designs of which are varied according to the function they need to fulfil.
Here also we need to consider the different materials used, as the saddles used on farms need to be stronger and last longer than for example those for pure representative purposes.

Here the main and the most important part of this saddle is the wooden tree which holds the whole weight of the shafts.
In case of riding saddles the leather is the main component, which is obvious considering such saddles must make both the rider and the horse as comfortable as possible.

Kentish cart saddles, for example do not even have the leather housing (Keegan 1973).
Therefore it is possible to classify them not only according to purpose but also region.

The introduction and the development of saddler certainly affected the society in one way or another.
Looking at this aspect from a Marxist perspective it is possible to analyse the new division of labour commencing this event.

Prior to that, men had to be more involved in the fieldwork.
The weight of a cart that a horse could pull was limited by how much it could stand.

Also the time for which it could work was certainly shorter when the burden was heavier.
But when the cart saddles were introduced the labour became easier for the horse, which meant a single animal was more effective.

That might have had some serious effects on the economic situation of a farm or estate.
The next step of this ladder would be the introduction of a tractor and cars for ploughing and simple farm transportation.

This phenomenon could be looked at locally or in different parts of the world as the saddle is certainly different within other societies.
Some might still not use it at all.

How their economy and agricultural system looks like?
This brings us to ethnographic examples of the use of cart saddle.

A question arises, how different ethnographic societies deal with the problem of the weight of carts?
The harness as a whole might look completely different and there might be many different ways in which to make a horse comfortable with pulling a heavy cart.

Treating this saddle as an artefact makes it possible to consider it from an archaeological perspective, which I will focus on in the next part of this work.
A saddle as such was probably introduced as early as 2 nd century BC, which is proved by the finds from a Scythian kurghan burial of Pazyryk excavated first in 1929.

These nomad people hugely relied on horses in their daily lives, therefore were the first ones to develop a so called 'horse culture' and master the management of these animals.
The first societies to use chariots were the Hittites, Aryans and Celts, who then spread this technological advancement further into Europe, Asia and Americas (Edwards 1987).

This approach creates a possibility to arrange an exhibition of the development of horse management from domestication to the present use throughout cultures and regions of the world.
"Horse - the past and the future".

This would also link with the ethnographic evidence to form a nice, coherent entity in terms of the picture of the role of horses in the development of civilizations.
It is surprising that the ancient cart saddles already consisted of a pad that was made of two cushions placed on horse's ribs in order to protect its spine (Edwards 1990).

This is evidence that ancient Scythians already possessed a fair amount of knowledge about the anatomy and the needs of horses, which allowed them for such effective management.
Since the most easily accessible artefacts will obviously come from England, the local saddles would form a core of the exhibition, of which the ancient and ethnographic examples would be just nice introduction and context.

Therefore we can distinguish such types like Yorkshire or Lincolnshire, Scottish, Kentish saddles and so on according to county.
They vary in their construction and design, but the general rule and purpose is essentially unchanged.

That depends hugely on the type of horse people are using in different counties, as well as on the predominant local industry or area of agricultural production.
That is because different industries or agricultural activities required different types of equipment, within which horse was an important part.

Heavy industry or any kinds of activity required a heavy horse, which was naturally stronger built, therefore needed a slightly larger pad.
Different races of horses were also differently built, hence for example an unusual shape of the Scottish saddle, which was also affected by the type of collar used in that region.

These are the main ways of interpretation one can think of analyzing a cart saddle.
It is also useful to look at the marketing aspect of such object and the possible ways of advertising it.

There were qualified people who were producing saddles like this one and then undoubtedly selling them to the farmers.
Firstly, when this producti0on was performed by individual craftsmen, the saddles had many designs, according to the individual taste and techniques.

When mass production was introduced the number of designs of for example the trees was reduced to about twelve basic ones (Brown 1991).
This craft inevitably required some sort of advertising as well as an organisation of a selling point.

Saddlery was one of the crafts performed in the countryside in the past and it certainly required the whole marketing and advertising system around it, which might also be worth looking at.
It is also essential to think of an interesting way of displaying an object like this one, given that leather might be especially difficult to preserve and protect as an organic material.

An attractive idea that occurred to me was to make replicas of different parts of harness from different time periods and different cultures and actually let the visitors equip a horse by themselves.
That would be a model of a horse, of course.

But this would allow people to understand fully the roles played by different parts of harness and why each of them is important.
While the real object would have been placed safely behind a glass, the hands-on experience would provide a useful lesson.

Conservation matters also need to be considered.
Especially the lightning, as leather is sensitive for bright light and can easily loose its colour.

Also relatively new objects would draw attention and fingers of kids and some adults, causing potentially some damage.
Conclusions:

Given the character of the object and the information about it, that is possible to gather, it is rather impossible to display it on a detailed kind of exhibition about the history of this particular saddle.
But as each cloud has a silver lining there are many other possible ways to present it, including the above mentioned exhibition on the techniques and designs in saddlery, the social changes within societies commencing the introduction and technological development of saddles as a whole or merely cart saddles, as well as the ethnographic or archaeological approach to the object.

Horses played an important role in the development of early agriculture and therefore also our own civilization.
However, without such equipment like saddles and harness they would have been much less effective.

For this reason these objects should be given a fair amount of attention in museums.
We can also associate the object itself with a whole group of people surrounding it, using it and being involved in its production.

We can also learn a lot from those societies that use such equipment even today or some local form of it that plays the same role in the management of horses as the above analyzed English style of cart saddle.
Due to the materials it has been made of there are limitations of how it can be displayed.

Necessary steps need to be undertaken to protect it, but there are still possibilities of creating an interesting exhibition, for example by using replicas of artefacts.
Different methods of learning need to be taken into account, as well as different audiences, when we think of organising a display of any kind.

For some people illustrations of how the saddles were used might be enough to memorise their purpose and role.
For some others some more hands-on experience is necessary.

Also children would probably prefer to check by themselves how saddles were used in the past, not to mention the fun that would give them and therefore contribute to the good outcome of the process of learning.
Even though a saddle is theoretically a rather 'boring' item of everyday life, it is not made of any precious material and does not attract so much attention as some other treasures, for example archaeological, it can still be displayed in a way to make it as attractive and interesting as possible.

Considering its long history, in my opinion it is worth stressing it, because it is well amusing that such technological advanced items like saddles were used by people such a long time ago, and in addition by people of which we previously though as backward and mainly agricultural.
That is why I chose to analyze this aspect of interpretation of the object, because of the potential attractiveness of the exhibition and because of my personal interests as an archaeology student.

The behaviour, lifestyle and biology of the species Homo Neanderthalensis can be inferred from their fossil remains as well as comparative anatomy with other members of the Homo genus.
Information can also be obtained from studying the Middle Palaeolithic archaeological record.

It is important to recognise that in many ways they were quite similar to modern humans.
The configuration of their limbs, hands and feet indicate that they stood, walked and manipulated objects in much the same way that modern humans do.

There is nothing in their vertebrae, joint structures or feet to say that they were anything but a fully upright bipedal species (this definitely detracts from the stooping ape - like beast of the early 20 th century).
Probably most important in disproving this rather old fashioned view are the postcranial (body) bones associated with La Ferrassie 1 (approx 70,000 kya) a French site containing the remains of several individuals.

An earlier reconstruction of a Neanderthal skeleton by Boule (from his work at La Chapelle-aux-Saints in France) portrayed Neanderthals as stooped, brutish creatures which are misconceptions that have stayed with our notion of Neanderthals to this day.
The morphology of the leg bones and the foot demonstrate without any doubt that the posture and gait of Neanderthals differed very little from modern humans.

Today the skeleton of La Ferrassie 1 is considered the "classic" example of Neanderthal anatomy by many archaeologists.
(Fagan 1991: 159) The first and possibly most important part of Neanderthal anatomy to look at when considering anatomies influence on behaviour is the brain.

The Neanderthals were a relatively large brained species; Endocasts taken from the inside of fossilised skulls have been compared to that of other species in the Homo genus.
The brains of Homo Erectus were between 1,150 and 1,250 ml, modern human range from around 1,200 to 1,500 ml in volume.

This is curious when compared to Neanderthal brain sizes of around 1,300 ml for female specimens and 1,600 ml for male (Stringer and Gamble 1993: 81 - 82).
It can be argued that these figures suggest that evolution (at least where brain size is concerned) does not seem to follow a step by step pattern.

In other words as our species progressed brain size and intelligence increased in tandem.
On the other hand it can be argued quite strongly that brain size does not necessarily indicate intelligence within our own species, working with brain size alone is problematic at best.

Endocasts have shown researchers however that modern humans and Neanderthals do share at least one majorly important trait, they both have cerebral dominance which basically means that the left and right hand sides of the brain perform distinct, separate and unique function (Stringer and Gamble 1993: 83).
Although we can determine the actual size of the Neanderthal brain we cannot learn anything about its internal structure.

Archaeologists have studied various other aspects of their anatomy such as the size and shape of their endocranial cavities, as well as their vertebral spinal canals with do indicate (possibly) that the full range of cognitive abilities was available to them (Walker and Shipman 1996: 142) It is however virtually impossible to make any real and scientific judgements based on brain size or Endocasts alone.
In order to gain more insights concerning Neanderthal behaviour it is probably more pertinent to look at some other aspects of their anatomy.

Another important aspect of Neanderthal anatomy to consider when studying the relationship between anatomy and behaviour is their dental remains.
In comparison to modern humans the average size of Neanderthal teeth are much larger.

The front teeth were especially prominent and research has begun to show that they played an important part in Neanderthal life.
Archaeologists have studied and analysed the use - wear patterns found on fossilised teeth which appear unusually rounded, worn and deep, showing that they were subjected to heavy use.

Microscopic analysis has found traces of material, organic in nature, embedded in the teeth.
The patterns and alignment of this material are suggestive of material being gripped between the front teeth and pulled tight.

This evidence could point to the fact that Neanderthals used their strong front teeth and muscular jaws as a vice to secure a variety of objects and possibly food.
An example of how this might work could be the preparation of animal hides (Stringer and Gamble 1993: 77).The Neanderthal could of used his/her front teeth to grip one end of the hide whilst pulling it tight with one arm and then using the free arm to manipulate and prepare the hide possibly using one the many types of scrapper tool (some evidence points to the right arm being used for this task) (Walker and Shipman 1996: 142) Microscopic evidence from fossilised teeth found at Atapuerca in Spain support the theory that stone tools were being held in the right arm.

The action of the stone tool being used in conjunction with the item being manipulated left distinctly different scratch marks on the teeth indicating the direction of the tool (Stringer and Gamble 1993: 77).
Evidence such as this gives archaeologists valuable insights into Neanderthal behaviour and everyday activities as well as the role their unique anatomy might have played.

An interesting and unique piece was excavated at the French site of La Quina (approx 50,000 kya) where the fragmentary remains were found of several individuals were found (Scarre 2005: 149).
The same scratch marks were found on the milk incisor of a young Neanderthal.

This provides researchers with a tantalising insight into the early lives of Neanderthal children.
The use wear marks are extremely suggestive and show that at least to a certain degree children were being taught the skills necessary to survive as well as taking an active part within their family units or communities (Stringer and Gamble 1993: 77).

Apart from the Lithics that make up the vast majority of the Middle Palaeolithic archaeological record, the skeletal evidence provides the greatest insights into Neanderthal behaviour.
Neanderthal bones were extremely well built and robust with thick walls and large joints.

The average Neanderthal had much shorter forearms and leg bones (when compared to modern humans that is) and large barrel chests.
Therefore Neanderthals would have been rather short and squat in appearance, heavily muscled with broad chests (Stringer and Gamble 1993: 93).

Archaeologists have compared these traits to that of modern living peoples and have found that they actually have the most in common with the Inuit people (a race of people that have adapted to the cold climates of the frozen north with climates similar to that of ice age Europe).
This goes some way to proving the theory that Neanderthals evolved to survive in cold and extreme climates (other evidence to support this includes their highly effective nasal system thought to regulate temperature) and is reflected in their anatomy (Walker and Shipman 1996: 151).

The robustness present in Neanderthal fossils suggests a skeletal structure that had evolved to withstand a lot of punishment and hardship.
The bones themselves supported immense muscles, which made the Neanderthals into virtual powerhouses and left their mark.

The ligaments and tendons needed to support this musculature left deep marks on the bones at the point were they connected to the joints, the size of these marks are very suggestive of the sheer bulk of this species.
There is evidence from sites such as Le Moustier and Roc de Marsal in France that this well developed muscularity was present even in childhood.

It can therefore be argued that this is an inherited evolutionary trait as opposed to a trait developed over a period of time.
It does however prove that the Neanderthal required massive amounts of strength and endurance in order to survive (Stringer and Gamble 1993: 93).

An example of this could be their legs, which show great endurance and strength, this implies frequent and prolonged movement across the landscape carrying large burdens.
As far as anatomy goes and musculature goes the emphasis really was on brawn.

Neanderthal anatomy can also inform archaeologists about diet and subsistence strategies.
Their fossil remains show abundant signs of nutritional depravation during growth and around 75% of all fossils found show defects in the teeth as a result of this.

Stable isotope analysis of the bones chemical composition shows that they were primarily carnivores, with meat accounting for approximately 90% of their diet (although recent excavation with more up to date scientific processes have begun to uncover evidence for at least some plant resource exploitation, though this is mainly is the Southern parts of Neanderthal territory) (Scarre 2005: 149).
It is therefore entirely valid and reasonable to assume that hunting (and some scavenging) would have played a large part in the average Neanderthals life, in fact their lives and behaviour patterns could well have been dominated by changing weather patterns and the movements of their prey (the archaeological and skeletal evidence does support this i.e. stone tools with no other obvious use apart from hunting and cliff drive sites like Jersey) The difficult existence of the Neanderthals is reflected in the high frequency of traumatic injury found within the fossil record.

Amazingly almost all adult Neanderthal fossils found to date show an astounding array of serious wounds, sprains, fractures and break (even an amputation at one site called Shanidar in Iraq).
Many of these injures are healed fractures and breaks occurring around the torso (upper body).

Comparative studies by Berger and Trinkaus in the mid 90's have thrown up some interesting data, a fair number of the injuries found closely resemble the kind of injuries sustained by modern day rodeo riders.
In other words the kind of injuries one might expect when coming into close contact with large mammals.

Archaeologists have suggested that the dangerous close quarters hunting of large herbivores such as Woolly Mammoth, Rhino and Aurouchs could very well account for injuries of this type (Scarre 2005: 149).
The tricky and sometimes unreliable ageing of Neanderthal remains show that life expectancy was very low (again when compared to modern humans, biologically speaking they may have had a much shorter life span).

Few fossils have been found of individuals over the age of 40 and none whatsoever have ever been found over 50 (so far anyway!) This shows that the dangerous lifestyle and world of Middle Palaeolithic Europe and South West Asia clearly had its costs, in terms of stress, injury and life expectancy.
Fossil remains show that over 70% of Neanderthals experienced times of great stress during their development (Fagan 1991: 159)..

There are certain sites which provide archaeologists with vital evidence and clues concerning Neanderthal lifestyles and the trials they in their ice bound world.
One important site worth mentioning is the French site of La Chapelle - aux Saints (approx 60,000 - 50,000 kya), discovered and excavated by A and J Bouyssoine in 1908.

It was here that the now famous "old man of la Chapelle - aux saints" was discovered, the reason this find was so historically and scientifically important was the fact it was the first relatively complete skeleton discovered.
The nickname "old man of la Chapelle - aux saints" was used because the age of the skeleton is approximately 40 years, which as previously discussed is the upper age limit for the species (Fagan 1991: 159).

The "old man of la Chapelle - aux saints" is an interesting and useful find because his anatomy provides researchers with a wealth of information about his life.
Many of his teeth were missing completely but the bone that had surrounded the teeth had been given more than enough time to heal after tooth loss.

This means that he lived a considerable of time after he lost so many teeth.
He also suffered from debilitating arthritis which would have certainly left him incapable of performing any tasks that required strenuous physical labour.

It has been argued that in order for him to survive for so many years he would have needed the help and support of the other members of his group with processing food and maybe even moving from place to place.
This fossil is considered to be the first example of Neanderthal altruism to be discovered.

(Fagan 1991: 159) The fossil remains found at the Shanidar cave site in Northern Iraq (the extreme Eastern edge of known Neanderthal territory) were excavated during the 1950s by Ralph S. Solecki and Rose L. Solecki. Stratified occupation layers were identified in the cave dating to the Middle Palaeolithic and Upper Palaeolithic periods, as well as a later component dated to the Pre Pottery Neolithic (10,600 BP).
The lowest levels at Shanidar (approx 50,000 kya) included some accidental, and some apparently deliberate burials of Neanderthals including the Shanidar 1 skeleton.

The fossil was male and like the "old man of la Chapelle - aux saints" was aged around 40 - 45 years at the time of death, his skeleton displayed signs of severe deformity.
His right arm was severely atrophied (withered) a condition he had for most of his life, possibly even from birth.

His right leg was also crippled and deformed (which means that he probably had a painful limp throughout his life) and had a healed fracture on one of the metatarsals.
At some point during his life he suffered from a severe and crushing injury to his right eye socket (clearly visible when the skull is examined) which could have resulted impartial or complete loss of sight in that eye, even brain damage to a greater or lesser degree.

All of these injuries combined would have almost definitely led to at least partial paralysis throughout the right hand side of his body.
What is clear however is the fact that these injuries were sustained long before death and he survived for a good number of years despite them.

He would not have been able to contribute very easily towards his community or family group and would have been a strain on resources and a burden.
Archaeologists believe that this situation (and that of la Chapelle - aux saints) infers that Neanderthals looked after their sick even when they were not an active part of the group, this denotes implicit group concern (Fagan 1991: 160).

This says a lot about Neanderthal social structure and organisation, as well as suggests levels of care, emotion and compassion (coupled with the evidence for the possibility of burial practice) previously only attributed to modern humans.
There could however be an alternative argument, the Shanidar 1 fossil and the "old man of la Chapelle - aux saints" could both have been either the group's leader or have had access to knowledge and memories that the group needed in order to survive.

Variation must also be taken into account, just like with modern society, different groups might have had different practices, whereas one group might leave an injured member behind another might save him.
It is unfortunate that the answers to these questions are unanswerable.

The Neanderthals were a highly evolved and well adapted species, who flourished for millennia, is the harsh conditions of ice age Europe.
Archaeologists have gleaned vast amounts of data from studying their anatomy.

Their skeletal structure tells of a species with the strength to handle the dangerous situations they faced on a daily basis.
They must have relied heavily on the heavily musculature, which must have been at least as important if not more so than their intelligence.

They probably had at the very least a form of proto language (evidence for this comes from the hyoid bone found at Shanidar which is very modern human in appearance) as they almost certainly had the cognitive functions required.
Their children grew up quickly and had to use their unique anatomy to adapt to their environment as well as learn the skills necessary to survive in the short time they lived.

In many ways their behaviour seems very human in others it seems completely alien.
We will never know their complete story, but through careful scientific study of their anatomy as well as the archaeological record we can learn as much as possible and their behaviour and everyday lives.

Deposition of artefacts (often valuable metalwork), or hoarding as it is more commonly known, within the landscape is a well known and studied aspect of prehistoric behaviour and culture.
Deposits are found in many varied locales and are disposed of in different manners; they consist of myriad artefact typologies as well as cover a wide range of forms and functions.

They also help archaeologists to form chronologies and chart the history of metal production as well the formation of certain societies.
Deposition is a world wide phenomenon and exists in many eras from the Mesolithic to the medieval periods.

But it is most well known and studied within the European arena (as well as the most stunning artefacts).
Archaeologists have over the years presented many theories concerning the motivations behind deposition and have also proposed a set of categories to work with.

One of the major theories concerning Bronze Age hoarding that has been presented by and argued for by archaeologists is relatively straightforward and logical, is that hoards are a direct response to warfare and periods of political and social unrest.
This theory basically states that the fear of war and the associated plundering and raiding would result in people 'hiding away' valuable goods in order to protect their wealth.

The goods deposited in this circumstance would have been deposited with the intention of being retrieved at a later date and would have been placed (in most situations) in a pit or posthole for safe keeping (Grinder - Hansen 23rd March 2001, URL ).
It can therefore be argued (but never proven beyond a shadow of a doubt) that if the people responsible for this form of deposition did not return it was for reasons that were beyond their control (this can be also be arguably applied to Founders, Merchants, and Personal hoards).

They may not have survived the turmoil or had to the leave the area quickly and were unable to return, but this is ultimately just conjecture (Bradley 1996).
A good example of deposition (although rather late) during times of warfare and unrest is the Hoxne treasure (pronounced 'Hoxon') which is comprised of over 15,000 gold and silver coins, gold jewellery (comprises of a body-chain, a small group of necklaces, three finger-rings and 19 bracelets), silver tableware such as pepper pots, as well as ladles and spoons (obviously items for everyday domestic use with no ritual significance attached to them).

The previously mentioned items were found in the remains of a large wooden chest.
It was discovered in November 1992 by Eric Lawes, who immediately reported the find and did not remove all the objects from their contexts.

This responsible conduct enabled the Suffolk Archaeological Unit to carry out a controlled excavation of the deposit, which has greatly enhanced the importance of the Hoxne Treasure for research in the future as well as preserved vital contextual information (Bland R, Johns C.M, 1993; 17).
The latest of the coin issues in the hoard establishes that its deposition took place some time after 407 A.D.

This was the period when Roman rule was starting to break down in Britain and was extremely unstable, and the Hoxne hoard could well be related to these events.
The careful burial of this treasure suggests that it was temporary in nature, meaning that the owner intended to return and recover them (Bland R, Johns C.M, 1993; 19).

One of the only ways to provide evidence to support this theory is to take a comparative approach.
To do this it is possible to take into account hoards from later better documented (not always!) periods such as the early medieval and middle to late medieval periods.

The hoards would have to be as accurately dated as possible and then compared to surviving records (both documentary and archaeological) of political and social unrest from that particular period.
It can then be argued (if a correlation is found between periods of warfare and unrest and an increase in hoarding/deposition in that region) that hoarding does indeed intensify during times of warfare and political and social upheaval (Grinder - Hansen 23rd March 2001, URL ).These results can then be extrapolated to the Bronze and Iron ages and used as a model for future arguments.

Richard Bradley argues against this approach and says that it has "led to contradictory interpretations, and has obscured some of the most striking developments that took place over time" (Bradley 1996).
which means that it can lead to false conclusions which would obscure scientific fact, what is true in one context is not necessarily true in another.

Some hoards are representative of the systems concerning Bronze and Iron Age metal production, manufacture and manipulation.
This type of deposition is commonly known as a Founder hoard (Minster in Kent and Ashbury in Oxfordshire are two such examples) and is generally associated with the production of metallurgy.

For the most part they consist of broken, used or unfit metal objects, bronze/iron/copper/tin ingots, casting waste and in many circumstances complete or newly finished objects (Champion, Gamble Shennan, Whittle 1984: 288 -289).
Hoards of this variety are another example of depositions that are temporary in nature as the person or persons responsible probably did intend to return and retrieve the objects.

Hoards that include metal ingots are generally indicative of raw material usage, distribution/exchange networks, supply and material composition which provide archaeologists with vital data; they also provide key insights into standardization (Bradley 1996).
Whereas hoards that are composed of valuable objects of a similar design and style that were deposited near to or soon after final stages of production and manufacture are argued to be the stock of a metalworker being held securely in reserve whilst awaiting distribution and trade (Champion, Gamble Shennan, Whittle 1984: 288 -289).

This argument (if true) has important ramifications and provides vital data concerning the demand for certain objects during the Bronze and Iron Ages.
It can be asserted that metalworkers were producing and manufacturing items in anticipation of demand as opposed to waiting until the items were needed, it shows that in some cases and regions a surplus was required.

The two previously discussed types of founder hoard are mainly dated by archaeologists to approximately the 13 th century B.C.
It is not until approximately the 10 th century B.C that a new and distinct form of founder hoard appears (Champion, Gamble Shennan, Whittle 1984: 288 -289).

This new type of deposit is composed of objects that are to all intensive purposes scrap metal.
The deposits are comprised of objects that are old and worn, some showing heavy long term usage and have in many cases been rendered useless.

Some of these particular deposits are made up of hundreds, even thousands of metallic items, many of which have been broken up into smaller pieces.
The largest deposit ever found in England is a founder's hoard known as the Isleham hoard, numbering 6500 pieces is the most famous example.

Hoards such as these, some archaeologists have argued; contain vital information and key data respecting the nature of metal production and manufacture in the Bronze and Iron Ages (Champion, Gamble Shennan, Whittle 1984: 288 -289).
They argue that deposits of this nature prove that Bronze and Iron Age metalworkers were concerned enough about the supply of raw materials that they secured reserves of material that could be recycled if necessary.

This argument is supported by supplementary evidence, the location of the deposits are often found in direct association with sites of metal production.
(Champion, Gamble Shennan, Whittle 1984: 288 -289).

Merchant hoards are another form of deposition associated with the temporary placing of valuable objects in a secure location.
They are usually composed of objects that have been newly completed and are similar to founder hoards because they are being stored (in part at least) in anticipation of future demand (there is also an element of security in order to prevent theft by storing the objects in a place known only to the merchant).

The type of objects that may well be included in such a hoard include functional items such as tools, weapons of varying types and a certain proportion of luxury or prestige goods intended for the elite classes (Bradley 1996).
Another argument that has been presented concerning hoarding is that they are acts of consumption and can be used as political tools of control.

It is believed by some groups of archaeologists that high status metallic goods were at least part of the basis for the establishment of control in the emerging stratified societies that are indicative of the Bronze and Iron Ages.
The elite 'ruling' classes of the time period in question would have turned the supply of high status goods to their advantage using them as tools to confirm their status, Limiting the quantities available would have served a distinct and useful purpose as too many high status goods in circulation would have lowered their value and status and led to difficulties in restricting access.

(Champion, Gamble Shennan, Whittle 1984: 294).
By taking older high status goods out of circulation and depositing them where they could not be easily retrieved it meant that more items could be acquired whilst keeping their intrinsic value and status at a premium.

The method of deposition (as well as the types of goods deposited) varies from region to region, ranging from large scale burials to deposition in inaccessible locations (such as lakes, rivers and peat bogs) to simple pit burials.
High status burials (for example Hochdorf, Germany and Vix in eastern France) would have had a threefold purpose, firstly to reiterate the power, status and wealth of the individual interred, secondly to fulfil any ritual and religious requirements concerning burial practice and thirdly to take certain items out of circulation (Bahn P 2004: 205 & 506).

Archaeologists can arguably use this type of evidence very effectively to explore and chart the ever changing role of status in the hierarchical societies of Bronze and Iron Age Europe (Champion, Gamble Shennan, Whittle 1984: 294).
Votive deposits are deposits that are ritual in nature or have had some form of religious significance attached to them.

This does of course leads to difficulties concerning the identification of ritual which is notoriously difficult to pinpoint.
They are considered to be different from the vast majority of other forms of deposition; partly because of their ritual nature but also because of their permanence.

The permanent (placed for the main part in non - retrievable locations) nature of votive deposits is in direct contrast with the temporary or accidental nature of other forms of deposition (such as merchant or founder hoards).
A large number of Votive deposits have been found in places which provide supporting evidence concerning their permanent nature (Harding A F 2000: 326 - 329).

They have been recovered from a wide variety of locations such as peat bogs (Vikso, Northern Zealand, Denmark), rivers and lakes (Ljubljanica River, Ljubljana, Slovenia) and other 'watery' places (Cunliffe B 1997: 330; Kaufmann C 2007).
Other locations are also utilized such as mountain passes (Italy, Alpine Regions) and even the bases of cliffs, areas of metal production also receive some attention.

A good example of this is the Bronze Age hoards from Dieskau, Saallkreis in central Germany (approx 2000 B.C) an area adjacent to the rich copper (Cu) sources of the Harz mountains (Cunliffe B 1997: 256).
It is fairly obvious that retrieval of these deposits would be very difficult or even impossible.

Votive deposits can sometimes be identified purely for the fact they sometimes contain a wider range of goods that need not be 'manufactured remains' but can also include organic remains such as animal (antlers, skeletal) remains as well as human remains (sacrifices, burials) (Bradley 1996).
The range of manufactured goods can be very far reaching and cover several distinct typologies.

A British examples may be seen at Flag Fen near Peterborough in Northern England.
Metal detectors (one of the principle methods employed to locate hoards, whether accidental or purposely) were used to great effect to reveal a remarkable series of beautifully worked metallic (bronze) goods.

These objects included swords and rapiers, spearheads (see front page for photographic example), chapes, knives, awls, razors, rings, pins, a gouge and an axe, as well as other smaller objects and a small but noteworthy collection of iron objects (Byford J 20th February 2007.
URL ).

Whilst many votive objects are complete or finished, there are on the other hand many that show signs of previous wear and tear, and some that are broken or damaged (comparable to those found in founders hoards but not entirely the same) or show some casting flaws.
It can be argued that in some cases the act of deposition coincided with the end of the objects 'life' or when it had outlived its usefulness.

Some archaeologists (including Francis Pryor of Time Team fame) argue that in some deposits the objects were deliberately damaged before deposition (comparable to the Roman practice of piercing pots before depositing them in wells) again reminiscent of objects coming to the end of their usefulness and then being symbolically destroyed before they enter the 'supernatural' world (Harding A F 2000: 326 - 329).
Some objects appear to have been made specifically for ritual purposes and even for the act of deposition itself, in some cases this can be characterised by poorer manufacturing techniques or material usage.

For example in some deposits axe heads have been found that have been cast from lead (Pb), a weak soft metal that would have rendered the object useless for everyday practical use(Harding A F 2000: 326 - 329).
The Battersea Shield (now residing in the British Museum) is an example of a specifically made ritual item; it was dredged from the Thames near Battersea Bridge and has been dated to the Iron Age, approximately 350-50 BC.

The Battersea shield was not made for serious warfare and is too short to provide sensible protection.
The thin metal sheet and the complicated decoration would be easily destroyed if the shield was hit by a sword or spear.

Instead, it was probably made for flamboyant display and ritual.
The highly polished bronze and glinting red glass would have made for a great spectacle.

It was finally thrown or placed in the River Thames, where many weapons and other items were offered as votive deposits and sacrifices throughout the Bronze Age and Iron Ages (Stead I.M, 1985: 21).
It is therefore entirely plausible to argue that its purpose was distinctly ceremonial possibly intended specifically for votive deposition.

This highlights the importance of ritual in Bronze and Iron Age Europe.
Votive deposits represent one of the key interfaces between the peoples of these periods and their gods as well as providing key insights into their belief systems (Champion T 1999: 106 -108).

The significance of 'watery' locations is unavoidable and highly indicative of votive deposits, the earliest deposits date to the Mesolithic (Scandinavia, Slovenia).
These locations were obviously of great importance to the people who made these offerings, it is however unknown exactly how they viewed these places (theories include portals to the 'supernatural' realm, mirror images of our own reality) and why they were so specifically chosen (Stead I.M, 1985: 29).

Some locations received offerings over extended periods of time (Dowis, Co.
Offaly, Ireland) even stretching into the hundreds and thousands of years (Bahn P 2004: 130).

The Ljubljanica River in Slovenia is one such example; depositions have been recovered from the river for over 20 years numbering 10,000 - 13,000 or more.
The earliest being Mesolithic in origin through the Bronze and Iron ages, the Roman occupation and even a 17 th century flintlock pistol has been recovered.

This proves the argument that certain locations had a certain sacred aspect that held a special place in the belief systems of the Bronze and Iron Ages.
They were visited time and time again and must have been centres of religious focus (Kaufmann C 2007).

However it is not enough to merely identify ritualistic behaviour, it must be explained and understood which presents a whole new set of problems.
In some cases deposits are viewed as boundary markers, these deposits mark all kinds of boundaries in the ancient world as well as significant or special places within the landscape.

For example these kinds of deposits have been found in niches in walls or buried in postholes within settlement locales.
It is however difficult to determine whether or not these locations had some kind of cultural, ritual or social implication or the deposits were just personal in nature.

Some deposits have been found in seemingly random locations, but when viewed as part of the wider landscape they appear to have some form of significance (Bahn P 2004: 174).
In conclusion archaeologists have devised many theories to account for the deposition of metallurgy in Europe.

Each theory looks at the major (and minor) differences in form, style, typology, location, scale and condition of the artefacts in each assemblage and then assigns them to a particular category.
Each category has its own approach and definition which helps to formulate theoretical viewpoints concerning the reasoning behind the deposition of metallurgy.

Ultimately these theories are just conjecture and the true reasoning behind the act of deposition is elusive.
This paper is distinctly post - processual in its approach to settlement archaeology and urges the reader to take a broader view of the Neolithic and its people, the author has a definite background in the archaeology of landscape as well.

This background is key in understanding the point the paper is trying to make, to make his point Pollard uses many examples and is highly argumentative in his approach.
As the title suggests this paper is primarily concerned with settlement practices in the British Isles.

The author, Joshua Pollard draws on his extensive knowledge of settlement in the British Neolithic as well as his apparent intimacy with previously excavated Neolithic sites.
His examples cover a wide geographical range, comparing and contrasting sites as diverse as Skrae Brae in Orkney, Hurst Fen in East Anglia and Hazelton North in Gloucestershire.

Pollard uses these examples effectively and they help to support his arguments.
He acknowledges the fundamental differences between settlement practices in the far North and Southern Britain.

He uses a comparative approach to support his arguments concerning Neolithic settlement variability and mobility, he states that; "Nowhere is this seen more dramatically than by comparison between the rather ephemeral and impermanent character of settlements in southern England and the permanent or semi - permanent 'farmstead' and 'village' scale sites of the Northern Isles of Scotland." Although the geographical range is quite wide ranging from North to South, it could have used some examples from continental Europe and even Ireland.
These examples could have been used to broaden and develop the comparative nature of the paper and the arguments presented.

The paper is mainly concerned with looking at different aspects of Neolithic settlement within the British Isles.
The paper has three distinct areas dividing the arguments, 'temporality and mobility', 'scale', and remembrance and reference'.

Pollard begins by pointing out the fact that the relationship between people and the experience of settlement and landscape is a highly important yet relatively un - studied aspect of Archaeology.
He states that "as archaeologists we rarely reflect on this sense of personal experience when engaged in studying settlement".

His arguments try to instil a new dimension into the understanding of settlement.
Pollard tries to expand the readers viewpoint by arguing that the decision making process when applied to Neolithic settlement practice is extremely fluid and variable, it is in fact a "skilled practice involving experiential knowledge, social and ontological risk, decisions about where and how to live, with whom and at what cost or benefit".

He then goes to criticise archaeological thought and practice by pointing out the fact that "the central problem with the archaeology of Neolithic settlement rest not with the nature of the information we recover, but with the way it is understood".
This is a prime example of the whole basis of Pollards argument, he wants the wider academic community to approach Neolithic settlement archaeology in a whole new manner, to reinterpret the current schools of thought.

At times he also argues that archaeology can be too influenced by the times and different 'waves' of thought and that this needs to change in favour of a more open viewpoint.
The central argument of the paper as a whole is primarily concerned with broadening understanding of the largely dismissed (in favour of large scale ritual sites) field of Neolithic settlement.

Pollard is trying to change the way in which the wider academic community classifies and approaches this particular field.
Pollard views Neolithic settlement systems as highly variable, he brings attention to the fluid (a word that stands out very prominently throughout this paper) nature of the era (in his view anyway).

The author's arguments are very strong and it is plain to see that he is attempting to guide the reader's viewpoint and make them aware of the mobile nature of settlement and how it was affected by various factors such as season, location and even geographical region.
In order to support his argument concerning the fluidity of Neolithic settlement pollard uses a range of examples drawn from his extensive knowledge of the field.

One such example is Trelystan, Powys near the Severn Valley, it was here that a series of "flimsy" stake - built structures was excavated.
Archaeologists suggested that the reason for the structures "flimsy" construction was the fact that they were not expected to be occupied for more than a decade or so (argued to be an example of seasonal transhumance).

A structure of this construction and type would definitely not be an example of long term residency and planning.
Pollard also makes use of material evidence such as lithic scatters and middens to help support his theories surrounding the lack of apparent permanence in Neolithic settlements ( it is here he uses the comparative approach to great effect again when comparing Neolithic material culture to that of the Mesolithic).

"Tightly defined, high - density lithic scatters of the sort that frequently define Mesolithic occupation sites are not common.
The poor survival of Neolithic middens, even under the protection of alluvium or the mounds and banks of later monuments, suggests that these rarely had the chance to accumulate to any great extent.

Where it is possible to gauge duration of occupation at particular locales, this seems to be measurable in years or a few decades, rather than centuries".
There is however a flip side to Pollards argument, he states that by its very nature Neolithic settlement practice was highly varied.

This therefore leads to the conclusion that not all populations were quite highly mobile, in some cases and geographical regions certain population groups were sedentary in nature, Pollard approaches this aspect as well and he warns of the problems that could arise from ignoring this fact; "However, by focusing too much on settlement practice that embodied varying degrees of 'inbuilt' mobility, there is a danger of making recourse to general models for the period and consequently ignoring clear regional variations in practice".
This clear acknowledgement of two distinct variables in Neolithic settlement practice helps to bring Pollards argument together before he moves on to discuss ritual and domestic boundaries.

Pollards main argument concerning the boundaries between the ritual and the domestic, is simply and clearly stated, he views Neolithic monuments as acts of remembrance as part of the peoples place within the landscape.
He uses chronological examples to support his arguments that (as with most elements of Neolithic settlement practice) the boundary between the ritual and the domestic was again rather fluid; "the frequency with which earthen and chambered long barrows were built over pre - existing occupation sites during the fourth millennium BC suggests that the construction of monuments may have deliberately made reference to acts of earlier settlement..........a link could be established between several states of being: that of the present, a generalized ancestral past, and the specific social biography of a particular place".

Pollard appears to be arguing that monuments were placed as acts of remembrance over occupation sites that had been abandoned, that the people how lived there viewed disused places of occupation as ritual in nature, they had a special significance that needed to be remembered.
This would (it can be argued) mean that the boundaries between the ritual and domestic were to all intensive purposes non existent, that they were one and the same.

His arguments also hint at the people of the Neolithic having a sense of history, which extends our insight into the Neolithic mind.
Pollards paper ends with a final important reminder of the importance of the domestic arena as it "is after all where people would have spent most of their lives" This is in general a very well written and argued paper, it lacks the theoretical 'blurb' that some papers are weighed down by, which adds to it success.

It is clearly and eloquently written and Pollards arguments are supported by strong evidence and prime examples, which clearly shows a well researched paper.
The examples cover a good range of geographical locations which help to provide the reader with insight into his arguments.

The paper in general is highly successful; it provides the reader (and hopefully the wider academic community) with a fresh viewpoint and a new way of looking at settlement in the British Neolithic.
It provides vital insights into the behaviour of the people of that era and will provide researchers with a new perspective to work with.

In my opinion the arguments presented should be further expanded and applied to a wider range of archaeological areas, as I have gained a lot from reading this paper.
Archaeological theory is a very wide ranging and multilayered part of archaeology as whole.

It covers many subjects and methodologies, and is constantly evolving new ways of thinking as well as new approaches to archaeology itself.
The term theory, when applied to archaeology (as well as other social sciences) generally has a greater diversity of meaning than most other natural sciences and most definitely covers more ground.

The range of different types of theoretical approaches to archaeology can often be confusing and difficult to separate, this an aspect of archaeological theory that is very often criticised by the wider academic community.
Each theory has its own unique approach which is almost certainly always influenced by the archaeologists that thought them up in the first place as well as their politics and cultural backgrounds.

The theories cover many aspects of the human past from society to cultural background and even material culture.
The theories themselves (and the archaeologists don't forget) as well as the unique and particular approach they adopt or undertake is very often influenced by current political trends and 'fashion' or even new technologies and associated ways of thinking.

An example of how this can happen is Gender Archaeology which was definitely shaped and influenced by the increasing popularity and rise of feminism in the 1980's.
As feminist issues came to the forefront of public and even academic society, gender archaeology and its unique views and interpretations soon followed and became and increasingly important part of archaeological theory (Fagan 1996: 244).

Gender archaeology at its most basic level looks at the roles of males and females, their activities, their relative positions within past societies as well as assesses the symbolic and cultural meanings attached to them throughout the whole of human history.
It works on the assumption that all historical (as well as modern!) societies require a definite collaboration between men and women (Fagan 1996: 244).

The main aims pursued by gender archaeologists include attempting to correct the male orientated bias that has and still does exist in archaeology; this basically means changing the assumption that all past societies were male dominated and that women were second class citizens (some gender archaeologists also try to change the use of the word 'man' when referring to humankind, a distinctly feminist sounding agenda).
They also criticise existing practices within the world of practical archaeology (another supposedly male dominated area) and attempt to suggest possible new ideas, practices, methodologies and approaches.

Another aspect that they seek to change is the distinctly male dominated academic world and encourage a more open, 'politically correct' and equal approach.
Lastly they study gender throughout history using the archaeological record to provide them with supporting evidence.

They use the full range of material remains to assess the impact gender has had on the structures of society as well as the possible cognitive processes concerning gender and possible gender roles (Fagan 1996: 245).
The role of gender archaeology within the subject as a whole is an arguably extremely useful one, even if one could say that it is influenced by politics.

It causes the archaeological record to be looked at in a new and exciting manner.
Areas of archaeology that had been previously dismissed by the academic community are now being studied adding to the sum of human knowledge.

As a theory it shares a lot with feminist theory (at least as far as its political aspects go) and to a certain degree Marxist archaeology.
It is, however, distinguishable from other theories because it looks at a particular and specialised area of archaeology that has previously been dismissed.

(Bahn 1996: 290).
Marxist archaeology is another obviously politically influenced theory, which at its simplest level is the interpretation of human history using a set of principles laid down by Karl Marx.

The theory is primarily looks at social change and structure throughout history by examining the connections between forces of production, for example, technology and the power it has over the people within a society.
It also looks at social organisation, for example, it would be concerned with feudalism in the middle ages and how it affected society as a whole (Bahn 1996: 292).

Marxist archaeology is heavily influenced by 'classical' Marxist political theories and is concerned with the struggle between socioeconomic classes within particular social groups or societies.
Marxist archaeologists also suggest that a particular society's economic base or foundation can invariably be responsible for its basic ideologies, which in turn effects the amount of direct control over and the lifestyles of the people within that society (Bahn 1996: 292).

Marxist archaeology is clearly directly attributable to Marxist political thinking and the associated theories.
This is something that proponents and supporters of the theory do not deny; they admit to being fully cognisant of the nature of the theory but still think it has immense relevance in modern archaeology.

This particular aspect of Marxist theory is the most commonly criticised by the wider academic community with the vast majority of archaeological theorists (well academics in general) preferring to keep their personal political belief system separate from their professional lives (Bahn 1996: 292)..
One other aspect (or even sub - discipline) of Marxist theory worthy of mention is the Neo - Marxism movement which places emphasis on the ideological aspects of social hierarchies and social structures as opposed to the economic aspects (Trigger 1989: 93) Both of these 'schools' of thought are arguably flawed theories.

They are, it can be argued, (and has been many times) too influenced by out - dated political ideas.
Marxism is (in my opinion) a relic of WW2 politics and cold war thinking and has no real place in the 21 st century, even if we do live in very politically motivated times.

There are as with most theories some elements than can be applied to archaeology and must be noted and studied.
It is essential when studying ancient societies to look at power distribution and the relationships between different classes and elements within those societies.

But this does not mean that Marxist ideologies have to be brought into the equation.
Marxist theory is distinguishable from other theories purely for the fact it is so heavily influenced by real - world politics and serves as an example of how politics and archaeology are not necessarily mutually exclusive.

The first important and noteworthy phase in the history of archaeological theory and thought is commonly known as cultural or culture history.
The aim and practice of culture history was quite straightforward, archaeologists grouped sites into distinct social groups or cultures, they then looked at the geographical locations, the period of time in which they existed as well as the length of time and studied any possible links and interactions that may well have occurred.

Archaeologists who support or supported this theory looked at each culture and determined that each one was governed by a set of rules which in turn went on to govern the behaviour of the people within that particular society.
An example of how this might work is looking for patterns in style and craftsmanship, for instance an arrowhead with fluted barbs should theoretically belong to a different culture than an arrowhead without barbs at all (Trigger 1989: 99) This theory is flawed is some ways, an approach such as this leads to people viewing the past in a possibly incorrect manner.

It is possible that the past would be viewed as nothing more than a collection of different and separate population groupings only classified by their differences (such as material cultural, architecture e.t.c) as well as any influences they may or may not have had on each other.
In reality the evolution of culture and social groups is a lot more complicated than this and archaeological thought has definitely moved on from this rather old fashioned viewpoint.

During the revolutionary 1960's new ideas and viewpoints were being established in almost every sector of the academic world and archaeology was no exception.
A group of primarily American archaeologists, which included Lewis Binford, looked at the well established cultural history theories and decided a new approach was needed.

(Trigger 1989: 99) They proposed a "New Archaeology" which would be more scientific in its approach as well as draw ideas and methods from the closely allied field of anthropology.
Binford and his supporters saw the formation of culture as the result of a set of behavioural processes and traditions.

It was this study of processes that led to this new archaeology being called processual archaeology.
Processulalists (as they came to be called) took the idea of hypothesis testing from science and applied it to archaeology.

They would formulate a hypothesis and try to apply to the particular culture they were trying to study and try to prove its veracity.
Excavations would also be undertaken with the hypothesis in mind in order to test and prove their ideas.

Another aspect of culture historical thinking they tried to change was the emphasis of culture over people, in other words large social groups dominating research instead of individuals.
They also began to prove that modern ethnic groups were not always easily comparable or similar to the cultures within the archaeological record.

(Hodder 1991: 106) Processual archaeology was extremely revolutionary in its time and literally rewrote the book where the study of cultures was concerned.
The analytical (through computer modelling of population exchanges, this shows how technology can effect archaeological though as well) and scientific approach was easily adopted into archaeology and shows how truly interdisciplinary archaeology can be.

It forced many academics to reassess their previous conclusions and as with most theories looked at past societies in a new and exciting way (Hodder 1991: 106).
Archaeological theory as previously mentioned is a greatly varied field that has viewpoints and new approaches to almost every single facet of archaeology.

There are many 'schools' of thought covering many different aspects, some are in direct opposition and compete directly with each other (processual and post - processual theory), whereas others work in tandem (gender and feminist theory) trying to establish new methodologies.
For the layman it can be very difficult and even confusing to see the relevance of all these myriad theories and the place they have in modern day archaeology.

Some theoretical archaeologist tend to cloud their writings in dogma and overly complicated 'jargon' (Alison Wylie for one!), but ultimately they all to a certain extent have a valid point to make.
Some theories have overlapping agendas or ideas which can make it even harder to distinguish them from one another.

Unfortunately (like many other areas in science) a lack of objectivity can be a problem and researchers must be aware of this so that it can be dealt with otherwise any data compiled could be misinterpreted and corrupted.
In order to make use of these theories they have to be looked at separately so that one can be distinguished from another.

When studying a specific site, society or culture one or more theories can possibly be taken into account.
For example if an archaeologist was studying a fifth century Anglo - Saxon graveyard, theories concerning gender, landscape and even Marxism could arguably be considered, even the scientific hypothesis testing of processual archaeology could be useful.

In order to do this however archaeologists must have a clear picture of each theory, its strengths and its weaknesses if they are to successfully complete their research.
Theory is extremely useful to the academic community as whole as it encourages new lines of inquiry and new approaches that previously may have been dismissed or in some cases not even acknowledged (gender archaeology is an example of this).

It can on the other hand be argued that theory can all too easily be susceptible to the trends and fashions of the time (feminist, gender and Marxist archaeology are all to a certain degree 'victims' of this) in which they were invented, be it politically (this can be dangerous or even in some cases misused) or even technologically speaking.
It must be argued that just because it is not necessarily easy to wade through each theory and their potential conflicts it can and must be done so that historical accuracy is kept intact.

Critical Review of Barbara Benders paper Theorising Landscapes and the Prehistoric Landscapes of Stonehenge.
Barbara Bender is an archaeological theorist who takes a distinctly post - processual approach to her work.

She looked at the work of other people before she started her own.
The aim of her work appears to link buildings with their landscapes using spatial analysis, she attempts to look at the overall picture and states that buildings should not be isolated within that landscape and that landscape is more fluid than previously thought.

She criticises other archaeologists saying that they: "Fail to recognise that the way in which people understand and engage with their worlds depends upon the specific time and place and historical conditions ; it depends upon gender, age, class and religion.
At any given moment and place landscapes are multi - vocal" This quote shows that she is attempting to say that the scope of understanding where landscapes are concerned is far too limited and that many more factors must be taken into account even if they are not outwardly obvious.

She uses the work of two landscape "pioneers" W.G Hoskins and Raymond Williams as examples of how there are two views of landscapes in cultural and historical contexts she says that "Hoskins writings can be used to legitimise this sort of disembodied frozen past" meaning that he appears to not understand the fluidity of landscape.
She goes on to say that in Hoskins writings "the past is cut off from the present" Whereas Raymond Williams "provides a politicised theoretical framework that is less open to such abuse and serves to enlarge our understanding of how landscape 'works'".

She then goes to say that his "theorizing of 'structured feeling' moves backwards and forwards (without positing causality) between people's changing social and economic conditions and their experience and understanding of their world".
It is not difficult to see which view of landscapes she finds the more pertinent.

The paper in question uses the world famous prehistoric monument Stonehenge as an example of how building can become static within their landscapes.
She puts forward the idea that the landscape surrounding than monument was ever changing that settlements and field boundaries were fluid in nature.

She uses evidence to back up her argument, for example, the fact that archaeologists have found signs of "flint working, planting and grazing washed up to the very edges of the monuments".
There uses and significance were not always straightforward.

Her theoretical scope is very definitely post - processual with a vaguely Marxist tint, she also puts theory before data and uses examples she knows will support her theory.
She aims to place whole contexts of settlements within their landscapes.

She looks at the effect individuals have on the landscape and how they mould and distort it over time.
The data she uses is prehistoric in origin, using Stonehenge, Avebury and Silbury Hill comparatively.

She does however successfully bridge the gap between theory and data, she does put theory first but uses actual scientific data to support her arguments.
Her style is at times confusing and presents her case in an overly complicated manner.

She does inspire the reader to look at landscapes and buildings/monuments in a new and exciting way.
Her post - processual viewpoint is very obvious and can easily be criticised as it pulls together so many arguments it is not necessarily very concise.

Critical Review of Alison Wylies Putting Shakertown Back Together: Critical Theory in Archaeology
Alison Wylie's paper is based upon critical theory and her own views concerning objectivity and subjectivity.

She looks at the reconstructed settlement of Shakertown in a very critical manner as well as reconstructions in general.
She says that: "museum reconstructions not only misrepresent the past, as was the case with the historical reconstruction discussed by Handsman, but, in doing this, they directly serve the interests of the present; they are a medium for the self - definition and self - legitimation of those create and view them" Her views are very extreme and have a very aggressive tilt to them, she is insistent that Shakertown and other reconstructions are only built on one persons views and are not objective enough.

She believes that reconstructions are very ethnocentric which means that ones values and cultural aspects hold true throughout other time periods and other cultures.
It is also the belief that ones culture is morally superior which is something that all archaeologists must try to avoid entirely.

She thinks that we are in danger of viewing the past with modern viewpoints and all the cultural bias that follow, personal biases and prejudices are also a problem in her view.
Her argument is not very well supported by her data set because she only chose one very limited example which does not appear to bridge the gap between theory and data very well at all.

Her theories concern subjectivity and objectivity and the fact that as an archaeologist you cannot be either but in reality a little bit of both is required.
Her opinion of Shakertown is very low, she thinks it is static and not true to life, she puts this point across very firmly.

Her paper is written in a very complicated and difficult read style and is in my opinion nothing more than a rant at something that in her opinion is very poorly done.
She does not appear to understand that although difficult archaeologists must at least try to be as objective as possible, but this is not always possible.

Reconstructions will never be entirely realistic (without the aid of a time machine anyway!) but must be appreciated for what they are, a way to look at the past visually and give archaeologists (and the public) a chance to feel the past.
She is right when stating that objectivity is a problem that must be addressed and avoided.

The Viking raids of the 9 th and 10 th centuries are amongst the most well known and documented episodes of early medieval Europe.
Before this period Scandinavia was a distant outpost with little cultural, political and economic significance or value for the rest of the world.

The beginning of the Viking age starts traditionally around 793 AD to when the first historical account of a Viking raid was writted describing a raid on an English monastery at Lindisfarne on the east coast.
It is around this time that similar records of raids start appearing all around Europe.

The age of the Vikings had begun.
The popular image of the Vikings handed down through the generations is based on written accounts from the time.

These accounts portray them as brutal, merciless barbarians who were nothing more than heathen pagans that spent their lives killing and stealing from good Christian folk.
In many respects this image was true to a certain degree but it must be taken into account who wrote these depictions.

These 'hirtorians' were for the most part priests (one of the only actively literate groups at the time) and would have been particularly horrified by the pagan Vikings and their practices.
On the other hand however it is important to realize, that the Church itself would have been a relatively defenceless and extremely wealthy target.

Many monasteries were positioned along the coast which would have made attacking easier for the seafaring Vikings; they would have made an extremely tempting target.
Another point to consider is that the Vikings were pagans and did not believe in or fear the Christian's God, something that would usually have gone a long way to protecting a monastery from attack.

However as with any historical account it is inevitable that a certain amount of bias would creep in and the priests would not have had the opportunity to see the situation from all sides.
This is where archaeology steps in, to find physical evidence of all aspects of Viking life.

The Vikings were a seafaring people who relied on their ship building knowledge as well as their practical knowledge of the seas and oceans on which they sailed.
Just the sight of a Viking warship off the coast of many countries was enough to spread terror and panic.

The ships themselves were of exceptional design and came in two distinct classes, the longship and the warship.
The longship ranged from 70 to 140ft in length while the more practical and easier to maneuver warship ranged from 70 to 80ft in length.

Both had extremely shallow drafts which led to the ships having the ability to enter river systems as well as allowing them to come close to shore and even be beached for extended periods.
The shallow drafts also allowed for speed and manoeuvrability which was essential in sea battles, over all ships of Scandinavian design had the advantage.

Generally the ships were rowed by the warriors within and when they were adequately rested they could reach speeds of 8-10 knots.
In the late 1960s the remains of five 11 th century longships and warships were found at the entrance to Roskilde Fjord near Roskilde in Denmark.

The ships were in a remarkable state of preservation and had been purposely sunk with stones placed inside their hulls.
The position of the boats, in the main navigation channel, suggested that they had been placed there to protect the nearby city (the Danish capital at the time) from sea borne assault.

This can lead to two conclusions, that the inhabitants were sufficiently worried about attack that measures needed to be taken and that they had ships to spare (Renwick 1979: 7) The excavation of these ships was an extremely lengthy and expensive process and required the building of a coffer dam which enclosed the area and allowed the water to be pumped out and drained.
This meant that the excavation could proceed as if it was taking place on dry land.

The results of the excavation provided archaeologists with very important data concerning Viking shipbuilding techniques and technology (Renwick 1979: 7).
It is obvious that the Vikings spent a lot of time, energy, experience and money on designing their warships.

This helps to prove that they were a warlike race; the ships themselves match the descriptions given by the previously mentioned priest, which means that their depictions were accurate at least in one respect.
Viking warships were perfectly designed for lightening fast raids and ambushes and were fearsome in aspect.

There is however another side to the story archeological evidence shows that warships weren't the only type of ship commonly used by the Vikings.
Things had begun to change by the 10 th-11 th centuries and European society was beginning to settle down as nations began to form.

The Vikings slowly began to turn from raiding to commerce, and they began to build a new generation of ships more suitable for this purpose and more characteristic of the generality of Viking shipping at that time.
The most characteristic of these trading vessels was the Knarr or Hafskip which was used for long distance trade and features widely in the Icelandic saga as the principal vessel used for exploration and colonization.

It was in these deep water high sides sailing ships that Bjarni Herjolfson and Lief Eriksson used in their voyages across the North Atlantic to the American continent.
References to the Knarr are abound in the Sagas, but there is even less evidence of them in Viking iconography, for much the same reasons that Greek and Roman Roundships are not well represented in classical iconography.

But it was the Knarr and its derivatives which constituted the basis of Viking Sea trading and transport.
These merchant vessels were built with a larger load capacity and for seaworthiness and stability rather than speed.

These ships had a wider deck, shelter for the crew (unlike the warships where the crew had to sleep open to the elements) and a cargo hold in the middle of the ship.
There have been a few archaeological sites where evidence for Viking trading has been found including the Askekarr ship which dates back as far the 8 th century.

This shows that the Vikings were not just interested in warfare and that they did indeed trade, hardly the sign of a society whose only interest was bloodshed there was another side to their culture.
(Greenhill 1976: 215) Due to their sea faring nature the Viking sphere of influence (as opposed to an empire comparable to the Roman and British empires) stretched from their homelands in Scandinavia to Constinople in the east to Greenland and North America in the west.

Through out these realms there is much archaeological and historical evidence of their activities and wherever they went they made cultural and social impacts.
Raiding was all very well for the yielding sporadic bursts of wealth but only trade could yield a regular income.

It can be argued that the achievement of the Scandinavians as traders rather than raiders proved the more enduring and successful.
The Vikings made use of various different methods for trading; they used simple fishing camps or minor trading stations, through to seasonal markets, densely settled towns and trading ports (Renwick 1979: 20) Recent excavations in Viking Age towns and market places have revealed a myriad of evidence for craft industries including debris from the making of everyday and luxury objects such as glass, amber, bone, antler, wood, iron, bronze and precious metals, as well as for subsidiary industries such as ship repairing and agricultural industry.

The blacksmith, woodworker, bead-maker, specialist jeweler, horn-and antler-worker, leather-worker and stone carver all played an important part in Viking society and were the foundation of Viking society and trade.
Many warriors were in fact skilled in other professions and were only part time raiders and warriors seeking to gain a reputation.

(Sawyer 2001; 78) The eastern European island of Gotland with its central Baltic location was prominent as a centre for trade with both the east and west Baltic areas.
Naturally formed shelving beaches provided many landing places suitable for the shallow-draft vessels of the Vikings and allowed the development of good communications and trade routes throughout the Viking world.

Gotland was a trading port deriving its commercial success from the handling of transit trade through the Baltic and the eastern European regions.
Wealthy pagan graves and buried hoards of Viking treasure from all over the island suggest that native Gotlandic farmers had indeed participated in this rich commercial trade.

Several burials were uncovered in the area containing hoards of grave goods, helping to prove the extent of Viking influence and trade as well as the huge profits made.
The Gotland burial hoards contained large amounts of imported Arabic and European silver in the form of coins (about 40,000 Arabic, 38,000 German and 21,000 Anglo-Saxon coins), silver objects and jewelry, sometimes hacked into smaller pieces for ease of bartering and trade or melting down for use elsewhere.

Sets of bronze weighing scales with surprisingly accurate regulated weights, used for weighing out silver and precious metals in commercial transactions, were also found in the Gotland area (Sawyer 2001; 79) The Vikings also had trading centres in Western Europe, one the most important of which was Dublin in Ireland.
The Vikings first established a base at the mouth of the River Liffey in 841.

This was used as trading and raiding base which survived until 902 when the native Irish defeated and exiled the Vikings.
Dublin was at its peak around 1000AD and would have been a very busy trading port with many ships coming and going throughout the year.

There would have been ocean-going traders, coastal traders, and of course the fearsome longships or warships, all of which used the port as a base.
This helps to prove the duality of the Vikings, that they had interests in both trading and raiding.

Many different goods were traded, including wool and other woven materials, hides and furs, weapons, items for everyday use and even slaves (evidence of raids) for transport across the known world.
These items would have been traded for goods that would have usually been unavailable such as ceramics from England, soapstone for Shetland, silks from Baghdad, broken glass pieces from Germany (to make bracelet stones and other decorative items), tin from Cornwall (a centre for tin mining in Europe), silver from the Middle East, and ivory in the form of walrus tusks was imported from the Arctic colonies like Greenland and Iceland.

Evidence for the slave trade in Dublin includes chains and manacles which have been found on various excavated sites and as individual finds.
One find of particular note is a 10meter long iron chain with an attached collar which was found beside an iron spearhead, a bronze pin and a human skull beside a crannog at Ardakillen near Dublin.

The skull has signs of up to twenty axe or sword cuts, which is perhaps a testament to the brutality of the age and the possible mistreatment of slaves.
The Vikings also traded internally especially with their more isolated Northern Atlantic colonies like Iceland and Greenland.

Greenland relied on trade due to its very nature, a sparse and barren land with almost no timber; it can be argued that the inhabitants would have relied almost completely on traders and supplies from other more resource rich settlements.
Below is a description from the Kings Mirror a 13 th century 'handbook' written in the form of a father's advice to his son: "It happens in Greenland...that all that is taken there from other countries is costly there, because the country lies so far from other countries that people rarely travel there.

Every item, with which they might help the country, they must buy from other countries, both iron and all the timber with which they build houses.
People export these goods from there: goatskins, ox-hides, sealskins and the rope...which they cut out of the fish called walrus and which is called skin rope, and their tusks...

The people have been christened, and have both churches and priests..." (King's Mirror, translated by Hellevik 1976) The historical source is direct evidence of the difficulties facing the Greenlander's and proves that trade was essential for the survival of certain colonies.
This is also backed by archaeological evidence in the form of Viking longhouses and evidence of other buildings which were made from timber that couldn't have been native to Greenland.

Overall the Vikings were a much more complex and multitalented society than they were given credit.
An indication of the violent nature of a particular society is the fact that nearly all the graves of males should include weapons.

A well equipped warrior had to have a sword, a wooden shield with an iron boss at its centre, a spear, an axe, and a commonly portrayed in modern pictures, are extremely rare in archaeological material.
Helmets with horns, ubiquitous in present day depictions, have never been found amongst weapons, archaeologists find signs of more peaceful activities; sickles, scythes, and hoes lie along side of weapons.

The blacksmith was buried with his hammer, anvil, tongs, and file.
The coastal farmer has kept his fishing equipment and is often buried in a boat.

In women's graves we often find personal jewelry, kitchen articles and artifacts used in textile production.
This is all tied up in the belief systems that existed at the time, but also provides archaeologists with a window into the past, the goods and artifacts found have a lot to say about the lifestyle and profession of the deceased.

The answer to the question: Were the Vikings raiders or traders?
They were both, above all Viking warriors sought glory and a reputation (known as 'word-fame'), they sought fold and a place in the sagas, this was part of their belief system.

Although as time progressed they became involved in their own myth starting elite groups like the Jomsvikings.
Many Vikings who took part in the raids went on to set themselves up in business or in some cases to fund journeys of exploration.

The Viking conquests left lasting impressions on the areas they visited, their social structure, dialects, place names, personal names and even law codes.
However the Vikings in turn were exposed to different cultures and lifestyles which they took back to their homelands, which started to reshape parts of their society.

Their poetry, art, history and mythology still remain fascinating to this day.
Humanities first known art tradition appeared during the Upper Palaeolithic period.

The vast majority of Palaeolithic art appears in cave systems throughout the world, especially in areas like France and Spain.
Prehistoric are comes in two main forms parietal and mobiliary art.

Parietal art covers paintings and engravings on stationary surfaces, such as cave walls or on large blocks of stone.
A famous and well known example of parietal art are the spectacular cave paintings discovered in 1940 at Lascaux near Montignac in the Dordogne region of France (Lecture 8, 28 th October 2005).

There are many images of animals including Reindeer and Bison (animals commonly found throughout Europe in that period) which date back almost 22,000 years.
The reason these paintings have survived the test of time concerns the environment for artefact preservation.

The paintings and related artefacts are protected from the elements in a cool, dry environment, ideal for preservation (Aiello 1982: 80).
Mobiliary art covers art that is portable in nature, in other words engravings or sculpture on stone, clay, bone or even antler that can be carried easily by an individual.

This can include personal decoration and items such as beautifully carved spear throwers, which while practical, are also decorative.
One common piece of mobiliary art are small stylised figures of human females called Venus figurines, these are regularly found on Palaeolithic sites throughout Europe and the Near East.

The figurines are considered by many archaeologists to be fertility symbols and may have some religious significance (Aiello 1982: 80) The most important and hotly debated question surrounding the enigma of Palaeolithic art is quite simply; why?
What does it represent and what is it for?

Art is a landmark in the human journey and understanding it is essential in understanding our heritage.
Paintings, engravings and sculptures that are unquestionably art suggest that the artists were people who were to all intensive purpose like us, they thought and felt in the same way we do.

An animal would not be able to define a painting as anything other than an incomprehensible mess of random colours and shapes, art and the appreciation of art is unique to man.
Understanding the reasons behind its creation is a quest that many archaeologists have taken up over the years, and many theories have been proposed (Lecture 8, 28 th October 2005).

The first and possibly most obvious theory is 'art for arts sake', which means that Palaeolithic art is nothing more than 'primitive mans' first attempts at being creative (Lecture 8, 28 th October 2005).
Some archaeologists believed that the paintings were nothing more than decoration, making their immediate environment more appealing.

This theory was generally abandoned quite quickly as new theories were introduced (Leakey and Lewin 1977: 140) In recent times however, the theory has been revived by John Halverson an anthropologist from the University of California.
He suggests that the images are the product of a 'primal mind' and that the art is the 'human consciousness in the process of growth'.

He states that the reasons the images are relatively simple, depict no complicated scenes and are naturalistic in appearance are the result of a mind in the 'early stages of mental development', in other words, the paintings were experiments in imagination or creativity (Leakey and Lewin 1992: 318).
Another theory became popular when researchers realised that there was a definite link between more recent Australian Aboriginal art and Palaeolithic art.

Anthropologists realised that the Aboriginal paintings were part of ritual hunting magic.
They argued that both societies were hunter-gatherers and that there were many similarities in the number and composition of species in the paintings.

It is therefore reasonable to suppose that both societies produced rock paintings for the same reasons.
That painting images of certain 'prey' species was either some kind of offering to gods unknown or they could have believed that the paintings gave the hunter some power over the animal's depicted.

It could also have been a way of symbolically increasing the numbers of certain animals to ensure that a hunt would be successful or there is a more simple explanation, it was a record of kills from a particular hunt (Leakey and Lewin 1992: 318).
When considering this theory it must be taken into account the species that appear most often in rock art, their numbers and even the style in which they were painted.

Around 60% of European rock art is dominated by three main species, Horse, bison, oxen (all commonly found in the Palaeolithic landscape).
Animals such as Mammoths, reindeer, ibex, boars and goats make some appearances but only occasionally.

Images depicting plants, carnivores, fish, birds and people are rare, and some only appear on one or two sites.
The animals that do appear are 'prey' animals that would have been very common and part of life for the Palaeolithic hunter.

If taken as whole the images do not represent how the animals appeared in nature, the scope is just not broad enough there appears to be bias towards certain species (Leakey and Lewin 1992: 318) There is one major problem with the hunting/sympathetic magic theory; the fossil record does not in a number of cases support it.
One example is Reindeer; they were an extremely important part of Palaeolithic life, not just for food but for their antlers and hides as well and are not very commonly depicted in Palaeolithic artwork.

There are a few other species where this discrepancy shows up and casts doubt on the whole theory (Leakey and Lewin 1992: 319-20) The next major theory is based on work published and researched by Clive Gamble and Steven Mithen and is called Information theory.
The basis of this theory suggests that the rock paintings were in fact recording devices for passing on environmental and resource information to other hunter-gatherer groups.

These measures were required in order adapt to the extreme ice age environments that they lived in.
The paintings would have provided hunters with information about which particular animal species were edible and in abundance as well as their locality (Lecture 8, 28 th October 2005).

This could explain the absence of certain species of rock art; they just weren't immediately available at that time in that area.
The paintings could also have served a secondary function in regards to this theory; the marking of territory.

For example if a nomadic group enters a cave system they haven't previously encountered and it contains the art from a rival group they are made aware of the occupational history of that cave.
Another theory has in recent years gained popularity, ritual magic and shamanism.

Amongst modern hunter-gatherer groups like the San or Bushmen of Southern Africa (Namibia, Botswana, and South Africa) shamanism and rock art are closely intertwined.
Again, both the San and Upper Palaeolithic artists are hunter-gatherers so it could be argued as with the hunting magic theories that they would follow the same patterns and possibly ritual practices as well.

(Clottes 2000, URL ) One very important factor that must be taken into consideration when looking at Upper Palaeolithic religious practices and mythology is that they would have been completely different to our own.
We just would not recognise certain things as being significant as we have no firm point of reference from which to study religions of such great antiquity (Leakey and Lewin 1992: 318).

The central point of almost all shamanistic religions is the spirit world, shamans try to enter and connect with this world by entering a trance like state.
There are various methods for doing this the most common being the ingestion of naturally occurring hallucinogenic drugs.

The art could in fact be the neuropsychological outpourings of a shaman practicing religious rites.
A shaman would have fulfilled a key role in society; they would have been a mediator communing with the spirit world.

They would have portrayed could have been spirit guides or as previously mentioned been connected with hunting magic.
Some paintings exhibit geometric patterns and shapes which are present during the initial stages of a trance.

Palaeolithic shamans would have been Homo sapiens just like us and therefore had an identical nervous system, this is a great help as the effects of certain rituals can be compared with modern groups.
The caves themselves would have been places of deep cultural and religious significance, places of mystery and fear.

Many societies believe that the supernatural would lays beneath our feet, a realm of spirits and the dead.
When entering a deep cave system Palaeolithic people would have been aware that they were entering a world of spirits.

Some paintings have found in deep inaccessible passages in difficult to reach places, in small tight passageways, high up on ceilings; some archaeologists have noted that these locations are perfectly positioned for sound resonance.
The cave environment was without a doubt a central point for our ancestors.

In the Tuc d'Audoubert cave system in France two clay sculptures of Bison were found in the centre of a circular cave room far from the entrance.
Near the Bison a circle of small human footprints were, could these prints be from children being led to an initiation ceremony?

These questions are still left unanswered and will continue to tantalise archaeologists for many years to come.
(Aiello 1982: 80) The question "what was Palaeolithic rock art for?" is perhaps not as important as what it represents as a whole.

The theories that have been presented in this essay all make valid points and have a ring of truth.
Very few have any solid evidence to back them up; comparing ancient rock art to the work of modern tribes is very helpful in establishing theories but ultimately is not concrete.

Viewing rock art through western eyes inevitably can lead to bias, the mind of 'primitive man' is extremely elusive and whereas we are anatomically identical their mindset and belief system must have been incomprehensible.
Interpreting rock art is difficult, for example; some animals appear pregnant, is this an attempt to symbolically increase their fertility?

This is not a question easily answered.
Of all the proposed theories the shamanistic approach seems most likely, and the evidence seems to support it.

Rock art does represent one important thing; imagination.
The artwork shows that humanity had begun to develop their mental faculties and lay the foundation of the complex society that we live in today.

Humanity had begun to look at the surrounding world in a new and exciting manner using their imaginations to create objects of beauty and plan ahead.
The are could represent the human need to leave a mark after death, something that well be left behind to reach out to future generations.

What ever the reason the very fact it exists is of enormous importance that someone painted a picture that only existed in their minds eye is proof that we are different from every other creature that has ever existed.
An imagined world of their own creation.

Throughout the span of human history many societies and civilisations have risen and fallen, many of these societies have approached the disposal of the dead with a great reverence and ritual.
Generally the funeral rites and practices would take days, weeks, months and in some cases years to plan and put into action.

They were and still are seen as an important part of separating world of the living from the world of the dead.
The material remains of these practices provide archaeologists with invaluable data and evidence to interpret and analyse.

This provides the archaeological community with a unique and exciting challenge, every site, tomb and burial must be approached differently depending on the environmental and physical conditions.
Artefacts and skeletal remains must be treated with due care and consideration and catalogued in a precise and meticulous manner in order to aid scientific method.

If handled properly archaeologists find skeletal remains and their associated artefacts extremely useful for academic study.
The way in which a particular society or ethnic group handle the subject of death and the disposal of the dead can be very telling about their belief systems, religious practices and social structures.

(Stirland 1986: 5) The bodily remains themselves can provide a treasure trove of information about the individual or individuals found.
Detailed study of the bones themselves can provide information about lifestyle, migration patterns (stable isotope analysis) and the physical attributes of that particular person.

(Stirland 1986: 5) Anthropometric data can easily be obtained from skeletal remains, for example the measurement of long bones can help to calculate stature and body mass, and the skull can be used to recreate facial features.
Evidence of trauma is retained and can be studied and compared with statistical data to extrapolate information about the kind of activities undertaken by certain individuals.

Malformation, congenital disorders and joint wear due to hard labour can all be recognised, catalogued and studies with a high degree of accuracy.
(Stirland 1986: 5) Certain diseases leave their mark, such as tuberculosis, leprosy, arthritis and osteoporosis.

Once identified the data is extremely useful in ascertaining a possible cause of death as well as helping archaeologists to track the diseases evolution, infection and mortality rates and movement through ancient populations.
Other diseases are caused by poor diet or a lack of certain minerals, one example being Rickets (children) or Osteomalacia (adults) caused by a lack of vitamin D.

This particular affliction is generally associated with the industrial revolution and the subsequent problems with severe overcrowding and poor diet.
This type of information is very important to archaeologists as they can then learn a lot about the conditions in which people lived and the effect disease and illness had on them.

(Stirland 1986: 36) Another important aspect concerning the body itself is its orientation and the way it is arranged within the grave itself.
The orientation can be very suggestive and help to place the burial into religious and social contexts.

For example Muslim graves are always orientated with the body facing Mecca no matter which part of the world the burial site is in.
This would therefore indicate to archaeologists the importance of Mecca in the lives of members of the Islamic community.

Another example would be Christian burials with the grave orientation being with the head pointing west.
The theory behind this involves the Christian belief in Judgement day, that god will appear in the east and deceased will rise to meet him face first.

These examples go a long way to prove that grave orientation is rarely random and is usually associated with some form of ritual behaviour or religious practice; this therefore gives researchers a line of inquiry to follow in their investigations.
(Pearson 2003: 6) Within the grave itself the arrangement of body can be placed in varied positions, each representing different beliefs.

Bodies have been found in various burial sites in many different positions ranging from lying face up to sitting or even standing.
Many bodies are found lying on their backs with their hands crossed on their chests, this is a very peaceful pose and suggests a certain degree of care and tenderness in the placement of the body.

The bodies arrangement is in the majority of cases one of the first indicators of deliberate burial.
Archaeologists can also make a judgement of the speed and care taken with a burial, if the body is an awkward pose it can prove that the body was buried in haste or even just dumped (which in some cases, such as some European bog bodies, can smaller groups with slightly different practices within a particular area.

It is also reasonable to assume that the deceased had family, friends and had achieved some measure of respect during his/her lifetime.
(Pearson 2003: 6) Archaeologists can learn a lot from the grave of burial pit itself; the size, shape, depth and care taken or not taken whilst digging it can provide the archaeological community with vital information.

It can in some cases reflect the social standing or rank of that particular individual; even gender can have some role to play in shaping the processes of funeral practices.
Generally the more formal the funeral rites are the more effort is put in to the preparation and the grave itself.

A modern example are Christian graves, they are very formal and functional, and their dimensions vary according to coffin size and are mostly around 6ft deep.
The reasons for this depth is twofold, firstly to protect the body from tampering, secondly to prevent the spread of disease.

The graves are clearly marked with comparably little extravagance (usually just a headstone or a small statue or in some cases a small mausoleum) and set out in clear areas of grids.
This is designed to give relatives a place to visit and to stop newer graves intersecting old ones, a common problem in the poorly planned cemeteries of medieval Europe.

(Pearson 2003: 7) Not all burials required the digging of formal grave pits in specially prepared sites.
Some groups, especially in pre-Iron Age societies made use of natural features such as caves, fissures, rocky overhangs and ditches.

(Pearson 2003: 5) This could show that these groups observed funeral practices with very little ritual and were just a way of disposing of a carcass rendered useless in death.
It could be argued that the emphasis was on the spiritual rather than the mortal remains and if true says lot about the people who are represented by these types of burial and their belief systems.

It could also indicate a lack of time and resources as well as a possible nomadic lifestyle with a lack of permanent structures.
The most obvious and recognisable feature of burial archaeological are any above ground or semi - buried burial structures.

Burial structures range from simple headstones to burial mounds to full scale monuments like the great pyramids in Egypt.
When studying these monuments the general rule is the larger the structure the higher the social status and the more powerful the individual of individuals who are interned within.

The larger monuments were hugely symblois; they were statements of power and domination meant in some cases to remind the populace who's in charge.
They were built for a variety of other reasons, including the protection of the deceased and their grave goods and to help them ascend to the afterlife.

Archaeologists can learn a lot from burials of this type, they can study building methods and technology, the positioning of the building itself, the wealth and resources of the particular civilisation that built it.
Some burials were constructed to mimic structures from everyday life like houses or in the case of the change over time, for example, during the European Neolithic some burial mounds and barrows were used communally (although they did appear to contain individuals of higher rank) whereas the emphasis did shift towards wealthy individual burials during the Bronze and Iron ages.

These monuments stand as a testament to human beliefs and ingenuity.
(Hadingham 1976: 50) Grave goods are the items placed within the burial chamber or burial pit; they can be personal in nature.

They can also be gifts from family members, friends and even subjects (in the case of royal burials).
Grave goods can range from simple everyday items to the bodies of slaves and concubines.

The goods themselves can be representative of the deceased's profession, in other words, a warrior may be buried with his swords and other relevant items.
Although this is not always accurate and the items present could be an idealised view of the individuals life and achievements.

In many cultures the presence of grave goods serve the purpose of providing materials necessary for starting a new 'life' in the afterlife.
The first Chinese Emperor Qin Shi Huangdi had a vast terracotta army built in order to protect him in the afterlife, this shows a strong belief in the religious ideals surrounding death, a true belief that death is not an end but another beginning.

It is physical evidence such as this that helps archaeologists to build a picture of a particular culture and its relgious and social belief.
The artefacts themselves can provide vital information about lifestyle, religion, the extent of trade, artistic standards and technology, and social standing rank and power.

The artefacts also reflect the growth through time of material wealth within a civilisation.
As a civilisation grows it begins to develop wealthy powerful classes, an elite, which as they grow more powerful their burials generally grow more extravagant.

This can be used as a method to chart the rise and fall of a civilisation (for example during times of war and disease epidemics mass graves occur more regularly) throughout a certain period of time as well as record changes in belief, attitude towards death and ideology.
This information is vital in understanding a particular burial and being able to compare and contrast with similar burials and place it in a wider context.

(Pearson 2003: 9 - 11) The Hochdorf burial is an Iron Age tumulus located near Stuttgart in Baden - Wurttemberg, Germany was discovered in 1978.
The Hochdorf tumulus was 30m high (100ft) and surrounded by a low stone wall, it was dated to the final quarter of the 6 th century BC.

The interior consisted of alternating layers of planks and stones forming a chamber which contained a single body about 6ft high of a male, aged between 30-40 years old.
The grave goods contained within the chamber were of such magnificent quality that its occupant earned the nickname 'The Hochdorf Prince'.

The burial is clearly of 'Celtic' origin but unlike most important 'Celts' of the time he was not a warrior, the only weapon found was a small iron dagger which was more likely an essential accessory rather than a tool for killing.
The 'prince was laid out in luxury on a Greek style bronze couch covered with expensive furs and finely woven fabrics.

Around his neck was a golden torc of typical 'Celtic'; design, and placed just above his head was a strange conical hat made of birch bark of unknown design and origin.
All around him lay items of immense value, suggesting great wealth, influence and power.

Within the burial chamber itself a massive four wheeled iron decorated was found filled with feasting equipment such as drinking horns and pottery vessels, suggesting that feasting was a big part of this individual's life.
At the foot of the couch stood another object of Greek design and origin, a huge bronze cauldron hold more than 70 gallons or 318 litres of wine (an essential part of 'Celtic' life).

(Bahn 2004: 295-206) This site is incredibly important, it helps archaeologists to understand the early 'Celtic' world and al lot can be learned about their role in central European history.
The 'Celts' did not have a writing tradition so sites like Hochdorf are very useful for filling the gaps in our knowledge.

The archaeological evidence helps to prove that trade between the Greeks and the 'Celts' was taking place and the 'Celts' were in touch with what was at the time the most advanced civilisation in Europe.
Due to the fact that such a high status person was buried in such a manner without being a warrior suggests that there was more to 'Celtic' culture than the filthy 'barbarians' described by the invading Romans.

Dr Erwin Keefer of Stuttgart Museum suggests that the 'The Hochdorf Prince' could have been a cultured diplomat or trader, who traded in Greek goods and possibly Greek ideas as well.
The relationship between the Greeks and the 'Celts' has been previously noted in the Histories by Herodotus a 5 th Century BC writer known as the 'Father of History'.

Men like the 'Hochdorf Prince' would have previously been made rich by the trade in iron using the river Danube as a supply route.
The burial provides physical evidence of this relationship and goes some way to disproving the traditional theory of the blood thirsty head hunting savages that rampaged through Europe murdering indiscriminately.

It also helps to build on the store of knowledge of burial practices at the time.
(Bahn 2004: 205-206) Burials in general provide archaeologists with huge amounts of information about past societies; there is a vast amount of variation between cultures (and even within cultures).

Each group had different approach, from cremation, to mummification and simple burial a whole set of beliefs and rituals accompany each method from the highly extravagant to the downright bizarre and mundane.
1.0 Introduction.

This investigation into spoken discourse will focus on the interactional approach to sociolinguistics in relation to male and female speech.
The aim of spoken discourse analysis is to verbalise, or make explicit, information that would otherwise be taken for granted (Gumpertz, 1982: 22) and 'to show what talking accomplishes in peoples lives and society at large' (Cameron 2001).

Two transcriptions of speech will be analysed in order to take an in-depth look at how people speak, how language use differs in different social environments, and whether speech style alters with groups of males and females.
One transcript to be used consists of four female students, whilst the other is a transcript taken from BBC2's Newsnight programme consisting of four male speakers.

The report will start with a literature review of previous relevant interactional sociolinguistic studies in order to provide a basis for analysis.
From the literature review a research question and hypotheses will be formed.

Under the methodology the sociolinguistic approach will also be reviewed, with reasoning for its use within the context of this investigation and the particular features that will be analysed.
Reasoning for not employing the other approaches will also be given.

The methodology will also include the source of the data used, a brief discussion of the subjects involved and comments on the variables, such as those mentioned previously.
The Interactional sociolinguistic approach will then be applied to the transcripts, results presented and consequently discussed.

The conclusion will assess how far the results go to support the literature and the limitations will be discussed.
2.0 Literature Review.

Maltz and Borker (1982, Litosseliti, 2006: 37) suggested that linguistic differences between genders are similar to those that exist across cultures, with each gender being socialised differently.
This seems to be supported through the evidence that between the ages of five and fifteen children mainly socialise in same sex groups (Crawford, 1995: 88) and therefore learn different rules of friendly interaction (Graddol and Swann, 1989: 90).

These separate social spheres lead to different approaches to communication, due to the genders functioning in different ways dependant on situation with boys opting for a hierarchical structure within a large group, whilst girls prefer smaller groups or pairs with the focus being on closeness (Tannen, 1990: 43).
Gender differences are clear from an early age, with Tannen noting them in children as young as 3;0 (Tannen, 1990: 44) and seem to explain miscommunication through socialisation differences.

Socialisation is also responsible for the ways in which men and women are treated differently from an early age and the consequent affects of this in later life (Crawford, 1995: 14).
Extra linguistic features such as tag questions, minimal responses, interruptions and hedgers demonstrate the differences between male and female speech and consequently seem to go some way to explain the different conversational styles adopted by men and women.

Women are said to take a more collaborative approach to language (Edelsky 1981 as in Romaine 1999: 159) supporting each other by providing encouragement to their conversational partners (Holmes, 2001: 297) whereas men are more competitive (Aries 1976 as in Coates 2004: 126/7) for the floor often being found to disagree with and challenge the previous speaker (Holmes, 2001: 299).
The content and purpose of male and female speech also differs with men using language to convey and obtain information, whilst women use language to establish, nurture and develop relationships (Holmes, 1995: 2)- these differing styles have been coined as report talk and rapport talk (Tannen, 1991 as in Talbot 1998: 98).

Men's competitive style has been said to be responsible for males dominating public speech, however Holmes (1995: 2) reports that women often find it difficult to get men to talk in their own homes.
In mixed gendered conversation men have been found to be responsible for 96% of the total interruptions (Zimmerman and West as in Crawford 1995: 41).

However, interpretations of interruptions may differ between men and women due to women's collaborative style seeming to identify these violations as a form of solidarity instead of competition for the floor (Romaine 1999: 158).
Tag questions have been said to be a female feature (Lakoff 1975 as in Holmes, 2001: 286) expressing uncertainty and mitigating threat, thus being tentative and weak (Coates and Cameron 1988: 76), however, not all tags have been found to have the same function.

Tags have been divided into two categorise by Homles (as in Talbot 1998: 41): affective and referential, whilst referential tags indicate uncertainty, affective tags have been found to indicate solidarity (facilitating affective) and mitigate threat (softening affective).
Although women have been found to use twice as many tags than men (Fishman 1978, Litosseliti, 2006: 33), the function of these tags has been suggested not to indicate uncertainty with Holmes (1984, 2001: 290) finding men use tag questions to express uncertainty 61% of the time, whilst women only do so 35% of the time.

Similarly women have been assigned the stereotype of asking lots of questions (Brouwer et al 1979 as in Coates 2004: 93), however it has been suggested that these too are used to maintain conversation (Maltz and Borker :213 whilst men's use of questions supports Tannen's (1991,Talbot 1998: 98) report talk.
It has been found by Dubois and Crouch (1975, as in Lakoff : 139) that men used more questions than women at a conference with Bashiruddinn et al (1990), Swaker (1979) and Holmes (1988, 1995) finding similar results in academic or presentation situations (Coates 2004: 94) seeming to support men competitive speech style and dominance in the public arena.

Similarly, hedgers have been suggested to be unassertive with women being found to use twice as many as men (Fishman 1978 as in Maltz and Borker 1982: 33), yet Holmes found the use of hedgers with a falling tone to show certainty (1984 as in Coates 2004: 88).
The addition of hedgers, such as like, act as a form of politeness that detaches the speaker from what they are saying, therefore avoiding negative evaluations of others or positive evaluations about themselves, although Deuchar (1988 as in Holmes, 1995: 2) suggests that those who are powerless must be polite hence women using more features of polite speech.

Politeness is, however, divided into positive and negative forms, with positive politeness indicating solidarity and shared attitudes, whilst negative politeness avoids intrusion and demonstrates respect (Holmes, 2001: 268).
Unsurprisingly, women have been found to use more features of positive politeness (Holmes, 1995: 6).

Minimal responses have also been found to be used differently (Cameron, 2001: 117) with women using them to indicate attention whilst men se them as a sign of agreement (Mesthrie 1999: 233).
Hence the tendencies for women to feel men are not paying attention and men's confusion when women use minimal responses without agreeing (Coates and Cameron 1988 70).

Research question.
After considering all the literature a research question can then be formed as follows: If the emergence of a dual culture between the genders, as suggested by Maltz and Borker (1982, as in Coates and Cameron 1988 124) can be founded and men have been found to favour public speech (Tannen, 1990: 76/7) does this suggest that men use more extra linguistic features in public speech, in the same way that women have been found to use more extra linguistic features in private or rapport talk?

From this question the following hypotheses have been formed: Men use a greater or equal number of hedging devices in public conversation as women do in private conversations.
Men use a greater or equal amount of tag questions in public conversations as women do in private conversations.

Men use a greater or equal number of minimal responses in public as women do in private conversations.
Men interrupt equally or less in public conversations as women do in private conversations.

4.0 Methodology
4.1 Approaches.

The four approaches studied have all been found to have their strengths and weaknesses, with certain approaches being favoured for different variants of speech.
It was felt that, for the purposes of this investigation, the pragmatic approach, although effective to an extent due to its shared belief in miscommunication and emphasis on polite forms of speech, identified as female traits; would not be a successful approach to use due to its focus on implicit meaning being generated through what is said.

The nature of the male transcript, it was felt, would be rife with "minimally explicit" (Holmes, 2001:263) which may be lost in translation due to the political content of the transcript and the analysis only being carried out on a selection of the interview.
Similarly, conversational analysis was not favoured due to it not being compatible with simultaneous talk, a common feature of female speech.

Conversational analysis assumes all overlaps and interruptions as a violation which seems to contradict suggestions of female's collaborative style and therefore it was felt that the female transcript may be compromised.
Ethnography of speaking was also thought to be a limiting approach due to the fact that gender is not considered enough for it to be the most beneficial to this particular investigation, focusing on the reasons behind the differences.

The interactional approach to sociolinguistics was favoured as it was felt it would give a comprehensive overview of the transcripts.
4.2 Interactional Sociolinguistic Approach.

Although speech seems to function as a means of exchanging referential information, there are more implicit functions that aid our understanding of others (Holmes, 2001: 259).
When listening to someone speak we are establishing who they are, their purpose and possibly even identifying features that we misunderstand.

Interactional Sociolinguistics is an approach that considers the different speech features that are used during naturally occurring conversations (Tannen 1984 as in Schiffrin 1994: 106), considering whether different subcultures, such as gender, class or ethnicity; use different speech styles and, if so, what effect do these speech style have on others (Gumpertz, 1982: 1)?
Do these different styles cause miscommunication?

The interactional approach to sociolinguistics highlights the significance of small differences in spoken discourse (Cameron, 2001: 106) with Maltz and Borker's (1984) suggesting that males and females belong to different subcultures leading to the decision to look at an all male and an all female transcript.
The approach will allow me to investigate whether certain characteristics, noted in the previous literature review, explicitly characterise a speaker as one gender or another, or whether formality and public speech acts as a variable.

Tannen (1990) felt men were more comfortable in public talk, and women more comfortable in private talk due to each genders dominance in each arena (Tannen, 1990: 76/7).
The approach looks at, both, non-verbal and verbal features.

Non-verbal features include prosody (stress, rhythm, intonation etc), paralanguage (voice quality, hesitation, speed etc), body language and vocalisations such as laughing, crying or coughing (Cameron, 2001: 109).
Verbal features that are studied within this approach include hedges, minimal responses, tag questions, commands and directives, interruptions, overlaps and simultaneous speech, swearing or taboo language and so on.

However, the data includes no visual aids and therefore aspects of non-verbal features will be impossible to investigate e.g. body language.
For this reason verbal features will be the focus of the analysis.

The key features that will be analysed will be interruptions and overlaps, minimal responses, hedgers, use of questions and tag questions, due to their significance in gender research; however other features may be commented on briefly.
4.3 Data

The first transcript is an all female sample of informal speech recorded and transcribed by the students who feature in the transcript.
The participants are all university students in their early twenties and were all participating in the recording as part of an assignment.

The second transcript is an all male sample from a political interview.
The four participants adopt the roles of interviewer, interviewee and two members of the public who act as challengers to the interviewee's responses, via their recalling personal experiences.

The transcript source is BBC 2's Newsnight programme.
The reason for analysing a public male transcript and a private female transcript is to look at extra linguistic features in the settings which both genders are said to be most dominant in (Tannen, 1990: 76/7).

5.0 Results.
5.1: Interruptions in male conversation

5.2: Interruptions in female conversation
5.4: Questions (including tag questions)

5.5: Tags
5.6: Minimal responses:

5.7: Hedging results:
Results for hedgers considered the context as "like" is commonly used as he demonstration of speech

6.0: Findings
Research question and hypotheses Men use a greater or equal number of hedging devices in public conversation as women do in private conversations.

Men were found to use fewer hedging devices than women in these transcripts, with women using hedging devices 8 times and men using hedging devices 6 times.
Men use a greater or equal amount of tag questions in public conversations as women do in private conversations.

Men were found not to use tag questions in this transcript, whereas women were found to use them at 3 intervals.
However, the tag questions used by women appear to be affective.

Men use a greater or equal number of minimal responses in public as women do in private conversations.
The men in this transcript were found to use no minimal responses, whilst women used them 9 times.

Men interrupt equally or less in public conversations as women do in private conversations.
The transcript does not show any evidence of men violating turn-taking conventions, whilst women are found to interrupt15 times, however 11/ 15 of these constitute as overlaps as the original speaker did not yield their turn the majority of the time.

7.0 Summary.
The aim to answer the research question using the interactional approach to discourse analysis has proven male speech to use fewer interruptions, minimal responses, hedgers, questions and tag questions than female speakers.

The verbal features under analysis were noted and illustrated quantitatively, as shown previously, to show how often the features were present within each speech situation.
8.0 Discussion.

Analysis shows that women use all of the extra linguistic features identified as significant to this investigation more than men involved.
Women were found to use more interruptions than men in their conversations, with men demonstrating no violations of turn-taking conventions.

However, of the fifteen violations made by women 11/15 were classified as overlaps therefore supporting women's collaborative speech style (Edelsky 1981 as in Romaine 1999: 159) On one occasion in the male transcript and just let me try to explain was used before a statement, suggesting the speaker anticipated they would be challenged and possibly interrupted; this seems to support the notion that men try to hold the floor.
The occurrence of more female violations contradicts the findings that men interrupt more than women, however many of these finding come from mixed gendered research- a sample that was not analysed for the purpose of this investigation.

Women were found to use more questions and tag questions than men, which adheres to the stereotype (Brouwer et al 1979 as in Coates 2004: 93) despite the interview context of the male transcript.
In an interview the interviewer controls the topics and evaluates the responses, whilst the interviewee offers information dependant on what they feel is expected of them (Coupland, 2000: 9).

Men's use of fewer questions, despite this could be due to men's use of more directives and aggravated phrases (West1998, Coates 2004: 115) e.g. FORMULA Women have been found to fail in their attempts to interact by rephrasing what they would normally phrase as a question as a statement (Fishman 1978: 404) however this could be due to women being found to use less direct speech than men (Lakoff 1975,Tannen in Gumpertz 1982: 225).
The interviewee demonstrates negative politeness in response to the interviewer's aggravated phrase in example 1: e.g. FORMULA Here the use of I'm sorry seems to be being used as a mark of respect and to mitigate any offence that may be caused by saying that the interviewer is speaking nonsense, supporting the idea that negative politeness avoids imposing on others (Holmes 1995: 5) Minimal responses were also shown to be used by women in these transcripts with the male transcript producing no evidence of this feature.

The distinction made between men and women's perceptions of minimal responses may be the reason for this with men perceiving minimal responses to indicate agreement, and in this context of debating the effectiveness of the health service, both sides are arguing very different points and therefore may provide reason for their absence, whilst the female transcript has nine instances of minimal responses supporting the solidarity associated with female speech.
Hedgers were used by both men and women in the transcripts analysed, however, once again women were found to use more than men, thus supporting the literature.

The hedgers used in the male transcript were all used by the interviewee suggesting that he did not wish to commit himself to certain statements which could not necessarily be proven or followed through at a later date e.g. FORMULA The use of a hedger here avoids any future backlash as it avoids asserting that this is a fact known to the interviewee.
Despite women being found to use more hedging devices, as the research suggested, the classification of hedgers did prove difficult in the female transcript due to the use of like acting as a demonstration of a speech or action e.g. FORMULA Characteristics such as these have been labelled "mall talk" with the suggestion that the speaker appears unassertive (Cameron, 2001: 112).

Therefore, men have not been found to demonstrate female speech characteristics across the public domain in the transcript analysed as the research question suggested.
9.0 Limitations.

The male transcript used, although public, was also very formal and errors on the interviewee's part would have created negative publicity.
This suggests that an element of planning was more than likely present, as opposed to the female transcript which demonstrated natural interaction.

The male transcript did not follow the same written conventions as the female transcript.
This meant that interruptions had to be assumed by the end of prose.

Some features may have been demonstrated non-verbally e.g. nodding, therefore visual aids would have been beneficial Therefore, if the investigation were to be repeated the same transcript conventions would be used throughout to ensure accuracy.
1.0 Introduction

The aim of this investigation is to assess the language of a child determined as medically premature in comparison to popular child language data, of children born at term, by the likes of Fletcher (1985).
The idea for this investigation emerged whilst researching for a previous assignment into child language, which will be discussed further in the literature review.

The majority of research into premature infants has focused on a sample where prematurity is not the only variable and the assessments have followed an assessment battery approach with the aim to gain a representative sample without extensive qualitative analysis.
Briscoe et al (1998) used the Bus story (Renfrew, 1991) to assess narrative skills, Luoma et al (1998) used a variety of subsections from the CELF-3 (Semel, Wiig, and Secord, 1995), whilst picture naming tests, such as the Boston Naming Test (Kaplan, Goodglass, & Weintraub, 1983) have also been used to assess the receptive and expressive skills of a child born prematurely (as in Luoma et al, 1998: 47, 54; and Jansson- Verkasalo et al 2004 :1).

Popular data and clinical tests used by therapists and clinicians illuminate the development of children's language, as well as possible differences.
However the setting in which the tests are carried out may contribute negatively to the final results and therefore a naturalistic setting will be used in the current investigation.

This will look to avoid the affect of unfamiliar environmental factors.
The tests typically used to assess a child's language in a clinical setting also act as a means to assess individual aspects of language, such as expressive and receptive vocabulary, phonological working memory and ability to comprehend instructions and information.

These areas are, of course, critical to look at in order to track a child's progress; however data may not be truthful in representation.
The individuals character and how they react to unfamiliar people and situations may affect their response; with the child possibly being anxious to answer questions or ask for any necessary clarification.

Due to these reasons an analysis of naturally occurring speech using SALT (Miller and Chapman, 1985) will be used for the purpose of this investigation, with recordings being taken in the child's home environment.
SALT stands for Systematic Analysis of Language Transcripts, and assesses language via a computer programme.

The programme is designed to analyse transcripts which need to be manually coded to identify utterance types.
SALT will allow a full representation of the child's every day language to be recorded and analysed with attention being paid to length of utterances, use of bound morphemes and frequently used verbs.

Errors that may occur in the child's language, such as errors in the use of irregular verbs and plurals, can also be noted in SALT.
These errors can often be classified as incorrect by clinical tests without identifying that the child may have a comprehensive understanding of the regular rules of language and be over-generalising these rules were irregular forms should be.

Marking such responses as incorrect could consequently lead to the child being seen to have diminished linguistic ability, which does not necessarily represent their true abilities or correspond to normal development where such errors are common.
The study will commence with a review of literature into premature infants language, cognition and social abilities whilst also looking at the demographics and environment that the infants studied were exposed to daily.

These factors will be taken into consideration with the subject of the current study with significant findings being commented on in the concluding discussion.
The literature review will be followed by the methodology in which the procedure will be reviewed and a description of the subject given.

Details of the subject will relate to their gestational age and other factors which the literature suggests may be significant in premature infants- such as medical history and genetic influences.
From the methodology and subsequent reasoning the analysis and results of the investigation will be presented.

The results will be compared to Fletcher's (1985) data on Sophie.
The consequent discussion will then review the previous literature to assess whether the findings from previous studies into premature infants language abilities do, indeed, support the findings of the SALT analysis or whether the analysis will prove to be a relatively typical representation of a child aged 2; 6 speech.

2.0 Literature Review
2.1 Origins of the idea.

Trask (1995: 167) observed that human infants are the only mammals born unable to communicate or move from A to B independently, emphasising that research has suggested that humans should not be born until after eighteen months gestation.
This seems to be supported anatomically due to the skull not being totally formed and therefore unable to protect the brains neurons at the time of birth, yet it is thought that a longer gestational period would make giving birth impossible due to the weight and size of the human head and brain.

This notion led to the thought that if the brain is thought not to be totally developed at nine months gestational age then what would the implications of premature birth be on both the brain, in the ways that information transmitted; and on the premature child, in infancy and in later life, in relation to language ability?
2.2 What classifies prematurity?

The term "premature" refers to any child born at or below thirty-seven weeks gestational age (Stenzel, 2004); however prematurity has sub-categories depending on gestational age and birth-weight of the child.
Anything between thirty-four and twenty-six weeks gestational age is classed as "severe to moderate prematurity" (Stenzel, 2004) due to the possible health implications that can come with being born at this time.

Birth weight categories include low birth weight (LBW, <2500g), very low birth weight (VLBW, < 1500g) and extremely low birth weight (ELBW<1000g) (Aylward, 2005: 427).
Although medical science has progressed in recent years, resulting in increased premature babies survival rates; this has also led to an increase in central nervous system (CNS) injury and therefore neurological development deficiencies (Aylward, 2005: 428), which Magill- Evans et al (2002: 42) emphasise often go unidentified and therefore have no developmental follow-up, or possible intervention, to assess that the child is developing normally

2.3 Health implications vs. prematurity.
As mentioned previously premature birth can often have health implications.

These include respiratory distress syndrome (RDS) and intraventricular haemorrhaging (IVH) (Menyuk, 1995: 3) with premature infants also being at increased risk of infection and conditions such as Cerebral Palsy, hearing and visual impairments (Briscoe et al, 1998: 656) with increased risk directly correlating with low birth weight (Aylward, 2005: 433).
These possible health implications, along with findings from the past thirty years of research into this area, have led to the theory that possible deficits in language amongst premature individuals may be due issues affecting an individual's health, such as cerebral palsy and visual and hearing impairments, as mentioned previously.

These possible health implications may, in turn, affect social and cognitive skills suggesting that these delays are the cause of medical complications rather than their gestational age directly (Menyuk, 1995:1).
This suggestion implies that premature children with no health complications may have no significant language problems, whilst full term children who have been diagnosed with the disorders previously mentioned may experience language deficits.

Therefore, it appears important to distinguish whether premature birth or the possible health implications that can, but don't necessarily, come with it is the more significant feature in these language deficits.
However, Magill- Evans et al (2002) found many premature children did experience difficulties in learning without any major health problems being prevalent.

This suggests that premature children may have a predisposition to academic and social deficits, despite these difficulties often not being noticed and addressed until they are at school age (Magill- Evans et al, 2002: 52); and suggests that gestational age is more important than health complications.
Aylward (2005) reports that premature children demonstrate more problems than their full-term peers with neurological functioning, visuo-motor integration, IQ, academic achievement, language abilities, executive function and behavioural issues such as attention- deficit hyperactivity disorder (ADHD) (Aylward, 2005: 428.) with these so called "high prevalence, low severity dysfunctions" being said to occur in between 50% and 70% of VLBW infants (Aylward, 2005: 428).

Problems associated with executive function, commonly described as executive dysfunction, affect a variety of impairments including conceptual reasoning, verbal working memory, spatial awareness and planning abilities and have been found extensively in ELBW samples in comparison to full-term peers (Anderson and Doyle, 2004 :434).
However it is vital to obtain a complete medical, genetic and social history when attempting to isolate the cause of a language delay (Dyer, 2003) as Yliherva, Olsen and Maki-Torkko (2001 as in Aylward, 2005: 434) found a family history of speech and language delay in 11% of their premature sample.

2.4 Social influences.
Factors of social interaction that have been suggested to play a significant role in the development of premature children's language primarily focus on dyadic interaction between mother and child, that are thought to lay the foundations for later language learning and turn-taking norms (Barratt et al, 1992: 1194).

Barratt, Roach and Leavitt (1992: 1195) assessed interaction between mothers and their four month old children, twenty-four of whom were full term and twenty-four preterm.
The preterm participants had a mean gestational age of 34 weeks, mean weight of 2099g and were all relatively healthy, whilst the mean gestational age of the full-term sample was 40 weeks and mean weight was 3493g.

The experimenters controlled for possible variables by using an all Caucasian sample from two-parent homes and by matching the participants according to their socio-economic status via the Hollingshead Social Status index (1978 as in Barratt et al, 1992: 1195).
The study looked at mothers and infants vocal, affective and attentional responses (Barratt et al, 1992: 1193).

Previous research by Bowlby (1969 as in Barratt et al 1992: 1195) suggests that maternal responsiveness to her child's gaze, vocalisations, smiles and cries is critical to a child's development as these all act as attempts to obtain attention from the caregiver.
Research has found that infants born at 23-27 weeks exhibit fewer babblings and more frequent stuttering in their language (Luoma et al, 1998) with similar findings being identified in premature infants at 0;3- 0;6.

Premature infants have been found to demonstrate more 'fussing' (Crawford 1982, Crnic, Ragozin, Greenberg, Robinson and Basham 1983 as in Barratt et al, 1992: 1194).
The maternal response to the infants 'fussing' seemed to depend on whether the infant was born prematurely or full-term.

Both pre-term infants and their mothers were found to have heightened vocalisations with premature mothers responding to their child's fussing with a vocalisation more frequently than mothers of full-term infants (Barratt et al, 1992: 1201).
This has been suggested to be a compensatory strategy (Barratt et al, 1992: 1202) as mothers of premature infants may be particularly sensitive to their child's need possibly due to a parental awareness that their child may not have been prepared for the "onslaught of stimulus that hits them when they are born", due to their early arrival (Magill- Evans et al, 2005 as found on McMaster, 2006).

2.5 Socio Economic Status and premature birth.
Barratt et al (1992) used pairs matched by Socio Economic Status (SES) in the previous investigation into dyadic interaction.

In many studies SES has been assessed in connection with premature birth as well as general studies into child language acquisition.
Socio-economic status is reported to be significant to premature infants due to poorer women being said to give birth preterm more frequently than those of higher social classes, due to nutrition and health care received (Menyuk, 1995: 4).

However, these issues may not be as prevalent in the United Kingdom due to health care being accessible to all citizens, unlike in America where insurance or payment for health care is necessary.
Kilbride et al (2004 as in Aylward, 2005: 433) suggest that a high- economic status counteracts the affect of prematurity on cognitive scores as premature infants classed as coming from a high socio-economic status family scored a mean of twelve points higher than premature infants identified as coming from a low socio-economic status family on cognitive abilities.

2.6 Premature birth and Cognition
Menyuk's 1991 study into cognitive and linguistic development in full-term and premature infants from birth to 3:0 found significant differences between the results of the full-term and premature participants (Menyuk, 1995:163).

The study used clinical tests such as the Bayley Scales, a test which broadly assesses motor and mental development (Bayley 1969, as in Menyuk, 1995: 17) and the McCarthy Scales (McCarthy, 1970 as in Menyuk, 1995: 18) which also assesses motor and mental skills but are more suitable for subjects aged 2;0 plus.
These tests were used to establish each group's average capability to recognise contrasting phonemes, produce vocabulary, babble and cry.

The findings showed that both groups were performing above the norm on all measures; however, the full term participants were achieving production of ten and fifty word an average of a month and a half ahead of the premature participants.
The full term participants were not ahead of the premature participants on receptive language tasks though, despite both groups being found to be equally capable of comprehending 10 words (Menyuk, 1995: 175).

Of the four participants whose results caused concern in Menyuk's study, one was full-term whilst the other three were premature, however, two of the latter were identified as having either particularly low birth weight or cerebral palsy (Menyuk, 1995: 164).
2.7 Impairments amongst premature and full-term infants.

These findings suggest that premature children can perform within the norms of child language data, whilst highlighting the possibility that severe neurological deficits and low birth weight can alter the results of investigations.
The significance of neurological deficits within language tasks has been demonstrated by Luoma et al (1998) in a study that looked at premature and full term infants.

It was discovered that after removing nine of the premature children who had severe neurological disabilities from their study; full-term children were seen to demonstrate specific language impairment more frequently than their premature peers.
Premature infants did, however, demonstrate deficits in certain areas such as naming skills and had problems understanding relative concepts (Luoma et al, 1998).

This suggests that although those with severe neurological impairment who were removed from the investigation belonged to the premature group, the majority of premature infants will not necessarily perform any worse than a full term group.
Greenberg, Carmichael-Olsen and Crnic (1992 as in Briscoe 1998: 655) observed that premature infants do score within the normal range on cognitive tests but these scores still appear to be lower than their full-term peers (Field et al 1983 as in Briscoe, 1998: 655).

2.8 Premature infants and neurological development and mapping.
Research into the area has put forward different findings as to whether premature infants are in fact more deficient in their language than their full-term counterparts; however, the neurology of premature infants in comparison to full term infants is often ignored in the composition of these studies.

It has been suggested that "normal" premature infants need a year to catch up with their peers (Dyer, 2003) which may explain diminished ability in some areas as aspects of language learning are thought to be fixed by age three (Dyer, 2003).
However, Peterson et al (2003 as in Aylward, 2005: 431) reported that "altered semantic language processing" was found in eight-year old premature children when an MRI scan was carried out.

It has been suggested that the neural pathways in these premature infants were different to those that would be expected, however the reasons for this are unknown (Aylward, 2005: 431).
The investigation did suggest these 'structural' differences to be the reason for poor comprehension abilities.

However, if the learning deficiency had been identified earlier and those affected received additional help, language functions may not have been compromised in the long term with those affected developing typically but via different pathways.
This is a theory that is emphasised by Jansson- Verkasolo (2004: 179) who believe that identification of deficiencies at an early age and consequent intervention may change academic abilities in those who experience difficulties in language learning, particularly those born prematurely.

2.9 Intervention and schooling
Aylward (2005: 427) states that premature children will generally have more problems with IQ and academic achievement.

It is reported that over 50% of VLBW children need special assistance in school, with at least one fifth of these requiring individual learning disability placements.
This increases in ELBW children, with 60-70% needing special assistance in school (Aylward, 2005: 428).

By middle school premature ELBW children are thought to be three to five times more likely to have reading, writing, spelling or mathematic problems than their peers (Aylward, 2005: 433) with 16-20% repeating at least one grade (Aylward, 2005: 428).
Aylward reports that maths and broad reading skills are most severely disrupted, finding that maths was more delayed than reading in a LBW sample (Aylward, 2005: 433).

It has been suggested that by adolescence premature children's requirements for special classroom assistance and resources is eight to ten times that of what a full term child would require.
This is a significant difference and therefore Aylward's estimation that 32% of VLBW and ELBW children attend mainstream school and are performing a below their peers by as much as one academic year is concerning (Aylward, 2005: 433).

2.10 Foetal brain development in the late stages of gestation.
Neurological development can to some extent explain why premature children often have lower IQ's and more behavioural problems than their peers, as demonstrated previously through Peterson et al's findings (as discussed in 2.7).

However few cognitive impairments have been specifically linked to prematurity.
The brain increases in volume 2.7 times from 29 weeks gestation to 40 weeks gestation, with grey matter, responsible for IQ and cognition increasing to four times its size and white matter, responsible for attention, increasing to five times its size (Aylward, 2005: 429, 431).

By being born prematurely an infant is being deprived of these vital developments, with premature subjects being found to have less grey matter, and in brain structures, for example smaller basal ganglia and corpus callosum mass; than full term controls on an MRI (Aylward, 2005: 430).
Being deprived of these developments in the womb is suggested to have serious implications on an individuals functioning, with the absence of grey and white matter affecting parts of the brain responsible for language, comprehension, attention and muscular and general anatomical control (Aylward, 2005: 431).

Although the brain does continues to develop after birth, because the infant was born early their neurological development would be behind an infant born at term at the same time.
This full term counterpart, who will one day become the premature infants peer, will also continue to develop and therefore the premature infant will always be expected to develop slightly slower.

2.11 Non-verbal suggestions
The notion of a low attention span acts as a non-verbal feature of language.

Due to the highly complex nature of language, verbal communication is generally accompanied by non-verbal communication (Swets and Zeitlinger, 1976: 325) in the form of body language.
Therefore, it is implied that non-verbal features, also, may be a cause for investigation in premature infants due to their differing neurological patterning.

As mentioned previously premature infants can demonstrate behavioural difficulties such as ADHD, however Magill-Evans et al (2002) state that non-verbal learning disabilities are not identified until higher grades, suggesting that 10;0 may even be too young for a parent to recognise that their child suffers from them (:52).
2.12 Limitations of the literature.

In 1991, Aram et al (Menyuk, 1995:4) reviewed previous studies into premature birth and it's affects on language and cognition and found that all fifteen studies reviewed had a different area of focus, different age group and/ or different methods.
Magill-Evans et al reports in Cognitive and Language Development of Healthy Preterm Infants at age 10 (2002) that Schothorst and Van Engeland (1996) found LBW premature infants to have vulnerability to social and academic functioning where heavier premature infants had none (Magill- Evans, 2002: 43).

As much of the research into this area is relatively new, and because each study has a different focus no real conclusions seem to be universal.
Individual difference and details regarding social, genetic and biological variables all contribute to the research that presently exists (Menyuk, 1995: 166) and these factors may, or may not, be significant in determining a premature child's capacity for academia (Taylor et al, 2002, as in Aylward, 2005: 434); with Aylward (2005: 434) emphasising that skills necessary for social development must not be overlooked.

The majority of the research suggests that premature birth will result in the subjects' language being vulnerable, however, the majority of the research also focuses on LBW samples, some of whom have or have had medical problems.
2.13 Gaps in the literature

Schothurst and Van Engeland (1996 as in Magill- Evans, 2002: 43) report that heavier, and therefore possibly healthy, premature infants do not experience the same difficulties as their LBW peers.
This approach uses premature birth as the only manipulated variable, and therefore the results can go some way to suggesting whether premature birth is significant; however, there is little supporting evidence within the literature for this claim.

3.0 Methodology:
3.1 Case Study

The decision to conduct this investigation as an individual case study was made due to the literature into premature birth seeming to focus on larger samples and using standardised tests as a means of assessment.
It was felt that by using a case study the data collected could be looked at in greater detail in terms of relative strengths and weaknesses of the subject.

The data was collected via recordings which were then transcribed.
Although this method does allow for more detailed analysis than standardised tests, the method can be more time consuming, and therefore larger samples are difficult to study.

This limitation means that data collected, although representative of the subject, can not be generalised to the population at large (Wray et al, 1998: 190) and therefore can not form conclusions, although suggestions are possible.
3.2 Observational Method

The investigation aimed to collect representative samples of the child's speech and therefore the case study was conducted through the use of an observational method.
The observational method aims to record naturalistic data through the use of the subjects caregiver as an elicitor (Bennet- Kastor, 1988: 27), with the presence of an investigator being a variable thought to compromise the data.

Due to the possible influences the investigator may have it has been said that by a caregiver taking on dual roles and acting as the investigator as well as the elicitor, the observations are less naturalistic and more experimental (Bennet-Kastor, 1988: 27).
Although the method is thought to be representative of the subjects abilities its limitations must also be taken into account.

It is important to take into consideration that the data collected in such investigations is only a sample and therefore may not reflect the subjects full expressive abilities (Bennet-Kastor, 1988: 36) as the contexts, dates and times of data collection are subject to external variables; this largely means that the child's production is left to chance (Bennet-Kastor, 1988: 35).
The child was recorded within her natural home environment, without investigator interference, whilst going about a typical day's activity.

The data collected was thought to be representative of the child's abilities due to the role of the caregiver as the elicitor.
The setting used for the recordings was that of the family home.

This was thought to be advantageous as it is a familiar environment and therefore would be expected to act as a controlled variable.
The factors taken into consideration all aimed to avoid the observer's paradox, whereby "we want to observe how people talk when they are not being observed" (Labov, 1971: 461, as in Bennet- Kastor, 1988: 70).

However, the extent to which observer's paradox was avoided is impossible to say.
3.3 Equipment.

The data was recorded on a small recordable MP3 player.
The MP3 player was chosen over a tape or CD recorder due to its size.

It was thought that by using a smaller device, and one which may already be familiar to the child, observer's paradox could be avoided more successfully.
The device is comparable in size to a small toy car and was placed near the child when recording and was controlled by the parents.

The possibility of the child wearing the MP3 player around their neck was considered but was thought to draw unnecessary attention to the task in hand.
3.4 Subject

The subject of the investigation is a girl aged 2;6, who shall be referred to as L throughout the investigation.
L was born at 36 weeks gestational age and can therefore fall into the category defined as premature in the literature previously presented.

The subject had no health complications at birth relating to her prematurity or otherwise and has had no significant illnesses to date.
L weighed 2812g at birth, thus eliminating low birth weight as a variable.

The subject lives with her parents and younger brother, aged 1;3, in the south of Birmingham and attends a local nursery, with her younger brother, five days a week.
L's father is White British and a student nurse, whilst her mother is Jamaican and works in a care home for the elderly.

The family have friends from both ethnic communities whom L is regularly in contact with, however most family contact is through her father's family.
3.5 Procedure

The child's parents were given the recordable MP3 player and asked to record L on various occasions, such as play times, bed times and meal times.
The parents were asked to note the dates of the recording and the context in which they occurred, this was requested taking into consideration that data is reliant on sampling period and situational variables (Bloom 1974: 87 as in Bennet- Kastor, 1988: 36).

It was emphasised that the data must be as naturalistic as possible, with L interacting as usual with various family members and friends.
The parents were asked to record every two or three days for a period of approximately four weeks.

Once the data was collected it was then listened to and transcribed.
The transcriptions were reviewed and samples of one- hundred utterances analysed.

The samples analysed were often part of a larger transcript.
It was felt that a warm up period was not necessary for the purposes of this investigation due to the familiarity of the assessment environment and the primary elicitors being care givers.

The one-hundred utterances from the transcripts were coded and analysed using Systematic Analysis of Language Transcripts (SALT), a computer programme devised by Miller and Chapman (1985).
The SALT programme was used as it provides a comprehensive analysis of the subject's utterance types through the use of mean length of utterance (MLU), word roots and codes, type-token ratio (TTR) and Brown's stages (Bennet-Kastor, 1988: 91).

Only the child's utterances were transcribed and analysed.
SALT requires a coding procedure to mark types of utterance (phrasal, clausal, minor, lexical or unintelligible), function of utterances (question, statement or exclamation), bound morphemes and main verbs.

The transcripts analysed were all one-hundred utterances in length so that comparisons could be made with Fletcher's data on Sophie (1985).
The data on Sophie comprises of TTR, VTTR and MLU scores; number of bound morphemes used and Brown's stages, as well as information on utterance types.

Fletcher used 100 utterance and 250 word transcripts to analyse Sophie's language between the ages of 2;4 and 3;11 and this investigation aimed to replicate Fletcher's analysis using samples of 100 utterances and 250 words from L's language samples so that comparisons could be made.
As L is 2;6 Fletcher's data of Sophie at 2;4 and 3;0 will have to be considered, however it is important to emphasise that this data will be used only as a point of reference and not strictly as a comparison due to Sophie's language being found to be delayed in some areas.

Brown's stages will act as the significant marker throughout the investigation due to the age comparisons that are presented; as the majority of the literature into premature children's language uses standardised tests and therefore chronological age comparisons.
Each transcript will be analysed individually, however the averages calculated will act as the main data.

All results are taken from the clear and intelligible (C&I) utterance analysis set.
3.6 Hypothesis

Infants who have prematurity as their only significant variable, will not necessarily experience language deficits.
Therefore, I hypothesise that the subject of this investigation will be achieving an appropriate Brown's stage (stage 2), as illustrated by SALT.

4.0 Results
L's parents collected twelve samples of L's speech over the four week period provided, at various times of day and within different contexts as summarised in table1.

These recordings ranged from approximately seven minutes to forty minutes.
When attempting to transcribe the recordings it became clear that some of the samples were unsuitable due to there being too few utterances to replicate Fletcher's method, as discussed previously; or due to difficulty in deciphering the recordings due to L's range of pitch and interference from background noise within the home environment.

Due to these limitations only seven transcripts were used in the investigation, as shown in table 2.
The transcription process proved difficult due to difficulty in interpreting what classifies an utterance.

It was felt that each of L's turns should be classed as a new utterance, with pauses also acting as a break in L's turn and therefore being transcribed as separate utterances.
Although this proved a difficult thing to identify, consistency within these categorisations is the determining factor.

Due to the investigation using Fletcher's data (1985) as a reference point, the analysis of L looked at the same features as had been used in the analysis of Sophie.
The main focus of the results will be on MLU, TTR, VTTR, Brown's stages and bound morpheme usage.

Types of utterances used in reference to the SALT transcript summaries and utterance codes will also be discussed.
The results summary will list L's average scores, along with those of Sophie at 2;4 and 3;0 and will illustrate where L was ahead or behind Sophie at either, or both, of these ages.

As L is 2;6 the expectation would be that she was achieving Brown's late stage 1 or early stage 2.
5.0 Presentation of data.

The data presented will follow the format presented in table 1.
The results will be supported with evidence from the SALT analysis and L's relative strengths and weaknesses in reference to Sophie at 2;4 and 3;0 will be presented.

5.1Transcript summary
Each of the seven transcripts of 100 utterances was analysed, in order to replicate Fletcher's data.

From the individual transcript summaries an average was calculated and it was found that 81% of L's utterances were clear and intelligible (C&I).
This percentage is lower than that reported by Fletcher of Sophie at both 2;4 and 3;0.

However, had L's language samples been transcribed by one of her caregivers, her language may have been reported as more intelligible as exposure to a child's language could aid comprehension of their utterances.
Of the C&I utterances L was using statements most frequently, as with Sophie; as well as demonstrating ability to command and ask questions.

L's average number of commands was equal to that of Sophie at 2;4, however the focus of the transcript summary is L's ability to use questions.
L was found to use an average of 6 question utterances per 100 utterances, which appears to be relatively few in comparison to Sophie, who, at 2;4 where she was recorded as using 4 questions per 100 utterances and at 3;0 used 25 questions per 100 utterances.

SALT analysis requires utterances marked as questions to adopt a wh- question format and therefore the analysis of L follows this procedure; however, evidence of L asking questions was also evident.
In order to assess L's question formation the Language Assessment, Remediation and Screening procedure (LARSP, Crystal, Fletcher, and Garman, 1989) was used.

LARSP allows a more comprehensive view of where L is in relation to her age and abilities by assessing her language at word, clause or phrase level (Fletcher, 1985: 48) from the fifty-first to one-hundredth utterance.
Of the questions used within this range of each transcript L's ability ranged from stage 2 (1;6- 2;0) to stage 4 (2;6 -3;0).

FORMULA Throughout the transcripts L was found to use stage 3 (2;0-2;6) questions a total of ten times, and stage 4 (2;6-3;0) questions nine times demonstrating that she is using questions suggested to be appropriate for her chronological age.
L is relatively accurate in her question formation with only one error appearing throughout the data, FORMULA In this example L is substituting what with who, therefore making the question formation incorrect.

This particular question structure belongs to stage 4 of LARSP and therefore does not suggest any cause for concern, as chronologically L should only just be embarking on this stage and therefore errors would be expected.
5.2 Mean Length of Utterance (MLU).

Mean Length of Utterance (MLU) is based on a one-hundred utterance sample of clear and intelligible (C&I) utterances (Bloom, 1994: 45).
Each transcript was analysed individually and then a mean MLU was calculated (for each transcripts MLU refer to contents page).

L's MLU was 2.76.
This MLU score appears to be consistent with the scores achieved by Sophie at both 2;4 and 3;0 (see table 3).

Although L's MLU appears to be consistent with Sophie's results it is important to remember that Sophie was found to be delayed in certain areas and therefore using her as they only comparison may result in flaws.
For this reason Brown's stages are employed.

Brown's stage (1973) attempt to categorise certain features of language into stages, relating to the chronological age, at which they commonly appear (Bennet- Kastor, 1988: 85).
Brown admitted that divisions were 'arbitrary rather than true stages'; however through MLU scores Brown devised stages that correspond to both age and features acquired.

As L is 2;6 she would be expected to achieve Brown's late stage 1 or early stage 2, however L's mean MLU score converts into Brown's stage 3, expected at 3;0 to 3;6.
At stage 3 acquisition of irregular past tense, possessive - s and uncontractible copula's are said to emerge; the following are examples from L's utterances:

Irregular past tense say( FORMULA Possessive - s FORMULA Uncontractible copula FORMULA
L is demonstrating progression in her language acquisition through the development of stage 3 skills, and the good command of stage 2 features that are presented throughout the transcripts.

Errors and omissions are present within the transcripts, however Fletcher emphasises that between the ages of 2;0 and 4;0 imperfect production and comprehension is to be expected as the child only has a 'partial linguistic and knowledge system' (Fletcher, 1985: 3).
5.3 Utterance codes

As mentioned previously a mean of 81% of L's utterances were C&I.
Those utterances that were thought to be intelligible were coded, according to SALT conventions, as lexical, minor, phrasal or clausal utterances.

Lexical utterances, where the utterance is a single word utterance, appeared 13 times on average in L's transcripts- a score similar to that of Sophie at 2;4.
Minor utterances, including greetings and vocatives such as 'Daddy' when seeking attention, occurred 15 times on average- equal to that of Sophie at 3;0.

However, the main focus of utterance codes is L's use of clauses and phrases.
5.3.1 Phrases:

L averaged 14 phrasal utterances within a one-hundred utterance transcript, which echo Sophie's use of 14 at 2;4 and 13 at 3;0.
However, the types of phrasal utterances used by L are the most significant feature to be analysed.

Once again, as with question analysis, LARSP was employed to analyse phrase structure.
Of the phrase structures listed within the LARSP chart L appears to be performing most consistently within the stage 2 bracket (1;6-2;0) with very little progression after stage 3 (2;0- 2;6).

DN's (determiner + noun) were the most frequent used, appearing thirty times throughout the transcripts.
Examples of such phrases are: FORMULA These examples illustrate a variety of determiners in use which seems to suggest that L's vocabulary is relatively good.

L's use of both definite and indefinite articles at various points throughout the transcripts seem to suggest that her vocabulary is developing at a normal rate due to Brown's stages suggesting that these features do not start to appear until stage 4 (3;4-3;10).
5.3.2 Clauses:

L averaged 39 clausal utterances within a one-hundred utterance transcript, which is marginally less than Sophie at 2;4 where she was found to be using 40.
As with phrasal utterances, the types of utterances used are the most significant thing to be analysed and therefore the same procedure as was applied to phrasal utterances was applied to clausal utterances.

L's use of clauses were most consistently within the stage 3 bracket (2;0-2;6) with L beginning to use some structures typical of stage 4 (2;6-3;0).
Most frequently L used the Subject Verb Object (SVO) structure, which would be expected due to this being the canonical structure in English.

Subject Verb Complement (SVC) structures were also relatively common with utterances such as, You're fatty (TRANSCRIPT 6)and You're cheeky (TRANSCRIPT 5) demonstrating these structures.
Many of L's utterances that were classified as clauses omitted function words such as auxiliary verbs regularly, with transcript 2 being a particularly good example of this.

As L and her father as discussing the ethnicity of family members L often omits the auxiliary is resulting in Subject Complement (SC) clause structure: FORMULA Although this is a regular error, it is not a consistent one with L using the appropriate function words at various points throughout transcript 2 and the other transcripts analysed: FORMULA
5.3.3 Telegraphic Speech:

Theses omissions of function words are said to create telegraphic speech.
Similarly, omission of subject also results in telegraphic speech (Bloom, 1994: 7)and L is also seen to do this on a number of occasions with many of her clauses adopting the Verb Object (VO) or Verb Object Adjunct (VOA).

Although the subject is omitted, the intended meaning is often clear with utterances such as:
VO FORMULA VOA FORMULA VO FORMULA

, being understood due to the context in which the utterance was said and the child's use of directed speech towards her conversational partner.
5.4 Type- token Ration (TTR)

Templin (1957) devised TTR to look at lexical diversity of a subject's vocabulary by assessing 250 word samples of speech.
The total number of words within the sample (tokens) and the number of different words within a sample (types) are collected and then the types are divided by the tokens: FORMULA L's mean TTR was 0.31, slightly lower than Sophie at 2;4 and 3;0 where she achieved 0.36.

Both L's and Sophie's TTR's would be said to be low as Templin suggested that a TTR lower than 0.45 was cause for concern.
However, the research which TTR is based on was conducted on children aged 3;0- 8;0 (Fletcher, 1985: 47) and therefore the same expectations may not be anticipated.

L uses a lot of repetition throughout the transcripts sometimes repeating the same utterance four or five times, as demonstrated at the end of transcript 1.
This repetition will act as a major contributor to L's low TTR score, however due to the investigation being a case study and the data only being a product of a limited time span it may not act as a true representation of L's language due to given contexts and tasks often dictating the vocabulary used.

Again this can be demonstrated through the data collected, with transcript 2 seeming to be the clearest example of a topic dictating language choice.
The transcript show's L repeating the words black and white throughout, however this is due to her father attempting to teach her about the different ethnicities within their family so by her father asking the question What colour is x? L has little option but to reply either black or white.

5.5 Bound morphemes.
Analysis of bound morphemes uses 250 word samples.

The samples were analysed and the results of the analysis set (C&I utterances) will be used.
L was found to be using a mean of four bound morphemes per 250 word sample, which is slightly fewer than Sophie at 2;4 where she was found to be using five bound morphemes.

However, types of bound morphemes used are more significant than how many bound morphemes are used.
Table 3 shows the bound morphemes that Sophie was using at 2;4 and 3;0 but due to L's bound morphemes being calculated as a mean, examples of her bound morphemes were not previously given.

Table 4 Shows bound morpheme types and the distribution amongst L's transcripts: L demonstrates a range of bound morphemes across her transcripts, with a greater bound morpheme diversity than Sophie at 2;4 and 3;0.
L's use of different types of bound morphemes is relatively sophisticated for her age, as by 2;6 the expectation is use of - ing and plural - s only.

Therefore,despite not being able to compare L's bound morphemes to the data on Sophie; it appears that L is acquiring bound morphemes successfully.
5.6 Verb Type- token Ratio (VTTR)

VTTR looks at lexical diversity of verbs within a child's language.
As with TTR a score of below 0.45 is considered a cause for concern, but as mentioned previously these expectations were based on a slightly older sample than L, which may have some effect on the significance of the results.

VTTR is assessed using a 250 word sample, and once each transcript was analysed a mean score was calculated.
L is achieving a mean VTTR of 0.35 which, as with TTR, is suggested to be cause for concern.

As mentioned previously, repetition influences type-token scores massively and this has been a major characteristic of L's speech throughout the transcripts analysed.
In transcript 1 L uses the verb love repeatedly, and similarly she uses make repeatedly in transcript 3.

However, the most influential factor in this VTTR assessment appears to be L's use of is as a main verb, as it is used a total of twenty-one times in transcript 2 alone.
L is, however, found to be using a relatively large range of verbs and seems aware of many implicit syntactic rules which accompany many of these, such as verb valency.

Verb valency looks at the number of arguments a single main verb takes with the final clause structure depending on the verb choice.
L is demonstrating use of monovalent, divalent and trivalent verbs with the verb taking one, two or three arguments respectively.

Monovalent verbs do not take a direct object, with an example within L's utterances being Lie down there, I want to (transcript 6).
L's repeated use of love acts as a divalent verb through it taking two arguments- who is doing the loving and who is being loved (I - love - you), whilst the trivalent variation is demonstrated through the utterance, Daddy you have to put me to bed.

This demonstrates the subject, verb and two arguments in the form of the object and adjunct.
Use of such features shows that L has acquired implicit syntactic rules, however due to her age errors would still be expected.

6.0 Discussion.
6.1 Summary of L's relative strengths and weaknesses in reference to Sophie at 2;4 and 3;0.

6.2 Similarities and differences between L and Sophie.
6.2.1 Similarities.

L's TTR is suggested to be cause for concern by Templin's criteria (Fletcher, 1985: 47).
Sophie's scores on this measures were also shown to be delayed, again suggesting cause for concern.

The average TTR is suggested to be 0.50, with anything below, or indeed above, 0.45 being said to show atypical lexical diversity (Fletcher, 1985: 47) L's score is significantly below this marker, which does indicate concern; however it is important to remember that TTR averages were formulated on an older sample (3;0-8;0) and therefore may not be as representative of a younger child's language (Fletcher, 1985: 47).
Bennett- Kastor (1988: 87) stresses that TTR is sensitive to sample size, and therefore suggests that factors such as repetition which effectively decrease the sample size may be partly responsible.

However, due to repetition being expected within any language sample, it appears that L's vocabulary may be developing at a slower rate than Sophie's due to the sample sizes assessed for both L and Sophie being equal.
This is only a suggestion and would need to be assessed in more detail, and using larger samples, to form any significant conclusions.

A similar situation arises when assessing L's VTTR, both in comparison to Sophie's achievements and with Templin's suggestions.
Again L is performing significantly below Templin's(1957) suggested 0.45 average, achieving 0.35.

Again this is suggested to be a cause for concern and once again the same limitations of Templin's criteria being based on older children are faced.
L is found to repeat some verbs regularly within a transcript, as referred to in section 5.6, which again limits the sample size.

In comparison to Sophie's achievements, L is developing steadily, bridging the gap between Sophie at 2;4 and 3;0 with her score.
The contexts in which the utterances were produced will also be an influential factor with different conversations possibly dictating specific language, relevant to the task being carried out, to the speakers.

As table 2 demonstrates, the majority of the contexts were related directly to play or interacting with family members which, although beneficial for the purposes of the observational method, may not have represented L's language abilities fully.
Fletcher reports that Sophie uses SVO clause structures most frequently at both 2;4 and 3;0 (Fletcher, 1985: 69, 99).

L was also found to use SVO structures most frequently, (refer to LARSP charts within appendix) with the majority of her utterances falling into the stage 3 category.
Similarly, this is the category occupied most by Sophie at 2;4 and 3;0, with a gradual progression towards more stage 4 clauses being shown at 3;0.

Stage 3 is estimated to be the equivalent of age 2;0-2;6 and therefore it's dominance in L's and Sophie at 2;4's language is to be expected.
It is apparent though that as Sophie is showing signs of more stage 4 clauses at 3;0 (2;6-3;0), L is also beginning to do so at 2;6.

This may suggest that L's syntactic development is in fact ahead of Sophie's, however as previously emphasised this is only a suggestion and further investigation would be necessary to form any significant conclusions.
6.2.2 Differences

Although Sophie's MLU at 2;4 and 3;0 are within the suggested age appropriate categories of Brown's stages, L is found to be achieving above the expectations for her age.
As shown in table 5, Sophie is reaching Brown's stage 2 (2;4-3;0) at 2;4 and stage 3 (3;0-3;6) at 3;0, therefore Sophie is producing language typical of these early stages.

On the contrary, L is achieving stage 3 (3;0- 3;6) on average and therefore is seen to be commanding language typical of a child six months above her chronological age.
The number of bound morphemes used by L was marginally below the number used by Sophie at 2;4 and 3;0.

However, due to L's bound morpheme's being calculated as a mean score the types of bound morphemes used had to be analysed individually and the results have been shown in table 4.
L is seen to use two or more types of bound morphemes in the majority of the two-hundred-and-fifty word analysis sets, whilst Sophie at 2;4 is only shown to use possessive /s.

This may suggests that L is using a wider variety of bound morpheme types than Sophie at 2;4.
This suggestion is impossible to support without further investigation, due to the number of bound morphemes used varying in each of L's transcripts.

This demonstrates that the results are largely dependant on the sample that is being analysed.Sophie was also found to have more intelligible language than L at both 2;4 and 3;0, with L's average intelligibility being 81%.
Although there is no strict guideline for intelligibility, the expectation would be that L was achieving intelligibility similar to Sophie at 2;4, or slightly above.

In reality, L is achieving 6% below Sophie at 2;4.
However, it is possible that L's lower intelligibility is not actually a reflection of her abilities but a reflection of the limitations of the investigation and methodology- this will be discussed further in section 7.2.

Hypothesis
The original hypothesis predicted: Infants who have prematurity as their only significant variable, will not necessarily experience language deficits.

Therefore, I hypothesise that the subject of this investigation will be achieving an appropriate Brown's stage (stage 2), as illustrated by SALT.
This hypothesis was proved to be true, with L achieving Brown's stage 3.

L did show delay in some areas, such as TTR and VTTR, however conclusions can not be formed from these delays without further investigation and consideration of the measures criteria.
Does the literature support the findings?

Menyuk states that researchers over the past thirty years have found premature birth to contribute negatively to language development (1995:2).
This statement does not support the findings of this investigation, with L's language appearing to be developing at the normal rate, producing the expected two and three word sentences (Wilkinson, 1971: 59), despite her prematurity.

This investigation used a child born at 36 weeks gestational age, so that prematurity was the only significant variable.
This isolates the findings in the literature somewhat as much of the previous research, as reviewed in chapter 2, has used a LBW sample.

This acts as another variable to consider and for this reason the subject of this study was also a healthy weight when born.
The issue of birth weight is an important one when investigating child development of any kind, as smaller children, premature or full term, are at "increased risk of biological risk factors" (Menyuk, 1995: 3), especially those born below thirty weeks gestational age (Briscoe et al, 1998: 654).

These biological risks include respiratory problems and infections, with premature children also being said to be at risk of hearing and visual impairments and cerebral palsy (Briscoe et al, 1998: 654).
Luoma et al (1998) refers to these biological risks as "associated hazards" and says that they are thought to result in those affected being more vulnerable to specific language impairment.

It has been reported that premature children often have lower performance on many expressive and receptive language tasks than their full term peers, with this low performance sometimes being indicative of further language problems (Magill- Evans, 2002: 54).
Menyuk (1995: 164) reports premature children to be within the average range on expressive language tasks, however despite this not being a cause for concern they were not progressing as quickly as their full term peers.

This investigation used the data on Sophie (Fletcher, 1985) as the control, and showed that, the majority of the time, L was demonstrating language equal with that of Sophie.
Although L was found to be behind on some features of the assessment, such as TTR, she was also found to be ahead of Sophie on some features, most significantly MLU and the consequent Brown's stages.

Aylward (2005: 434) reports that in children born below 32 weeks gestational age, particularly males, experience deficits in MLU, whilst vocabulary is also found to be lower than in full term children (Luoma et al, 1998).
Although the previously reported results were based on studies where the subjects were born four weeks gestational age prior to the subject of this study, this result seems to support the original hypothesis.

Due to L's MLU appearing to be unaffected by her premature birth it seems that L has avoided the developmental difficulties detailed in the literature.
L does appear to be developing as normal, acquiring function words, personal pronouns and bound morphemes as would be expected.

Although slightly deficit in some areas, L's results do not appear to be cause for concern.
This suggests that premature birth alone is not the cause of different and/ or deficit language development in premature infants; with biological factors associated with LBW seeming to be the contributing factor significant to such delays, as well as possible cognitive effects of premature birth.

It is vital to remember that stimulation within a linguistic community leads to language acquisition (Kess, 1976: 53) and in order to do this effectively the learner needs to have "the prerequisite cognitive development" necessary (Swets and Zeitlinger, 1976: 324) for language acquisition.
Therefore, this investigation suggests that providing an infant is healthy and does not have a LBW or cognitive difficulties, language development should not be affected providing language stimulation is frequent.

7.0 Conclusion
7.1 Relevance of this investigation.

By identifying a gap in the literature on premature birth and language development, this study aimed to look at the language of a healthy child, aged 2;6, who was born at 36 weeks gestational age.
This investigation did not aim to make any firm conclusions due to it being a case study, but rather aimed to go some way to identify whether premature birth alone can have the significant effects that have been proposed in the literature in order to suggest future research.

The suggestion that premature birth alone is not the cause of language deficits in premature children could be expanded upon and studied on a larger scale in order to establish whether the data presented in this study is, indeed, representative of the majority of healthy, premature infants.
7.2 Limitations and suggestions for future research.

This investigation did not use a control of the same age.
Although the data on Sophie was thought to be satisfactory for the purpose of this case study, in hindsight an age matched control should have been used.

Future research should consider using matched pairs of premature and full term infants in relation to SES, age and family history of language delay.
L's utterances were often difficult to decipher, resulting in many utterances being marked as unintelligible.

This may not necessarily have been the case.
Noises within the natural home environment may have contributed to these difficulties, as well as L's use of varying pitch often causing utterances to become distorted.

These factors add to the naturalistic environment which the investigation chose to adopt, however it may have been beneficial to use more sophisticated recording equipment which would not influence the subject or cause observer's paradox.
Coding system used within SALT is often problematic to use, with Brown (1973 as in Bennett- Kastor, 1988: 78) stating that he was "never confident that he knew what the full coding should be" The use of the Sophie data proved difficult to compare to that of L at times.

This was mainly due to L's data being based on a series of mean scores, so features such as TTR, VTTR and bound morphemes which had to be assessed in each individual transcript, could not be compared adequately as examples could not be given.
There is no way of telling if the data is representative of L's full language production abilities due to it being a sample, and there is no way of assessing comprehension directly through SALT (Bennett- Kastor, 1988: 36).

Comprehension assessments may be a useful tool for future research in order to assess whether receptive language is affected in a healthy, premature sample.
The form of a case study does not allow any conclusions to be drawn from the investigation.

In order to establish whether the finding presented are true of the majority of healthy, premature infants a larger sample of premature infants is required.
Introduction.

Aphasia has been of interest to the psychological, linguistic and neurological world since the late 1800's when the disorder was first identified.
The acquired language disorder literally translates as the total loss of language function, despite patients not necessarily losing all of their language abilities (Carlson et al, 2004: 388)., and has been found to be the result of damage to the brain and nervous system (Obler, 1999: 1).

Libben states that "the study of aphasia is the most important tool in the investigation of language and the brain" (1996: 424) with many types of aphasia being identified in recent years spanning all modalities of language.
Aphasia is split into two main groups, the fluent and the non-fluent syndromes with fluency being categorised as the ability to express five or more words uninterrupted (Goodglass, 2001: 67).

Non-fluent and fluent syndromes demonstrate very different symptoms and are thought to be the result of different injuries.
This assignment aims to focus on two types of aphasia that have been influential in the area of neurolinguistics within the modalities of speech and comprehension- Broca's aphasia, a non-fluent syndrome: and Wernicke's aphasia, a type of fluent aphasia.

These two syndromes will be discussed and other factors that have aided knowledge of brain localisation considered.
Features of Broca's Aphasia.

It has been thought that by observing those with aphasic syndromes language functions can be identified within the brain (Libben 1996: 425).
This theory supports the approach taken by Broca who observed that of the two cerebral hemispheres of the brain that one, the left in 97% of cases (Obler, 1999: 28), is dominant in language functions, with specific parts of the dominant hemisphere being linked closely to particular language skills (Obler, 1999: 9).

The notion that specific areas of the left hemisphere play particular roles in linguistic ability was supported by a patient, referred to in the literature as "Tan", whom Broca observed to have near perfect comprehension, yet was unable to speak.
After the patient's death, Broca performed an autopsy which showed a lesion to the lower area of the left frontal lobe leading Broca to identify this region as responsible for speech production (Libben 1996: 421) and therefore the aphasia experienced was labelled Broca's aphasia.

Typically Broca's aphasic's have lesions in the lower areas of their frontal lobe (Broca's area), said to be responsible for planning, speech and prediction, which consequently influences their fluency, prosody and production and retrieval of words, especially function words (Obler, 1999: 39/41).
The speech of Broca's aphasics is described as "slow, deliberate and effortful" (Obler, 1999: 39) with "limited vocabulary, restricted grammar and awkward articulation" (Goodglass, 2001: 61).

Patients have been found to use one or two word utterances, often simplifying more sophisticated speech sounds, with attempts to generate full sentences failing due to lack of syntactic support (Goodglass, 2001 : 62), however the words used are meaningful (Carlson et al, 2004: 389) Broca's aphasics often suffer from agrammatism, meaning that they lose the ability to express themselves in complex ways, and to understand others who choose to express themselves in this way due to disruption of grammatical features such as word order (Carlson et al 2004: 390).
The lack of complex syntactic comprehension can be demonstrated through the use of passive sentences where the patient finds difficulty in understanding reversible components, for example: FORMULA In this example both the mother and daughter are capable of carrying out the action of hugging, and are therefore reversible; yet the underlying grammar of the agent, in this case the mother, and the theme, the daughter, are lost due to the syntactic deficits within the patients' language.

However, should a sufferer of agrammatism be given the following example: FORMULA The patient would have an increased chance of understanding the sentence correctly as Broca's aphasic's who suffer from agrammatism retain lexical semantics and therefore would recognise that the apple was not realistically capable of eating the boy, and therefore the boy must be the agent (Libben 1996: 427).
Comprehension was not originally thought to be a characteristic of Broca's aphasia, as "Tan" demonstrated relatively good comprehension skills (Caplan 1987: 44) and non-fluent aphasics being found to be able to match heard words to the correct picture 62% of the time (Schwartz et al 1980 in Carlson et al 2004: 390).

However the difficulties with complex structures could be related to the suggestion that Broca's, and other non-fluent, aphasics struggle with comprehension of unfamiliar, less frequent and longer word retrieval, which seems to be supported by Boller and Dennis (1979 in Carlson et al 2004: 390) who found difficulties in sequencing tasks amongst non-fluent aphasics.
The presence of syntax errors in Broca's aphasia does, however, suggest that Broca's area is not only responsible for language production (Libben 1996:427) as does the identification of patients with lesions within Broca's area who have been found to have no speech or language problems (Caplan 1987: 49) These cases suggest that language may not be contained in the way that Broca believed it to be, or possibly support the notion of brain plasticity that Broca suggested was capable in adults, should the correct rehabilitation and encouragement be given, causing the right hemisphere to adopt the language functions of the damaged hemisphere (Caplan 1987: 47); however plasticity lessens as we grow into adulthood with the brain becoming more stable and the ability to learn language being lost (Obler, 1999: 4).

Those suffering from Broca's aphasia have also been suggested to lose control of non- speech oral movements in some cases (Goodglass, 2001: 62) due to the areas adjacency to the motor strip that controls facial muscle movement (Libben 1996: 426); however damage to Broca's area has been found only to produce mild weakening of facial muscles contra-laterally on a temporary basis, thus it seeming reasonable to revert back to the suggestion that Broca's area has a specific responsibility for language functions (Libben 1996: 426).
Features of Wernicke's Aphasia.

In contrast to Broca's aphasia, Wernicke's aphasia is a fluent aphasia, and the most common of the fluent aphasias (Goodglass, 2001: 69).
Fluent aphasia retains normal pace and intonation (Obler, 1999: 41) with language appearing fluent at first glance, yet on closer inspection it is found to have no meaning (Obler, 1999: 9).

The lesions related to this type of aphasic syndrome are typically in the posterior temporal lobe (Goodglass, 2001: 69), which is responsible for "hearing and time relation" (Garman 1990: 75) and is commonly known as Wernicke's area.
Patients with Wernicke's aphasia produce very nonsensical language (Caplan 1987: 50), being found to use phonemic paraphasia (Obler, 1999: 43) in which the patients wrongly select their speech sounds; Wernicke emphasises that patients do not mispronounce phonemes (Caplan 1987: 50) and that aphasia is not a production disorder.

The errors in sound selection lead to the use of non-words and neologisms (Libben 1996: 429), however phonemic deficits are not the only characteristic of Wernicke's speech with various grammatical and semantic deficits also being apparent.
Word finding difficulties within Wernicke's aphasia are incredibly severe and a constant feature of this type of aphasic syndromes speech (Goodglass, 2001: 69) with content words, particularly open class words, being sparse and the content words that are used seeming not to make sense (Carlson et al 2004: 390), for various reasons of paraphasia or the context in which they are used.

Semantics are described as unusual (Obler, 1999: 41) with lexical semantic deficits being a common occurrence (Obler, 1999: 58), unlike Broca's aphasics who typically have good lexical semantics.
However, the main deficits of Wernicke's aphasia seem to be comprehensive with the patients seeming to understand very little of others speech and their own self monitoring seeming to echo this.

Lack of comprehension means that sentence structure often seems bizarre (Obler, 1999: 41) with Wernicke's aphasics use of function words, complex verb tenses and subordinates (Carlson et al 2004 : 390) seeming to be in vain due to the lack of relevant content.
When attempts are made to employ content words circumlocutions are often used in the place of the single word, as word retrieval proves difficult (Goodglass, 2001: 69), resulting in elaborate descriptions being used.

Similarly, when asked to repeat structures "paraphasic distortions" occur with the patient, once again, elaborating and using more difficult structures than the examination requires (Goodglass, 2001: 69).
Carlson et al (2004: 390) suggests that comprehension begins at the auditory level with the idea that in order to understand we must first hear and process information, seeming to suggest that the temporal lobes role in hearing may influence the syndromes lack of comprehension, this would seem to be supported by the fact that, unlike Broca's aphasics, Wernicke's aphasics are unaware of any of their errors (Goodglass, 2001: 68) and the notion that sufferers have difficulties converting thoughts into words (Carlson et al 2004: 391) as the difficulties with comprehension and recognition of words would also be internalised, by which I mean self- monitoring and planning through thought (Libben 1996: 429).

Severe damage to Wernicke's area can produce pure word deafness, whereby patients cannot understand oral language despite having normal hearing (Obler, 1999: 45).
Those with this type of damage have been found to recognise non-speech sounds, such as a door-bell or dog bark and emotion expressed through intonation (Carlson et al 2004: 391) suggesting that aspects of association related to comprehension can still remain in extreme cases of left hemisphere damage.

Metabolic Scanning and Brain localisation.
Jackson (1878 in Obler, 1999 : 33) stated that "to locate the damage which destroys speech and to locate speech are two different things" and in order to locate language features and processing in the brain psycholinguists think that it is important to look at the normal brain, that is one without trauma (Obler, 1999: 2), as developing an awareness of the language-brain relationship could prove helpful (Caplan 1987:32) In recent year technology has become available that can aid knowledge of language localisation within the brain, some of which will briefly be discussed.

Computerised Axial Tomography (CAT scanning) has been used to identify lesions and tumours within the brain (Lebbin: 422).
Whilst these identification aids are useful, CAT scans only produce static pictures or X-rays of the brain at rest and therefore do not aid knowledge about language localisation specifically.

The WADA test, however, is used to discriminate which hemisphere is responsible for language functions by injecting anaesthetic into the brain and observing whether language is paralysed (Obler, 1999: 28).
If the injection does result in temporary paralysis it can be concluded that the side injected is dominant in language functions, with normal language functions being found to resume shortly after (Obler, 1999: 29).

This is an invasive way of identifying language functions and is generally done prior to brain surgery so as to avoid damaging critical language areas of the brain.
The more recent models have proven less invasive than the WADA test, with Event Related Potentials, or ERP's, using electrode attached to the scalp to record any brain activity (Obler, 1999: 35).

The patient carries out various language tasks that act as the stimulus (Obler, 1999: 35) whilst the results are quickly measured (Caplan 1987: 29).
The results gained will include conscious and unconscious language functions, however irrelevant actions, such as blinking and jaw movement, can also be recorded and therefore must be controlled (Caplan 1987: 29).

Positron Emission Tomography (PET) is also another non-invasive way tracking language, this time via blood flow.
The patient inhales a small, harmless amount of radioactive gas which then attaches itself to red blood cells in the blood (Libben 1996: 423).

When a particular task is being done blood will rush to the area responsible for that language function, thus indicating localisation of particular language functions.
The blood flow is shown via different lights on a computer screen with different colours representing different levels of involvement (Obler, 1999: 10).

PET's have shown that when a patient speaks there is increased blood flow in Broca's area (Libben 1996: 423); however the distribution of language functions illustrated is often not seen to be as localised, as originally suggested, to Broca and Wernicke's areas respectively.
The use of these metabolic scanning measures allows more precise observations and conclusions to be formed (Oblen: 31) with elements such as cerebral blood flow in PET scans aiding knowledge of language functions.

Discussion
Studies into aphasia have suggested that particular areas of the brain are responsible for particular language functions by correlating localisations of lesions and deficits a patient exhibit (Goodglass, 2001: 57).

Broca's area, in the lower frontal lobe, was suggested primarily to relate to speech production due to this being the main deficit in early case studies, whilst Wernicke's area was primarily involved in comprehension issues which were thought to be controlled by the temporal lobe.
These studies have focused mainly on observations of a living patient and their autopsy results as to where their particular lesion lay, until the introduction of metabolic scanning.

The early research and theories in the area, as proposed by Broca and Wernicke, have proved essential in the development of the field and many of the symptoms presented to be synonymous to their aphasics can still be discovered today, as demonstrated by the identification of Broca's area in speech production found in PET scans (Libben 1996: 423) and also Broca's identification of the left hemisphere as dominant in the majority of people (Obler, 1999: 28); however it is important to remember that locating damage and locating speech are vastly different things (Jackson,1878 in Obler, 1999 : 33) Aphasic study has, typically, focused on language as a uni-lateral function when via many modalities the non-dominant hemisphere, in most the right hemisphere, plays a vital role, for example in reading.
Non-verbal linguistic features, such as visual information from facial expression and gesture, have been suggested to be typically right hemisphere functions, as are features of prosody, seen to be so important in Broca's aphasia; and aspects of comprehension relating to jokes and metaphors (Libben 1996: 419).

Language disorders have also been identified in patients' non-dominant hemisphere and therefore outside the stated language areas (Obler, 1999: 5), with the majority of aphasics demonstrating word retrieval difficulties regardless of the position of their lesions (Libben 1996: 434).
These findings suggest that the whole brain contributes to language functions (Obler, 1999:12) and that language may not be as localised as originally thought with.

However, aphasic studies have informed our knowledge as to the localisation of language functions in the brain.
Had the original theories not have been suggested by the likes of Broca and Wernicke further research would not have been done and technology would not have developed to aid our understanding of neurolinguistics via mapping of brain activity.

Although Broca and Wernicke's suggestion that language is specifically localised is now being challenged by new information about language distribution in the brain, it has not been totally disproven, as mentioned previously; however it seems vital to remember that different tasks require different functions and that "localisation of symptoms is a product of brain maturation and years of language use" (Goodglass, 2001: 57) thus proposing that not all brains necessarily follow the same paths in relation to their deficits, which in turn reinforces individual differences and therefore the possibility that language pathways can also differ.
Introduction.

The term 'levels of language' refers to semantic, phonological, lexical and pragmatic development across language acquisition and use.
Typical child language acquisition maps stages at which each of these 'levels' develop and become gradually more sophisticated, however, in atypical language development these 'levels' often appear impaired.

Kaiser et al (2001:145) state that children with developmental disabilities exhibit cognitive, perceptual and social characteristics that result in modest to severe disruptions of the normal language learning process.
These disruptions are mainly manifested through language delays which, although detrimental, offer an insight into language development of atypical individuals in a more detailed way due to each stage developing at a slower rate (Tager-Flusberg, 1999: 311).

This assignment aims to look at deficits in spoken language associated with Autism and Down's syndrome.
Each of the disorders will be presented separately, with a brief description of what they are before the relative strengths and weaknesses of language acquisition and use is discussed and a conclusion formed.

Autism
Baird et al (2003: 488) describe Autism as a "behaviourally defined disorder characterised by qualitative impairments in social communication, social interaction and social imagination".

The disorder is, generally, diagnosed on behavioural deficits as a biological cause has not been identified as of yet (Tager-Flusberg, 1999: 328) with the majority of diagnosis taking place after the age of 3;0 (Baird et al, 2003: 490).
Due to the relatively late diagnosis there is little evidence from the pre-linguistic period or early vocal development (Tager- Flusberg, 1999: 328), however social deficits have been noted as early as 18months with Baron-Cohen et al (1996, as in Kaiser et al, 2001: 145) reporting that eye contact was only used for requests or regulation.

2.1 Communication and Pragmatics
Children diagnosed with Autism are found to lack joint attention skills (Wetherby, 1986 as in Kaiser,2001: 145), whereby the child seems somewhat reclusive in their own interests demonstrating more interest for their environment than for the people they are sharing it with.

This is supported by Ornitz et al (1977 as in Tager-Flusberg, 1999: 329) who reported that autistic infants show no interest in interacting with other babies.
These deficits seem to consequently lead to autistic children lacking social language but acquiring and using environmental language (Wetherby, 1986 as in Tager- Flusberg, 1999: 330).

Whilst much research into child language development has suggested that pre-linguistic infants favour familiar voices of their own caregivers, autistic children show no preference for their mother's voice over a stranger's, again supporting the notion that autistic children are disinterested in people (Klin, 191 as in Tager-Flusberg, 1999: 329).
However, it has been suggested that Autistic children are more limited in their functional communication abilities than normal children, or other congenital disorder subgroups (Prizant and Duchan, 1981 and Shapiro, 1977 as in Tager-Flusberg, 1999: 329).

This has led to the suggestion by Ricks and Wing, 1976 as in Tager-Flusberg, 1999: 329) that perhaps autistic children communicate their needs differently to the majority of children.
The possibility that autistic children communicate in a different way to most children, whether they are typically developing or suffer from another congenital disorder; seems to be supported by evidence of autistic children in conversation and also their spontaneous speech.

Autistic children showed evidence of requesting objects and protesting against situations but were not found to comment on situations, request information, acknowledge listeners or boast and show off (Wetherby and Prutting, 1984, as in Tager-Flusberg, 1999: 330).
Similarly, in conversation with others autistic children introduced irrelevant topics to the conversations with their mothers, often repeating themselves but not elaborating on subjects (Tager-Flusberg, 1999: 330).

The apparent inability to elaborate on conversation and the need to change topics seems to emphasise the fact that autistic children do not identify with or acknowledge their conversational partners or the listeners.
2.2 Phonology

Both verbal and non-verbal aspects of language are impaired in Autism, however phonological deficits are relatively few and appear to follow similar patterns to those of a normally developing child (Bartolucci and Pierce, 1977 as in Tager-Flusberg, 1999: 328) with similar errors appearing and some functional language appearing by middle childhood.
The main phonological deficits that those with Autism tend to exhibit is unusual use of intonation and voice quality with many children, and adults alike, with Autism have difficulty varying their intonation (Pronovost et al, 1966 as in Tager-Flusberg, 1999: 328).

2.3 Lexical development
Tager- Flusberg (1999: 332) reports that autistic children experience similar success rates as a normally developing control group on conceptualisation and organisation of words and phrases at a basic and more sophisticated, superordinate level.

This is though to suggest that lexis develops systematically in those with Autism as it is thought to in the typically developing population.
However, there is evidence that autistic children do suffer certain lexical deficits through their lexis often seeming immature due to their use of neologisms and errors in the production of words and phrases seeming to mirror that of younger children.

In a typically developing child these deficits would not be as consistent as they appear to be in those with Autism (Tager- Flusberg, 1999: 333)
2.4 Morphological and syntactic development

The morphological and syntactic development of autistic individuals seems to be an area where much more research is needed due to the massive variation that is reported within the literature and research.
Tager-Flusberg (1999: 334) states that syntax of autistic children has been found to range from no functional language at all to "high normal range language".

It has been reported that those with Autism demonstrate a lower mean length of utterance (MLU) than typically developing children, with development happening over a longer period of time but in the same pattern acquiring simple structures first before building on them to create relatively complex grammar (Tager-Flusberg, 1999: 333).
Downs Syndrome

Downs Syndrome is the most common neurodevelopmental disorder (Tager-Flusberg, 1999: 312) which is typically diagnosed at birth (Tager-Flusberg, 1999: 313).
The disorder has been found to be due to an individual developing an extra chromosome before birth (Tager-Flusberg, 1999: 312) with the disorder varying greatly from relatively few deficits and a near normal IQ to severe impairments (Tager-Flusberg, 1999: 313).

Abbeduto and Hesketh (1997: 327) suggest that those with Downs Syndrome "see language as a vehicle for performing the same interpersonal functions as do typically developing children, albeit at a considerably later age" with Downs Syndrome children focusing on interpersonal relationships more than their environment.
3.1 Communication and Pragmatics.

Pragmatics appears to be a relative strength in those with Downs Syndrome (Tager-Flusberg, 1999: 316).
Infants with Downs Syndrome tend to avoid mutual eye contact and vocalisations initially (Tager-Flusberg, 1999: 315), however, by the end of their first year these infants have begun to catch up with typically developing infants demonstrating use of more vocalisations than other infants and fixating on peoples eyes (Berger and Cunningham, 1981 as in Tager-Flusberg, 1999: 315).

Unlike Autistic children, the centre of infants with Downs Syndromes attention is social interaction, with the suggestion that fewer object request statements parallels the pre-linguistic period where language is used as a means to engage people socially rather than to engage with the surrounding environment (Tager-Flusberg, 1999:316).
Children with Downs Syndrome have also been found to be able to maintain topics of conversation for a longer period of time than those developing typically.

Coggins and Stoel-Gammon, (1982 as in Tager-Flusberg, 1999: 317) report that when a breakdown in communication occurs, a person with Downs Syndrome will not just repeat their utterance, but rephrase it from an early age, however these breakdowns may be related to limited syntax and intelligibility (which will be discussed further 3.2 and 3.4) which could ultimately cause difficulty in interpreting the intentions of speech acts (Tager-Flusberg, 1999:317).
3.2 Phonology

Unlike Autism, Downs Syndrome is diagnosed at birth and therefore there is evidence as to the early implications that the disorder has on language development.
The first indication of language delay in an infant with Downs Syndrome is their delayed onset of canonical babbling.

Oller (1986 as in Tager-Flusberg, 1999: 313) reported that infants who had been diagnosed with Downs Syndrome began babbling approximately two months after the control group of typically developing children.
This delay was not found to be related to any other factor or impairment, such as hearing loss, which are often associated with Downs Syndrome (Tager-Flusberg, 1999: 313), and can affect language.

However Dodd (1972, as in Tager-Flusberg, 1999: 302) found no significant difference between typically developing infants and those with Downs Syndromes acquisition of babbling at the pre-linguistic stage, despite evidence of delayed cognitive skills.
Lynch et al (1990) suggested that delays amongst those with Downs Syndrome in the general population may be related to motor delays which were suggested to be more vulnerable in Downs Syndrome children and adults.

Smith (1977, 1984 as in Tager-Flusberg, 1999: 303) presented evidence that meaningful speech is delayed by approximately 7 months in those with Downs Syndrome.
Similarly, Stray-Gunderson (1986 as in Tager-Flusberg, 1999: 303) found first word acquisition across a wide range of ages within a Downs Syndrome population, varying from a typical 9 months to a severely delayed seven years.

This demonstrates the massive variability within the disorder which was referred to initially in 3.
Delayed language development in Downs Syndrome does follow the same phonological pattern as typically developing children follow, however development occurs more slowly (Tager-Flusberg, 1999: 303) with Downs Syndrome children using a greater variety of substitutions, than typically developing children, before they settle of the correct phoneme (Stoel-Gammon 1981, as in Tager-Flusberg, 1999: 304) Abbeduto and Hangerman (1997) emphasise the fact that those with Downs Syndrome often have anatomical abnormalities due to the disorder which can affect speech (: 314) as the vocal tract can develop abnormally (Abbeduto and Murphy, 2004 as in Rice et al, 2005: 21).

Possible anatomical abnormalities are thought to offer reasoning as to why many with Downs Syndrome are unintelligible as the atypical formation of the vocal tract could limit a person's capacity for communication (Stoel-Gammon, 1997: 301).
However, Crossley and Dowling (1989, as in Tager-Flusberg, 1999: 314) suggest unintelligibility may be due to syntactic delays relating to MLU.

3.3 Lexical development
Abbeduto and Hagerman (1997: 315) state that there is little known about lexical development in Downs Syndrome with those with the disorder being found to perform below their chronological age on both receptive and expressive vocabulary (Madison et al, 1986 as in Abbeduto and Hangerman, 1997: 315) but no research being presented for a possible correlation between cognitively matched pairs.Cardosa-Martins et al (1985 as in Chapman, 1997: 308) however, identifies that when Downs Syndrome children are matched according to mental age they develop their early words at a similar rate.

Vocabulary does seem to be acquired at a relatively basic level with comprehension of subordinate and superordinates being poor (Mervis et al, 1985, Tager-Flusberg, 1985 as in Tager-Flusberg, 1999: 318) and more sophisticated vocabulary not seeming to develop, even in adolescence.
3.4 Morphological and Syntactic development

Those with Downs Syndrome experience difficulty with comprehension, particularly of complex sentences (Chapman et al 1991 as in Tager-Flusberg, 1999: 318).
Shorter, more basic sentence types are generally used with more sophisticated forms often failing to be acquired (Tager-Flusberg, 1999: 319).

Development does continue into adolescence allowing some to progress slightly (Chapman et al 1992, as in Tager-Flusberg, 1999: 319), however most with the disorder will not achieve beyond early grammatical development as most Downs Syndrome children do not start combining words until they are 5;0 or 6;0.
Conclusion- similarities and differences.

Whilst both disorders did demonstrate deficits in language acquisition and use across the different levels of language the deficits experienced in both differed dramatically.
Impairments in the language development of Autistic children have been identified with an unusual use of language and poor non-verbal and comprehension abilities being reported (Baird et al, 2003: 490).

Lower verbal, than non-verbal scores have been found in preschool children with Autism (Tager-Flusberg and Lord, 2002 as in Rice et al, 2005: 15), with Allen and Rapin (1980 as in Rice et al, 2005: 16) reporting that 67% of autistic children have expressive and receptive language delay.
It is thought that language and concepts that engage what are referred to as "theories of mind" are impaired, whilst lexical semantics remain less impaired (Tager-Flusberg, 1999: 334) within autistic individuals, with Tager-Flusberg (1999: 328) seeming to suggest that language deficits lead to the social deficits autistic people experience.

The association between Autism and lack of interpersonal communication seems to provide reasoning for the emergence of linguistic terms that refer to and request objects and actions in the environment.
It also supports the additions of irrelevant information and common initiation of subject change (Tager-Flusberg, 1999:330) as the child appears to have a relatively egocentric cognitive and linguistic capacity.

Phonologically, few deficits appear in Autism with the phonological system appearing to develop typically.
Similarly lexis is relatively strong with little evidence being proposed of any significant delays, and neologisms seeming to be the main cause for concern.

Syntactically there is little evidence; however there does appear to be a large range of ability across the Autistic populations studied.
On the contrary, those with Downs Syndrome experience poor phonology, lexis and syntactic development; whilst pragmatics appears to be a relative strength.

Phonologically delays are rife, possibly due to anatomical deficits such as abnormally formed vocal tract or hearing loss.
However, there is evidence of a significant delay which is not remedied over time despite a typical developmental pattern occurring.

Lexically and syntactically language is acquired at a very basic level with progression being slow.
The syntactic delay is most probably a result of the lexical deficits experienced with multiple word utterances appearing late.

Pragmatically those with Downs Syndrome are relatively strong, although their other deficits can often cause confusion with identifying the intent of a speech act.
Eye contact and maintaining conversation is done well, however initiating conversations is rare.

Therefore, when language develops atypically patterns of deficit differ across different congental disorder.
Autism is mainly found to be deficit in pragmatics and social features of language acquisition, whilst pragmatics is a relative strength in Downs Syndrome.

However, those with Downs Syndrome experience greater difficulties with lexis, syntax and particularly phonology.
Despite these dffernces those with Autism and Downs Syndrome have been found to acquire language via a normal pattern, although at a slower pace.

Introduction.
The definition offered for the classification of SLI is one which is ruled by categories of exclusion (Botting and Ramsden, 2001: 421).

Donaldson states that SLI is a diagnosed when a child presents language difficulties despite this impairment not being associated with any other problems, such as cerebral palsy and hearing difficulties (Donaldson, 1995: 21).
A non-verbal IQ of 85 or above is expected of those with SLI in order to establish that the deficits produce by the child are indeed language specific (Leonard, 2000: 10) and that otherwise the child is intellectually 'normal' (Johnston: 1994: 107); however in recent years researchers have felt that the definition of SLI by exclusion is not satisfactory and that non-verbal and behavioural skills are also vulnerable in those categorised as having SLI (Stevenson, 1996: 83).

This has led to the suggestion that SLI is not in fact a specific disorder of language but a more general disorder affecting cognition also (Hill, 2001: 167).
The literature proposing those with SLI will also have non-verbal deficits will be presented.

From the given information a discussion will be generated in an attempt to establish a conclusion as to whether those described as having SLI do indeed demonstrate no non-verbal deficits or whether the disorder involves elements of cognition that have been largely ignored in the earlier research into SLI.
Research into the notion of non-verbal deficits in SLI.

Snyder (1982, as in Leonard, 2000: 17) states that a significant percentage of the population who attain a normal non-verbal, or performance, IQ can be found to demonstrate diminished IQ on language measures by approximately 15 points.
Such a statement describes the patterns of deficit proposed for those with SLI and supports the view that SLI does not include deficits of non-verbal functioning; however it has been reported that many children with SLI show weaknesses on tasks that require minimal language ability, hence suggesting a possible cognitive vulnerability (Leonard, 2000: 119) with the suggestion that these differences between verbal and non-verbal ability via IQ scores are not significant enough to classify children as having a specific impairment in language alone confidently (Leonard, 2000: 16).

Research into the non-verbal abilities of those with SLI have taken various approaches with some focusing on physical ability (e.g. limb praxis) and others focusing more on a child's ability to use abstract concepts and stretch their imaginations (e.g. symbolic play, mental imagery).
A variety of evidence will be discussed in relation to potential correlations with SLI as a general processing disorder.

Symbolic play in SLI.
The concept of symbolic play was explored by Morehead (1972, as in Leonard, 2000: 120) who looked at SLI children's ability to project concepts of 'real play terms' (in this case a doll, cot and blanket) onto items which could be used to represent them by using a shoe box and a piece of paper along with the original doll.

The expectation would be that the SLI children would be able to map these concepts onto objects that could represent the initial play items due to SLI's exclusionary criteria; however this was not the case.
Morehead found that children with SLI demonstrated very little symbolic play suggesting a limitation in their non-verbal intelligence (Leonard, 2000: 120).

Similar findings have been reported by Brown et al (1975, as in Leonard, 2000: 120) who discovered that preschool children with SLI had more difficulty than age matched controls using objects in a pretend manner.
The ability to inter-relate items was also found to be problematic in SLI children with Udwin and Yule (1983 as in Leonard, 2000: 121) finding age matched controls to perform better on tasks which aimed to elicit concepts of time and space using a miniature toy set.

However, this concept was also adopted by Terrel et al in 1984 (as in Leonard, 2000: 121) but using a control group who were matched, not by age but by expressive vocabulary ability (i.e. MLU).
This study found that those with SLI out performed the language matched controls, thus seeming to support the original criteria for SLI.

Further investigations followed the notion of age matched versus language matched controls in a symbolic play situation and the research has proven relatively inconclusive on one front.
Those children with SLI do demonstrate a diminished performance in comparison to their age matched peers; however the language matched controls have proven to differ greatly with some studies finding SLI children to have poor performance whilst others show equally good or even better performance than the MLU controls.

Despite the inconclusive nature of the language matched groups the age matched groups were found to be significant with non-verbal deficits seeming to exist within SLI children on symbolic play tasks and the suggestion that poorer play correlates directly to less developed language (Leonard, 2000: 123).
Mental Imagery in SLI.

Mental imagery tasks were also found to pose a difficulty to those with SLI with similar patterns of results regarding age matched versus language matched controls appearing (Kamhi, 1981; Camarata et al, 1981).
Those with SLI were found to have difficulty in predicting the direction of water in a tilted glass with many believing that the water would not remain horizontal but parallel to the bottom of the glass regardless of the direction it was being moved in.

On an assessment battery of non-verbal tests, Johnston and Ramsted (1983 as in Leonard, 2000: 123) found mental imagery proved to be the area of most deficit with SLI participants failing to identify shapes they had blindly felt correctly.
2.3 Conservation and Seriation ability in SLI.

Tests on conservation and seriation have also been performed on those with SLI and present mixed results.
Whilst Siegel et al (1981 as in Leonard, 2000: 126) report those with SLI perform lower than their age matched peers on both measures, Johnston and Ramsted (1983) only found seriation tasks to be deficit with the SLI children involved performing approximately two years below their chronological age.

Kamhi (1981, as in Leonard, 2000: 126), however, comments that the differences found on these measures are not statistically significant implying that the deficits found in conservation and seriation are not as poignant as those discussed previously although some pattern of deficit can be assumed.
2.4 Hypothesis testing and Analogical Reasoning.

Hypothesis testing and analogical reasoning tasks have also been found to be problematic for those with SLI when compared to age matched controls.
Hypothesis testing found that when the SLI child was presented with a new and unfamiliar situation they found it difficult to make the appropriate distinctions regarding size, colour and shape requiring more trials and therefore input before correctly identifying features (Leonard, 2000: 126/7).

Similarly, analogical reasoning tasks were often found to require more processing time for SLI children to reach the correct answers despite the need for them to use language and concepts that would be assumed to be known to them.
For example, the participants' in Nippold et al's (1988) study were given sentences such as "Ear goes with radio, as eyes go with...." (Leonard, 2000: 127), with television.

being the target response.
Nippold et al and also Kamhi (1990) have reported that in such tests SLI children perform better when there is little or no language demands, i.e. the task demands are set out or supported using images and/ or toys, however it is unclear whether deficits in processing speed are the cause of better performance when linguistic input is limited, or whether the linguistic input of the child is the determining factor as to why these children struggle (Leonard, 2000: 128).

2.5 Auditory Processing in SLI.
Other evidence supporting the notion of a possible processing deficit comes from the research into auditory processing in children with SLI.

Those involved had no previous hearing impairment, hence their classification as SLI.
Rosenthal (1972, as in Leonard, 2000: 133) found SLI children capable of correctly identify single phonemes produced at intervals of 200ms, however reporting the order in which different phonemes were sequenced proved problematic.

The participants' ability to recognise the phonemes independently indicates that the difficulty is not one of auditory discrimination but rather appears to be a processing deficit.
However, this approach has been criticised due to its use of verbal stimuli with Rees (1973, as in Leonard, 2000: 134) asserting "what positive results have been reported could well be interpreted to reveal what is already known- that these children have a language disorder".

Non-verbal stimuli have therefore also been studied with evidence that children with SLI show poorer performance on tactile perception tasks (Tallel et al, 1985) with more complex sounds and rhythms requiring significantly more time in order for the child to decipher what the sound was or where it was coming from in the case of dichotic listening tasks (Haggerty and Stamm, 1978, as in Leonard, 2000: 136).
Tallal and Piercy (1974) suggest that correct identification of vowels more frequently than [ba] and [da] is due to vowel formants being steady throughout their articulation whilst consonant groupings were only steady for just under one fifth of the total duration (Leonard, 2000: 137).

2.6 Motor Ability in those with SLI.
Motor ability is also suggested to be a potential non-verbal deficit associated with SLI.

Nicolson and Fawcett (1994, as in Hill, 2001: 150) report difficulties in motor control of developmental dyslexics with an apparent overlap occurring between motor ability and language skills across SLI.
Hill (2001) provides two possible accounts for these overlaps, one being that elements of language, attention and motor deficits exist in everyone with developmental disorders with specific disorders being suggested to be rare in comparison; or that the co-occurrence of such deficits is superficial and largely insignificant (Hill, 2001: 150).

However, both of these views seem relatively extreme and a middle ground is suggested to be necessary.
Children classified as having SLI were assessed for their motor abilities after being differentiated from those with Developmental Coordination Disorder (DCD) - children with DCD have higher verbal IQ than non-verbal IQ, unlike those with SLI (Hill, 2001: 152).

Although children with SLI have been found to have movement difficulties, these movement difficulties were not found to limit all motor abilities with Johnston et al (1981) and Preis et al (1997) reporting that fine motor tasks in SLI children tend to be unimpaired.
Different tests of motor ability seem to affect different subgroups of children with SLI, for example, children with inconsistent phonological errors and verbal dyspraxia performed worse on peg moving and motor accuracy tasks than those who had been found to make consistent phonological errors and those with phonological delay (Hill, 2001: 158).

Limb Praxis in SLI
When testing for limb praxis children are generally being tested on familiar and unfamiliar gestures which sometimes require the use of a prop (transitive gestures) (Hill, 2001: 159).

The children who gestures were trying to be elicited from were cued verbally or through the prior knowledge that they must imitate or pantomime another's actions.
It was that children with SLI would naturally compensate for their language deficits by using gesture, as with deaf children.

However, Bartok, Rutter and Cox (1975) have found this not to be the case reporting that over 40% of their sample with receptive language deficits did not use gesture (Bishop, 2000: 103).
Dewey and Wall (1997) identified limb praxis deficits in familiar situations which did not require a prop, which contradicts Aram and Horwitz's (1983) research which showed the majority of their subjects demonstrated some level of impairment on all familiar gesture tasks (Hill, 2001: 159).

The errors experienced by those with SLI are said to be similar to those one would expect to find in a normally developing child, however the frequency of the errors is much greater in those with SLI.
This could be due to SLI children typically requiring more time to process information as discussed previously in 2.4 with Archer and Witelson (1988, as in Hill, 2001: 161) supporting this by stating that SLI children are significantly slower in their accurate production of gesture.

3. Discussion.
From the evidence presented on non-verbal deficits in SLI it seems clear that the deficits children with SLI face are not exclusively related to language (Hill, 2001: 162), therefore causing speculation as to what the deficit does indeed entail.

Although non-verbal IQ appears to be in tact on initial examination Conti- Ramsden and Gunn (1986 as in McTear and Conti-Ramsden: 1994: 65) suggest that tests used to assess cognition are too 'global' to identify any specific deficits within a persons cognitive system.
All the measures discussed previously demonstrate some level of non-verbal deficit in those with SLI with some of these being found to be more pronounced than others; for example symbolic plays clear deficits in those with SLI in comparison to conservation and seriation where deficits are thought to be less significant statistically.

However, all of these deficits point towards SLI as a more general deficit in processing ability with Tallal et al (1993) and Kail (1994) seeming to support this notion with their theories on processing deficits in SLI (as in Hill, 2001: 162).
Tallal's suggestion that integration deficits are a significant contributor to those diagnosed with SLI supports the evidence presented previously by Udwin and Yule (1983 as in Leonard, 2000: 121) in 2.1.

Whilst Kail (1994) suggests that all verbal and non-verbal processes slow and therefore create the patterns of deficit that can be attached to SLI (as in Hill, 2001: 163), which would support the notion that children diagnosed as having SLI require more processing time and therefore are found to have deficits in auditory perception, hypothesis testing and analogical reasoning tasks.
Some areas discussed have noted that the research is not totally conclusive as of yet, such as the findings on mental imagery tasks that identify SLI children as poorer than their age matched controls but better than their language matched controls.

However, even subtle and inconclusive evidence must be considered as it all leads to a clearer pattern of deficit with these small discrepancies being found so frequently in those diagnosed with SLI that they cannot be ignored (Leonard: 2000: 237).
The suggestion that SLI includes these non-verbal deficits add strength to the suggestion that cognitive ability and language are more closely related than previously thought, with Leonard emphasising that through the study of those with SLI non-verbal deficits have been identified and through these deficits being investigated the cause of language impairments experienced by those with SLI may be feasible (Leonard, 2000: 119).

However, whether non-verbal deficits are the likely cause or effect of language impairment is unclear with the suggestion that low non-verbal IQ may in fact be found in response to poor comprehension of syntactic structures (Leonard, 2000: 128).
Whilst non-verbal deficits have been suggested to be secondary to language impairments, (Leonard: 2000: 237, Stevenson, 1996: 83), low general IQ has been reported by Silva et al (1983, as in Donaldson, 1995: 21) in 25% of children aged 7;0 with SLI.

Therefore, although the exact relationship between non-verbal and verbal functioning capacity are still unknown it is increasingly apparent that non-verbal deficits do exist in SLI, with these deficits requiring greater investigation in order to establish whether SLI is predominantly a language disorder or a more general disorder of processing capacity.
It is now well documented and widely accepted that the Earth's interior consists mainly of molten rock and exists as layers of differing content and density, ranging from a solid iron core to a molten, mineral rich mantle.

This, however, has not always been the case and many hypotheses on the nature of its structure have been put forward throughout history.
Nineteenth century science fictionalist, Jules Verne, proposed that, to account for the flawed calculation of the earths density, vast internal caves existed as hollows below the surface and in his popular children's story, 'Journey to the centre of the Earth' he exploited this fact by allowing the characters to descend through these air filled passages toward the centre of the Earth.

Of course this sounds preposterous now but many of the arguments made by Verne hold true and at the time in which it was written, people believed that this was the knowledge of the scientific community.
When describing the journey to be undertaken in Chapter Six, the young Axel states that,"the internal temperature rises one degree for every seventy feet in depth" which equates to a geothermal gradient of around twenty eight degrees Celsius per kilometre, well within the accepted value of between twenty and thirty.

However, this gradient only holds true for the continental crust, a vital error which leads to an overestimation of the temperature at the centre by a magnificent factor of forty.
Despite his inaccuracy, he does state his assumption that the gradient remains constant, in keeping with scientific procedure.

The argument between him and the professor develops and counter theories expressed as expected.
It is true that an internal heat of such magnitude would indeed lead to unpredictable and explosive consequences and so other avenues must be explored.

The professor disputes even the concept of internal heat, he believes that, "The Earth has been heated by combustion on its surface, that is all" and that any heat which once was held internally as a result of this combustion must now have all dissipated through volcanic vents.
There are several obvious errors in his argument but again, each has been presented from a scientific and informed viewpoint.

We know that heat within the Earth is, as the professor said, a result of its violent creation.
However, it is now known that this heat was formed in the outer regions by the conversion of kinetic energy of extraterrestrial bodies on impact with our young planet, whilst gravitational energy from contraction heated its interior so, although the vigorous reactions mentioned between atmospheric water and what he proposes composed the Earth's surface, sodium and potassium, do occur as he states, this is inaccurate.

It is important to note though that although erroneous, the argument is supported by an experiment he claims was performed by Humphrey Davy, the chemist who discovered the afore mentioned elements, and so is in keeping with scientific methodology.
He was also correct when saying that the depletion of volcanoes over history indicates that the earth is cooling down, but the process is extremely slow and he has assumed that no additional heat is being generated, which we know is not the case.

The decay of radioactive isotopes in the mantle is constantly producing heat which goes into powering the solid state convection cycle associated with plate tectonics.
The professor concludes, inaccurately, "that the interior of the globe is neither gas nor water, nor any of the heaviest minerals known." We know that all three of these components contribute to the Earth's internal structure.

The mantle is predominantly peridodite, a rock made up mainly of the silicate minerals, pyroxene and olivine.
There is also evidence that there is water within the mantle and these vapours and gases can be observed during volcanic eruptions.

I think, when reading this book, it is important to remember that its sole purpose is to entertain and excite the reader.
Although many of the facts stated have scientific support and the way in which they were presented have academic credibility, there are several, and in my opinion, intentional flaws.

If Verne had been accurate when describing the internal structure of the Earth he would inevitably have had to conclude that the mission was impossible.
Temperatures and pressures of such great magnitude would not allow such a thing.

The molten interior would be impenetrable.
I think the book is a wonderful introduction to geology, providing much insight into the field but it is by no means a substitute for modern day text books.

I think also, at the time it was written, it was to be taken somewhat with a pinch of salt.
I believe Verne represented his true feelings through the voice of Axel when presenting his arguments against the journey as he knew such a feat could not be undertaken but the professor, with his convincing and logical arguments, persuaded him, as much as the reader, that it could and no budding geologist would turn down such an adventurous quest for knowledge.

Stage One:
Aim:-

the aim of stage one is to prepare a sample of acetanilide which can later be brominated.
The reason for this is that aniline is too reactive to produce p-bromoaniline directly and so we must first reduce the activating influence of the amino group by acetylation.

Procedure:-
Aniline (10ml, 109.9mmol) was dissolved in glacial acetic acid (25ml, 437.1mmol) and acetic anhydride (12ml, 127.3mmol) added slowly with swirling.

The reaction was exothermic and yielded an orange-brown solution which was allowed to stand for five minutes then diluted with water (approximately 200ml) until crystallization occurred and a dense cream suspension formed.
The precipitate was filtered, washed with water and allowed to dry.

The filtrate was then re-crystallised from the minimum hot ethanol.
Results: -

Our re-crystallised product was a white solid.
It was found to have a melting point in the range of 110 - 114 o c, slightly lower than the literature value for acetanilide of 113 - 115 o c (taken from the Fisher Scientific Catalogue of fine chemicals '05 - '06) but clearly the correct product was formed, ready to be brominated in stage two.

4.78g of product was obtained, equating to 35.4mmol out of a possible 109.9 (this was the number of moles or our limiting reagent, aniline and the reaction occurred with a 1:1 ratio).
The percentage yield was calculated as only 32%, which is very low due possibly to loss in the re-crystallization process in which some of our product may have remained soluble in the filtration stage.

Stage Two:
Aim: -

to brominates carbon four of the benzene ring of acetanilide to form p-bromoacetanilide.
Procedure:-

Acetanilide (4.78g, 35.4 mmol) was dissolved in cold, glacial acetic acid (25ml, 437.1 mmol) and added to it slowly was a solution of bromine (2.1ml, 40.7mmol) and glacial acetic acid (30ml, 524.5mmol) with constant shaking.
An orange brown solution formed whose intensity and density increased, over the 15 minutes it was left to stand, to a bright orange suspension which was filtered after the addition of cold water (200ml).

The filtrate was stirred with a further portion of water (150ml) containing sodium metabisulfate (2.02g, 9.8mmol) and re filtered, yielding a pale yellow solid, which was then re-crystallized from hot ethanol and a little water (~2ml).
This was allowed to cool, then filtered and dried in a desiccator over several days.

Results: -
Our re crystallized product was a white solid, weighing 4.39g.

It was found to have a melting point in the range of 167 - 170 oc which confirmed it's identity as p-bromoacetanilide, whose melting point is quoted as 166 - 170 oc.
20.5 mmol of this were obtained out of a possible 35.4mmol, which is how much acetanilide we began with, generating a yield of 57.9% for this stage.

Stage Three :
Aim: -

to remove the protecting acetyl substituent of the p-bromoacetanilide to form our final product, p-bromoaniline.
Procedure :-

p-bromoacetanilide (4.39g, 20.5mmol), water (25ml) and concentrated hydrochloric acid (25ml, 288mmol) were heated under reflux until the solid dissolved and then for a further 10 minutes.
The solution was then cooled in ice water, generating a cloudy white suspension which thickened on addition of 25% sodium hydroxide (added until pH read above 14).

A white solution settled above a brown precipitate which was then filtered to reveal two products of differing nature: one a white powder, one a pink - brown solid.
Both were dissolved in the minimum hot ethanol and water added drop wise until it became slightly turbid.

The solution was then allowed to crystallise, filtered and the filtrate dried in a dessicator over several days, yielding a beige powdery solid.
Results :-

Our re crystallised product was a beige solid of mass 2.15g.
It had a melting point of 54 - 61 oc, just slightly under the literature value for p-bromoaniline of 56 - 62 oc but we were satisfied that the correct product had been obtained.

The yield for this final stage was 61% and the overall yield (comparing the number of moles of product obtained with those of our starting material) was 11.4%.
Conclusion: -

The overall percentage yield for this experiment is extremely low but unsurprising due to the many processes we undertook where loss of product was unwanted but expected.
To improve the loss at re crystallisation a different solvent could be used which would be more specific to our products.

Aim:-
to determine, experimentally, the solubility product of Potassium Periodate and study the effects of salting in and salting out on the molar solubility.

Background:-
there exists a heterogeneous equilibrium between a saturated solution of a slightly soluble salt, MX, in contact with excess solid.

FORMULA The equilibrium constant for the equilibrium between the undissolved salt and its ions in a saturated solution is known as the solubility product, Ks, and like any equilibrium constant is the same at any one temperature.
FORMULA The concentration of the anion and cation are equal and are aknown as the molar solubility, s. FORMULA

Salting Out:-
this refers to the common ion effect by which the solubility of the salt is reduced by the addition of a common ion of concentration, c, of M+ ions, for example.

For the equilibrium constant to remain the same, [X-] must decrease and hence the molar solubility, which can now be called s' : FORMULA
Salting In:-

this refers to the inert ion effect by which the solubility of the salt increases by addition of other ions due to ionic interactions.
For example, the removal of M- ions by a cation would lead to, according to Le Chatelier's Principle, the generation of more M- ions to oppose the change.

The effects can be quantitatively described in terms of activity co-efficients for each ion.
The true thermodynamics solubility, ksth, is the quantity which is truly constant at any one defined temperature: FORMULA For dilute solutions, there are few interactions between ions and so the effects can be ignored as (a + -) tends to one, ksth tends to ks.

However, for solutions between 0.01M and 0.1M, the value of the activity co-efficients are more or less independent of the nature of the electrolyte and (a+ -) can be calculated from the Debye-Huckel limiting law equation: FORMULA For concentrations above this, (a+ -) can be approximated by considering the mean value of the activity co-efficients for several similar electrolytes.
Procedure:-

Sodium thiosulfate (12.4037g) was made up to 250ml with distilled water and a trace amount of sodium carbonate added.
A 25ml portion of this solution was then extracted and made up to 250ml.

Three stoppered bottles were labelled A-C, and into each, potassium periodate (1g) was added with, in bottle A, distilled water(100ml), in B, sodium nitrate(0.2M, 100ml) and in C, potassium nitrate(0.2M, 100ml) and shaken vigorously for 3 minutes before allowing to settle.
At this point the temperatures were noted.

each solution was then filtered and a 25ml portion taken for titration.
This sample was then treated with potassium iodide and sulphuric acid to liberate the iodine.

Solutions A and B were titrated against the more concentrated sodium thiosulfate(1.999*10^-1M) and solution C against the more dilute(1.999*10^-2M).
The iodine acted as an indicator, turning the solution from red-brown to colourless when it had all reacted.

Each sample was repeatedly titrated until concordant results were achieved and a mean titre calculated.
Results:-

Calculations:-
FORMULA

Errors:-
FORMULA

Conclusion:-
the true thermodynamic solubility product of potassium periodate at room temperature was found to be approximately 2.718*10^-4 (this was the average value taken from our three solutions).

It is this product which is said to be truly constant at any one temperature regardless of the environment as it takes into account the interactions between ions in solution.
Our results strongly support this, deviating only very slightly from the mean, whereas the solubility product, ks, has a much greater standard deviation, varying for different solutions.

It is important to note that our three solutions were filtered when at slightly different temperatures which may have led to the discrepancy in ksth.
Our results also demonstrate very well the effects of both salting in and out as mentioned before.

When other ions were introduced they reacted with the dissolved salt ions, removing them from the established equilibrium and hence forcing further generation (solubilization).
This was illustrated by the rise in concentration of salt ions in solution B and demonstrates "salting in".

Solution C introduced common ions which pushed the equilibrium towards the solid salt, lowering the concentration of iodate ions and hence the solubility decreased - an example of "salting out".
Aim:-

to calculate, experimentally, the heat capacity of our calorimeter and use it to determine the enthalpy of vaporisation of a sample of propanone by recording the drop in temperature of the surrounding fluid in our calorimeter.
Background:-

the molar enthalpy of vaporisation is the heat energy required to vaporise one mole of a liquid substance.
during calorimetry, the energy needed is absorbed from the surrounding fluid and so a drop in its temperature is observed, which is equal to the enthalpy of vaporisation divided by the specific heat capacity of the calorimeter system.

FORMULA The heat capacity of the system can be determined by supplying it with a know amount of energy and measuring the increase in temperature.
the heat capacity is the number of Joules of energy needed to increase the temperature by one degree.

All the relevant formulae will be exploited in this practical.
Apparatus:-

Procedure:-
petroleum ether (180ml) was transferred to an insulated, glass cylinder and a magnetic stirrer bar added.

the cylinder was placed in its insulated jacket on a magnetic stirrer.
The lid of the vessel contained a thermometer, heater and an evaporation tube.

The heater was connected to a heat monitor and power supply and was switched on while a note was made of its current and voltage.
Propanone (3.9684g,68.42mmol) was added to the evaporation tube and connected to the water pump to ensure a flow of air over it to remove any vapour produced and encourage the generation of more vapour (by Le Chatelier's Principle).

Before turning the pump on, the magnetic stirrer was started and the temperature of the petroleum ether taken at thirty second intervals.
The water pump was then turned on and further temperature readings were taken until the propanone had evaporated and the temperature ceased dropping.

At this stage the temperature was monitored for a few minutes to see how effective our insulation was (eg. how much heat was absorbed from outside).
The heater was then switched on and temperature readings taken until it exceeded the starting temperature.

It was then turned off and again the temperature monitored for a few more minutes.
Results:-

Calculations and Errors:-
FORMULA

Conclusion:-
the molar enthalpy of vaporisation of propanone was found to be 30.72kJmol^-1, which is slightly higher that the literature value of 29.10kJmol^-1, with a discrepancy of 5.27%.

Obviously the equipment error cannot account for this as it is much smaller.
Other sources of error include our interpretation of the graph and disturbances of our calorimeter system.

It is apparent that either the flask was knocked, moving the insulation, or the flow of air into our system wasn't constant because the three sections of our plot that show external influence on the system should have best fit lines of equal gradient.
The system appears to be well insulated between 0 and 20 minutes as the temperature stays constant before and after the petroleum evaporates, however at 20 minutes the petroleum ether began to heat up leading me to suspect that the system was tampered with, letting outside heat in.

This pattern of heating was observed again after the electrical heater was switched off.
To try and minimise the effect of this, the lines of best fit of these periods were extrapolated and the temperature taken along these at the midway time.

A great source of error, I believe, was the way in which the graph was manipulated.
The lines of best fit were just predicted by eye.

A possible solution could be to use a computer program to plot the graph and determine them with it.
Random errors in this practical also include our reading of the temperature and timing the thirty second intervals.

Aim:-
to hydrogenate the unsaturated fatty acid, methyl ester, Methyl Oleate to form Methyl Stearate and to identify differences in the physical properties of the two using thin layer chromatography.

Due to the extreme flammability of Hydrogen gas, Ammonium Formate was used, which is readitly converted to Hydrogen in the presence of a catalyst.
It was formed in situ and so the Hydrogen formed went on to hygrogenate the Methyl Oleate at once, again with the help of the catalyst.

Procedure:-
to a solution of Methyl Oleate (0.6ml, 1.76mmol), 10% Palladium on Carbon (44mg) and Methanol (10ml, 247mmol) was added Ammonium Formate (219mg, 3.48mmol) and swirled frequently as an exothermic reaction proceeded.

Three thin layer chromatography samples were taken ten, thirty and sixty minutes after its addition to track the prgress of the reaction.
Hexane (10ml, 76.1mmol) was then added, yielding two liquid phases which were mixed thoroughly and settled with the clear Hexane layer floating on a black Methanoic solution containing the catalyst.

The Hexane layer, which should have contained our product, was obtained by filtering the solution through a small plug of cotton wool and Celite in a pipette and then we attempted to remove the solvent on a rotovapour to get a solid product.
However, what remained was an oily mixture of what we believed to be Methyl Oleate and possibly our product solubilised in it.

Once 0.5g remained in the flask we realised we would not obtain any crystals because we would expect to gain this amount of solid product so evaporation was ceased.
Results:-

The ten and thirty minute thin layer chromatography samples yielded very similar results when stained as shown on the opposite page with Rf values of around 0.5 for both substrate and product, although it is likely no product had formed and the plate had simply entered the eluent tank at an angle.
At sixty minutes, Iodine stained all three columns as shown.

No further results were gained as a solid product was not obtained.
Conclusion:-

we appear to have gained no, or very little, product due to, we believe, inactivity of the provided catalyst.
There were indications that the reaction was not occurring as hoped from the start.

When Ammonium Formate was first added we expected Carbon Dioxide and Ammonia gas to be evolved yet nothing was observed.
To have been satisfied that our reaction had gone, there would have been distinct spots in the co product column of the thin layer chromatography plates that matched with a single spot in the substrate and product columns.

It was the case with ours that one large spot developed over all three columns making it difficult to detect individual marks and the Rf values calculated appeared very similar in this solvent.
It may have been beneficial to use a different eluent for clarity.

When our sixty minute plate was exposed to Iodine we would have hoped to see staining only on the substrate and co product columns due to the addition of Iodine to the double bond.
However, some staining was detected on the product showing it to be saturated still.

Analysis of all these things led us to believe we may not have been successful.
The proof came when evaporating our final solution.

It was expected that once all the Hexane had been removed, a solid would be left of mass ~0.52g.
An oily substance remained and when weighed it was found to have a mass of 0.50g.

It was clear then that we would not gain a solid and that Methyl Oleate had been collected and our catalyst had indeed been corrupted.
Aim:

To make up n approximately 0.1M potassium hydrogenphthalate solution accurately and use it to standardise the approximately 0.1M solution of sodium hydroxide provided in the laboratory.
Through doing this I will:

Familiarise myself with what a primary standard isAcquaint myself with the balances used in the laboratoryRemind myself of the techniques employed in carrying out a titrationRemind myself how to calculate the result of a titrationGain a realistic estimate of the errors associated with standardisation
Theory:

When carrying out a titration, a solution of unknown strength is titrated against a standard solution that has a known concentration.
The standard solution e.g. sodium hydroxide, is usually obtained from the laboratory and its concentration (molarity) is given on a label.

To make this solution solid sodium hydroxide pellets have been weighed out and dissolved in a known volume of water.
However the pellets are very hygroscopic (water absorbing) and this process occurs during weighing so that the exact amount of NaOH used is unknown.

The final solution is also frequently contaminated with carbon dioxide which reacts to form sodium carbonate.
Sodium hydroxide is therefore not a primary standard.

A primary standard is a stable, non-hygroscopic, pure solid material which can be weighed out accurately and dissolved in water to give a solution of accurately known concentration.
A useful primary standard acid is potassium hydrogenphthalate, a monoprotic acid derived from the diprotic phthalic acid.

It has a fairly high molar mass (reducing weighing error) but a low solubility in water (<0.5M at 25 oC).
Potassium hydrogenphthalate: The following reaction occurs when sodium hydroxide is added to potassium hydrogenphthalate:

Method:
Accurately weigh out between 1.8 and 2.2g of solid potassium hydrogenphthalate into a tared weighing boat.

Transfer the solid to a 100cm 3 granulated flask, washing off any powder on the boat into the flask.
Dissolve the solid in about 80cm 3 of distilled water.

When the solute has dissolved completely (this may require shaking of the flask), make up to the mark with distilled water.
Transfer some of the 0.1M sodium hydroxide solution provided in the lab to a 50cm 3 burette.

Pipette 25cm 3 of the potassium hydrogenphthalate solution into a 250cm 3 conical flask.
Add two drops of phenolphthalein indicator to the flask, (this changes from colourless to pink at a pH of 9) and with a white tile under the flask carry out one rough and two accurate titrations.

Swirl the flask regularly during the titrations and place a piece of paper behind the burette to accurately read the meniscus.
Results:

Mass of potassium hydrogenphthalate weighed: 1.8320g
Calculations and Discussions:

(a) Moles of potassium hydrogenphthalate used: FORMULA One mole of potassium hydrogenphthalate reacts with one mole of sodium hydroxide.
Therefore 2.2427 x 10 -3 moles of potassium hydrogenphthalate reacted with 2.2427 x 10 -3 moles of sodium hydroxide.

Average of the two accurate titrations = 25.2750cm 3.
So 25.2750cm 3 of the sodium hydroxide solution contained 2.2427 x 10 -3 moles of sodium hydroxide.

Concentration of sodium hydroxide solution: FORMULA (b) An estimation of the percentage uncertainty in my answer: Weighing uncertainty: ± 0.00005g = 0.00273% Volumetric flask uncertainty: ± 0.05ml = 0.05% Pipette filling uncertainty: ± 0.006ml = 0.024% Burette reading uncertainties: ± 0.05ml = 0.19782% x 2readings = 0.39565% End point detection uncertainty: ± 0.05ml = 0.19782% B-grade glassware uncertainties:0.2% x 3(volumetric flask, pipette, burette) = 0.6% σ = √∑ each uncertainty 2 FORMULA (c) The concentration of the sodium hydroxide solution can only be given to three significant figures because this was the lowest precision of all the raw data (the burette readings).
(d) Calculation from section 1.6: A titration of a solution of H 2SO 4 (in flask) against a 0.1079M solution of NaOH (in burette) is carried out.

It requires 26.43cm 3 of NaOH to reach the end point of the titration with 25.00cm 3 of H 2SO 4.
We want to find the concentration of the H 2SO 4 solution.

FORMULA One mole of H 2SO 4 reacts with two moles of NaOH.
So the amount of H 2SO 4 in the flask was: FORMULA This was contained in 25.00cm 3.

FORMULA
Aim:

To look at the effects three different acids have on the freezing point of water.
Through experiment the depression of water's freezing point will be studied and the results used to calculate the Van't Hoff factor for each acid and therefore the degree of dissociation of each acid.

Theory:
A colligative property is one that depends only upon the concentration of a solute and not upon its nature.

The colligative properties of electrolyte solutions are more complicated than those of non-electrolytes because the solute dissociates into free ions and because the ions in the solution are then subject to strong interactions.
However colligative properties can be used to give a rough indication of the number of particles present in a solution, and hence the extent of dissociation of an electrolyte in solution.

The depression of a freezing point is one example of a colligative property.
The depression, ∆T = T fo - T f is proportional to the concentration c of the solution.

When defined in this way ∆T is positive.
In the case of an electrolyte which dissociates into i particles when it dissolves, ∆T is proportional to ic.

Hence: FORMULA k f is the molal freezing-point depression or cryoscopic constant for the solvent.
For water, k f = 1.860 K mol -1 dm 3, but it is different for other solvents.

i is the Van't Hoff factor.
It is 1 for non-electrolytes and 2 for strong 1:1 electrolytes like NaCl which are fully dissociated into two ions.

For a weak acid HA, with a degree of dissociation α: FORMULA For a solution of a monoprotic weak acid of concentration c and degree of dissociation α, the acidity constant K a = α2c/(1- α) ≈α2c if α is small.
Different equations apply for polyprotic acids.

Strictly colligative properties depend on the molality (moles of solute per kg of solvent) rather than the concentration or molarity (moles of solute per dm 3 of solution), but the difference between the two scales is small for aqueous solutions and was neglected in the experiment.
Molality is independent of temperature and pressure whereas concentration is not, so k f was taken as 1.860 K mol -1(dm solution)3, properly it is 1.860 K mol -1(kg solvent).

The value k f can be calculated from the relation: FORMULA R is the gas constant, 8.314 J K -1 mol -1.
T f is the freezing point of pure solvent.

M is the molar mass of the solvent in kg mol -1.
∆H ofus is the molar heat of fusion of the solvent.

Method:
150cm 3 of; distilled water, 1.5M ethanoic acid, 1.5M hydrochloric acid and 1.5M sulfuric acid were placed in separate 250cm 3 conical flasks and all were immersed in a bowl of crushed ice.

The flasks were shaken frequently until the temperature of each solution was below 3 oC.
Four Dewar flasks were filled two-thirds full with crushed ice.

Just enough of one chilled solution was added to each flask to cover the surface of the ice.
Each ice solution mixture was stirred thoroughly.

After 5 minutes the calibration of a -20 to +20 x 0.1 oC thermometer was checked by immersing it in the Dewar flask containing ice and the water, stirring, and waiting until the reading was steady.
The thermometer was then transferred to the Dewar flask containing the ethanoic acid-ice mixture.

The mixture was stirred and the temperature recorded once the reading had steadied, with the thermometer in the middle and not the bottom of the ice.
Three 5cm 3 samples of the liquid phase were then removed using a 5cm 3 pipette with a polymer-wool filter.

The three samples were set aside and later titrated against 0.3M sodium hydroxide from a 50cm 3 Teflon tap burette, using phenolphthalein as an indicator.
This operation was repeated for each of the other acids.

Results:
Exact concentration of 0.3M NaOH: 0.29765 mol dm -3 Reading of thermometer in melting ice: 0.10 oC Results for each acid: FORMULA Mean Volume delivered = 19.63 FORMULA Mean Volume delivered = 17.50 FORMULA Mean Volume delivered = 37.20 (3rd titration anomalous so not included in mean)

Calculations and Discussion:
(i)

Concentration of CH3CO2H: Moles of NaOH titrated: FORMULA One mole of NaOH reacts with one mole of CH 3CO 2H, so 0.005843moles of NaOH reacted with 0.005843moles of CH 3CO 2H.
0.005843moles contained in 5cm 3.

FORMULA Concentration of HCl: Moles of NaOH titrated: FORMULA One mole of NaOH reacts with one mole of HCl, so 0.005209moles of NaOH reacted with 0.005209moles of HCl.
0.005209moles contained in 5cm 3.

FORMULA Concentration of H2SO4: Moles of NaOH titrated: FORMULA One mole of NaOH reacts with two moles of H 2SO 4, so 0.01107moles of NaOH reacted with 0.02214moles of H 2SO 4.
0.02214moles contained in 5cm 3.

FORMULA
(ii)

Values of i for: FORMULA There is a high percentage error in these results for the Van't Hoff factor because of the inaccuracies in the method.
The volumes of the acids and the ice and water were not measured accurately, the thermometer only read the temperature to 0.1 of a degree and there was a time delay between extracting each of the 5cm 3 samples for titration.

There were also inaccuracies due to the b-grade glassware used, and noting the end point of each titration.
(iii)

The calculated values of i (the Van't Hoff factor) for each acid give an indication of the extent of dissociation of each acid.
CH 3CO 2H had the lowest value of 0.824, this suggests that it is a weak monoprotic acid, weaker than HCl and H 2SO 4 because it only partially dissociates.

HCl had a value of 1.613, suggesting it is a stronger monoprotic acid which is more fully disassociated.
H 2SO 4 had the highest value of 1.720, this would suggest it is the strongest acid of the three but the high value could be due to the fact that it is a diprotic acid and therefore can loose two protons per molecule and so would dissociate into a greater number of aqueous ions.

(iv)
Ka value and degree of dissociation for each acid: FORMULA HCl is a strong acid and is fully dissociated so i = 2.

FORMULA The first stage of dissociation is complete so i = 2 Second stage: FORMULA My values for i are lower than the calculated values but they are in the same order for the degree of dissociation.
The lower values may just be due to inaccuracies in my method.

(v)
Molality of a solution of hydrochloric acid of concentration 0.5000mol dm -3, which has a density of 1.0090g cm -3 or 1.0090kg dm -3.

Molality FORMULA
Aim:

To separate the components of a simulated pharmaceutical preparation.
Theory:

Most commercial preparations are mixtures of many different substances.
To obtain a pure organic compound from such a mixture, one must separate the wanted compounds from other components by using the differences in physical and chemical properties.

Organic materials Tends to have very different solubility's in differing organic solvents and can often be separated by filtration/extraction.
Organic compounds with functional groups such as amino and carboxylic acid can be converted to their water soluble salts, which can then be separated from insoluble components of a mixture, these salts can then be converted back to an organic soluble material and recovered.

Reaction equation:
FORMULA

Saftey/Hazards:
Pharmaceutical preparation (hazards unknown)Ethyl Acetate (highly flammable)Sodium Hydroxide (2M) (corrosive)Hydrochloric acid (2M) (corrosive)Ethanol (Flammable, toxic)

Precautions to be taken, appropriate lab wear to be worn including lab coats, goggles, gloves and hair to be tied back.
All work is to be carried out in a fume hood.

Experimental method:
To begin the sample of pharmaceutical preparation - sample A (4g) was placed in a conical flask (100ml) with ethyl acetate (50ml) and swirled thoroughly; a semi-cloudy solution was formed.

The insoluble material was followingly filtered at the pump; left to dry, weighed (2.93g) and its melting point taken.
The filtrate was transferred to a separatory funnel and 2M Sodium hydroxide (25ml) was added.

The funnel was stoppered and shaken frequently opening to release any pressure.
The two layers were then allowed to separate and the aqueous layer was run off.

This process was the repeated with a further portion of 2m Sodium hydroxide (25ml) and the two aqueous layers combined.
Aqueous 6M hydrochloric acid (20ml) was added slowly, whilst shaking to the combined aqueous layers.

The solution warmed and a white precipitate was formed.
The Ph of the solution was taken with indication paper (PH1) to check the Ph was below 2.

The mixture was then cooled on ice and the precipitate collected by vacuum filtration.
The filtrate was washed with distilled water, and the solid left to dry under suction for 10 minutes.

The weight of the crude sample was taken (1.4g) and followingly it was recrystallised from ethanol.
The purified product was weighed (0.93g) and a melting point and Ir spectrum taken.

Magnesium Sulphate was added to the ethyl acetate (remaining) layer and swirled creating a "snowstorm", This was then filtered at the pump into a pre-weighed round bottomed flask (100ml) washing with ethyl acetate.
Following this the solvent was evaporated on the rotary evaporator.

Mass of the crude solid was taken (0.8g) and then recrystalised from ethanol/water mix (ethanol added under heat until product dissolved, water added until solution went cloudy, ethanol added to clear solution).
Finally weight of the purified product was taken (0.77g) and a melting point and IR spectrum recorded.

Results:
From these spectrums we can check the identity of our final products.

Both spectrums clearly show the carbonyl group (C=O).
In addition the spectrum of Aspirin shows peaks representative of O-H (3020cm -1) and C-O (1216cm -1).

Acetanilide also shows a peak of 3296cm -1; representative of a N-H bond.
These spectrums clearly support the presence of our product.

Discussion:
The sodium salt is much more soluble in water than the organic solvent.

Hence the compound will be found in the aqueous layer.
By addition of HCL; the organic acid is re-created; and being far less soluble in water it precipitates out of solution.

Aim/Experimental purpose
To prepare p-bromoaniline, by facilitating the mono bromination of aniline through the addition of a protecting acetyl substituent - this reduces the activating influence of the amino group.

Reaction equation
Safety hazards

Material HazardAniline Toxic, cancer suspect agentAcetic anhydride Corrosive, IrritantGlacial Acetic Acid Corrosive, irritantAcetanilide Unknown hazard, product of stage 1Bromine Highly toxic, corrosiveSodium metabisulfate Irritantp-bromoacetanilide Unknown hazard, product of stage 2Conc.
Hydrochloric acid Corrosive25% Sodium hydroxide solution Corrosive

Precautions: Appropriate lab wear; lab coat; goggles; gloves.
Work carried out in a fume cupboard.

Experimental details/Method
Stage 1

To begin aniline (10ml) was dissolved in glacial acetic acid (25ml), following this acetic anhydride (12ml) was added and the reaction mixture shaken (the reaction vessel warmed).
The mixture was allowed to stand for 5 minutes, and then diluted with water (~150ml) until crystallisation occurred (white precipitate formed), this was then filtered at the pump, washed with water and allowed to dry in air.

The crude product was re-crystallised from ethanol, dried, weighed (4.53g) and the melting point taken (69-75 0c).
Stage 2

The purified acetanilide (4.53g) from stage 1 was dissolved in cold glacial acetic acid (25ml) in a 250ml conical flask.
A solution of bromine (2.1ml) was made up in glacial acetic acid (30ml) and added to the acetanilide solution.

The mixture was left to stand for 15minutes, and following this poured into cold water (300ml), (an orange precipitate was formed).
The precipitate was filtered at the pump and returned to the original conical flask, this was stirred with further water (150ml) containing sodium metabisulfate (2g).

This mixture was again filtered at the pump, left to dry and weighed (4.06g), 2.1g of this was taken onto stage 3.
The remainder 1.96g was re-crystallised, using ethanol as a solvent, filtered at the pump, weighed (0.87g) and a melting point taken (174-177 0c).

Stage 3
The 2.1g of p-bromoacetanilide was treated in a 100ml round bottomed flask with water (25ml), concentrated hydrochloric acid (25ml), and the addition of a few bumping granules.

A condenser was fitted to the flask and following this the mixture was heated under reflux until the solid dissolved, heating for a further 10 minutes after the solid had dissolved.
Subsequently the mixture was cooled on ice and 25% sodium hydroxide solution was added until the PH was alkaline.

Cooling on ice and whilst scratching crystallisation occurs, this was then filtered at the pump.
The product was re-crystallised from hot ethanol and the pure p-bromoaniline filtered at the pump, weighed ( 0.3g), and a melting point taken (61-65 0C ).

Results
Criterion of purity

FORMULA FORMULA
Percentage yields

Stage 1
FORMULA Theoretical yield FORMULA Percentage yield = FORMULA

Stage 2
FORMULA Theoretical yield FORMULA Percentage yield = FORMULA

Stage 3
FORMULA Theoretical yield FORMULA Percentage yield = FORMULA

Mechanistic equation for each step in the preparation of p -bromoaniline from aniline
STAGE1 STAGE 2

Why does the bromination of acetanilide stop at the mono-bromo stage?
The bromination of acetanilide stops at the mono-bromo stage as the nitrogen atom in an amide is much less basic than in an amine as it is conjugated with the carbonyl group.

Amine is more reactive due to the increased electron density in the ring however in the amide, nitrogen cannot donate electrons as effectively to the ring system due to the conjugation and hence substitution is far more controlled and ends at the mono-bromo stage.
Why does this bromination occur only at the para - (4-) position?

Bromination occurs only at the para position, as the substitution of bromine deactivates the benzene ring - withdraws electrons from the ring and therefore stops the resonance at the para position.
In addition to take the ortho position the electrophile would encounter significant steric hindrance due to the large substituent e.g. amide, and hence the para position is more energetically favourable.

Aim:
To carry our diazonium coupling reaction between a Diazonium Salt and phenol with reaction at the para position.

Reaction via an electrophilic aromatic substitution mechanism.
Theory:

Diazonium coupling reactions are examples of electrophilic aromatic substitution reactions.
In this experiment the diazonium salt is the electrophilic, which reacts with an electron rich nucleophile (phenol).

This reaction occurs mostly in the para position however is can be facilitated in the ortho position by protecting groups placed in the para positions.
The product of this experiment, an Azo-coupled product are commonly used as dyes as the extended conjugation of the Π electron system causes them to absorb in the visible region of the electromagnetic spectrum.

Safty/Hazards:
Chemicals:

Anthralic acid (toxic) C. HCL (corrosie) Sodium Nitrite (Toxic) Phenol (Toxic) Sodium Hydroxide(Corrosive)
Precautions to be taken, appropriate lab wear to be worn at all times including, lab coat, goggles, gloves and hair tied back.

All work is to be carried out in the fume hood.
Experimental method:

To begin anthranilic acid (1.7g) and distilled water (25ml) were mixed in a conical flask.
Concentrated sulphuric acid (4ml) was then added whilst swirling the solution.

The mixture was then warmed until the amine dissolved, and then cooled on ice until below 10oC.
Following this Sodium nitrite (0.9g) and distilled water (10ml) were missed in a conical flask and cooled on ice until below 10 oc.

The sodium nitrite solution was then added to the amine solution, slowly and with swirling, ensuring the solution temperature did not exceed 10 oC (A yellow solution is created).
Next Phenol (1.2g, 12.7 mol) and Sodium hydroxide solution (2.5M, 25ml) were heated together in a conical flask until the phenol dissolved.

This solution was then cooled on ice until below 10 oC.
The diazonium alt solution was then added to the phenoxide solution ( a lumpy precipitate formed) and left to stand for 5 minutes.

The mixture was then acidified with hydrochloric acid (4M) to PH1.
This solution was then filtered at the pump, and washed with ice cooled water (3*10ml), leaving the crude products to dry under suction for 5 minutes.

This product was followingly transferred to a conical flask and ice cold methanol (15ml) was added, the mixture was then swirled and filtered at the pump, leaving to dry under suction for a further 5 minutes, The yield and melting point were then taken and the product recrystalllised from methanol.
The purified product was then weighed, a melting point taken and a TLC plate run (eluting the plate with a 1:1 toluene: ethyl acetate mix)

Results
TLC analysis - Rf values 0.8 - product; our TLC plate was inconclusive and did not seem to develop properly.

Discussion:
Reaction mechanism to rationalise the formation of the Azo dye: To begin substitution of hydrogen by NO2 through Nitration (reaction with concentrated Sulphuric acid and Nitric acid, at a temperature below 55oC) To follow this reduction with h2/Ni, Nitro groups are deactivating and meta directing.

After this Nitrobenzene is reacted with concentrated sulphuric acid under reflux.
Finally Thionyl chloride is reacted with the reaction mixture; SOCl2; then (NH4)2CO2, an Amine source.

Aim:
To study the results of titration's with a sparingly soluble salt, and to investigate the effect of differing electrolytes.

Theory:
The equilibrium established between a saturated solution of a slightly soluble salt in contact with excess solid is a heterogeneous one.

The product of the concentrations of the cation (M+) and anion (M-) in a saturated solution of a slightly soluble electrolyte is constant at a given temperature (solubility product).
The relationship between the solubility (molar concentration) and the solubility product (Ks) depends on the charge type of the electrolyte (MX, MX2).

Different electrolytes can have differing effects upon solubility; for example an electrolyte that already contains concentrations of the salt cations/anions will lesson the solubility of the respective salt.
The common ion effect reduces the salt solubility but the solubility product remains unchanged (salting out).

The addition of "inert" ions increases solubility of a sparingly soluble compound (salting in), this is due to ionic interactions and the effect is calculated via the activity coefficients for each ion.
For solutions below 0.01M it is fair to dis-regard ionic interactions however more concentrated solutions or were inert salts are present this is not a reasonable assumption.

Values of activity coefficients are independent of the electrolyte at ionic concentrations below 0.1M, above this concentration values differ.
The true thermodynamic solubility product, Ks th, takes into account these mean ionic activity coefficients.

In this experiment, the slightly soluble salt is Potassium Periodate.
Three saturated solutions are also prepared, one being in pure water, another in a solution containing NaNO3 and our final being in a solution also containing KNO3.

The concentration of IO4- in each saturated solution is obtained by converting IO4- to I2 (reduction by acidified I-) followed by titration of the I2 with standard Sodium Thiosulfate.
Procedure

Hazards/Safety:
Chemicals: Potassium Periodate, Potassium Iodate, Sodium Thiosulfate, Sulfuric acid (1.5M).

Sodium Nitrate (0.2M), Potassium Nitrate(0.2M).Precautions : Appropriate lab wear to be worn - Lab Coat, Goggles, Gloves, hair tied back.
All work carried out in a fume cupboard.

Procedure:
To begin 250cm3 of Sodium thiosulfate (0.2M) was prepared and a trace of sodium Carbonate was added.

A portion of the Sodium Thiosulfate solution was diluted tenfold to give a 0.02M, Sodium Thiosulfate solution.
Three bottles of solution were made up, in each bottle, 1g of Potassium Periodate was placed.

Following this in bottle A 100cm3 of distilled water was added, Bottle B - 100cm3 of NaNO3 (0.2M), and bottle C - 100cm3 of KNO3 (0.2M).
Each bottle was vigorously shaken for 3 minutes, allowed to settle and the temperature taken.

The samples were the filtered into conical flasks.
To carry out each titration 25cm3 of solution was taken with the addition of KI (2g) and sulphuric acid (20cm3, 1M).

-Solutions A and B were titrated against the more concentrated Thiosulfate -Solution C was titrated against the more dilute Thiosulfate 2 concordant results were obtained for each electrolyte.
Results:

FORMULA Our results demonstrate the common-ion, "salting out" effect, this can be seen in solution C, KNO3, The Solubility of KIO4 is greatly decreased, and therefore as seen above the concentrations of the cation and anion are greatly decreased and hence the values of Ks and Ks th are greatly reduced.
Aim:

To investigate how the boiling point of Cyclohexane varies with pressure.
Introduction and theory:

The temperature at which the vapour pressure is equal to 1atm is the normal boiling point of that liquid.
At some temperatures the energy of molecules is great enough for bubbles of vapour to form - the liquid boils.

However at any lower temperature the bubbles collapse as soon as they form because the atmospheric pressure is greater than the vapour pressure inside the bubbles.
Hence the boiling point of a liquid is the temperature at which the vapour pressure equals the external pressure.

When varying pressure; if a lower pressure is exerted on the liquids surface, less kinetic energy is needed to form bubbles and hence the temperature of the boiling point is less than for a higher pressure.
Boiling point therefore depends on applied pressure (Vapour pressure increases with temperature) The Clausis-Clapeyron equation relater the temperature dependence of the vapour pressure of a liquid to the heat of vaporisation and can be used to find this figure.

(dIn p/d (1/T) = -ΔHvap/R).
Troutons rule states that for most un-associated liquids, ΔHvap is roughly independent of temperature; this is found by, ΔHvap being roughly equal to KT b.

This is found to be true for most un-associated liquids other than those with extremely strong intermolecular forces, eg. Hydrogen bonding.
Saftey/precautions:

Chemicals: Cyclohexane (Flammable)Electrical: Vacuum pump (avoid getting mains connection wet), Heater.Other: Glass vacuum apparatus (wear safety glasses)
Appropriate lab wear must be worn including lab coat, gloves and goggles.

All Chemical work should be carried out in a fume hood.
Procedure:

To begin the apparatus are set up; the boiling vessel (Cottrell pump), condenser and ballast flask are connected; in turn this is connected to a pressure gauge and the pressure gauge to the vacuum pump.
Both taps on the pressure gauge are tightened and the pressure gauge switched on (the reading was 1000mbar).

Cyclohexane (70cm3) and pumice granules were placed in the boiling vessel and the joints lightly greased.
The pressure was adjusted to 400mbar; and the tap connected to the closed pump.

The water supple to the condenser was turned on,.
And the thermometer greased with glycerol.

The Cyclohexane was then heated until boiling freely and the temperature at this point recorded.
Pressure was then adjusted (by opening pressure gauge to pump tap) in steps of 100mbar to 1000mbar and using a viewer the temperature taken when the liquid boiled freely.

Results:
A graph of pressure against temperature was plotted (graph 1), it can be seen that as the pressure increases; the temperature of the liquids boiling point also increases.

A graph of Inp against 1/T was plotted (graph 2), from this graph we can obtain a value of ΔHvap; by working out the gradient and inserting this figure into the Clausis-Clapeyron equation.
Discussion:

Our value for -ΔHvap = 0.9kjmol-1, in comparison to a literature value for the -ΔHvap of cyclohexane = 30.8kjmol-1.
From our experiment cyclohexane appears to obey Troutons rule.

Troutons rule states that for most un-associated liquids, ΔHvap is roughly independent of temperature.
From our results we can gather that the normal boiling point of cyclohexane at 1000mbar =62.5 oC.

Aim:
The aim of this experiment is to determine the composition of a commercially available alcoholic beverage (strongbow) by analysis using the technique of gas chromatography.

Introduction/Background:
The basis of chromatography lies in a seperative process between 2 phases, these being the stationary phase and the mobile phase.

The mobile phase, and injected sample flow across the stationary phase, and each component of the sample will be distributed between the two at differing amounts due to their relative affinity for each phase.
The distribution ratio is determined by the physical and chemical characteristics of the sample, however it is also affected by operating conditions such as temperature and pressure.

Temperature must be carefully selected, as a compromise between elution time and resolution quality.
A high temperature is needed to cause vaporization of the volatile liquid however elution time is very fast and resolution is therefore resolution is poor.

However lower temperatures cause spreading of the peaks and therefore sensitivity is reduced.
Ideally a chromatogram will show a series of separate peaks, each of which corresponds to a component of the injected sample.

The sequence of elution corresponds to the retention time of each component upon the stationary phase of the GC column.
The area of each peak is related to the concentration of each component and therefore gas chromatography can be used as a quantitative method.

During the specific process of gas chromatography, a sample is injected into a stream of gas, which is programmed at a specific temperature; usually 50 oC above the boiling point of the least volatile component.
The column temperature can then either be kept constant (isothermal) or the temperature can be programmed to progressively increase to improve the separation.

The components of the sample will then distribute themselves according to their relative affinities for the stationary phase, and are then recorded and quantified by measuring a physical property.
The most common detector to find in a GC system is a flame ionisation detector.

Method:
To begin a calibration mixture was prepared in triplicate of the following composition: 1000L of 25mg% propan-1-ol internal standard solution and 400L of 80mg% ethanol standard solution; this was taken by micro syringe into clean dry clip top vials which were flowingly capped to prevent loss of ethanol.

In an identical manner, a second calibration mixture was prepared in triplicate, this was of the following composition: 1000L of 25mg% propan-1-ol and 400L of 200mg% ethanol solution.
Vials were then capped, again to prevent loss of ethanol.

A sample of the alcoholic beverage (strongbow) was then prepared.
To do this 20mL of the alcoholic beverage was measured and placed into a volumetric flask, this was then degassed using an ultra-sonic bath (for 15minutes).

From this degassed sample a solution was prepared in triplicate by pouring the solution into a beaker and taking 2mL of the degassed drink by micro syringe, placing this into a 50mL volumetric flask and diluting to volume with distilled water (dilution factor of 1:25).
The samples for analysis were then prepared by taking 1000L of 25mg% propan-1-ol internal standard solution and 400L of the diluted drink (also from a beaker) by micro syringe.

These samples were also made up I n triplicate and the vials stoppered to prevent loss of ethanol vapours.
Moving on a "Perkin Elmer - auto system, gas chromatograph" was set up to the following parameters: a pressure of 16psi, an injector and detector temperature of 200 oC and a column temperature of 110 oC (isotherm).

The carrier gas used was Helium, the column type was a capillary tube composed of fused silicate coated in polyamide being 30m long and 0.25mm in diameter and having a 0.25m film coating inside.
Hence the stationary phase is 0.25m in diameter and was composed of carbowax polyethylene glycol base.

The detector used in this experiment was a flame ionisation detector (FID).
To utilise the gas chromatograph a sample of 1L was taken of each mixture, to do this the sample was taken by syringe plunger, the syringe was filled with the sample mixture 3 times before a sample was injected (to ensure an uncontaminated sample).

The sample was then injected through the septum on the top of the instrument, by a swift movement and then the integrator stripped by pressing the "run" button on the chromatograph.
The syringe was withdrawn after 3 seconds, and the created chromatogram printed.

This process was repeated for all prepared samples.
Results:

Questions:
Briefly explain the mechanisms of the seperative process that is occurring within the column in a GC system.

What factors can be altered to affect the separation?
Within a GC column there are two phases, mobile and stationary.

Generally in Gas chromatography the stationary phase is composed of a solid (inert) support coated with a liquid phase.
The liquid phase is chose after considering the polar characteristics of the analyte.

The sample/analyte will be passed across the stationary phase as a component of the mobile phase (a carrier gas), each component in the sample will have a relative attraction/interaction with each phase, this will determine each components retention time on the column.
The retention time is determined by the chemical and physical properties of each component and instrumental/method differences.

The following factors affect the separation and hence the retention time sof components on the column.
Volatility of compound: The more volatile a component, the faster it will travel through the column, and hence the faster its elution time.

Polarity of compounds: Polar compounds will travel more slowly especially if the column is also polar, this is due to increased interactions between the analyte and the stationary phase.
Column temperature: At higher temperatures sample components will spend most of their time in the gas phase and hence elution time will be quicker.

However this means that chromatogram resolution is poor.
C olumn packing polarity: Non-polar liquids tend to be non-selective and therefore elution order is determined by the boiling points (volatility) of the components, the lower boiling point components being eluted first.

However with polar liquids volatility is not so easily determined due to interactions between the analyte and the stationary phase, hence polar columns tend to show slower elution times but also a larger effect on relative retention times.
Flow rate/Pressure of the gas through the column: An increase in either of these factors will provide faster elution times of the sample components.

Length of the column: Longer columns will mean a heightened elution time, this in turn also means better separation.
Describe a method by which non-volatile alcohols or carboxylic acids may be analysed by GC.

To analyse compounds, which are non-volatile in nature, chemical derivation is required.
Alcohols and Carboxylic acids are low in volatility due to their ability to hydrogen bond, this creates a locked together structure which requires much energy to break.

Derivation works to mask the hydrogen bonding, a common way to do this for alcohols and carboxylic acids is to derivitise to an ester.
This can be done by the addition of 1M solution of potassium hydroxide in 95 % ethanol (2 mL), refluxed for ~1hour.

The solution is then cooled and water (5 mL) is added, following this the mixture is extracted thoroughly with hexane-diethyl ether (1:1, v/v; 3 x 5 mL).
The solvent extract is washed with water, dried over anhydrous sodium sulphate and the non-saponifiable materials are recovered on removal of the solvent in a rotary evaporator.

The water washings are added to the aqueous layer, which is acidified with 6 M hydrochloric acid and extracted with diethyl ether-hexane (1:1, v/v; 3 x 5 mL).
The sample is recovered after washing the extract with water, drying it over anhydrous sodium sulphate and removing the solvent by evaporation.1

Conclusions
Analysis of our results show there to be a 2.43mg % alcohol content of the commercial alcoholic beverage.

The quoted value is of 4mg%, our results may be subject to many experimental errors, such as the injection of our sample, caused by different operators injecting each sample, and errors in pipette use.
In addition there may be slight error caused by the computer integration of the peaks produced in the chromatogram.

Aim:
The aim of this experiment is to determine the concentration of F- in a series of metal fluoride solutions.

Introduction/Background:
Electrochemical methods of analysis are based on the theory of galvanic cells.

In a galvanic cell two electrode systems are interested into a medium of ions.
The electrodes are then joined externally forming a circuit and the electron flow, which is established, causes a potential difference.

This works on the theory of a spontaneous chemical reaction which sets up a potential difference, created by the 2 half reactions at each electrode.
The electromotive force of the galvanic cell is dependant upon the difference between the potentials of the 2 electrodes.

Cells are normally formed by combining a standard/reference electrode with an electrode whose potential or ionic behaviour is under study.
Potential is dependant upon solution concentration therefore this technique can be used to determine concentrations via calibration plots of known concentrations.

The most used reference electrode is the calomel electrode; this is composed of mercury and mercurous chloride (calomel) in contact with a solution containing chloride ions (potassium chloride).
The concentration of the chloride determines the absolute potential; the electromotive forces however are also dependant upon temperature.

The measuring electrode can be one of many types:
Metal in contact with a solution of its own ions.Inert material in contact with ions in 2 different oxidation states.Membrane separating 2 solutions of the same ion at different concentrations.

Experimental method:
To begin a solution of NaF (0.1M) was taken and diluted to the following concentrations, 10mM, 1mM, 0.1mM, 0.01mM, and 0.001mM.

This was achieved by using a pipette to transfer 10mL of the NaF solution into a 100mL volumetric flask, and diluted to volume.
From this was then taken 10mL and further diluted to volume, and so on for the other concentrations (dilution by a factor of 10).

Into separate beakers (Polythene, 100mL), labelled A-E was transferred 25mL of each of the standard solutions.
Following this into polythene beakers labelled F-J was added 25mL of each metal fluoride solution (BaF 2, MgF 2, PbF 2, SrF 2, CaF 2), care was taken not to shake the saturated fluoride solution and the solution was pipette-ed from the top to avoid uptake of un-dissolved solid.

To each of these polythene beakers was added 25.0mL of TISAB reagent, this was mixed well and subsequently placed in a thermostat-ed water bath, allowing the solution temperatures to equilibrate.
Following this each solution was taken in turn and its potential difference measured.

Electrodes were connected to the potentiometer, washed with distilled water and placed in the first standard solution (10mM), the potential was allowed to stabilised (around 1min) and the potential difference read.
This was then repeated with the other standard solutions (B-E) and samples (F-J), washing the electrodes with distilled water between each measurement.

Results:
Questions:

Describe the construction of the fluoride selective electrode.
The Fluoride ion selective electrode is one of the most important and useful as the detection of the fluoride ion is rather difficult by most other methods.

The ion selective electrode (solid state electrode) falls into the category of a membrane-separating electrode.
This in the case of the fluoride selective electrode is comprised of a single crystal of lanthanum fluoride doped with some europium (II) to increase the conductivity of the crystal; this forms the membrane of the cell.

The electrode contains an electrolyte solution that is typically composed of NaF and NaCl, the membrane of the cell separates different concentrations of this solution.
The ion selective electrode is then used in conjunction with a standard Calomel electrode, both being dipped together into a solution containing Fluoride ions.

State the Nernst equation with respect to the fluoride ion
Nernst equation FORMULA With respect to the Fluoride Ion: FORMULA

What gradient would you expect for the calibration of PD vs. log[F-] and how does this compare with your results?
The gradient of our plot is calculated to be, -0.0209, I would expect a value very close to zero due to the (Expected) linear nature of the calibration plot, linear increase of potential difference with fluoride ion concentration, this relationship is proportional.

The R 2 value is found to be 0.9806, standing as proof and as a measure of the linear relationship.
Why is the TISAB reagent required?

TISAB serves to minimise interferences with the fluoride electrode.
Dilution with this buffer will provide a high ionic strength background, which will swap out any moderate variations in ionic strength between solutions.

This keeps the activity coefficient of the fluoride ion constant from solution to solution.
The buffer contains CDTA, a chelating agent that prevents any complex formation with the F- ion, therefore also maintaining its activity from solution to solution.

Comment on the shape of the curve for [F-] vs. ionic radius.
This plot can be seen in results - "Plot of [F-] vs. Ionic radius", the shape of this curve is that of a parabola.

Why has CaF2 the lowest solubility?
Calcium Fluoride has the lowest solubility due to its high lattice enthalpy of 2602kJ mol -1.

It occurs naturally as the mineral Fluorite, this adopts the calcium fluorite structure, the fluoride ions lie in a cubic arrangement with the fluoride and calcium ions being of comparable size, the insertion of the calcium ions into the structure causes an expansion of the cubic arrangement and the fluoride ions are no longer in contact with one another.
Fluorite contains calcium ions in a cubic closest-packed arrangement with fluoride ions being in a tetrahedral environment, the unit cell consists of 4 atoms and 8 tetrahedral sites all of which are occupied by fluoride ions., Due to this close packed structure with all the tetrahedral holes being occupied this makes it difficult for any substance to penetrate and form a solution of calcium fluoride.

Aim:
The aim of this experiment is to determine the composition of the analgesics and stimulant found in a commercial tablet formulation via the method of HPLC.

Introduction/Background:
HPLC - High Pressure Liquid Chromatography is a separative process, this separation occurs due to the differing specific affinities if each analyte for the mobile and stationary phases.

Specific affinities are dependant upon the chemical and physical characteristics of each individual component.
Seperation efficiency is related to the size of the particulate packing material and hence modern chromatograpy systems utilise microparticles less than 5micrometers in diameter.

This development affected the efficiency of the entire system due to the gravity fed columns used, therefore the technique was developed by the application of pressure upon the column - HPLC.
To begin this technique the mobile phase solvent is thoroughly degassed (bubbling with inert gas/ ultrasonication), removing dissolved gases and also filtered to remove particulate material.

The mobile phase can be manipulated to achieve different compositions; it is pumped under constant pressure onto the column via a sample injection port.
These injection systems are designed to ensure injection witout introducing air into the system, this utilises a sample loop that will hold a defined volume.

Elution from the column will depend upon the relevant interactions between each analyte and the stationary phase.
A suitable detection system is used and in the case of this experiment it is a UV detection system as most organic compounds absorb in his region.

The detector response is then converted into an electrical output, which is used to generate a plot of detector response Vs time, a chromatogram.
Ideally this will show a series of peaks separated by a baseline, each peak corresponds to a single component of the mixture under analysis.

The information can be used to determine the concentration of each component in the mixture.
Method:

To begin an Internal standard stock solution was prepared by weighing out 0.0577g of Phenacetin into a 250mL graduated flask.
To this was added 1:1 Methanol : HPLC grade water from a 300mL beaker, the stock solution was filled to just below the volumetric mark.

The mixture was shaken and following this sonicat-ed thoroughly (for 15mins) to ensure dilution and the degassing of the solvent.
The solution was then allowed to stand for 1minitue and solvent added in a drop wise manner by Pasteur pipette to the volumetric mark.

Following this a standard solution was prepared by weighing out 0.0372g of the standard mixture (equal amounts of aspirin: paracetamol: caffeine) into a 50mL graduated flask.
Using the internal standard solution as the solvent the above procedure was followed again for the preparation of the standard solution.

A sample solution was then prepared by taking a commercial analgesic tablet (0.6252g) ad crushing this into a fine powdered using a pestle and mortar.
A 0.0310g sample of this tablet was weighed and placed into a 50mL graduated flask.

Taking the internal standard as the solvent the solution was prepared for analysis using the same procedure as stated for the Internal standard preparation.
The UV spectral characteristics of each of the components on the standard mixture were analysed to select the optimum wavelength for analysis (...nm).

The optimum mobile phase composition was determined by adjusting the mobile phase composition from 70%methanol: 30% of 1% acetic acid (aq) to 50%methanol: 50% of 1% acetic acid (aq) and finally to 40%methanol: 60% of 1% acetic acid (aq) and analysing the standard solution.
During the process of adjusting the mobile phase the acetic acid component is increased in 10% increments and a period of 10minutes was allowed for the readjusting of the mobile phase compositions to ensure that the column was at an equilibrated state.

The 50%: 50% mobile phase composition was chosen, it provides good peak/baseline separation, not as good as the 40%: 60% mobile phase composition, however as a compromise the 50%: 50% phase is chosen as the analyte is separated far quicker than during the 40%: 60% phase.
Finally our commercial analgesic solution was analysed using the 50%methanol: 50% of 1% acetic acid (aq) mobile phase.

Results:
Questions:

Briefly explain the mechanism of the separative process that is occurring within the column of a HPLC system, Use a reverse phase column as the stationary phase in your answer.
A reverse phase column is a HPLC that is based on a non-polar column packing/stationary phase, with a polar mobile phase.

During the process of HPLC a sample is pushed through by high pressure, and the different components of the mixture are distributed to different extents between the mobile and stationary phase.
The distribution ratio of each component is dependant on its relative attraction for each phase.

Overall the differences in affinity for the stationary phase will cause elution from the column at differing times and therefore a mixture can be separated into its components.
The strength of the interaction with the stationary phase governs the retention time and this is in turn dictated by the interactions with polar functional groups.

A reverse phase column is non-polar and therefore will have maximum interaction with other non-polar analytes; therefore the more polar analytes will be eluted first.
Describe mode of operation of a column that is packed with size exclusion media for the separation of polystyrene oligomers.

Size exclusion chromatography serves to separate molecules based on their size by passage through a porous structure stationary phase.
The stationary phase is termed as a molecular sieve; smaller molecules enter a porous media (gel) and will take longer to elute from the column.

An oligomers is a molecule with an infinite number of monomer units, the separation process works along the basis that polystyrene oligomers larger than the largest pores of the swollen gel cannot penetrate the gel particles and, therefore will pass straight though the column through the spaces between the individual particles.
Smaller polystyrene oligomers will penetrate the open network in the particles to a varying degree depending on shape and size, and therefore are eluted in order of decreasing molecular size.

What is the role of Guard columns in modern HPLC systems?
A guard column is about 1-2cm long and has the same stationary phase as the analytical column.

The guard column is mounted before the analytical column and serves the purpose of protecting the analytical column from anything that may block it, for example solvents that are no de-gasses fully.
The guard column is cheap and disposable being replaced frequently and serves no real selective process.

Predict the elution order for the following HPLC conditions: 10: 90 Water: CH3CN; flow rate 1mL/min; Hypersil ODS 3micro meters 25cm*4.6mm i.d.
@ 28oC; 5 micro litres injection; UV detection (254nm).

As the analytical column is non-polar this means that the analyses that are more polar will have a shorter retention time than those that are of a more non-polar nature.
Therefore the phthalates that are more polar in nature will be eluted from the column first, and hence I predict the following elution order to be: C, D, E, A, B.

Conclusions
Analysis of our results show there to be a 2.43mg% alcohol content of the commercial alcoholic beverage.

The quoted value is of 4mg%, our results may be subject to many experimental errors, such as the injection of our sample - caused by different operators injecting each sample, and errors in pipette use.
In addition there may be slight error caused by the computer integration of the peaks produced in the chromatogram.

Forensic science refers to any science that is used in a court of law.
There are many analytical methods used to process evidence, and one should consider the advantages and drawbacks of each method to determine its suitability for the evidence in question.

X-ray powder diffraction is a commonly used analytical method for the identification of evidence being frequently used in patents, criminal cases and other areas of law enforcement.
X-ray powder diffraction is a technique, which provides a high degree of accuracy in the measurement of interplanar spacings.

X-rays were discovered by 1895, being electromagnetic radiation with a wavelength of around 1Å, comparable to the size of an atom they were utilised to probe crystalline structures at the atomic level.
Materials to be analysed by X-ray powder diffraction should be crystalline or at least partially.

The method works with a focused X-ray beam probing the substance/evidence and this interacting with the regular repeating planes of atoms that form the crystalline lattice.
The X-ray beam may be absorbed, refracted and scattered or diffracted.

Around 1% of all X-rays are scattered or diffracted whereas around 99% will pass straight through the sample.
X-rays are diffracted by each mineral uniquely, diffraction depends upon which atoms form the crystalline lattice and their arrangement.

To begin this method X-rays must be generated, this is accomplished by applying a current that heats a filament within a sealed tube under a vacuum.
The higher the current, the greater the number of electrons emitted from the filament.

A high voltage is applied to the tube, which accelerates the electrons emitted onto a target.
When electrons hit this target X-rays are produced, they are then collimated and directed onto the powdered sample.

A detector is set up to record the signal X-ray output from the sample and this is processed into a count rate.
An X-ray scan of the sample is produced by altering the angle between the X-ray source, sample and detector.

When an X-ray is diffracted one can apply Braggs law ( nλ = 2d sinθ) to determine the spacings (d) between atom planes.
One will know the wavelength of electromagnetic radiation used, λ, and the angle of incident radiation, θ; n is simply and integer and by use of this information, the spacings between atoms, d, may be worked.

Each mineral will give a characteristic set of d-spacings providing a fingerprint reference to the compound.
This is therefore the information utilised in compound identification, in fact there is an accumulation of data for over 550,000 reference materials represented by the "international centre for diffraction data".

This analytical technique is used most commonly for "contact trace analysis", this focuses on Locards exchange principle, "with contact between two items there will be an exchange".
This principle is applied to all crime scenes and investigators work along the concept that any persons at the scene would have both taken something from the scene with them and left something of themselves at the scene.

Contact traces include substances such as paint flakes, glass fragments, hair fibres, metals, soils, and building materials.
Recovery of trace materials can be achieved in an infinite number of ways and it is not unusual for a Forensic scientist to improvise methods.

For example cello-tape was not used until the early 1970s to recover debris from garments.
Evidence may be collected by shaking; therefore removing loose particulate material from garments onto a sheet.

Also combing/brushing surfaces such as shoes or pocket linings where shaking has not been effective.
Taping is a method used mainly to recover hairs or fibres from clothing, in addition to this vacuuming has also been used mainly to recover firearm residues from the surface of clothing.

Swabbing is also used to trace firearm residues, however most commonly used for smeared material evidence such as blood.
In addition material can be picked by tweezers and all evidence is stored in the relevant sealed uncontaminated containers or bagging.

There are 7 main areas of evidence types that may be analysed by X-ray powder diffraction.
To begin there is soil analysis, this can be used to link persons or objects to a specific area.

Secondly there is the analysis of explosives and post blast residues, also pigments and paint analysis again for linking persons or objects to a scene.
Goods identification is another branch, helping to detect counterfeiting and the origin of products.

On a truly investigative level there is the identification of unknown substances such as poisons and contaminants, this area is used for industrial accidents and leakages, and incidents becoming more common in this day and era such as white powder scares.
Another area of analysis is that of degraded bones, used in degraded skeletal recovery for confirmation of composition.

Finally there is the area of drug analysis, incorporating investigations into quantitative analysis, and identification of any adulterant helping to ID the supplier.
The identification of any compound under these areas can lead to or help conviction or exoneration.

Evidence such as combings can be compared to a sample taken from the scene, this can prove a person was present at the scene of a crime, or in the case of unknowns the X-ray diffraction data can be compared to a database of reference compounds.
When analysing a substance which may be formed of many compounds, each will produce its own unique diffraction pattern, the substance will therefore produce a diffraction pattern that is the sum of all components.

Such a use for X-ray diffraction shown in a case study; is the analysis of drugs and other substances in an unknown.
These are dealt with by the Drugs section of the Forensic Science Service, during this particular case study an off white powder was submitted for analysis with a preliminary analysis of Heroin, glucose and sodium chloride content.

A sample was subjected to XRD to determine the form of heroin and confirmation of the substances present.
Results confirmed the presence of Heroin and sodium chloride but not glucose.

A strong XRD was recorded but was unidentifiable, the d-spacings and relative intensities were kept on record.
A subsequent case showed the pattern occurring together with that of glucose monohydrate.

It was thought that glucose may be forming a complex with Sodium chloride.
A complex was synthesised and an X-ray powder diffraction pattern run; this was found to match the unidentifiable substance.

This shows the analytical value of a technique such as X-ray powder diffraction, and its routine use in substance identification of areas such as drug seizure.
Another example of the use of X-ray powder diffraction is shown in the case of a dockyard burglary committed by an employee.

The employee decided to steal a 4-blade, brass tugboat propeller from a neighbouring repair shop.
To carry out his steal, is was necessary to cut the propeller into four parts; however during his crime the gasoline fuelled cutter he was using ran out of fuel.

The man was seen fleeing from the scene and his license plate recorded.
Police later traced the mans dwelling and found him at home minus his overalls, from the neck upwards the suspect was covered in fine brass shavings.

Samples of the brass shavings were taken and compared to controls from the scene of crime, these were analysed using X-ray powder diffraction.
Although brass was evidenced in both samples the phases were not matched, however due to the heat of the cutter used this had manipulated the brass.

A comparison of control sample heated in a similar way to the suspect brass shavings were found to match and the man was convicted of this crime.
As with any technique X-ray powder diffraction has its advantages and drawbacks.

One advantage of the technique is that it doesn't require the growing and mounting of a single crystal; powders are utilized and this saves time and widens the scope of evidence this technique can be used in conjunction with.
This allows the sampling of larger objects, in comparison to single crystal work, with this technique a larger object can be ground up and the sample taken.

Also, X-ray powder diffraction can be used in situ, modern day demands have arisen as to identify more at the scene of the crime, aiding a faster legal process.
Miniaturisation of the equipment is in working and therefore can be used more easily at the crime scene.

Portable databases of the reference compounds are also easily at hand.
A major advantage to X-ray powder diffraction is that it is a non-destructive method; therefore the evidence can be preserved.

This is useful during legal proceedings as Counsel may call for the specimen to be re-analysed by their own scientists.
X-ray powder diffraction also saves time as there is no extensive sample preparation and therefore one can quickly analyse any unknowns.

The method can be used to identify compounds as well as specific elements; X-ray powder diffraction will identify many materials including both organic and inorganic materials.
In addition the technique can identify polymorphs; different crystalline forms of the same substance, and can distinguish between hydrated forms of the same element, this heightens the methods effectiveness at compound identification.

X-ray Powder diffraction can be used to measure a compound or element quantitatively and qualitatively, therefore is a very useful technique with a wide range of applications.
As with any method, X-ray Powder diffraction holds its disadvantages also; the technique turns information of 3 dimensions into a collapsed form of a 1 dimensional diffractogram.

Therefore some information is disregarded in the final analysis data.
In addition the material must be crystalline or at least partially and this means some materials, those which are for instance amorphous cannot be analysed by X-ray powder diffraction.

The method is also fairly insensitive, and therefore not a method of trace analysis, for instance a compound present in a mixture of around 1-5% may go undetected by X-ray powder diffraction.
Overall this technique has many advantages and fewer disadvantages, however one must question the use of this technique as a trace analysis tool, due to poor detection at the presence of a substance at 1-5%.

The general analytical approach to prove results are reliable beyond reasonable doubt is to use two analytical methods, the first being relatively cheap and quick, but also reliable, then a second test can be used to give unequivocal proof of a compounds presence.
Perhaps it can be seen that X-ray powder diffraction would be most suitable for use as a primary test method in smaller sample sizes.

X-ray powder diffraction is a useful analytical tool, it can identify many types and forms of substances and is seen to be useful in many situations were unknown substances are recovered.
The physics on which the method is based provides a high degree of precision and accuracy in the measurements gained.

Overall X-ray Diffraction holds a prominent place in forensic science and can be useful in many cases.
Introduction:

Polymers based on the silicon-oxygen linkage are the only major commercial polymers with an inorganic backbone.
These polymers are produced by hydrolysis of an organo-dichlorosilane, followed by spontaneous condensation of the product silanediol with elimination of water.

The final result is a long chain polymer known as polysiloxane (silicone).
FORMULA If the reaction takes place at low concentration then the cyclic compounds; cyclopolysiloxanes are formed, this is due to the kinetic favourability for the 2 ends of the chain to react with each other than another chain (at low concentrations).

In the experiment, from the starting compound, dichlorodiphenylsilane one can isolate the intermediate silanediol, as diaryl compounds are found to be more stable than their dialkyl counterparts.
Condensation of diphenylsilanediol in dilute solution gives two different cyclopolysiloxanes, one of them will be isolated depending on whether an acid or base catalyst is used.

Hazard assessment:
See COSSH sheet

Experimental procedure:
Preparation of diphenylsilanediol

To begin a mixture of toluene (10ml), t-amyl alcohol (25ml) and water (100ml) was cooled in a conical flask (500ml) to ~10 oC in an ice-water bath, whilst being stirred magnetically, solution turned cloudy.
To the vigorously stirred mixture, a solution of diphenyldichlorosilane (20ml) in toluene (10ml) was added dropwise over a 15-minute period.

The temperature was held between 10 - 15 oC throughout by adding more ice water when necessary, a white precipitate was formed.
The reaction mixture was stirred for a further 10 minutes and following this the crystalline solid was filtered off by suction.

The solid was washed with water (100ml), then washed twice with 5% sodium hydrogen carbonate solution (50ml X 2), and further washed with water (100ml X 2).
The crystals were then stirred gently whilst the vacuum was in operation.

After an hour the crystals where easily moved and free flowing.
Once dried the yield, melting point where measured and IR spectrum was obtained.

FORMULA
Preparation of octaphenylcyclotetrasiloxane

To a "Quickfit" conical flask (100mL), diphenylsilanediol (5.0189 g) and 95% ethanol (50ml) was added, in addition a solution of sodium hydroxide (0.4521g) in water (1ml) was further added with stirring.
The mixture was then transfered into a round-bottomed flask where it was heated under reflux in an oil bath, with magnetic stirring.

The solution was heated under reflux for ~30 minutes and then cooled in an ice bath, fine white crystals were formed from solution.
The crystals were then filtered off and air dried until free flowing.

The product was then dissolved in a minimum volume of heated toluene (~70 oC) following this a small amount of ethanol was added until crystals began to form.
The solution was then re-heated to take the newly formed crystals back into solution.

The solution was then cooled again by ice-bath, giving fine, needle-like crystals.
The crystals where then filtered off, a yield and melting point measured and an IR spectrum obtained.

FORMULA
Results:

The literature melting point for diphenylsilanediol is 201-202 oC; In comparison to our value of 149.1 oC - 166.8 oC; octaphenylcyclotetrasiloxane literature quoted to be 187 oC In comparison to our experimental value of 193.7 oC - 198.6 oC.
Differences can be accounted for by impurities in our products.

Discussion
Analyse both spectra obtained and the spectra given to determine whether they are fully consistent with the structures proposed for the products.

Interpret the fragmentation pattern observed in the mass spectrum of octaphenylcyclotetresiloxane, and use the pattern to identify the weakest type of bond in this compound.
1H NMR:

All the peaks are in the range 7-7.5ppm indicateing that the H's are all in aromatic environments.
Hydrogen's are found in a ratio of 2:2:1.

13C NMR:
The peaks on the spectra show there are four C environments; phenyl groups bonded to each of the Silicon atoms, and two phenyl groups bonded to one silicon atom.

Mass Spec:
Infra Red:

From the IR spectra the structures are fairly consistent, however there are anomalous errors between the two.
In the IR spectra of diphenylsilanediol we can see the O-H stretch and the C-H stretch, in the spectra for octaphenylcyclotetrasiloxane peaks for the C-H stretches can be seen however the O-H stretch is lost.

There were some unexplained peaks picked up in the IR spectrum, which shows that there were impurities.
The fragmentation pattern of octaphenylcyclotetrasiloxane shows that the weakest bond in the molecule is the Si-C bond.

This can be deduced as the main peaks are due to loss of phenyl groups; the phenyl groups are bonded to Si atoms by carbon.
Carbon-silicon bonds compared to carbon-carbon bonds are longer (186 pm vs. 154 pm) and weaker; bond dissociation energy 451 kJ/mol vs. 607 kJ/mol.

The C - Si is strongly polarized towards carbon due to higher electronegativity.
Describe the differences between compounds with the general formula R2CO and the corresponding organosilicon compounds having the general formula [R2SiO]n.

Why do carbon-based compounds of this type adopt a monomeric structre whereas silicon-based compounds are invariably polymeric or cyclic?
Compounds of formula R 2CO and R 2SiO differ due to their difference in bond strength, Carbon is the only group 14 element that is able to form strong pie bonds with oxygen.

The effects of this bonding can be seen when comparing their physical properties to their silicon counterparts.
Carbon is bounded to oxygen by both σ and pπ-pπ bonds, however for silicon it is more energetically favourable to form single bonds.The C=O bond has good p-orbital overlap due to carbon being of a similar atom size relative to silicon being a lot smaller, the silicon atom is too big for p-orbital overlap to be effective.

The silicon-oxygen sigma bond is highly stable because it does not differ greatly in electronegativity.
The carbon-oxygen is also stable, however due to a bigger difference in electronegativities the bond becomes polar which is open to attack.

Silicon based compounds are most likely to be cyclic due to their favourability for forming single bonds, their structures become based on O-Si-O bridges, this is in comparison to Carbon based compounds in which pie bonds are more preferable and therefore a monomeric structure is adopted.
A possible reaction mechanism for the base-catalysed condensation of a silanediol to a cyclo-siloxane.

The reaction is base catalysed and therefore a silicon atom becomes hypervalent (an atom having more bonds/electrons in its valence shell then would be allowed by the octet rule) resulting in loss of a hydroxyl group.
This causes the two ends of the molecule to react forming a cyclic structure.

A possible method of converting your cyclic siloxane to a high molecular weight linear polysiloxane.
To convert a cyclic siloxane to a linear polysiloxane one would use sulphuric acid under heat to protonate the oxygen atoms, therefore breaking the Si-O bonds.

The heat provides the energy to break this highly stable bond and make the reaction thermodynamically/kinetically favourable.
Conclusion:

From the data collaborated we can see that we have prepared diphenylsilanediol and octaphenylcyclotetrasiloxane, however both compounds do contain impurities.
These may have been encountered at various stages of our experiments, and one must also question the purity of our starting materials.

In order to analyze the aims of Carthage during the First and Second Punic Wars; it is necessary to examine a number of aspects, which may have determined Carthage's policy towards the conflicts in question.
These aspects are: the extent of Carthaginian influence during this period, the resources and man power available to her (and also the Romans) and perhaps more importantly, the system of government which ultimately controlled Carthaginian policy.

All of these various factors will be examined throughout the course of this essay to determine the extent that Carthage's aims differed during the two conflicts, if at all.
The first task is to examine the political situation at the time between the powers of Rome and Carthage.

Livy describes that both powers were at the peak of the power and prosperity during this period.
This is indeed the case for Rome, but there is a direct contrast with this claim found in Polybius: 'But at the time when the Hannibalic War began, the political state of Carthage was in decline, while that of Rome was growing better...

The power and prosperity of Carthage had developed far earlier than that of Rome...' However, Carthage was still in a powerful position concerning her influence abroad.
She could largely dominate the whole Western Mediterranean due to her control over Sicily.

Hoyos states that the Carthaginians were still a match for Rome in terms of her manpower.
Not only this but Carthage had possession of a large empire and received a large annual income as a result from trade.

Scullard explains that Carthage exported salt, wax, honey, figs, textiles and pottery.
Details from 193 BC show that Carthage gained an approximate annual revenue of 1,400 -1,500 talents which equates to between 8,400,000 and 9,000,000 drachmas.

Compare this to the figures of Rome during the early decades of the same century, when the Roman Republic is receiving approximately 2000 talents.
By analyzing the situation prior to the First Punic War, it is possible to examine what the aims of Carthage were.

Polybius states that Carthage ruled the sea and Tarn provides a rough total of 130 ships in the Carthaginian fleet prior to the First War.
He also comments that Carthage was capable of creating 200 ships in times of intense national effort.

This can be further reinforced with a statement by Ennius that: 'Alter nare cupit; alter pugnare paratust.' 'One desires to swim; another is prepared to fight.' The extent of Carthage's influence is shown on the map below: Yet, despite her power, Carthage during 264 BC did not want to fight the Romans and it is probable that it was the same for the Romans.
The tension between the two powers would have been intensified by Roman expansion in Eastern Sicily and here the first difference between the two wars can be encountered.

Polybius informs us that the First Punic War was fought over Sicily and that Carthage; wishing to maintain the vital region of western Sicily sent troops there.
Carthage's aims in the First Punic War were only to prevent the loss of Sicily to the Romans but after the First War, Livy states that 'bitter resentment at what was felt to be the...

tyrannical attitude of their conquerors' was the driving force behind the Second War.
After their defeat in the First Punic War, the Carthaginians lost Sicily to the Romans.

So in essence they had lost one of their vital commercial power bases.
During the peace between the First and Second Punic War, Carthage's political situation had changed.

The map above shows the territory gained by the Barcids in Spain.
This new region was a valuable boost to Carthage's income and also Spain opened up a new avenue in terms of manpower.

Livy provides us with an example of the Carthaginian exploitation of their new province: 'An officer was sent with Mago to Spain for the purpose of enlisting mercenaries to the number of 20,000 foot and 4000 horse, to be added to the forces already in Spain itself and in Italy'.
The settlement of New Carthage also contributed greatly to Punic power.

So how did this new region affect Carthage's long term aims?
Polybius and Livy credit Hamilcar's conquest of Spain as his intended base, from which he would attack Rome.

So now under Hamilcar, Carthage's aim was to avenge the loss of the First War by attacking Rome itself.
This being the case, Hamilcar died before the Second Punic War and so a new political figure would now influence Carthaginian politics.

It is also necessary to remember that the aims of one individual will not represent the intentions of all the Carthaginians.
Having examined the conditions of the First Punic War, a contrast now can be made with Carthage's aims during the Second Punic War.

There is a remarkable difference between these two wars concerning the expanse of land which became theatres of war.
The First War essentially focused on the area around Sicily, while the Second Punic War had theatres of war in Spain, (later Africa) and Italy.

Hoyos explains that Hannibal launched his invasion from his capital of New Carthage and intended to march over a thousand miles without reinforcements or provisioning by Punic ships along the way.
Livy gives a concise summary of the differences between the two wars: 'But this second war was felt by the Romans to be a much deadlier struggle than the first, partly because it was being fought on Italian soil and partly because of the frequent terrible losses in men...' Since the First Punic War, Carthage's naval supremacy had been overturned.

Hoyos states that the Romans had a navy of 220 ships compared to 100 Punic.
Giving a slightly different figure, Tarn points out that the rough total of the Carthaginian fleet in 215 BC was 120 ships in all.

However, the reduction of Carthage's naval power could be exaggerated.
Compared with the total of 130 ships in the First War, there seems only a small reduction.

Hannibal chose to attack by means of a land campaign, ironically reversing Ennius' statement concerning Carthaginian War Policy in the First War; but this does not mean that Carthage abandoned the usage of naval operations altogether.
Livy describes a number of Carthaginian fleets that were sent to attack the coasts of Italy.

The critical difference was the state of Roman naval power, which was a key difference in the Second Punic War, not so much the Carthaginians themselves.
Polybius records that during the First Punic War, the Romans had no decked ships or warships of their own and that they simply used ships belonging to their allies.

The Romans also endeavored to build a huge number of newly fitted ships in sporadic bursts when they needed more.
So it was this increase in Roman sea power that firstly diminished the Carthaginian naval advantage and then subsequently enabled them to win the First Punic War.

It is necessary to investigate Hannibal's intentions and how his leadership affected the aims of the Carthaginians.
According to Bickerman, Hannibal expected to remain the Carthaginian general in Italy after the victory and ensuing peace.

There is seemingly some debate over Hannibal's intentions regarding Rome.
Polybius comments that Hannibal intended to bring the war to Italy, this would divert the Romans from attacking the coasts of Africa which had been done towards the end of the First War.

It would also give Hannibal an opportunity to raise more recruits from the Gauls and the Roman allies.
There is also a marked contrast between what Hannibal would have liked to have done and what he actually could do when it comes to the invasion of Italy.

He had to tread very carefully because success hung on his ability to bring the Italian Allies to his cause.
As Hoyos comments, he could not afford to be seen as an invader to them because then this would end his prospects of victory beyond all doubt.

Hannibal was, therefore, taking a huge risk, he had to recruit the Allies when he arrived in Italy, but before that, he would have to take as small a force as practically possible.
Polybius records that he emerged from the Alps with an army of 12000 African and 8000 Spanish infantry and not more than 6000 cavalry in all.

Taking such a small force into Italy was indeed a risk but having brought many more Carthaginian reinforcements would only serve to alienate the Italian Allies.
After all, given the choice, the Italians would rather have an alliance with the Romans than with a far more exotic Carthaginian power.

In addition to the subject of man power, Hannibal also had to consider the requirements of his army in terms of rations, supplies and especially wages as his army (as is the case with all other Carthaginian forces) was essentially comprised of mercenary soldiers.
So this aspect would be paramount to him throughout his campaign.

Meiklejohn points out that the most decisive move of the war was made by Scipio, when he invaded Spain, cutting off Hannibal from his base.
This in itself created a vicious circle for Hannibal.

In order to pay and feed his troops he would need both supplies of food and money.
The nearest source of both food and money would be from any captured towns in the form of war booty and harvest crops.

So to acquire these supplies he would inevitably have to occupy towns or even if necessary take them by force.
This in turn would have alienated the Italian Allies unless they were highly supportive of Hannibal.

As a result of this problem, Livy reports that Hannibal did suffer financially and had difficulty in acquiring enough food; though this was not always the case.
The clearest possible indication of Hannibal's aim is after Cannae, when he had the opportunity to march on Rome.

In terms of investigating any possible aims of the Carthaginians, this is one of the better events to choose.
Both Polybius and Livy state that Hannibal did not feel that attempting to capture the city would be possible; Livy certainly believes that he should have done: [Maharbal 'said' to Hannibal] 'You know, Hannibal, how to win a fight; you do not know to use your victory'.

However, Hannibal could hardly challenge Rome by a siege or an assault because of the Fabian tactics which were employed.
Furthermore, the Romans did not surrender after Cannae like Hannibal had imagined.

Moreover, the strength of the allied forces was a major problem for Hannibal.
As Hoyos remarks, he lead an army of 26,000 to challenge a power which had 750,000 men of military age.

Details from Polybius illustrate the proportion of the Romans' forces that was comprised of allied soldiers: 'Each consul had two legions, each of 5,200 men and 300 cavalry'.
Allies consisted of 30,000 Etruscan infantry and 2000 Etruscan cavalry, Sabine cavalry of 4000 with infantry of 50,000.

The hill tribes of the Apennines are listed as providing 20,000 [infantry] as well as 20,000 by the Veneti and the Cenomani.
The Roman reserve was 20,000 infantry.

This is one occasion mentioned by Polybius, and the allies were called upon on a number of occasions to provide more troops.
Not only was the sheer number of the reinforcements which could be summoned to Rome a problem for Hannibal; but even more crucially their loyalty to Rome cost him the war.

Polybius gives some information concerning the Celtic tribes and he attributed the lack of support for Hannibal to Roman military positioning: 'The rest of the Celtic tribes...
had been eager to join the Carthaginians from the offset, but the Roman legions...

now stood between them and their would-be allies'.
As Livy explains, worse was to come for Hannibal: 'Nevertheless not even the panic caused by these depredations, not even the flames of war on every side of them, could move Rome's allies from their alliance'.

So supposing Hannibal had actually taken Rome, what would this have achieved?
Well the answer depends on what the Carthaginian aim was regarding Rome after the war.

From the claims that the Carthaginians wanted to avenge the loss of Sicily, one could believe that the destruction of Rome was the most likely outcome.
Had Hannibal have done this, it is possible that then the allies would have resisted no longer.

Merely occupying Rome was hardly likely to make any difference.
More importantly, Hannibal was unable to attempt this because of the Roman forces that still were at large.

Yet if the Romans had met Hannibal in pitched battle continuously and had not adopted the Fabian tactics, it is likely that they would have had to withdraw to Rome itself.
Had the opportunity presented itself, Hannibal would have laid siege to Rome.

In terms of Carthaginian aims, Donaldson remarks that the only opinion that mattered was Hannibal's.
His aim was simply the destruction of Rome.

However, Bickerman suggests that there is evidence to believe that Hannibal intended to spare Rome and actually allow her continued existence.
Bickerman suggests that Hannibal 'admitted that even after the conclusion of peace, Rome would remain a military power.

Regarding the aims of the Carthaginians, there is a difficulty.
In the Ancient World, there was no real sense of national interests or aims as Donaldson explains.

The only way to fully answer such a question would be to examine who was in power at Carthage itself and to analyze the form of government that operated there.
So the first question to ask is who wanted to achieve these aims at Carthage.

Who wanted to regain Sicily and who wanted to take the war to Italy?
Evidence concerning the constitution of Carthage is not great but significant details can be located in Aristotle's Politics.

The Carthaginian government consisted of Kings or Sufetes (mentioned in Livy) and also a Senate body of sorts.
Aristotle describes the relationship between the Sufetes and the Senate body (called the Elders in his work): '...

at Carthage the Kings, acting in conjunction with the Elders, have sovereign power to refer or not refer a matter to the people, provided they are unanimous...' In effect the people can be left out completely if both the Senate and the Sufetes agree with one another.
So this extract would lead us to believe that the people may not be behind the aims of the Carthaginian Empire.

There is ample evidence though to suggest that the people may well have had quite a powerful role in Carthaginian aims.
There are a number of extracts from both Polybius and Livy, which illustrate the relations between Senate and the Sufetes, was usually tense.

Polybius indicates that the Sufetes would have to have popular support by both the Senate body and the people: 'The capture of the city [Saguntum] would provide him [Hannibal] with ample funds and supplies...
and earn the goodwill of the Carthaginians at home by the spoils he would send there...

He had succeeded in making his men more enthusiastic...
and the Carthaginians better disposed to grant his request'.

It seems that the Senate body was the most powerful according to Aristotle and Livy reinforces this possibility by mentioning that communication with the Senate was possible during the Hannibalic War and that the Senate was very much in control.
Mago had to prove to the Senate that the war was going well: 'Then as evidence of the success of the campaign, he [Mago] had the captured gold rings poured out in the courtyard of the Senate House'.

Livy does provide an instance where there is negotiation between the generals and Senate: 'The terms of surrender were not observed very long; for Hasdrubal soon received orders from Carthage to march for Italy at the earliest possible moment.
The news once known diverted the allegiance of nearly all the Spanish tribes to the Roman interest.

Hasdrubal wrote immediately to Carthage, pointing out...
if the Carthaginian government were interested in Spain at all, they must send someone with a powerful army to take other from him...

The immediate effect of Hasdrubal's letter upon the Carthaginian Senate was considerable;...
Himilco was sent with a fully equipped force and an enlarged fleet to hold and protect Spain by land and sea'.

Yet Hasdrubal's orders were not changed and he was still to march to Italy.
There were also reported instances of division within the Senate itself concerning aspects of the War.

According to Livy some of the senators were against sending Hannibal to Spain in the first place.
There was also division after Mago's report to the Senate ; which shows that even the Senate did not always have one sole aim.

Livy believes that division in the Senate was a factor which caused Hannibal to fail in his objectives.
Another important fact was that Sufetes could only become politically powerful if they were victorious in campaigns.

We do have to bear in mind that Aristotle as Hoyos comments was recording a constitution a century before the period of the Punic Wars.
The people at this time were very important to any Carthaginian general as they elected them via the Public Assembly.

Polybius states that at the time, the people had become the predominant force in Carthaginian politics and also mentions that Hannibal hoped to get support at home.
In conclusion, the differences between the First and Second Punic Wars would cause the aims of the Carthaginian to alter.

While during the First War, the aim was to maintain control over Sicily and the immediate area around her; the Second War, according to our sources, was a fight to avenge the loss of Sicily and to attack Italy.
There appears a marked difference in terms of Carthage's aims.

However, if Hannibal intended to allow Rome to survive, then all he intended to do was to bring the war to Italy while Carthage managed to reclaim her lost territories.
Meiklejohn explicitly states that this is the case, as Hannibal, while he was in Italy persuaded the Carthaginians to stir up trouble in Sicily, so they could regain their lost territories.

This implies that in essence the aims of Carthage (if we can really use such a broad term) did not change between the two wars.
Still, that is only Hannibal's aim, we do not know what the other politicians at Carthage might have wanted, nor are we aware of the feelings of the general populace to a great extent.

We must consider that Hannibal's aims should have been popular with the people because normally a Sufete was elected for a short term; so he needed their support.
However this is not a normal situation, it would not be possible to try and recall Hannibal for an election and so he was probably left the command, without needing popular support after his initial election to the post.

From the material we have, all we can do is analyze the aims and ambitions of individual leaders at Carthage.
To be able to fully answer the question 'how far did Carthage's aims differ between the First and Second Punic Wars'; greater detail concerning Carthaginian intentions, other than just a few individual cases such as Hamilcar or Hannibal, would be required.

Throughout the course of this essay, I shall explore various aspects of Plato's proposed state in order to ascertain whether or not it can be regarded as elitist.
I hope to compare and contrast the most important aspects of the system and then give an overall conclusion.

Let us begin with the standard definition of Elitism : 'The belief that some people are inherently superior to others and deserve pre-eminence...' 'Belief that government or control should be in the hands of a small group of privileged, wealthy, or intelligent people'.
These are the characteristics that we are searching for in Plato's model state.

The elements upon which I shall focus are:
The Guardian Class and their integration with the so called Third Class.

The nature and selection of the Philosopher Kings and the Guardians.
Beginning with the justification for such classes, Plato argues that: 'Quantity and quality are therefore more easily produced when a man specializes appropriately on a single job for which he is naturally fitted...'(370c).

According to Plato's theory, then, some people are indeed better suited to rule other others and some people are not as capable.
Seemingly if Plato's whole model state is based on an elitist ideal, what hope is there for the rest of the model state?

These Philosopher Kings are selected by a rigorous regime of training and should only become rulers if they possess: 'Truthfulness.
He will never willingly tolerate an untruth, but he will hate it, just as he loves truth'.

(485c).
Bowra explains that Plato believed that government should comprise of an intellectual and moral elite.

He was not just satisfied with having them well trained, but also morally superior and there is a justification for such a line of thought.
Plato himself explains that such a provision prevents the Philosopher Kings from being mean or unreasonable because 'they must do whatever they judge best for the city'.

Although so far, such a regime seems elitist to the core, there is a provision which appears to contradict the concept of elitism.
As Schofield explains, the Rulers are chosen from the Guardian Class, which as Plato instructs are to possess no private property of their own, not even a dwelling.

Surely an elitist would grant the very best property to the Rulers and Guardians as they are 'inherently' superior?
While on one hand, the Rulers are in complete control of the laws and state policy; yet on the other, they completely lack private property and strive for the general welfare of those ruled.

Moreover we have another paradox regarding Plato's theory; how will the Philosopher Kings be able to endure one untruth, which is the one of the greatest imaginable?
The so called 'Noble Lie' should be a great source of angst amongst these rulers, or should it?

If they believe that they are ruling for the good of the city, they themselves would believe the Noble Lie.
This is where the problem worsens; these Philosophers are supposed to be able to: '...look directly at the sun itself, and gaze at it without using reflections in water or any other medium...'(516b).

Presuming that the rulers did this, they too would then be witness to the truth behind the Noble Lie itself.
It seems that Plato was not only aware of this prospect but he also expected that the Philosopher Kings would be quite content with ruling by 'a fairy story, like those the poets tell'with 'a great deal of fiction and deceit'.

'It will be for the rulers of our city, then, if anyone, to use falsehood in dealing with citizen or enemy for the good of the state; no one else must do so.
And if any citizen lies to our rulers, we shall regard it as a still greater offence than it is for a patient to lie to his doctor...

' (389 b-c).
This seems rather odd to me as only a few passages further Plato had himself admitted: 'And isn't it a bad thing to be deceived about the truth, and a good thing to possess the truth?'(413a).

Seemingly Plato is only concerned with enlightening the Philosophers and not at all concerned with informing the lower classes; as such knowledge would entitle them to rule, in essence a democracy.
From this respect, the so called model state seems very much an elitist one, although some elements seem to contradict the standard definition of elitism.

Now we have examined how the Philosopher Kings are selected and some of the attributes they are to possess, let us turn to the manner in which the Guardian class is selected from the whole city population.
According to the Noble Lie, the god added gold to the rulers and silver to the Auxiliaries and bronze to the farmers and workers.

How did Plato decide to winnow out the silver from the bronze?
The candidates were put through rigorous tests designed to lead them astray and tricking them to neglect their responsibility of protecting the city.

After testing the candidates during a period from childhood to adulthood, the candidates who would 'stick most firmly to the principle' would be chosen as Guardians.
This aspect of Plato's model state does not on the outset appear to be overtly elitist in policy.

The whole population has the potential of becoming a Guardian and possibly a Philosopher if they have the right attributes present.
Promotion and demotion seems to be an opportunity for all: '...occasionally a silver child will be born of golden parents, or a golden child of silver parents, and so on...

If one of their children [Guardians] has traces of bronze or iron in its make up, they must harden their hearts, assign it to its proper value, and degrade it to the ranks of the industrial or agricultural class where it properly belongs: similarly, if a child of this class is born with gold or silver in its nature, they will promote it to be a Guardian or an Auxiliary'.
(415 b-c).

Examining this extract would convince us that the system appears fair to everyone.
Any child who was golden, yet born in the iron or bronze class could climb the social ladder as befitted its qualities.

However it does not remain the case when we read later passages concerning the production of offspring.
Plato explains that the state needs to have a pedigree herd of people; the means of acquiring one is to: '...mate the best of our men with the best of our women as often as possible, and the inferior men with the inferior women as seldom as possible, and bring up offspring only of the best'.

(459e).
So this eugenic programme aims to have only potentially gold or silver children and to decrease the opportunity for the procreation of the population of bronze offspring.

It gets even more sinister concerning the 'undesirable' children; while the 'desirable' children are taken by officers to nurseries run by the state, the 'undesirables' will be taken away and secretly disposed of.
What is the intended result from such a regime of selection?

The offspring of the Guardian Class will be only the 'best' and will have no defective attributes; the offspring which were not desirable from the Guardian Class are to be disposed of.
In essence Plato wanted by means of the eugenic breeding, to improve the Guardian Class and in doing so even their worst offspring (provided they are healthy) will improve the lower classes as the standard will be ever increasing in excellence.

Or is there another explanation?
Does Plato mean that only healthy children of the Guardians are to survive birth and that the best of them become future Guardians, while the worst are relegated to the lower classes?

With such an overall scheme in mind; what would the opportunities of a bronze child really be?
Would they be able to be promoted at all?

Hourani takes up this question and he firstly provides opinions from both sides of the debate.
He quotes from the work of Cornford, summarizing his opinions on the matter: 'No explicit provision is made for their education; [children from the Third Class] but unless they share in the early education provided for the Guardians, there could hardly be opportunities for promoting their most promising children to a higher order'.

Hourani questions whether this was the case and confirms that there is no mention of the Third Class receiving the same education as the Guardians at all in the Republic.
The point as to which children are disposed of is enumerated by Hourani.

Seemingly the bronze civilians are allowed to marry as they please and as Hourani states the eugenic breeding programme would put the bronze children further behind as time progressed; 'the scales were weighted from birth'.
He also makes the point that the kindergarten education is not given to 'the best children', but 'the children of better parents': the Greek reads: .

Hourani also informs us that it seems likely that these bronze children would have had no education, as he states that the source for the description of the education makes no mention of anyone else but the Guardians: 'Given those natural qualities, then, how are the Guardians to be brought up and educated?'(376c).
Even though Hourani correctly states that even at birth the chasm between the bronze and silver children would be too great to bridge, I have one, possibly unfounded reservation concerning his treatment of the question.

While he explains that the Third Class is allowed to marry as they please, he omits to mention the so called mating festivals that were to take place at certain times of the year.
As it turns out, only the Guardians are to be entered into the breeding festivals; this is made clear by Plato: 'And we shall devise and ingenious system that of drawing lots, so that our inferior Guardians can, at each mating festival, blame the lot and not the Rulers'.

(460a).
To summarize then, the eugenic system was only directed towards the Guardian Class and so Hourani was quite right in his explanations.

Lee provides a brief summary of the intentions of this system: 'The real point is that what he wants is an aristocracy of talent.
He thinks, as we shall see, that he can get this largely by breeding, as one breeds race-horses; but the breeding process is not infallible, and so there must be provision for both promotion and demotion...'.

Now we have examined the selection of Rulers and Guardians from the city populace and the integration with the Third Class; we may attempt to answer the question of elitism.
I shall now summarise the elements that seem to convict Plato's model state of being elitist in nature.

These elements are for mostly the breeding programme which forms a chasm between the Guardian and Third Class at birth.
The idea that some are better suited to ruling is also thoroughly elitist.

The ruling elite would also be chosen on intellectual qualities.
These elements match the description that elitism is the:...belief that some people are inherently superior to others and deserve pre-eminence...' and furthermore 'that government or control should be in the hands of a small group of privileged, wealthy, or intelligent people'.

Yet there are also elements that do not conform to this ideal.
The ruling class does not have any property at all and in this respect the Third Class are 'superior'.

In addition, as Plato says, the idea of this model state was to ensure that all the classes of people were content, not just the ruling elite.
Balot reminds us that unlike the totalitarian states of the last century, it was not simply a matter of the elite seizing power for their own benefit.

Lee adds that: '...belief in an aristocracy of talent may be wicked; but it is not the same thing as belief in a hereditary caste'.
On these grounds then can we completely condemn Plato's model state for being elitist?

I would advocate that we should not even consider condemning him at all; his values of professional specialization are still part of our modern mindsets.
Do we not still wish to be treated by doctors who are qualified?

The same is true for lawyers, brain surgeons and politicians.
We still have expectations that certain people should and should not take on certain professions; a rather topical example is paedophiles in our schools; there we still have expectations regarding what people need to be like to be allowed to perform certain functions.

Returning to Plato and his aristocracy of talent, how can we even consider his model state as an aristocracy when we can not even define what an aristocracy is?
What are the traits that distinguish an aristocracy in the first place?

Merriam discusses the difficulties that the aristocrats themselves face when trying to legitimize such a term.
The main commonly perceived attributes of an aristocracy are wealth; birth and property.

Merriam explains that in the modern age all of these are not considered to be attributes even by the Aristoi.
He also states that: 'The historic categories of aristocracy point either in the direction of intelligence...or in the direction of...

religious organization; but neither the philosopher-king of the classical period or the prophet-priest type is seen in the aristocracy of the modern day'.
What is more, the only one of those attributes found in Plato's Republic is the elite of intelligence; the Rulers do not have wealth or property and have to rule on behalf of the ruled; doing so because they are forced to; not because they have political ambitions.

In conclusion, Plato's Republic does contain a model state that can be seen to be based on elitist principles; but we must remember that our political mindsets are different to those of the classical period.
Moreover, to condemn Plato for his elitist ideals would be to condemn our present day society for retaining them to a certain extent, after centuries of 'progress'.

If we were to condemn Plato for his viewpoints, it would be seen as an attack on our own belief that everyone is entitled to their own opinion.
Naturally we may supply the counter arguments to any viewpoint but we should never go beyond this; unless we wish to repeat the travesties that were the crusades and the witch hunts.

Chapter 1: Introduction
The purpose of this dissertation is to compare and contrast magical techniques employed by the Ancient Egyptians with the moral values found in the Wisdom Literature, popular stories and religious texts.

Lichtheim states that any Egyptian magical approach 'could not be but contrary to morality and piety'.
Yet, there is certainly reason to believe that the Egyptians did justify certain uses of magic within their own 'system' of ethical practice.

I shall therefore examine the Wisdom texts and popular stories to provide a list of values the Egyptians held concerning the use of magic.
The resulting criteria shall be compared with the magical techniques found in State Magic, Funerary Magic and Popular Magic, to grant us an overall comparison between moral thought and magical practice.

This introduction shall also serve as a discussion of several key Egyptian principles, such as Ma'at and Heka, which are crucial to the understanding of Egyptian magical practice.
Once these ideas have been discussed and the ground rules have been established; I shall proceed to analyse each field of magic individually and then conclude in a separate piece at the end.

There has been a vast amount of discussion concerning the definition of magic.
I shall not discuss these definitions here as it would involve another dissertation in its own right.

However, I shall provide my working definition of magic, at least in terms of Ancient Egypt.
The use of objects and texts that are designed to 'ward off the blow of events' will be classed as magic in this dissertation.

I hope that by using an Egyptian definition of magic, that of Merikare, I shall avoid modern definitions as much as possible.
Furthermore, much of the magical practice discussed in this work was indeed designed for such a purpose.

Next I shall focus upon two key concepts of Ancient Egyptian magic, Ma'at and Heka.
The Concept of Ma'at

Let us now move on to a brief introduction to the concept of Ma'at.
The Egyptians believed that Ma'at was the cosmic order that kept the universe functioning.

The word Ma'at itself can be translated into English as 'truth', 'order', 'justice' or 'balance'.
From a moral standpoint, people were expected to live their daily lives in accordance to Ma'at.

Allen explains that 'we distinguish good and evil from codes of law and commandments.
Ancient Egypt had no such codes: for the Egyptians the distinction was determined by practical experience'.

Gahlin informs us that rebellion, envy, deceit, greed, laziness, injustice and ingratitude were all crimes of Ma'at.
On a higher level, Pharaoh had to maintain Ma'at as his royal duty.

In his terms, Ma'at was to maintain justice, build or improve the temples and also protect the nation's borders from invasion.
Gahlin adds 'Life in Egypt was thought to be characterized by Ma'at, whereas outside the borders Isfet [chaos] reigned'.

All foreigners were considered to be elements of the forces of chaos and so they threatened to destroy Ma'at..Allen also comments that the function of the Wisdom Literature was the transmission of Ma'at.
He also makes the point that the Egyptians told stories for both entertainment and moral instruction, which legitimizes the use of such texts here to establish a set of ground rules.

Concepts of Heka and its Literary References
Concerning the idea of Heka, there needs to be a brief distinction between the modern day (and even the classical) opinion of magic and the Egyptian viewpoint.

Lloyd provides us with an excellent summary of this difference in two articles.
He states that 'magic in our sense, did not exist for much of Ancient Egyptian History'.

The idea of the universe according to the Ancient Egyptians was in fact a 'world of limitless possibilities' and those who had an understanding of magical rites could make their options considerably more expansive.
Whereas the Greeks and indeed modern society generally believe that magic is subverting the natural order; the Egyptians saw it as an integral force that bound the universe together and both man and god had access to it.

Naturally though, the gods had Heka as an attribute; while men would have to learn it over the course of their lives.
Turning to the texts themselves; we find that references to the use of magic are quite rare.

From the Instruction of Merikare there is one direct remark concerning the use of Heka: 'He [the creator god] made for them magic as weapons, to ward off the blow of events'.
Here we have our first ground rule; Heka can be used to avoid tragedy and misfortune.

The next reference to the use of Heka can be found in the Tale of the Eloquent Peasant; 'If only I had a potent divine image through which I could seize this peasant's goods'.
Lloyd explains that such an act would have been seen as totally reprehensible by the Ancient Egyptian audience as this was a device to indicate that he was a villain.

Nevertheless despite the fact it is a literary device, it shows clearly that the Egyptians felt that Heka could not be used in this manner.
The Tales from Papyrus Westcar show that Heka could be used to entertain Pharaoh; but the use of humans as guinea pigs was forbidden; Djedi objects to using a prisoner for his subject.

Though in an earlier tale; Heka was used to bring about vengeance.
Lloyd states that Heka could be used to bring about justice (i.e. punishment) for the crimes of Ma'at that the wrongdoer had performed.

The Admonitions of Ipuwer; may not directly inform us on methodology but he relates how much of the magical knowledge was stored: 'Lo, the Private Chamber [the House of Life], its books are stolen, the secrets in it laid bare, Lo magic spells are divulged, spells are made worthless through being repeated by people'.
Obviously it seems that magic was not something that Ipuwer wanted anyone to possess and this is indeed true as several rubrics found on spells instruct: 'Do not reveal it to the common man (it is )a mystery of the House of Life'.

There is also a direct mention of the Ritual of Overcoming Aepep in the Instruction of Amen-em-ope, where he says 'One spits on Aepep'.
Spitting on a figurine of Aepep was indeed part of the rite performed in that ceremony.

Using such a ritual for private magic however, according to Pinch would have been 'morally dubious'.
One last extract from the Wisdom Literature comes from the Instruction of Ankhsheshonq which is not a strong allusion to magic but it may be subtly connected; 'Do not scorn a remedy that you can use'.

Yet we can find additional ground rules in the religious texts, notably the Book of the Dead within the Declarations of Innocence or Negative Confessions.
One of the declarations is 'I have made no conjuration against the king'.

We also know that using Heka against Pharaoh was quite illegal due to the records from the Harem Conspiracy where a magician tried to use magic to harm Pharaoh; which of course is treason, a crime against Ma'at.
Towards a System of Ethical Magical Practice

These are the references to the use of magic that I have found in stories, 'religious' and wisdom texts.
There are quite a few guiding principles here with which we may work.

The complete list of ethical criteria is now listed below: Magic could be used to avoid personal tragedy or misfortune.
Magic could not be used to steal the property of another.

Magic could be used to avenge a wrongdoing and to bring justice.
Magic could be seen as a form of entertainment; but the use of humans as subjects was prohibited.

Magic could be used to defend the state against the forces of Chaos.
Magic could not be used to 'execrate' fellow Egyptians.

Magic against Pharaoh was forbidden.
Now I have established the afore-mentioned criteria, I shall compare them to the magical techniques used by the Ancient Egyptians to ascertain if their 'magical approach could not be but contrary to morality and piety'.

I shall examine any techniques or rituals which seem to contradict these ethical criteria; dividing the work into State Magic, Funerary Magic and Popular Magic.
We must bear in mind at all times as Ritner comments such divisions in magic are generally illusory; but nevertheless they aid me in structuring the work.

Chapter 2: State Magic
In this section I shall discuss the magical techniques used by the Priesthood on behalf of the Pharaoh.

Limited space prevents me from providing an exhaustive study of each and every one of the rituals employed; so I shall examine only those techniques which seem to contradict the ground rules that have been provided in the Introduction.
The other rituals and/or techniques, which have not been examined here, do not conflict with the given criteria.

The rituals and techniques that I shall examine in this section are:
The Execration Rituals and their legitimacy.The Harem Conspiracy against Ramesses III and evidence of other such conspiracies.The use of Royal Propaganda in terms of magical purposes.The magical practice of Damnatio Memoriae and its pre-requisites.

These categories are in fact interrelated as Damnatio Memoriae and the Harem Conspiracy are both extended examples of Execration.
The use of Royal Propaganda (particularly on stone) is related as it is often the prior magical technique used by the future victim, who will later fall foul of Damnatio Memoriae.

These techniques would seemingly contradict the ground rule concerning the Execration of Egyptian people and this is why they shall be examined; so we may determine the ethical complications within Egyptian legitimization of such rites.
The Execration Rites and their Legitimacy

Execration Rites were employed by the Egyptians to bring about the ritual and physical destruction of their potential enemies.
The idea of Execration can be attested as far back as the Pyramid Texts as Ritner demonstrates with the following extract: '...

Take it so that you may be strong and that he may be terrified of you - Break the red vases'.
The instruction mentioned above refers to a ceremony that was used throughout the course of Ancient Egyptian history; the Ceremony of Breaking the Red Pots.

This is only one method used by the Egyptians as an Execration Rite; there are several others which I shall examine, such as the Ritual of Overcoming Aepep and the Ritual for the Victory over Set.
The Egyptians had numerous rituals designed to keep their enemies at bay; some involved the binding or restraining of enemy figures; but the far most vicious rites involved Execration.

So what are the ethical implications of these rites?
In order to answer this question we must examine the subjects of such rituals as well as the ground rules we have established.

Firstly when the subjects of the rites are foreign enemies of Egypt; we can grant this purpose legitimacy because such rites form part of Pharaoh's duty of upholding Ma'at.
As I have mentioned, the foreign enemies of Egypt were seen as manifestations of Chaos and therefore such people would have to be kept under control.

When the rites are aimed to destroy the serpent demon Aepep, the argument of triumph over Chaos is still applicable.
As Aepep threatened to swallow the Sun each night, such a threat to the cosmic order had to be quelled.

We also know that the Egyptians thought the rite against Aepep was acceptable because a mention of it appears in the Instruction of Amenemope: 'Set your goodness before people; Then you are greeted by all; One welcomes the Uraeus; One spits upon Apopis [Aepep]'.
Here we have the comparison of two serpents: the Uraeus, the protector of the Pharaoh, often associated with the goddess Wadjet, and Aepep, the evil serpent who tries to destroy the cosmic (and therefore Pharaonic) order.

The ritual which aimed to destroy him was later adapted against another deity.
Over time the god Set was also targeted and demonized.

Set is described in the Book of Victory of Set as: '...Who did evil, who stepped outside the [prescribed] path, Robber, lord of the lie, ruler of deceit, Leader of criminals...
' This particular extract will be of more importance to the later discussion; but for now it illustrates the Egyptian discrimination against him.

So it is clear that, when the focus of the Execration Rites was either an evil god or a foreign enemy, the use of ritual destruction was considered a legitimate course of action.
Both instances also comply with the ground rules as no Egyptian is targeted.

In fact this is when Execration begins to clash with the ground rules; as we know the Execration Rites were used against Egyptians.
Pinch informs us that Egyptian traitors were included on Execration Texts and an example of such a section has been provided here: '...

all people, all nobles, all common people, all men, all women and all authorities who will rebel...
in this whole country'.

Yet one of our ground rules is that magic could not be used to execrate fellow Egyptians.
However this does not actually break our rule when we consider why such people are named.

Those who are placed on the Execration Texts are criminals who have gone against Ma'at by means of their crimes; particularly traitors.
So here we have an example of magic being used as a punishment which also matches a criterion on the list of ground rules.

An instance where Execration Rites were used upon an Egyptian who was not a criminal would therefore be immoral according to our ethical rules.
Pinch comments that an officiant of such rites was in a powerful position and could add personal enemies to the list; yet this was a 'private perversion'.

Examples of such a terrible crime exist in the form of the Harem Conspiracies.
The Harem Conspiracies

Of all the possible crimes one could perform with the use of magic in Ancient Egypt the worst was using Heka against Pharaoh.
This is directly stated as one of the Declarations of Innocence found within the Book of the Dead corpus; so we know perfectly well that it was illegal.

While the previous section examined the legitimate usage of the Execration Rites, this section will present the other side of the coin: the illegal usage of these rituals.
Fortunately small extracts from a court case survive; describing a Harem Conspiracy against Pharaoh Ramesses III.

The extracts from Papyrus Lee and Rollin describe the actions that were involved with such a crime.
Papyrus Rollin...

He began to make writings of magic for exorcising (and) for disturbing (and he began) to make some gods of wax (and) some potions for the laming the limbs of people...
Papyrus Lee...

swearing at every [time...
saying I have not given] any [magical roll] of the place in which I am to anyone on earth...

he gave him a writing of the scrolls (?) of [Ramesses III]...
Ritner summarizes the methodology of these conspiracies, saying that the conspirators had employed magical writings, inscribed wax figures and potions to exorcise, disturb, lame and enchant.

Moreover it seems that this occasion was not unique either; Ritner lists evidence for two other such conspiracies, seemingly against Amenemhat I and Sesostris III.
The conspiracy against Amenemhat I was undertaken by Intefiker; his name has been found on an Egyptian section within the execration lists of five alabaster plaques.

His name was determined by the hieroglyph for 'Enemy' which shows that he must have fallen from grace as Ritner notes.
Furthermore, the appended glyph is intended as part of a system of punishment for such criminals which I shall discuss in the next section.

We also have other evidence that Intefiker's family fell from grace because of the extent of destruction performed anciently on the Theban family tomb.
Concerning the conspiracy against Sesostris III, evidence has been found on the Berlin red pots.

These are part of an execration ritual where the names of enemies are written upon red clay pots and then ritually smashed.
From the fragments we have names which are according to Ritner characteristically of the 12 th Dynasty royal family and of women who seem to be queens.

Now that the Harem Conspiracies have been introduced let us now examine the system of punishment that was adopted by the Egyptians for such a crime- in effect a Damnatio Memoriae.
The Egyptian Damnatio Memoriae

There are two types of Damnatio Memoriae I shall discuss in the following section; the first concerns the punishment of criminals and the second concerns the destruction of the names of previous Pharaohs by another.
There are indeed strong similarities between the two variants but I shall examine them separately.

When crimes against the Pharaoh occurred, the Egyptians fought fire with fire; those who commit illegal Execration Rites would also be punished by them.
An extension of this punishment was also exacted by Pharaohs against previous ones and even against deities as we shall see.

Pinch supplies us with the system of punishment and it certainly fits perfectly with the evidence supplied in the last section.
According to Pinch: 'The names of some of the defendants have obviously been altered in the court records.

Divine elements in their names are replaced by an epithet of Aepep and another is called 'Ra hates him'.
This was the first step in identifying these political enemies of the king with the enemies of Ra and the whole cosmic cycle, just as was done in the magic ritual [i.e.

the Execration Rites].
The conspirators were either executed or forced to commit suicide'.

Therefore this magical punishment is one of the very examples of the legitimate use of magic to punish criminals which are discussed by Lloyd in his analysis of magic in Egyptian stories.
The story in question comes from Papyrus Westcar tells us of a certain Webaoner who used magic to avenge the townsman who was having an affair with his wife.

Such a usage is indeed one of the ethical criteria and therefore is legitimate as it is used in an attempt to uphold Ma'at.
The punishment of criminals by Execration also comes under this criterion and itself is a legitimate usage under the ethical system proposed in my introduction.

The purpose of these Execration based punishments was, as Pinch states, to prolong the punishment into the afterlife.
Killing the name of the subject was essential in giving him/her the second death and this is the common action of the Egyptian rite of Damnatio Memoriae.

This applies to both the punishment of criminals and also the attempts to induce the second death upon previous Pharaohs.
Now let us examine the legitimacy of both examples; do they both fit under the ethical system of magical practice that was outlined in the introduction?

Regarding the punishment of criminals, the answer is definitely in the affirmative.
The second use of Damnatio Memoriae causes more of a problem as it is a Pharaoh using it against another Pharaoh or deity.

Our first inquiry must be to ascertain what the Egyptian justification of such a rite was, if one ever existed; after all, as the upholder of the cosmic order, the Pharaoh, would hardly have had to legitimize his actions to anyone else.
Fortunately for us, one such justification has been found and is supplied by Bell.

His work on the Cult of the Royal Ka explains that it was a political device regarding the succession of the Pharaoh; 'all genuine kings possess it; no pretenders do'.
This is the justification of the second Damnatio Memoriae for which we have been searching.

It explains at the very least why a Pharaoh could use the rite of Damnatio upon a previous one.
As Bell states: 'Thutmose III could nullify the legitimacy of Hatshepsut by denying that the Royal Ka had, in fact, descended upon her...

and so the 19th Dynasty could simply ignore Akhenaten, Smenkhkhare, Tutankhamun and Ay, claiming the Ka had really fallen to Horemheb upon the death of Amenhotep III'.
If the Pharaoh could deny that the Royal Ka had been bestowed upon the target Pharaoh, he had all the justification that was necessary to classify his victim as an enemy of the state and could, subsequently, have his name destroyed as the standard punishment of criminals dictated.

We know that such an act was commonplace as we can see the traces of the destruction.
Ritner reinforces this notion by stating that the practice of widespread destruction of the images of extant enemies rendered the depictions magically useless and the effect was felt in the underworld.

He also provides a general summary of the victims of this type of Damnatio Memoriae; it included Hatshepsut (by Thuthmosis III), the god Amun (by Akhenaten), Akhenaten (by Horemheb) and also the god Set (by the later Priesthood).
Examining this list forces us to conclude that even gods were not safe from this malevolent practice, Amun and Set being among the Pharaonic victims.

How would we justify the attempted destruction of gods?
Set can be explained without much difficulty; as he was later equated with Aepep and became the subject of a similar Execration ritual.

Amun is far more difficult; he was 'execrated' only once during the reign of Akhenaten unlike Set, who was periodically execrated.
There is however, an idea located within the Ritual of Victory of Set, which might legitimize a Pharaoh, allowing him to believe he could destroy or punish a deity.

In the ritual, the gods curse Set for his chaotic behavior.
The curse reads as follows: 'You shall not be!

Your name shall not be!
Your grasp shall not be!

Your plan shall not come to fruition!...
[You] have been delivered to the foreign lands.

Loathing of Re, disgust of the great god!' There are two things to note here: first of all the obvious similarity to the Aepep Ritual, where Set has the name 'Loathing of Ra' as a criminal would have done and the second is that a god may in turn curse another god.
The Pharaoh was the son of Ra, an incarnation of Horus; by definition he was at the very least semi-divine and therefore could himself curse another god.

Surely this an extension of the ideas we have discussed and this is perhaps where Akhenaten found his justification for the attempted destruction of Amun.
Yet he would still have had to prove that Amun was committing crimes against Ma'at if it was to be an imitation of the execration of Set.

Presumably if Akhenaten himself fell victim to Damnatio Memoriae it would mean that not only was he considered a heretic by his own people but also he had committed crimes against the gods and therefore against Ma'at itself.
So we have seen that Pharaohs who either had not had the Royal Ka bestowed upon them or had committed crimes against Ma'at could also expect to suffer the same legitimate punishment of criminals of this kind.

The destruction of the images of these Pharaohs brings us neatly onto the final topic of the section; Royal monuments as magical propaganda.
Royal Monuments: Altering Reality

In the introduction to State Magic, I explained the relationship between royal monuments and the rite of Damnatio Memoriae.
The artwork to be found on such monuments could never be seen as just propaganda; it has a far deeper function compared to the modern examples of propaganda.

While the modern examples are intended to influence the way in which we regard an issue, Egyptian examples go further by intending to bend reality itself and not just the people to its way of thinking.
This magical usage can be demonstrated in a great many ways, firstly by means of the principles behind Execration figurines and images.

This was a type of sympathetic magic common to Egyptian magical practice which was extended to imagery.
Concerning the magical properties of such artwork, Ritner comments: 'Rendered tangible and permanent in stone, the image was designed not simply to reflect, but to create reality, guaranteeing by "sympathetic magic" the victory of the state and the gods.

So pervasive was this image [Pharaoh defeating his enemies] to become, that...
a rendition of the scene...

rapidly acquired more of a ritual than a historical validity'.
This usage of stone representations constitutes a form of magic under my definition, as they are objects made with the purpose of warding off the blows of events.

Now by examining the choices of scenes included in royal art, we may see that many can be equated with the ethical magical criteria in my introduction, namely the upholding of Ma'at, the cosmic order.
However, unlike twentieth century propaganda, the purpose of Egyptian propaganda was twofold.

The propaganda was there to enhance the reputation of the current Pharaoh; it was also made as sympathetic magic to ward off the possibility of foreign invasion and treason.
While at the same time a Pharaoh could attain glory for battles he may never have fought in his reign, he was displaying that he was upholding Ma'at.

To conclude; the artwork found on the temple walls also served as an execration technique, the enemies of the Pharaoh not only being destroyed by means of the traditional ritual but also by mere depiction.
Furthermore, an additional step of execration could be applied to the image itself to destroy the Pharaoh in question.

His name had to be chiseled out because as Wilkinson explains about hieroglyphics: '...from the Egyptian perspective the symbolic values of the signs obtain a virtual reality of their own'.
Chapter 3: Funerary Magic

This section would not be able to provide a broad survey of the huge expanse of funerary magic that was employed by the Egyptians; so I shall attempt to discuss only the elements, which would seem to contradict the ethical criteria set in the Introduction.
Seemingly the aspects of funerary magic most likely to conflict with the ethical guidelines are the spells which seem to modern authors to cheat destiny.

Spells used against potential enemies are no more troublesome to justify than the Execration Texts which were used on behalf of the state.
One such example of Funerary Magic that would contradict the proposed ethical criteria is Chapter 30B of the Book of the Dead.

The important aspects of the text are as follows: 'O my heart which I had from my mother!
O my heart which I had from my mother!

O heart of my different ages!
Do not stand up as a witness against me, do not be opposed to me in the tribunal, do not be hostile to me...

Do not tell lies about me in the presence of the god; it is well indeed that you should hear!' It is generally given a highly cynical treatment by the majority of modern scholars due to the nature of the spell itself.
The spell, as Gahlin, explains 'was thought to prevent the heart from owning up to any crimes the person had committed in life'.

Chapter 30B was the preferred text that was inscribed on a heart scarab which was then placed in the wrappings of mummies.
Andrews explains: 'However, at least as early as the 13th Dynasty the Egyptians had invented the heart scarab as a magical means of preventing such a dire event [i.e.

the Weighing of the Deceased showing the person to be guilty]; indeed it would allow anyone who possessed it to live a totally reprehensible life and still enter heaven'.
Surely, such a spell would clearly oppose the concept of Ma'at?

Apparently it is not as clear cut as we might think; for within the ethical criteria that have been established is an extract from the Instruction of Merikare: 'He [the creator god] made for them magic as weapons, to ward off the blow of events' It is probable that the greatest blow of fate for an Egyptian would be the second death; implying that magic could well be a legitimate tool to counteract this dreaded prospect.
Here then, we have an odd contrast of ideas: while Lichtheim was particularly adamant that using this kind of magic 'could not be but contrary to morality and piety'; the reference concerning the employment of magic against the blows of fate is found in a so called moral text.

Could it be that there is an explanation that would place Chapter 30B within the legitimate confines of our draft ethical system?
My hypothesis is that, if there is a way to legitimize Chapter 30B within our criteria, it would be given to people in the form of a reward for their services to Ma'at (essentially the present reign).

The methodology will be as follows: Firstly I shall collate various opinions of the morality of the Chapter by various authors.
I shall then proceed to examine the evidence from the various Books of the Dead that we have in order to establish what sort of people owned this chapter.

The data shall be interpreted to examine if there is any supporting evidence for my hypothesis.
Let us now analyze the current opinions from other scholars who have written on the issue to ascertain if the notion has any supporting evidence from this quarter.

Current Notions of Chapter 30B
There are seemingly two points of view regarding the usage of Chapter 30B within modern scholarship.

One argument is that using it is unequivocally reprehensible; Andrews, as we have already seen, regards the spell as completely incompatible with the morals found within the Instruction Texts.
Gahlin would seem to side with this argument as she comments that the heart scarabs and the chapter itself was only for those 'who could afford the luxury of extensive funerary equipment'.

Concerning the moral implications of using Chapter 30B, Pinch provides us with another perspective.
In doing so she makes the comparison with modern religious ideas: 'The use of this gift by the Egyptian Dead does not seem so different from the Christian doctrine that humans can not be justified by their own deeds, but must rely on divine grace to achieve salvation'.

Pinch then goes on to say that it is not that the Egyptians thought they were unworthy to pass the judgement, but that the beings of the underworld could not be trusted.
Here we have a parallel with the mistrust of enemies found in the Execration texts.

There were thought to be many demons inhabiting the Duat, who seemed to threaten any deceased person voyaging to the place of judgement.
This idea instantly brings legitimization to the spells to be used against potential enemies as they could try to impede the deceased's progress through the afterlife.

Such spells do show a resemblance to the format of the Execration texts.
Assuming then that these independent agents of the afterlife could not be trusted or they had unknowingly committed a sin; using Chapter 30B would be merely an extension of this principle; ensuring that even their heart will not betray them.

These are the current notions concerning the spell and now let us examine the evidence available to us to determine if only a certain type of person could receive such an award.
Who owned Chapter 30B?

This section shall present the data that I have acquired from the collection of the necessary funerary papyri kept at the British Museum.
The data I have included here in my discussion only include entries where the owner's name, occupation and selected spells are intact.

From a total of 97 entries a total of 20 people owned Chapter 30B.
The preceding values are the figures for the entries with all the details intact; when we consider all the entries including those who are incomplete we have from a total of 287 only 37 people who own the chapter.

This means that the overall number of people with the chapter is 12.5% from the whole set of data.
Please note that, of the 37 people who owned the chapter, 17 of them had no occupation recorded and so cannot be used for the next stage of the inquiry.

The remaining 20 have their occupations recorded and shall be used.
The kind of people who owned Chapter 30B from the data provided were senior officials such as Estate Overseers, High Priests, Domain Scribes, Granary Overseers and other important priestly posts such as the mysterious Illuminator of the Wedjat.

This would confirm Gahlin's statement; that only the rich could have afforded it.
Yet there is more interesting evidence; the chapter was also owned by other people lower down the social scale, such as Pure Priests, ordinary priests, God's Fathers (a term for a senior position) and also Copyists.

Surely if only the rich could have afforded such a spell how could people from lower levels of society have obtained them?
There are several possibilities which I shall discuss in the next section.

My next stage of the inquiry was to then produce a list of probabilities in the form of percentages for each occupation where Chapter 30B is attested.
I also divided the groups of posts into two groups.

The first group consists of the positions that were highly involved in the logistics of the state, such as Granary Overseers, Estate Overseers and Domain Scribes.
They would have been in a great position of trust and these I have named TP (Trusted Position).

The other group includes the posts that are more servile in nature; in other words they would not have come into contact with the logistics at all, such as God's Fathers and Copyists.
These I have simply named OP (Other Position).

These are the probabilities for each post of a person getting Chapter 30B: One important thing to note is the clear-cut divisions between the two groups; the TP group ranging from 66-100%; while the OP group only has a range of 12-50%.
Now the data has been supplied let us examine the implications of these results in the next section.

Was Chapter 30B an award for good service?
To answer this question, let us examine the data; if indeed the chapter could only be bought by the rich, how is it that a fraction of the people belonging to the OP group possesses a copy?

There are a variety of possible reasons why this is the case, one probable reason being that these particular people had a good personal fortune, perhaps inherited from rich family.
Alternatively we have set the qualification for the expense of Chapter 30B too high and that all of these people could well have afforded it.

That in itself would explain the results we possess, as the richer people had a better chance of getting the chapter for the very reason that it was more affordable.
Yet I would not have expected ordinary attending priests to be able to afford the chapter.

The next stage of the inquiry in pursuit of the answer to my question would be to produce an estimated wage rate of some of the OP posts and with average figures such as life expectancy and expenditure (which could be near impossible) calculate if these people could have bought a Book of the Dead of their own and if the data shows that they could not have afforded it, then we really are presented with the possibility that it was a gift or even a reward for their service.
If this was the case then using Chapter 30B would definitely fit within the confines of our ethical criteria.

It could also be the case that people who worked in the TP posts got Chapter 30B as part of the benefits of the trusted position.
It is most unfortunate as according to Cerny there has been no source mentioning the basic wage rates of a field labourer, a priest or even a government official.

Even if I was able to locate such a source, trying to determine if the person could have afforded the Book of the Dead would be near impossible.
So for the present my theory has no firm evidence and I can only advocate that the Chapter could potentially have been a reward for good service, even though the investigation will remain inconclusive.

At the very least, I hope to have added a new perspective on Chapter 30B that has seemingly not yet been touched upon.
Chapter 4: Popular Magic

The final sphere of Egyptian magic concerns the use of popular magic.
Within this section many questions need to be asked and shall each be addressed throughout the discussion.

The vast majority of spells from the Middle and New Kingdom generally do not conflict with the ethical criteria that were established in the Introduction.
Many are healing spells, while others combat potential threats.

Some are fairly aggressive in nature and share a marked similarity with Execration Rites which have been shown to be legitimately within the confines of the ethical rules.
Later in Egyptian history, this appears to change.

The greatest example of the change is the corpus of texts known as the Greek Magical Papyri; here the level of aggression seems to increase and in addition the intentions of the spell appear to be radically different from the older variants.
A great many of these spells stand in opposition to the ethical criteria and so I shall dedicate the section to investigate why this is the case and indeed if this is really the case.

My investigation shall comprise of the following elements:
A brief introduction to the Greek Magical Papyri and their composition.A comparison of the nature of these spells with the older variants from Egyptian history.A brief investigation concerning the identity of the magician who utilized these spells.

Let us now examine the Greek Magical Papyri in terms of the composition.
The composition of the Greek Magical Papyri

One aspect of the Greek Magical Papyri that makes them distinct from earlier texts of this kind is that it has a far greater diversity of cultural elements present within the corpus.
Betz comments that the main constituents of the corpus are Egyptian, Greek and Jewish among others.

It is in fact this cultural melting pot which causes the problem of assessing how many of the changes in terms of intention within the spells are purely Egyptian or otherwise.
This topic is still very much an ongoing process and currently it is difficult to separate all the various cultural elements.

The most essential piece of information for this investigation pivots around the methodology found within the spells.
It seems to be generally held that much of the methodology within the corpus is based on Egyptian techniques.

Johnson explains that: 'Perhaps even more telling is the fact that even in the spells written in Greek, the religious or mythological background and the methodology to be followed to ensure success may be purely Egyptian in origin'.
It seems that within the Greek Magical Papyri there is a mixture of texts some directly descended from older Egyptian techniques and others are also derived from Egyptian techniques but have been Hellenized in the process.

If this is the case then a comparison between the spells from the Greek Magical Papyri and the older texts from Egypt is perfectly possible, provided we bear in mind the points above in doing so.
The next point to consider relates to the aims of the spells in contrast to the older Egyptian spells.

If these texts do indeed display features that would contradict the ethical criteria; then they need to be discussed.
At the same time I shall ascertain if they can be legitimized under my current proposal just as I have done with the earlier case studies.

Continuation or Revolution: The Intentions and Methodologies of the Papyri
Once a spell from the Greek Magical Papyri can be identified as Egyptian-based, we are permitted to compare and contrast the varying purposes of the spell and the means by which they are accomplished.

I have selected a small number of spells from the Greek Magical Papyri and from those dated from the Middle and New Kingdom.
The first spell from Borghouts concerns the treating of a headache: 'Break out <vessel> of the eyebrows, a hidden part of the head, which spoils the joys of seeing!

See, if you come another time as far as this- see, then I will say: "Where is he, the one behind the door?" This spell is to be said four times'.
Another spell for curing a headache is taken from the Greek Magical Papyri: 'Osiris has a headache; Ammon has a headache at the temples of his head; Esenephthys has a headache all over her head.

May Osiris' headache not stop, may Ammon's headache at the temples of his head not stop, until he, NN stops everything...' It was common practice in Egypt throughout its history to symbolically equate the gods with the subject of the spell; the most common was associating someone with Horus so that they may be cured of an illness.
These spells from the Middle and New Kingdom allude to the mythology, whereas the spell from the Greek Magical Papyri threatens the named deities with a headache unless they remove it from the intended subject.

Here a difference in style can be noticed and this change of style is common to many if not all of the spells with Egyptian methodology within the Greek Magical Papyri.
As we have seen in the section discussing State Magic, only Pharaoh had any real legitimate justification for threatening the gods.

There seems to be a change of methodology in terms of negotiating with the deities.
Pinch points out a possible problem in making an assumption of this kind: 'The spells in the Graeco-Egyptian Papyri often achieve their goals by employing vicious curses or even death threats against the magician's enemies.

This love of aggression seems a new phenomenon, but may simply be finding open expression in private written magic for the first time'.
There are few means for us to prove or disprove this notion; the only course for us is to compare these texts with the older texts that have survived.

It is indeed possible that such texts had existed before the Greek Magical Papyri but none have survived.
Pinch also describes the main changes between the two styles of spells.

Seemingly it was the ends to which the spells were used and also the motivation that changed during this period.
Before the spells were generally for protection of a person, property, temple or field and healing.

In the Greek Magical Papyri they are motivated by desires such as sexual pleasure or financial benefit.
What could have caused these changes to the spells?

One possibility appears to be the identity of the practitioner who utilized these texts.
This shall be discussed in the next section.

The Identity of the Practitioner
A number of scholars have commented on the identity if the magician and it has been a hotly debated topic.

Betz explains that within the Greek Magical Papyri there are two types of magician one from the Greek cultural milieu and another from the more traditional Egyptian background: 'This type of wandering craftsman [i.e.
the Greek type] seems keen to adopt and adapt every religious tradition that seemed useful to him, while his knowledge and understanding of what he adopted was characterized by a certain superficiality'.

The other type was the Egyptian kind; presumably associated with the one of the temple priesthoods.
Therefore could it be possible that the changes in the Greek Magical Papyri and to an extent ethical practice were caused by a shifting of identity of the practitioner himself?

Such an evolution of identity must have occurred especially towards and during the Roman Period.
Ritner states that the shift from the Pharaonic word 'heka' to the Coptic version of the word 'hike' or the Latin 'magia' was far more than a linguistic alteration.

Whereas the older Egyptian priests would have had to assert their sanctity by expressing their prowess with heka; the later priests under the Roman Period would have had to publicly deny any involvement with hike to display their sanctity.
Here we can see the evolution of the perception of magic; it evolved from the perfectly acceptable tool during Pharaonic history to the 'subversive art' which was 'contrary to the laws of nature' as seen by those in the Roman Period.

This in turn would have dramatically altered the types of people who would be a magician.
The priests were no longer the local practitioners but the so called 'Greek type' would have replaced them.

This would explain why there is a change of ethical usage in the texts which can be witnessed in the Greek Magical Papyri.
A magician of this kind would not have needed any association with the temples and therefore would not necessarily be bestowed with the former guidelines of ethical practice which the Egyptians had once used in rituals.

Pinch explains that these magicians would have been feared and hated by the Egyptians and they were not restricted as much as the priests were.
This change of identity reveals why suddenly after a long history of spells that was essentially benevolent in nature or purpose; we suddenly witness a corpus full of potentially malevolent rituals.

However, Frankfurter provides a completely different possibility to the fore.
His work provides another explanation which will be outlined here.

He explains that the priestly network shows little sign of decline in the first two and a half centuries AD.
Furthermore, he argues that the practitioners at this time were still Egyptian priests: 'The role associated with...

the whole range of rites...
was no freelance 'magician' but the hry-hb, or lector priest, who fulfilled this role publicly...

by virtue of his association with the sacred books of the temple.
From Pharaonic times through the 4th Century, these collections of spells could belong to no other constituency than one versed in Egyptian writing.

And if Egyptian writing had once occupied scribes and clerks outside the temple proper, by the Graeco-Roman period literacy in Egyptian was the exclusive provenance- and requirement, no less- of the priesthood'.
Frankfurter also provides a reason why the papyri are mainly written in Greek by explaining that Greek had become a 'neutral lingua franca' and along with Greek mythology, became the central means for preserving Egyptian lore.

However, how does he explain the methodological and ethical contrast between the Greek Magical Papyri and the older texts, if they were used by the same priesthood?
His answer concerns economic and cultural evolution in Egypt in the Roman period.

The gradual closure and decay of the temple institutions would have caused the priests to focus more on community-based needs: '...perhaps the most resilient role for any ritual expert, the priest can diminish the scope of rituals performed, from a repertoire focused upon the temple-based festival rite to a repertoire encompassing more the crises and concerns of ordinary life'.
This would explain the increase in erotic spells and spells for success in business.

Curses and blessings and other such spells, which are either rare or not attested in the older Pharaonic texts, belong to the sphere or everyday life.
Yet how would the ethical usage of magic have changed in such a way?

In order to answer this question, we must look at the priest's economic situation.
The temples no longer provide the sustenance and so he or she would need a new source of income.

The wealth of Egypt now lay in the hands of the Roman elite and so, a priest would provide his magical services for a fee.
As Frankfurter states, the priests can be observed gaining power and prestige by assimilating the broader Mediterranean image of the 'magos'.

Seemingly the spells desired by Roman customers may well differ from the more traditional spells of Egypt.
This is where the ethical change occurs, not perhaps within the priest, but within the desires of the customer.

Yet to state that only the priesthood was involved with such a magical service would be misleading.
Even Frankfurter is prepared to admit that the traditional spells and amulets of Egypt may have 'devolved upon various non-literate local figures with no priestly background'.

No doubt there may well have been Greek magicians who had no connections with the temples operating in Egypt, but only alongside the more traditional (yet evolved) native priests.
This would readily explain Betz's statement of there being two types of magician.

With this in mind, how are we to interpret this case study if it was possibly performed by different people with a different mindset?
Furthermore, can we include the Greek Magical Papyri within the investigation, which is based on the older system of ethical practice?

These are questions which shall be discussed in the conclusion.
Chapter 5: The Legitimacy of the proposed Ethical System

Now we have looked at the aspects of Egyptian magic that could be considered to break the ethical values of my proposed system.
I shall briefly summarize the findings before examining the impact of this proposal to the current ideas about Egyptian magic.

The Execration Rites, although considered as morally dubious today, had legitimate uses under the Egyptian system.
As we have seen Execration Rites were perfectly viable means against the enemies of Egypt, natural and supernatural, and took the form of a punishment for the traitors within the state.

The Egyptian Damnatio Memoriae was simply an extension of this principle and generally the Pharaohs had a justification for such an act.
There were, however, illegal uses of such magic even in Egyptian times; the use of such rituals upon Pharaoh was considered the worst crime.

Moreover we have found that in the story of the Eloquent Peasant, the evil villain Nemtynakht wished he could use magic of a similar kind on the innocent peasant.
This vilification of Nemtynakht shows that generally the Egyptians would not have believed that using magic such as this on the innocent was permitted.

After all the words of Ipuwer demonstrate that he at least did not like anybody knowing the magical techniques of the temple magicians: 'Lo, the Private Chamber, its books are stolen, the secrets in it laid bare.
Lo, magic spells have been divulged, spells are made worthless through being repeated by people'.

Here Ritner translates the passage differently; the alternative view being that spells are 'made dangerous because they are remembered by men'.
It is clear some types of spell were guarded very carefully; the Execration Rites would have been this type.

Chapter 30B from the Book of the Dead has certainly been more problematic and, is the only aspect of magic that might contradict the system that I have proposed.
The main problem is that evidence indicating why the recipients acquired the spell is lacking.

What is more, there are seemingly no reliable sources concerning wage rates of the classes of people that are under investigation in my study.
I am currently unable to prove that the spell was a reward for good service or whether it was just a luxury for the richest Egyptians.

Perhaps the few cases which one might not expect to own such an expensive spell were richer than their counterparts from other parts of the country.
Yet we can still state that this spell does not break the ethical uses of magic that we gleaned from the texts.

The Instructions of Merikare told us that magic was a device to ward off the 'blows of fate'; could there possibly be a more terrifying blow of fate than the Egyptian second death?
Under this pretext we can allow Chapter 30B to conform to our ethical criteria; despite the fact I do not wish to use Merikare's extract as a blanket term, which would in turn allow any sort of magic under this justification.

The Greek Magical Papyri clearly are not compatible with the proposed ethical system.
The corpus has all sorts of harmful spells that would never have been part of a magician's work prior to this period; although this we can deduce from the surviving texts, so it is possible that we have not yet found evidence of these kinds of spells earlier in Egyptian history.

In the study we discovered that the change was caused by a possible change in the identity of the practitioner who was neither attached to the temples nor versed in the traditional ethical rules.
Conversely, it could just as well have been a native priesthood, which needed to adapt its traditional spells to earn an income from the new Roman elite.

It may be worth noting that I have found no references to magic within the Wisdom Literature of the later periods.
Perhaps this could be connected with the professional classes of priests and scribes vowing that they had nothing to do with Heka.

What is clear is that the change of morality and function of the spells are connected to the change of the identity of the magician or the change of economic circumstances.
Therefore if the Greek Magical Papyri were devised under a different ethical code then they do not fit with the system I have proposed which concerns the earlier periods.

This means that all of the examples that were practised in the earlier periods are indeed compatible with the ethical system I have outlined.
Weaknesses and Impact of the Study

Before I conclude it is necessary to discuss the potential weaknesses in the study.
The word limit has been a restriction concerning the extent of the material that I am able to include here, which does not give a general account of the various rituals that also fit neatly into the ethical system.

Therefore I have had to focus merely on the cases where the ethical system seemed to conflict with the magical practice.
However I hope to remedy that to a certain degree by including the material I have studied in the bibliography.

Upon consulting it, the reader will be able to assess the full range of magical rites and practices I examined as part of this study.
I would hope that the content of the study is broad reaching enough to be of use in the discussion of magic and ethical practice.

The bibliography should be able to draw the reader's attention to the aspects of Egyptian magical practice that were studied, but were omitted from the work, due to the limited space.
Now let us turn our attention to the results of the study.

The main aim of the dissertation was to examine Lichtheim's statement that any magical approach: 'could not be but contrary to morality and piety'.
Throughout the course of the investigation, we have found that magical techniques had, in fact, a certain ethical usage and even known illegal uses of such magic.

Seemingly then it appears that Lichtheim's conclusion may not be the case and that magic and ethics were set in harmony with each other in Ancient Egypt after all.
The main strength of this dissertation is that the ethical system was composed from extracts, not from magical literature, but from the ethical works or 'Wisdom Literature', which Lichtheim felt was in direct contrast to the magical approach.

If we can demonstrate that Egyptian magic was used with an ethical system that we can deduce from the ethical texts, then surely I would advocate that we should not place ethics and magic as polar opposites of each other, but integrate the two.
In this essay I shall examine the Wisdom of Solomon and comment on the role of God, the definition of piety, the construction of Jewish identity, cultural influences and the possible date of composition.

The Role of God
In this section, I shall examine how God is portrayed in the Wisdom of Solomon.

The text emphasizes that people should pursue wisdom, beginning with an uncompromising reverence to God, 'for he will be found by those who do not put him to the test'.
The description of the nature of God dominates the first chapter, with other scattered sections throughout the book.

One of the most crucial concepts expressed is that no deed, word or thought is missed by God and Justice is always achieved.
One idea, which I find surprising, is the claim that God did not create Death and that he takes no pleasure from destroying the living.

According to the author of the text, Death is an artificial construction of man, based on what he perceives to happen to those who 'die'.
God therefore is portrayed to test every individual and then either punish accordingly or bring them under his eternal protection.

Such a notion is certainly not new; for possible Egyptian influences on the text, see the cultural influences section.
God is essentially the creator of everything, shaping the universe from formless matter.

There is also a powerful description of God as he punishes the wicked; he has Justice for a breastplate, invincible holiness for a shield and a helmet of forthright judgement.
In essence there is no greater force for good.

God's power is demonstrated in the third section of the book dealing with the ten plagues of Egypt.
He is characterized as the 'Saviour of all', having a 'sweetness to [his] children' and an all powerful Word.

The text concludes with the assertion that God has never yet failed to assist the Jews.
In the Wisdom of Solomon, God is portrayed as omnipotent, omniscient and omnipresent.

He will show himself to those who have simplicity in their hearts and are pure from sin.
Despite the formidable description of his limitless power, God did not create Death and hates to see his children destroy themselves.

I would advocate that the author was attempting to come to terms with a question very much in people's minds today: why does God allow such suffering?
A question perhaps wide spread in the period in which the text was written, see the date of composition section.

In Wisdom, the answer asserted is that God designed people to be immortal, but the envy of the Devil created Death.
The Author's Conception of Piety

Piety in Wisdom is described predominantly in terms of seeking wisdom.
Much of the focus is upon kings and rulers of nations in the second section of the book.

Ruling justly, observing the law and following the will of God are forms of pious behaviour.
Another crucial aspect of piety is to desire to be instructed: 'Whoever gets up early to see her [Wisdom] will have no trouble but will find her sitting at the door'.

(6: 14).
In order to keep the laws, a ruler must love Wisdom; giving her attention will give the ruler incorruptibility.

At the beginning of the book, the core instruction of pious behaviour can be seen.
Any form of wrong doing, such as perverse thoughts, drives both Wisdom and therefore God away from the subject in question.

However the largest section on personal piety occupies the last section of the book.
One could not miss the fact that the author deemed tending divine images as a highly impious act.

What follows from 13:10 to 15: 13, as Nickelsburg states, is: '...a scorching polemic against idolatry in general and Egyptian paganism in particular'.
Yet surprisingly, during the polemic the author stumbles across a notion central to the Egyptian homage to divine images.

An image can not help itself and needs assistance from the worshipper.
The Egyptians held the belief that a statue was not a divine being in itself, but by means of certain rituals, the 'Ka' or 'spirit' of the deity it represented would reside in the statue.

Thus, serving the statue was a form of sympathetic magic; doing service to the statue was the equivalent of doing service to the deity.
Whether the author was aware of this or not, such notions were conveniently omitted from the polemic, as one would expect.

Instead the reader is told not to put his or her 'trust in lifeless idols'.
So piety, according to the author of Wisdom, is to seek wisdom, be willing to be instructed, to live by the law and not to worship idols.

The Construction of Jewish Identity
Concepts concerning Jewish identity are predominant in the third section of the book.

What better passage could be used to grant the Jews their key customs other than the Exodus?
The appearance of the Exodus has two functions in the text, only one of them being the construction of the Jewish community.

The other was to introduce the reader to the atmosphere that would later launch a slanderous polemic against idolatry.
The Jews are described in Wisdom as 'a holy people, a blameless race'.

Their resilience to hardship is a key feature of this section of the text and it is then followed by an emphasis on the rewards they received for their piety to God.
An important section concerning Jewish behaviour comes at 12: 19-22: 'By acting thus, you have taught your people that the upright must be kindly to his fellows, and you have given your children the good hope that after sins you will grant repentance'.

The next crucial section of Wisdom that constructs Jewish identity is 15: 1-6.
It serves as a distinctive difference between the Jews and essentially all the other peoples of the ancient world.

Jews do not worship idols of any form and that for the Jews 'to know...
[God] is indeed the perfect virtue'.

However it is within the criticisms of other cultures that we can analyze Jewish identity the most, for a great deal of information is conveyed not just by what the Jews believe, but also what the Jews do not, via the description of other peoples.
In this instance, then, the savage polemic against Egyptian practices can serve to illustrate Jewish identity.

Other examples within Wisdom of these insightful criticisms are God's forbearance with Egypt and Canaan.
Egypt was the case study illustrating Jewish contempt for the service to divine images, while the people of Canaan were an example of 'loathsome practices...

acts of sorcery and unholy rites'.
In Wisdom, then, Jewish identity is expressed both by assertions of Jewish culture and by scornful denunciations of the neighbouring cultures, such as Egypt and Canaan.

Therefore, Jewish readers would be constantly reminded of whom they are.
The reasons behind this will become clear when we deal with the date of the work.

Cultural Influences: Biblical, Hellenistic and Others
The Wisdom of Solomon was written in elegant Greek and as Gruen explains, was intended for an audience of sophisticated and hellenized readers.

Gruen also states that is was most unlikely that the general readership comprised of many Egyptians.
In Wisdom a high level of Hellenistic influence can be detected in two aspects of the text.

The first is the style of the literature itself and the second concerns the thoughts and beliefs behind the Wisdom of Solomon.
A number of Greek literary devices can be found within Wisdom, the most apparent is the use of synkrisis towards the end of the book.

Essentially it appears that the Wisdom of Solomon is a combination of philosophic wisdom with divine inspiration.
Hadas explains that: 'The Greek author who was at once an indubitable classic, a spiritual guide whose teachings were sympathetic to non-Hellenes, and a political theorist of high usefulness in the movement to reshape eastern policies in what was fancied to be the Greek mold was Plato'.

One example of Plato's philosophy in Wisdom is the belief of the pre-existence of the soul.
In addition Nickelsburg comments that some of the descriptions of Wisdom are 'beholden to Stoic conceptions'.

However the personification of Wisdom which extends from 6:12 to 9:18 is not believed to be a solely Greek concept.
Murphy explains that the description of the order Wisdom brings is paralleled in the Egyptian concept of Ma'at; while Nickelsburg states that the language used in praise of Wisdom are most likely drawn from the praises of Isis.

There is also a strong biblical influence throughout the Wisdom of Solomon.
The Exodus story itself is the subject of the third section of Wisdom and other biblical elements appear in other parts of the work.

Solomon has been used as the central character in the book and he is a Jewish king from the Bible.
There are both Hellenistic, Jewish and to some extent Egyptian influence found within the Wisdom of Solomon and this combination of cultures can be understood more clearly when we examine the possible dating of Wisdom.

The Date of Composition
There has been a great deal of dispute between scholars concerning the actual date of the work's composition.

The range of possible dates seems to be from the last third of the Second Century BC to the middle of the First Century AD.
Winston argues that the book was written after 30 BC when Egypt was brought under the control of Rome.

Gruen states that it was written by an Egyptian Jew in the early Roman Empire.
I would also agree with the scholars who advocate the First Century AD because of the style of the polemic against idolatry.

During this period onwards the Egyptian priests were suffering under the Roman occupation.
Such was the growing dislike of magic, Egyptian priests of this period had to prove their sanctity by declaring that they had nothing to do with the art of magic.

Formerly Egyptian priests before the Roman Period proved their sanctity by asserting they had mastery over magic.
Such conditions would appear to match the attitudes expressed in Wisdom.

Conclusion
To conclude, the Wisdom of Solomon appears to be a text written by an Egyptian Jew in Greek during the First Century AD.

The literary style is very much Hellenistic while the ideas contained within it appear to be a blend of Greek, Jewish and Egyptian ideas.
Jewish identity is expressed both by means of open criticism of other cultures and assertions of Jewish values.

The general concept of piety included the search for Wisdom, by means of an undying devotion to a God who had never failed his people and taught them the secrets of heaven to those who were pure enough.
This essay shall examine the evolution of the goddess Isis in the Egyptian, Greek and Roman environments in an attempt to ascertain whether or not she can be categorized as being strictly Egyptian, Greek or Roman.

In a similar manner; an attempt will be made to determine if Isis can be seen as an old or a new deity.
The examination of her evolution over time shall be divided into a number of categories which are:

Her appearances and depictions in Egyptian, Greek and Roman art wherever possible.Her titles that were used in the Egyptian and Graeco-Roman periods.Details of the rites performed on her behalf.
My sources for the investigation comprise of both classical texts by writers such as Herodotus and Plutarch in conjunction with Egyptian sources such as the Bremner-Rhind Papyrus and the Oxyrhynchus Papyri.

I shall attempt to analyze the sources in chronological order to aid any patterns of evolution found.
I shall begin with the Egyptian elements; depictions and accounts of Isis (or Aset/Auset as she was known in native Egyptian).

The account of the evolution of Isis in Egypt shall be kept short as it serves only as a common element of comparison upon which I shall base my discussion of her depictions in the Graeco-Roman world.
My small selection of extracts from the various Egyptian texts is by no means exhaustive.

One of the reasons why Isis became as popular as she did throughout the ancient world was because, as Wilkinson remarks, she was able to absorb the attributes of a multiplicity of deities.
Even within Egypt itself, Isis absorbed a number of goddesses before her cult spread throughout the Mediterranean.

Wilkinson provides a list of the various attributes which she came to hold in Egypt:
Sister-Wife of Osiris (Asir or Wesir in Egyptian).Mother and Protector of Horus (Heru).Mother of the Pharaoh.Goddess of Cosmic Associations.Great of Magic: 'Ur Hekau'.Mourner, sustainer and protector of the deceased.

Over the course of Egyptian History, the imagery of Isis did change and it is well worth observing the following transitions in her image.
I have provided a number of images of Isis from Egypt from varying time periods; in order to show her gradual transformations that resulted in a template, which the Greeks and Romans incorporated.

Image A is from the 19 th Dynasty and depicts Isis as suckling Seti I.
Here she has the double crown of Egypt and the vulture head dress.

Image B is older than A and is the oldest of all the selection provided here; dating to circa 1400 BC in the 18 th Dynasty.
Isis has the characteristic crown on her head which also is the symbol used to spell her name in Egyptian.

Images C and D are both Amulets portraying Isis, Image C again shows the crown unique to Isis, from the Saite Period; while Image D shows Isis with the solar disk and cow horns that she acquired from Hathor during the 18 th Dynasty.
This is dated to the Third Intermediate Period.

Finally the most recent image of the selection, E depicts Isis with both crowns and is dated to the Roman period.
From the images from Egypt alone it is clear that Isis evolved at least in terms of portrayal.

Now let us briefly examine references to Isis in a sample of Egyptian texts again from a variety of periods in Pharaonic Egyptian History.
According to Wilkinson Isis is mentioned 80 times in the Pyramid Texts most of the references concern Isis as a protectress alongside Nepthys such as the example in PT 690, 2098-99.

She is mentioned about 114 times in the Coffin Texts and in particular Isis claims to be 'a protection from fear' perhaps the later ideas of Isis as the saviour goddess can be linked to Egyptian theology.
Isis' protective role seems to be of a primary importance as she is described in a similar way to CT 775 in the Book of the Dead Corpus, BD 69; where she has 'saved me from my enemies who would harm me'.

In the Bremner-Rhind Papyrus, Isis is called 'Mistress of the Universe, who came forth from the Eye of Horus'.
This is a title that seems to have been continued in the Graeco-Roman descriptions, which are examined later in the investigation.

Now a selection of the Egyptian elements of Isis has been summarized briefly; let us use them to compare and contrast the Graeco-Roman interpretations of Isis.
Firstly beginning with the Greeks; Isis was identified with both Io and Demeter who both wandered the earth.

Io gave birth to her son Epaphus by the Nile and Demeter who searched for her daughter Persephone.
The identification with both Io and Demeter is confirmed by Herodotus: 'The statues of Isis show a female figure with cow's horns, like the Greek representations of Io' (II: 41).

'...at Busiris...a vast temple [is] dedicated to Isis, the Egyptian equivalent of Demeter...'(II: 59).
Even from this early stage in Greek History in the Fifth Century BC; Isis had been associated with Greek divinities.

Herodotus also provides us with insightful information about the spread of the Isiac Cult during his era: 'Even at Cyrene women think it is wrong to eat heifer's meat, out of respect for Egyptian Isis, in whose honour they celebrate both feasts and festivals.' (IV: 186).
We must of course be aware that this could be a case of Interpretatio Graeca on the part of Herodotus but after all this was probably the case for the extracts from Book II.

If what he says is true then by the Fifth Century BC; Isis had already begun to spread across the Mediterranean.
We also know that Isis had come to have a temple built for her at Athens in 333 BC due to a Greek inscription: '...

on which to erect their temple of Aphrodite, in the same way as the Egyptians have erected their temple of Isis'.
State Decree It is interesting that Aphrodite is mentioned here because we have an image of Isis-Aphrodite (Image F) which dates to circa 100 BC.

One of the events that probably enhanced the popularity of the goddess in the long term was Alexander's conquest of Egypt.
Alexandria became one of the most important cities in the ancient world and according to Arrian: '...he himself [Alexander] designed the general layout of the new town, indicating...

what gods they should serve- the gods of Greece and the Egyptian Isis...'(3:1) So how did the Greeks perceive Isis?
Some of her attributes are given to us by Aesop.

Dillery comments that Isis was generally associated with the invention of writing and language in general.
This is not as such a truly Egyptian attribute but similarities of Isis and the power over words can be seen in one of the Egyptian myths; namely the Secret Name of Ra.

For this reason it is not a radically new attribute conferred upon Isis and this may even be evidence that Isis remained very much the same as she spread across the Mediterranean.
From the Life of Aesop we find that Isis had the title 'Isis of the many names' ''.

Further evidence provided by Dillery (namely from one of the goddess' Aretologies from Cyme dated to the First Century BC), in which Isis proclaims that: ' '.
'I bestowed the languages to the Greeks and the Barbarians'.

The reason for conferring such an attribute seems to be that Isis was associated with Hermes (Thoth; Djehuty in the Egyptian) who was the god of writing.
According to Plutarch she was also called 'First of the Muses' as well as Justice.

This appears to be because the Ptolemies associated Isis with the constellation Virgo, which was thought by the Greeks to be Dike or Nemesis.
So to summarize; the Greeks associated her with the Muses as well as a goddess who not only invented writing but language in general and gave it to both the Greeks and the so called Barbarian peoples.

She was also associated with Justice which perhaps seems rather a new concept; if we consider that in Egypt Ma'at was the personification of justice.
However; it seems that the most commonly widespread Egyptian myth concerning Isis was the Death of Osiris and this is probably one of the main sources for the Graeco-Roman perceptions of the goddess.

In fact the myth recorded by Plutarch, in my opinion, can show that any new attributes of Isis were thought by the Greeks and Romans to be her original ones from Egypt.
So in their minds; little had changed about Isis from her worship in Egypt to her cults established in the Mediterranean.

This I hope to demonstrate throughout the remainder of this essay.
I shall now focus the discussion on the rites and festivals performed on behalf of Isis.

As this seems to form the best means of transition from Greek aspects to Roman aspects; as the rites are similar in both.
Furthermore we have sources for the festivals performed over several different periods and focusing on Roman Isiac Cults before completing the analysis would rather disturb the fluidity of a chronological based method.

Let us now return to Herodotus who provides an account of the so called 'Festival of Lamps' which seems likely to have precedents for the processions described in the novel by Apuleius.
The events at the Festival in Busiris and at Sais (probably on a national scale) are as follows: '...

it is here that everybody...
when the sacrifice is over, beat their breasts: in whose honour, however, I do not feel it is proper for me to say [Osiris]...

At Sais, on the night of the sacrifices, everybody burns a great number of lights in the open air...
The festival is called the Festival of Lamps'.

(II: 61-2).
I have attempted to find a similar festival within the official calendars of Egypt and although I can not find a mention of the Festival of Lamps; I have found an entry that reads: 'Landing of the Great Ones, the Upper and Lower Ones at Abydos; loud weeping and wailing by Isis and Nepthys, her sister over Un-Nefer [Osiris] in Sais'.

It is by no means a match but I feel it is a strong candidate for one.
Now let us compare the description given to us by Apuleius: 'There was also a large group of both sexes with lamps, torches, candles and every kind of man-made light to do honour to her from whom springs the stars from heaven.'(XI: 9).

Could there be a greater match between the accounts of Apuleius and Herodotus?
Personally I feel that they both describe the same rite; perhaps the mythological reasoning behind the two cases is different but the ritual actions are seemingly identical.

There is also another festival which I shall mention here which is known as the Navigium Isidis.
This was celebrated on the 5 th March each year and the details of the ceremonies are provided in Apuleius' account.

Essentially a procession lead down to the sea where a boat made in Egyptian style was loaded with offerings and cast out to sea.
Turcan comments that Isis of all the deities had a direct interest as she was the patroness of all sailors.

Williams also mentions that Isis was associated with the Pharos Lighthouse at Alexandria in Egypt.
The Navigium Isidis was based on Isis' wanderings in search of Osiris which again is found in the myth recorded by Plutarch.

It is clear to see that the later rites associated with Isis are in fact based on this Egyptian myth.
So far we have found evidence to show that the Greek perceptions of Isis; which can be considered as 'new' ones; were based on an Egyptian myth.

Now let us turn out attention to the Roman perceptions of Isis.
We have a variety of opinions from both Roman sources and scholars; we shall begin with Cassius Dio writing in the Third Century AD: 'Now he [Mark Antony] has abandoned his whole ancestral way of life, has embraced alien and barbaric customs...

Instead he makes obeisance to that creature [Cleopatra] as if she were an Isis...'(L: 25).
In fact it seems we have a debate between two scholars concerning the popularity of Isis.

Goodman states that the worship of Isis came from higher ends of society; but Wells contends that she was generally held in disrepute by the more respectable citizens.
One source about the public standing of the cult of Isis is from a statue in Ostia which was set up by Drusus Fabius Florus Veranus a Roman Senator.

So I would be inclined to agree with Goodman.
Other sources concerning the Roman perceptions of Isis (namely her attributes) are Apuleius and also a papyrus from the Oxyrhynchite Nome in Egypt.

Here are some of the attributes recorded in the account of Apuleius: '...
I, mother of the universe, mistress of all the elements, first born of the ages, highest of the gods, queen of the shades...

representing in one shape all gods and goddesses...'(XI: 5).
We also have two sources that reinforce the account of Apuleius which come from Egypt.

The first is provided by Walbank and is a prayer to Isis by a certain Isidorus; an Egyptian priest of the First Century BC.
It was inscribed at the temple of Isis in Medinat Madi in the Fayum.

It reads as follows: 'The Syrians call you Astarte-Artemis-Nanaia and the tribes of the Lycians Queen Leto, and the Thracians call you indeed the mother of the gods, and the Greeks mighty-enthroned Hera, and Rhea and Demeter, but the Egyptians Thiouis, because in your own person alone you are all the other goddesses named by the peoples'.
Here we have Isis once again essentially addressed as '' and the next source expands on this theme.

Also found in Egypt, a papyrus from Oxyrhynchus seems to be a longer version of the passage found in Apuleius.
The implications show that not only were these titles attributes of the Romans but also those who worshipped Isis in Egypt.

The papyrus is believed to predate Apuleius, being written in the early Second Century AD; whereas Apuleius wrote at the end of the Second Century AD.
The final aspect of this section is to examine the representations of Isis in Roman art.

I have provided the Egyptian images and then a Greek one earlier with which we may compare the following two images.
Image G is a depiction of an Isiac ceremony from Herculanuem circa 50-75 AD.

Image H depicts Isis on the left with very Romanesque attire and a pair of cow's horns.
We have seen that the iconography of Isis changed considerably over time from the sample of images I have presented.

She has evolved from the Egyptian Isis with the Aset crown to the infamous Isis with the solar disk and cow's horns; which she acquired from Hathor in the 18 th Dynasty.
She then was Hellenized as we saw in the image of Isis-Aphrodite and then Romanized with only the cow's horns signifying her identity.

In terms of her attributes, we saw that as Isis spread across the Mediterranean, she seemingly acquired new titles and was associated with a whole myriad of other deities.
To mention a few she was assimilated with Hathor, Bast, Io, Demeter, Aphrodite, Dike, Hermes and Astarte.

So was Isis an old or new goddess?
We can contrast the attributes from Pharaonic Egypt with those from the Oxyrhynchus Papyrus.

Still in the Roman Period Isis is addressed as the wife of Osiris and mother of Horus; as well as the 'Mistress of the Universe'.
Not to mention her funerary qualities are still expressed as 'queen of the shades'.

I believe that although the details in her iconography were adapted, the general identity and attributes of the goddess remained the same.
From the beginning in Egypt she was known as 'Mistress of the Universe' and this title continued down into the account of Apuleius.

Her festivals performed in Greece and Rome may not have been identical to those in Egypt but they were all based on the Death of Osiris Myth from Egypt, recorded in Plutarch.
However to use such terms 'old' or 'new' is not a valid procedure; as not only are such terms relative in usage but they are also force us to create a direct contrast that in the case of Isis and her 'evolution' does not apply.

As the evidence shows she hardly changed at all.
Regarding the question of whether Isis was Egyptian, Greek or Roman; it would be fair to say that her origins and surrounding mythology was generally Egyptian; but it is misrepresentative to make such a simple statement.

Such a question is dependant upon the worshipper; a Roman would have seen her as Egyptian, but a native Egyptian who witnessed the methods which the Roman worshipper used would not agree.
It would perhaps be advisable to dispense with all these terms, old, new, Egyptian, Roman altogether.

It would be more precise to narrow the focus down to one particular period.
For instance 'how Egyptian was Isis during the Hellenistic Age?' would be more suitable to determine the national identity of the goddess.

Even then the Egyptian beliefs seem to have changed slightly since Pharaonic times to the Ptolemaic and Roman Periods; so using these terms is a hindrance.
In terms if her evolution; I would agree with Potter's comment on the Isiac Cult bearing the last few points I have discussed in mind: 'Indeed, the cult of Isis remained as recognizably an Egyptian cult, as the cult of Yahweh remained a recognizably Palestinian one...

' Therefore I would conclude that although Isis evolved over the course of time in terms of iconography; absorbing various national identities; she essentially remained the same in many core aspects.
Plutarch's version of the Osiris myth in De Iside et Osiride has been incorporated within other native Egyptian myths in mythological publications for much of the twentieth century, if not earlier.

Pinch, in particular, feels that this habit has been damaging for the study of Egyptian mythology.
She asserts that reading Plutarch or a non Egyptian account has caused the habit of 'perceiving Egypt through Greek or Roman eyes'.

The objectives of this essay are fourfold:
To compare Plutarch's version of the Osiris myth with native Egyptian accounts, in order to assess the extent of dissimilarity between them.

To investigate the sources available to Plutarch and place his version in the context of the mythical tradition.To judge how damaging it would be to continue the habit of incorporating a Greek account within a body of Egyptian myths.To examine how Plutarch uses the myth to promote his theories of the divine and to compare this with roughly contemporary editors of mythological texts from Egypt, the Theban magicians.
Each of the objectives will constitute a section of the essay and the conclusion will combine the results of each inquiry to ascertain whether Plutarch's Greek version is damaging when included among native Egyptian myths or, on the contrary, a useful tool, which orders the chronologically scattered parts of the myth found in Egyptian sources.

The Predecessors of Plutarch's Version
Before we may assess how Plutarch uses the Osiris myth to his own ends, we must first examine the versions of the myth from Egyptian sources.

The Osiris myth appears in texts right across the spectrum of ancient Egyptian history, from the earliest Pharaonic dynasties to the Roman Period.
The belief that Osiris was not only proof of, but also the means, to reach immortality was retained by the Egyptians throughout their religious history.

Furthermore, the nature of the evidence is diverse, chronologically and thematically.
There appear remnants of several traditions of the myth, often within the same source or corpus of texts.

One common example of this phenomenon is that during the conflict between Horus and Set for the throne of Egypt, the two warring gods are sometimes depicted as brothers, and at other times, in accordance with Plutarch's version, Set was Horus' uncle.
Both of these seemingly once independent traditions are found together in the Contendings of Horus and S et.

Despite the fact that we have no ordered equivalent of the Osiris myth from Egyptian sources, we can create an ordered list of the key events of the myth from the texts.
The texts I have studied are: a) The Pyramid Texts (PT): the oldest religious texts from Egypt, which were inscribed on the pyramids of the last pharaoh of the 5 th Dynasty and most pharaohs of the 6 th Dynasty (c.2375- 2181 BC).

They consist of roughly 800 spells to ensure the pharaoh immortality.
b) The Memphite Theology: although recorded on a stone inscribed during the 25 th Dynasty (c. 710 BC), this text is believed to date back to the Old Kingdom (c.2686-2181 BC) and thus makes it roughly contemporary with the Pyramid Texts.

c) The Coffin Texts (CT): are essentially revisions of the Pyramid Texts, with new spells and date from c.2181 -1650 BC.
They consist of roughly 1200 spells with similar intent to their predecessors.

d) The Great Hymn to Osiris: dates from the 18 th Dynasty (c.1550-1295 BC) and contains the fullest Egyptian account of the Osiris myth.
e) The Contendings of Horus and Set: dates from the reign of Ramesses V (1147-1143 BC).

The text focuses on the legal battles between Horus and Set.
f) The Book of the Dead (BD): is the successor to the Coffin Texts and again is a revised body of spells with many new texts.

These selections of spells were used from c. 1550 BC and were standardized in the 26 th Dynasty (664-525 BC).
g) The Lamentations of Isis and Nephthys: was written in the Ptolemaic Period (332-30 BC).

It is believed to be a parallel version, not an abridgement to the text on Papyrus Bremner-Rhind.
h) The Metternich Stele: is covered with texts to protect the owner from snake and scorpion bites.

Parts of the Osiris myth featuring Isis make up much of the text.
It is dated to the reign of Nectanebo II (360-343 BC).

i) Papyrus Bremner-Rhind: is a collection of ritual texts related to the mysteries of Osiris.
It was written in the reign of Alexander II in the year 312-11 BC.

j) The Book of Victory over Set: is another ritual text, designed to execrate the god Set, written in the 30 th Dynasty (380-343 BC).
k) The Greek Magical Papyri: date from the 2 nd Century BC to the 5 th Century AD.

They are a collection of mainly ritual texts written in Greek, Demotic and Old Coptic.
Due to the space restrictions, I have compiled a table (below), which lists all the main features of the myth reported by Plutarch and shows the Egyptian texts which have the same statement.

In addition I have included some of the Egyptian traditions that feature prominently in the texts but not in Plutarch.
These few exceptions are 15-17 and 20.

I have listed the texts according to letter and arranged them chronologically.
Let us now draw a few observations from the table.

Firstly, many of the episodes described in Plutarch can be traced in Egyptian texts right back to the Pyramid Texts (A in the table).
One crucial observation is that the closest parallels to Plutarch's version fall within the 30 th Dynasty and Ptolemaic texts, (texts H, I and J).

We have thus established that Plutarch's version of the Osiris myth matches many episodes recorded in the Egyptian sources and that the Greek and Ptolemaic versions are closely linked.
This insight will now allow us to proceed to assess the possible candidates for Plutarch's sources.

Plutarch's Sources
Since Plutarch could not read Egyptian he had to rely on accounts concerning Egyptian religion written in Greek.

Two of these accounts were by Herodotus and Diodorus Siculus.
We can swiftly dismiss Herodotus as Plutarch's primary source for the Osiris myth because Herodotus hardly discusses it at all.

Herodotus' accounts related to the Osiris myth can be summarized as follows: '...
For not all Egyptians worship the same gods- the only two to be universally worshipped are Isis and Osiris, who they say, is Dionysus'.

(II: 42) 'Nevertheless, before their time Egypt was, indeed, ruled by gods, who lived on earth amongst men, sometimes on of them, sometimes another being supreme above the rest.
The last of them was Horus the son of Osiris- Horus is the Apollo, Osiris the Dionysus, of the Greeks.

It was Horus who vanquished Typhon and was the last god to sit upon the throne of Egypt'.
(II: 144) 'The Egyptians have a legend to explain how the island came to float: in former times Leto, one of the eight original deities, lived in Buto, where her oracle now is, and having received Apollo, son of Osiris, as a sacred trust from Isis, she saved him from Typhon when he came there in his world-wide search, by hiding him in the island'.

(II: 156) Herodotus does, however, display at least partial knowledge of the myth.
He is aware of the rivalry between Horus and Set and that Horus was hidden from Set while he was young.

Yet interestingly Plutarch does not mention the hiding of Horus in the marshes, but he does mention that Horus was raised by Isis in Buto.
It is possible then that Plutarch may have used Herodotus for this section of his treatise.

Let us turn our attention to Diodorus Siculus' account of the myth.
His version is of particular interest since Plutarch and Diodorus may well have shared a source.

In fact there are a number of similarities between Diodorus and Plutarch, the first is the account concerning the birth of the early gods.
Both accounts have Helius (or Sun) followed by Cronus and Rhea.

Both writers have knowledge concerning the five intercalated days of the Egyptian calendar and each state that a god was born on each day.
Furthermore, both use the same gods, Osiris, Apollo (Horus), Typhon (Set), Isis and Nephthys (Aphrodite); even though the order of births is marginally different.

What is more remarkable is that both writers continue to describe in varying detail how Osiris traveled the world and civilized many nations.
Whereas in the birth of the five gods, Plutarch provided more detail than Diodorus; Diodorus provides much more information on the wanderings of Osiris.

These close parallels continue along the course of the myth, each adding more or less detail than the other at various points.
The following points of the myth are both described in the same order in both accounts:

Osiris, Isis, Horus/Apollo, Set/Typhon, Nephthys/Aphrodite are born each on a single days for 5 days.
Osiris wanders the earth and civilizes mankind.Set murders Osiris.Osiris is dismembered and different members are buried in different locations.Isis had to make a replica phallus for Osiris.

Therefore, it is clear that Plutarch and Diodorus shared the same source, at the very least for their versions of the Osiris myth.
This means that the common source would have to predate Diodorus, but probably postdate Herodotus.

My reasoning for dating the common source after Herodotus is that Herodotus himself admits that his account is to recall the thaumata 'amazing things' and had he more information on the Osiris myth, his version (II: 144, 156) would have contained more detail to astonish his readers.
However, we have so far only examined the Greek evidence for the common source.

The Egyptian material also provides important clues concerning the possible identity of the shared source.
Within the Greek Magical Papyri parallels to Plutarch's version of the Osiris myth may be encountered.

This body of ritual texts has many allusions to both Egyptian versions and Plutarch's own version of the Osiris myth.
The following extract from the Greek Magical Papyri matches statements found in Plutarch: 'O lamp, I call to you while you are going up upon the great sea, the sea of Syria, the sea of Osiris'.

(PDM xiv: 150-231) Compare this to Plutarch (de Iside: 15), who states that Osiris' coffin washed up at Byblos (Syria).
This is the most important reference as the only other source to explicitly state that Osiris washed up near Byblos is Plutarch.

Among other references from the Greek Magical Papyri is the equation of Osiris with river water and Set or Typhon with sea water.
It would appear that the common source shared by Plutarch and Diodorus also found its way into the Egyptian tradition.

PDM xiv has been dated to no earlier than the beginning of the second century AD in a recent study by Dieleman.
This would make this particular papyrus roughly contemporary with Plutarch's account, which has been dated to c. 115 AD by Jones.

Therefore, we can now state the attributes of the shared source.
It was written in Greek, it has to predate Diodorus and Plutarch.

It may or may not postdate Herodotus, though from what we know of his motivation for writing, his lack of detail would suggest that it postdates him.
Furthermore, the source was used in Egypt and the details correspond perfectly with details from older Egyptian texts.

Thus, our source is from Egypt and dates between c.425 BC and c.56 BC.
Given that in the section entitled 'Plutarch's Predecessors' we established that the closest parallels to Plutarch's version dated from around the 30 th Dynasty and the Ptolemaic period, these proposed dates agree completely.

Our common source would most likely be Manetho, who lived during the reign of the first three Ptolemies.
The fragmentary nature of Manetho is indeed problematic, yet there are several points which suggest that he is the strongest candidate for the common source.

Firstly, from what we know about Manetho, he became an authority on the Serapis cult.
In addition it appears he was heavily involved in establishing the cult in places other than Egypt and may have acted as a consultant.

His links with the cult of Serapis, which was based around a combination of Osiris and the god Hapi, make him an ideal candidate for the common source.
Fortunately we have more to work with than speculation concerning his career.

A number of the fragments are virtually identical to account given by Plutarch and Diodorus on the early gods.
Fragment 2a, (F2, 3a, 3b and 3c in Jacoby's system), describe the first gods to rule Egypt: '1.

The first person among the Egyptians was Hephaestus, who also discovered fire for them.
2. From whom was Helios.

3. After whom was Kronos.
4. After whom was Osiris.

5. And then Typhon, brother of Osiris.
6. After whom was Oros, son of Isis and Osiris'.

Diodorus also includes a story in his section on the early gods in which Hephaestus teaches mankind how to use fire.
Furthermore, his account, which attributes each of the gods to a force of nature or a heavenly body, matches Manetho's fragment F24 exactly, both give the following equations:

Osiris = the SunIsis = the MoonZeus = the spirit who moves through all things/ the windsHephaestus = fire Demeter = the EarthOceanus = moistureAthena = air
One way or another, Diodorus used Manetho even though he never cites him.

On the other hand Plutarch cited Manetho on a number of occasions throughout his account.
Plutarch's version of the early gods does indeed differ slightly from Diodorus, but it is still plain to see from their shared version of the Osiris myth that whatever changes Plutarch may have made, he shared the same source.

Plutarch's Place in the Tradition of the Osiris Myth
In this section, we will examine the differences and similarities between Plutarch and his Egyptian predecessors more thoroughly.

In doing so we will also assess just how damaging it really is to place Plutarch's version of the Osiris myth among other native Egyptian myths.
Thus far, we are able to reconstruct a rough tradition of the Osiris myth, which in its current state is depicted in figure A.

The diagram is not intended to show all the other possible sources that Plutarch may have employed in composing his treatise.
For instance we know that Plutarch does occasionally use material from Herodotus.

Also it is well known that the Greek Magical Papyri were influenced by many other Greek ritual sources, but here I am showing merely the proposed tradition of the Osiris myth.
Now let us examine the topical differences that can be found in Plutarch's or possibly even Manetho's version compared to Egyptian texts.

There are a number of episodes in Plutarch's myth which employ themes from older Egyptian sources but are used in a new way.
The episodes in question are:

The cursing of Nut (Rhea) and the intercalary days.Set's conspiracy at the banquet.The floating chest of Osiris subsequently washed up near Byblos.The chest becoming part of a tree, later cut down and used as a pillar.Isis and the encounter with the queen of Byblos.Osiris sleeps with Nephthys.Isis creates a replica phallus for Osiris.Set (Typhon) as the sea.
Most of these episodes may have been either the result of the merging of other Egyptian stories or may be based on similar episodes found within the pre-existing tradition of the Osiris myth.

The birth of the five gods on the intercalary days is essentially unique to Plutarch, in regard to the narrative behind the episode.
However, the order of the children of Nut and the involvement of Thoth (Hermes) has an Egyptian basis.

Thoth was associated with recording the calendar and the seasons of the year.
A number of Egyptian calendars also have the following order of births: Osiris, Horus the Elder (Apollo), Set (Typhon), Isis and Nephthys.

This order matches Plutarch's account precisely.
While Set's scheme at the party for Osiris is again absent from Egyptian sources; there is one instance of Set holding a party in order to humiliate Horus in the tribunal.

This is a common phenomenon and a number of the above episodes have Egyptian precedents although the subject of the episode is often different in Plutarch's version.
The floating chest of Osiris is a very intriguing episode.

Holley informs us that the floating chest is a recurring motif in Greek mythology.
There are about eleven such stories in Greek literature.

It is highly tempting to see the chest of Osiris as an adaptation from Greek mythology.
Yet there appear to be allusions of the chest in Egyptian texts and I feel it is misguided of Holley to say that the whole point of the story for Plutarch was to explain away Egyptian rites at Byblos.

A chest with entrails of Osiris is mentioned in the Book of the Dead and also a spell from Papyrus Chester Beatty VIII.
There is some form of link with Byblos in the Egyptian tradition beginning with the Coffin Texts.

In one spell Hathor, Lady of Byblos, makes a steering pole of a barge for Osiris and in another, Horus gives a steering pole of cedar from Byblos to Osiris.
There was a long history of importing cedar wood from the region of Syria, particularly Byblos.

In addition a 20 th Dynasty story features Byblos as the provider of timber.
Furthermore, Osiris had floated down to the sea in the Egyptian texts.

The first citation of this comes from the Pyramid Texts, the oldest religious texts from Egypt itself.
In the Middle Kingdom a text states that the god Sobek crossed the 'Great Green' (i.e. the Mediterranean) to find the body of Osiris.

The notion of the chest of Osiris being surrounded by a tree may also have an Egyptian precedent.
Pinch mentions that in the city of Herakleopolis the soul of the dead Osiris was believed to emerge from a sacred tree.

Lutz has argued quite convincingly that the Djed emblem of Osiris was descended from an older depiction of the tree of Osiris and that it may well be a borrowing from Mesopotamia.
One particular version of the Djed emblem has the pillar as the body of Osiris with his arms showing holding his characteristic symbols, the crook and the flail (below).

The episode of Isis and the queen of Byblos is similar to the story written on the Metternich Stele.
In Plutarch's version Isis becomes a nurse to the queen's child and every night burns a part of the child's mortal body away.

The story from the Metternich Stele includes an encounter with a noble woman who refuses Isis shelter and slams the door in her face.
One of the seven scorpions traveling with Isis bites the noble woman's infant child; no one answers the screams of the noble woman except Isis, who saves the child.

Details have changed but essentially Isis still interacts with a noble woman whilst on her search for the body of Osiris and in both stories is seen to help an infant child.
The episode of Osiris sleeping with Nephthys is seemingly a late tradition.

It is not mentioned at all in the Egyptian texts except in the Greek Magical Papyri.
It may well be a late popular story that has not survived.

The concept of Set as the sea may well be traceable to an Egyptian story, dating to the reign of Amen-hotep II (1427-1400 BC).
The story is very fragmented but there is some form of contest between Set and the sea.

It is possible that over time Set's role was reversed and he was then thought of as the sea itself.
The last episode is when Isis creates a replica phallus for Osiris as the original was eaten by a fish.

No parallel for this narrative exists in Egyptian texts, but there are two citations where Isis does create a replica member for someone.
In The Contendings of Horus and Set, Set holds a party so as to humiliate Horus.

He does this by having intercourse with him in an effort to impregnate him with his semen.
Horus, however, manages to catch the semen in his hands and shows Isis what had transpired.

She at once cut off his hand and threw it into the river.
It is this incident which is also alluded to in the Book of the Dead: 'I know the mystery of Nekhen; it is the hands of Horus of his mother's making which were thrown into the water...

And Re said: "This son of Isis is injured by reason of what is own mother has done to him; let us fetch Sobek from the back of the waters, so that he may fish them out...' To conclude this section, we have seen that Plutarch's version of the Osiris myth corresponds very closely to the known Egyptian versions of the myth and that where he appears to add totally un-Egyptian episodes, he may well in fact have based them on very similar episodes found elsewhere in other stories.
In effect, Plutarch's version summarizes and orders the many Egyptian versions of the myth, which are descended from the Pyramid Texts.

Therefore I would argue that it is not particularly damaging to the study of Egyptian mythology to include Plutarch's version among other Egyptian myths, since very little of it is not attested at some point in the older Egyptian tradition.
Reinterpreting Egyptian Mythology

In this final section, we shall examine how Plutarch uses Egyptian mythology to expound his cosmological theories compared with near contemporaries from Egypt, the Theban Magicians.
Let us begin with the Theban Magicians.

These individuals are the unnamed editors of one particular group of texts within the Greek Magical Papyri.
They worked on their texts from the Ptolemaic to the Roman period and the result of their work is now called the Theban Magical Library.

Dieleman has listed the following texts, which belong to the collection:
PGM IV (4 th Century AD?)PGM V (4 th Century AD?)PGM XII (2 nd Century AD)PGM XIII (4 th Century AD)PGM XIV/ PDM xiv (2 nd Century AD)

He also lists the following which may have been in the Theban Magical Library, but there is no proof to validate this:
PGM I (4 th/5 th Century AD)PGM III (4 th Century AD)PGM VII (3 rd/4 th Century AD)PGM LXI/ PDM lxi (5 th/6 th Century AD)PDM supplement (3 rd Century AD)

Other texts from the Greek Magical Papyri thought not to belong to the Theban Magical Library are:
PGM XX (1 st Century BC)PGM XXIVa (3 rd Century AD)PGM XXXVI (4 th Century AD)PGM LVII (1 st/2 nd Century AD)PGM LVIII (4 th Century AD)PGM LXXVIII (3 rd Century AD)

In the following graph I have shown the proportion of Hellenized Greek and Egyptian elements of the Osiris myth in each of these groups.
It also demonstrates the difference the second group of texts can make when added to either the Theban Magical Library or the group not from that collection.

As there is little evidence to place the second group of texts in either the Theban Magical Library group or the non Theban Magical Library group, I have displayed three scenarios on the graph.
Scenario A considers the second group as part of the Theban Magical Library, scenario B considers that the opposite is true.

Scenario C leaves out the second group altogether to show the two concrete groups as they stand.
It is clear that whichever scenario is true, the Theban Magical Library contains a much higher proportion of Greek elements of the Osiris myth.

This corresponds to Dieleman's theory that the demotic spells were copies from Greek spells with traditional Egyptian elements incorporated within the text, dating from the early Roman period.
The Osiris myth in the Theban Magical Library is not employed very differently from older Egyptian texts.

All Egyptian texts utilize the myth as magical historiolae, which equates their situation with that from the myth and thus secures the same outcome to occur in their own scenario.
The small difference is that one spell has the magician assisting Set to defeat Osiris, something unheard of from the older texts.

Let us now turn to Plutarch.
Richter claims that Plutarch uses the Osiris myth to create a Greek cultural superiority to Egyptian lore.

I am indeed happy to accept Richter's thesis generally, however, one particular point warrants further examination.
He states: 'In the de Iside, what is barbaric and false is consistently Egyptian- in the same way Greek allegorical interpretation, via citations from canonical Greek authorities, leads the soul towards knowledge of the divine'.

I would argue that it is not all things Egyptian that Plutarch consistently rejects but instead the accounts given by his Greek predecessors.
Let us examine some of the same extracts used by Richter to seemingly prove the opposite.

Plutarch dismisses accounts which equate a god to one particular body or natural phenomenon.
This is precisely what Diodorus and Manetho postulate.

The other major section that Richter uses is about sacred animals.
Two points Plutarch rejects are:

The soul of Osiris resides within the Apis bull.The gods hid from Typhon by becoming different animals.
Both of these accounts can be found in Diodorus (I: 85-6).

Most of the extracts where Plutarch rejects an account have either a parallel quoted in Diodorus (and sometimes in Manetho) and/or contain a citation to Manetho.
Given that Plutarch already wrote a reaction to the work of Herodotus prior to this work; I feel it is not unjustified to suggest that the de Iside is both written to return cultural superiority to Greece and a reaction against earlier Greek writers, who have not used Greek philosophical ideas to good use.

I would like to conclude the essay by demonstrating the difference between Plutarch and the Theban Magicians concerning the use of the Osiris myth.
The key difference between them is a conceptual one.

While the Egyptians emulate the behaviour of the gods in myth because myths describe the nature of the cosmos (for example: Set killed Osiris in myth because he did so in reality); Plutarch believes that they re-enact the myths, which in themselves are symbolic of the nature of the universe (Set tries to kill Osiris because the chaotic part of the soul always fights the ordered part).
Nor do I believe that he claims that Greek philosophers are the only ones capable of realizing Greek philosophical ideas as Richter suggests, since Plutarch on one occasion even praises Egyptian lore for containing ideas similar to Plato.

Conclusion
Plutarch's treatise contains a version of the Osiris myth that at best matches native Egyptian tradition perfectly and at worst uses episodes present in other Egyptian stories.

I would argue that his ordered account of the myth is not highly damaging to the study of Egyptian mythology, when placed among other native legends.
It also appears to be a reaction against earlier Greek accounts concerning Egyptian religious philosophy, while at the same time utilizing Egyptian mythology in only a marginally different way from the Theban Magicians and their Egyptian predecessors, advocating his Neo-Platonist theories concerning the divine and the nature of the cosmos.

Herodotus and Egypt
The sheer bulk of material written about Herodotus and Egypt has, unsurprisingly, constrained me to focus upon a particularly narrow aspect of Herodotean scholarship, namely the veracity of his historical methods and his relationship with his predecessor Hecataeus.

I have intended to produce a broad sweep of the most important literature on this topic over time from the late nineteenth century to the present.
Commentary

Debate concerning the veracity of Herodotus' historical methods and his relationship to his predecessor Hecataeus has been raging at least as far back as the late nineteenth century.
It would be almost impossible to comment upon both aspects separately as both are interrelated.

I shall provide a general sweep of the various schools of thought from the late nineteenth century down to the present day.
There are arguably three main schools of thought concerning our inquiry.

The third is relatively recent and the other two have been in competition with each other for a considerable length of time.
The essential arguments of the two older schools have remained unaltered over time, though in some cases slight details have been revised.

One school argues that Herodotus is telling us the truth and that the details of his account can be matched to corresponding Egyptian evidence.
They frequently attribute inaccuracies either to genuine misinterpretation by Herodotus, or alternatively genuinely innocent or deliberately construed misinformation by his sources.

I have managed to trace this school back to Sourdille and Wiedemann at the turn of the twentieth century, though it is probable that the school is older.
The other school, named the 'Liar school' by Kendrick Pritchett, advocates that Herodotus either created wholly fabricated accounts, due to the fact that he never went to Egypt, or similarly he copied and adapted passages from Hecataeus.

The oldest scholar I have found who claims that Herodotus was fabricating his account is Sayce who states that 'The majority of the statements made by Herodotus about Egyptian matters are now known to be false, and...
there are many in which we can trace a deliberate intention to deceive'.

It is clear that already some scholars had begun to suspect Herodotus' motives in the nineteenth century.
In my research I have found a greater number of scholars occupying the 'Misinterpretation School' over the course of time.

It is possible that the choice of books that I have made has had an effect on the statistics.
Fortunately I have found evidence within the literature that suggests this phenomenon is not merely an unbalanced selection.

Both Fehling and West comment that the (then) current trend was to take a more skeptical approach to Herodotus, while the last century or so has taken a 'more favourable view' of Herodotus' veracity.
Having mapped out the progress of both schools of thought from the late nineteenth century, it seems that from the 1910's until the 1960's, the 'Liar School' was outnumbered by the 'Misinterpretation School' at least by 2:1.

From the 1970's and 1980's onwards the numbers have evened out somewhat, quite possibly due to the emergence of a third 'school' that has gathered pace ever since.
This school shall be introduced once we reach the period of its emergence in our chronological sweep of the Herodotean literature.

The concept that Hecataeus was the source of much of Herodotus' account seems to have been present in Meyer's work in the 1892 and was quoted 'with approval' by Jacoby, writing in 1912.
Heidel developed the arguments further stating that some passages were a convenient fiction told in a witty manner by Hecataeus, which was then adapted by Herodotus, 'who was not only gullible but malicious'.

One of the most influential publications from the Liar School was by Fehling in 1971, 'whose forceful indictment of Herodotus' veracity is not easily refuted'.
In a review of Lloyd's commentaries on Herodotus Book II, Fehling states that particular passages within Herodotus, namely on Helen in this case, he 'would call a free creation'.

The two other prominent members of the Liar School, according to Kendrick-Pritchett, are Armayor and West.
Other members of the school mentioned by West are Swain, who according to West was 'the first to venture in this direction', i.e. that Herodotus' Theban Priest passage was an invented fiction on his part; and Erbse, who 'emphasizes the weaknesses of traditional views' of Herodotus.

We now encounter a problem.
So far our sweep has let us to believe that Fehling's publication has more or less overturned the arguments of the opposing school.

Now turning to the 'Misinterpretation School', we shall discover that this is not the case and in effect a scholarly stalemate is perceived, especially by members of the third school.
The 'Misinterpretation School' has had many members; I have traced the school back as far as Sourdille, who produced a highly influential work on the voyage of Herodotus.

His work was believed by this school for a great length of time and glaringly obvious errors, according to West, were only rejected in the late 1970's by Lloyd in his commentary.
Yet a crucial point is that Lloyd did not reject outright that Herodotus ever went to Egypt, nor does he openly reject the order of Sourdille's reconstructed itinerary, but states that the time of the year was wrong.

Lloyd is not the only scholar to have taken Sourdille's account as a valid reconstruction; his work was accepted by members of the 'Misinterpretation School' from Spiegelberg in 1927 to Lloyd in 1975-6.
Both Sourdille and Spiegelberg's works heavily influenced the scholars of the following decades such as Waddell, Griffiths and Zakbar.

Moreover, the belief that Herodotus based his account of Egypt on historical examples of Egyptian culture, which he had encountered, is still advocated by members of this school.
In 1989 Obsomer interpreted the campaigns of Sesostris in Asia Minor as a misinterpretation of the Nubian conquests by Senwosret III, a proposal that West refuses to accept.

Moyer in 2002 argued that Herodotus' passage concerning the Theban Priests was 'a peculiar Egyptian mirage constructed by the Egyptians for their own needs in the Late Period of their history'.
There are, however, a smaller number of scholars now occupying this school as the third school of thought has taken precedence over both of the original schools.

This third school was initiated by the work of Hartog in 1980, which has profoundly altered the status quo of the debate.
He argued that Herodotus' ethnographic passages inform us more about ideals of what it was to be Greek.

However, the point must be made that Hartog was preceded by Froidefond, who had written a chapter concerning Egypt, in which similar arguments to Hartog were expressed but not as explicitly as his successor, who was to be the main founder of the third school.
The 'Ethnographic School' as I shall address it, has had seemingly more adherents than the other two schools in recent years.

A current trend is to abandon the 'futile and infertile' arguments over Herodotus' veracity in favour of studies, which focus on Herodotus' internal coherence in his ethnographic accounts.
The main followers of this type of approach are Burkett, Cartledge, Hall and more recently Vasunia.

It appears that the current state of affairs has the 'Ethnographic School' enjoying a numerical superiority over the Liar and 'Misinterpretation' Schools.
Yet there are adherents for both of the older schools.

The irony is that the tide may well have turned against the Liar School.
Their attempts to overturn what they have deemed overly favourable interpretations by the 'Misinterpretation School', have been seemingly made obsolete by the 'Ethnographic School', which argues that both of the older schools are outdated and no longer worth pursuing in the intensive manner, which was undertaken by past scholars.

For many modern scholars, ethnographic coherence is far more important than the practically impossible task of determining whether passages originate from Hecataeus or Herodotus.
The world of Odysseus by M. I. Finley, (henceforth abbreviated to F.), has been acclaimed as a 'scholarly landmark'.

Such a claim is not exaggerated; the very fact that The World of Odysseus was first published in 1954 and frequently reprinted for just over half a century is testimony to the sheer impact of F.'s publication.
F. comments in the preface to his second edition that the work 'has been cited, discussed, attacked in innumerable books and articles' and 'has been the acknowledged starting-point of studies by other historians of society and of ideas'.

F. advocated that a close reading of both the Iliad and the Odyssey could produce a society, complete with a coherent system of institutions and values, which 'is neither an improbable nor an unfamiliar one in the experience of modern anthropology'.
In chapter 1, entitled Homer and the Greeks, F. places Homer in the context of myth telling in archaic societies and compares him with later writers such as Herodotus.

Homer's techniques concerning the narration of myth are 'expanded and elevated' by the succeeding centuries.
F.'s second chapter, Bards and Heroes, maintains that while, on the one hand there are some anachronisms within the poems, these fragments do not prevent a meaningful study of the archaic society.

Yet he is cautious to emphasize that when reading the poems in the proposed fashion, the reader must be constantly aware of the temptation to 'ignore the implications of the poet's selectivity' and 'to brush aside apparent confusions and contradictions in social and political matters...
as nothing more than the carelessness of a bard who did not care'.

The next three chapters then begin to concentrate on a variety of social elements, such as the economic centres (chapter 3), community and kin (chapter 4) and moral values (chapter 5).
F. states that the oikos is the centre of economic activity and a man's position in relation to an oikos was fundamental in establishing his rank in society.

Associated with the oikoi was the systematic and hierarchical giving of gifts.
Both of these crucial elements played a vital role in interactions within the local community.

As F. states, 'The coexistence of three distinct but overlapping groups, class, kin and 'oikos', was what defined a man's life, materially and psychologically'.
One aspect of family life that I admit shocked me was the relationship between husband and wife in F.'s account.

F. contrasts extracts depicting the emotional reaction between men and between a man and a woman.
The results of his inquiry led F.

to write, 'While Odysseus was absent the loss to Penelope, emotionally, psychologically, affectively, was incomparably greater than the loss to her husband.
The grief of Achilles was nearly matched by the sorrow of Hecuba and Andromache at the death of Hector, son to one and husband to other'.

F.'s treatment of the Homeric religion did not meet with my total approval.
It is quite possible that I have misinterpreted F.

on this occasion, namely on his account of the evolution of Greek religion.
He asserts that the humanization of the gods and the abandonment of the older divine forms was a rapid process, akin to a revolution.

F. takes for granted, in my opinion, that we are unable to prove that humanization of the gods occurred around the time of Homer.
It is perfectly possible that this had already taken place in earlier phases of Greek civilization.

The evidence, however, of such an occurrence is lacking, statues from older temples may well have perished and the Linear B tablets were not concerned with the recording of myth.
This does not mean to say that humanization of the gods could not have happened in this period.

There has been much scholarly debate concerning F.'s thesis.
Hornblower states that F.

has been attacked on two counts, firstly, that there was never a single or coherent society in the Homeric period and secondly, his preferred dating of this society is wrong.
The second charge is countered by F.

in his first appendix, where he remarks 'What is there in the similes that say this is eighth-century winnowing, not ninth?
' F. makes it clear that to give a precise date is probably unlikely and Hornblower reinforces this by casting doubt over the probability that modern scholarship will remain at a consensus with the currently favoured date.

Overall I believe that F. produced a work that will continue to cause debate.
Even though I have my reservations about the validity of employing two sources, the relationship with each other is still debated, to construct a society such as F.

has achieved, his style is such that I am quite prepared to follow him in his leap of faith and only at the last moment do I mentally apply the brakes and halt, looking down at the chasm that is Homeric scholarship, murky and teaming with pitfalls and dangers.
Such is F.'s power to convince.

F.'s work is enlightening both in terms of the application of source material and the combination of classics with other disciplines, which his publication was one of the first of this kind.
This essay is essentially a reaction to an article by R. David entitled: 'Rationality versus Irrationality in Egyptian Medicine in the Pharaonic and Graeco-Roman Periods'.

I would argue that David's article lacks a considerable amount of balance concerning the medical practices of the Greeks and Egyptians and, furthermore, at certain points may well misguide a reader who is not well acquainted with the evidence.
There are several issues within the article, which I hope to re-examine, these are:

The ancient and modern attitude towards 'progress'.The distinction between 'doctor', 'priest' and 'magician'.The 'rationality' of Greek medicine.The interaction between Greek and Egyptian medicine.
The inadequacy of the terms 'rational' and 'irrational'.

David, when discussing the trends of the Egyptian medical papyri over the course of history, concludes the paragraph with a description of the London-Leiden Papyrus, stating that: 'some parts follow the pattern of earlier medical papyri, there are also references to some entirely new drugs and, most significantly, the content is predominately magic and indicates no advance in medical science'.
This suggests at the very least that we expect the Egyptians to make medical progress over the course of time.

Perhaps this is not unreasonable of us to think so, but one point David fails to make is that the very notion of 'progress' was completely alien to the Egyptians.
In fact as Ritner explains, the Egyptians thought seemingly the reverse: '"Progress" is a notion alien to Egypt and most ancient cultures, which view the current and future ages as "regressions" from an ideal "Golden Age" when mankind and divinity more closely interacted.

In Egypt, the notion of an ideal past is linked to complex philosophical understandings of time itself, viewed as a repeating cycle or spiral capable of being manipulated by ritual.
On the theoretical level, "progress" was neither desirable nor necessary'.

Therefore, to expect a continual improvement in medicine as we expect of modern western society, is anachronistic and particularly unhelpful.
Once we have understood the Egyptian notion of time, it immediately becomes clear why the medical papyri often have rubrics claiming an ancient pedigree for a particular procedure or ritual, as can be found in the Book of the Dead.

Similar problems with definitions occur with the various healers who operated in Egypt.
David divides Egyptian healers into three categories as have other scholars before her.

The divisions are 'doctor/physician', 'priest' and 'magician'.
Such a division is not anachronistic as such, since the Egyptians themselves had several terms for healers: swnw- equated to 'doctor' or 'physician', wa'bw- the 'priest', and sau- one term used to mean 'magician'.

David's error is to then treat each of these categories of healer as fairly distinct and independent of each other, as well as stating that the swnw practiced 'conventional medicine'.
Quite what conventional medicine is David never explains and I think it is wrong to overlook this since, in my opinion, defining what constitutes as 'medicine' is just as notoriously difficult as defining what constitutes 'magic'.

An attempt to separate types of Egyptian healers by modern functional conceptions is more of a hindrance than a help despite the fact Egypt had a number of terms available.
Ghalioungui and David both separate the healers into 'magical' and 'rational'.

Ghalioungui separates the swnw from the wa'bw and the sau; David separates the sau from the other two, on the grounds that the wa'bw occasionally practiced 'conventional medicine'.
However, there are two reasons why this practice is not advisable; firstly at least two types of healer seemed to have access to the medical papyri, which have both 'rational' and 'magical' sections.

The acclaimed Edwin Smith Papyrus states: '[As for] the heart, there are vessels from it to [every] limb.
[As for] that [on] which any lay-priest of Sakhmet and physician puts his hands or his fingers...' This implies that no one type of healer performed what is conjectured as feeling the pulse.

David also states that a cache belonging to a magician (the precise type of healer, she does not state, I assume here sau) had medical papyri among other texts and magical objects.
Furthermore in his work Nunn has collated tables listing the known healers from Egypt and his work should clearly demonstrate why functional distinction is flawed.

Nunn lists magicians (sau) that were also priests (wa'bw) and physicians (swnw) that were also magicians (sau) and physicians that were also priests!
Several terms there may have been, but essentially one could and would be involved in another's form of healing.

The Egyptian texts themselves demonstrate that functional distinction is futile.
Let us take a case in point, the Edwin Smith Papyrus, which according to David is 'a model of pragmatism and proves that the Egyptians had developed an empirico-rational system at an early date'.

David neglects to admit the true composition of the papyus, stating that it only had one spell.
In reality the forty eight cases do only contain one spell, but after these are a further eight spells, written by the same scribe.

Moreover, this failure to mention texts that show a combined 'magical' and 'rational' approach is not a single occurrence, but is to be found consistently throughout the article.
Two further examples concern the application of substances such as honey to wounds, which David sees as part of conventional treatment.

Yet a spell was recited during the application of honey and several spells survive that were recited when medicine was being applied, one even confirms the unity of 'magical' and 'rational' cure: 'The medicine has come; that which dispels the substances from this heart of mine, has come.
The magic is strong on account of the medicine- and vice versa...

' However, it is not just the Egyptian sources that suffer from a lack of balance, David's discussion of the relation between Greek and Egyptian medical practice is problematic.
David rightly questions the theory that Greeks from the Hippocratic School at Cos established a school at Alexandria, which was more rational.

Yet only the Egyptian evidence is examined and not the Greek sources.
Much effort is spent in demonstrating that many features of Egyptian medical practice predate Greek practices but nothing is said of 'magical' cures in Greek civilization.

Reading through the article one would still assume that Greek medicine was wholly rational, even though some of their techniques may have derived from Egypt.
This is a considerable mistake on two counts, as will be seen.

Firstly, we should not allow ourselves to assume that all or even many Greeks shared the same 'rational' attitude toward medicine as the Hippocratic writers.
Still more intriguing is the recent work of Collins concerning the Hippocratic writers' attitudes to nature itself.

He states that: 'For the Hippocratics, then, there was no discontinuity between mechanical and divine causes of bodily ailments.
Nature incorporates divine forces, and even if some physicians tried to understand the mechanical processes of disease, it is mistaken to claim that what separates "magical medicine" (zaubermedizin) from the Hippocratic approach is the former's belief in personal divine influence'.

He goes on to argue that efficacy of magic was still provisionally acceptable and so demonstrates that even the medical thinkers seen as the most 'rational' are not devoid of some magical theory.
Let us turn our attention to 'magical' cures in Greek culture.

The existence of magicians who claimed to be healers is verified by Plato and also later in the Greek Magical Papyri.
The question I wish to ask is how different was 'magical' healing in Greece compared to the procedures of those like the Hippocratic writers?

The distinction commonly made between 'rational' and 'magical' healing is the approach taken by the healers.
Only one is based on logical and natural causes and is grounded in the result of observation, this is the definition David applies to the work of the Egyptian swnw as we have seen.

Yet we have also seen that at least in Egypt, 'magical' cure was connected to complex philosophy concerning the nature of time and the cosmos.
This also appears true of Greek 'magical' cures as Graf points out, both 'rational' and 'magical' approaches are similar if not identical: 'The therapy of the physicians makes use of the same rationality as that of the seers: both first observe the symptoms, in order to arrive at an adequate therapy.

But the two sorts of therapy start from different symptoms and therefore arrive at different therapies.
The cathartic priests, who understood epilepsy as a spirit possession...

all this is perfectly within the laws of causality...' This is intriguing since Collins, when referring to the Hippocratic notion of divine influence, cites his observations from the Hippocratic text On the Sacred Disease, which is normally seen as a reaction against 'religious' or 'magical' cures.
Essentially then, the 'rational' and 'magical' approaches in Greece were both based on observation and complex theories and thus neither should be called 'irrational'.

It appears that the 'rational' and 'magical' approach to medicine are very closely related in Greece and although perhaps not as integrated as in Egyptian medicine, the two can not be separated by methodology or religious philosophy.
Dickie makes an additionally fascinating observation about the nature of Greek 'rational' medicine.

From one of the poems of Pindar we have a description of the medical activities of Asklepios, the father of medicine.
These activities are:

The use of incantationsThe use of potionsThe application of antidotes in bandages on limbsThe use of cutting or surgery.
At least concerning the mythology of Asklepios, 'rational' and 'magical' medicine appears to be intertwined.

None of these considerations are mentioned at all by David when arguing that Egyptian medicine had a 'rational' side.
This in hindsight is surprising since these insights would be beneficial to her case attacking the theory that we should assign the term 'rational' and 'irrational' to Greek and Egyptian medicine respectively.

Let us proceed to examine the interaction between Egyptian and Greek medicine.
There are two relatively minor points to be observed and may well simply boil down to personal conjecture.

The first concerns David's insistence that the Greeks inherited many elements of medical practice from the Egyptians.
Personally, I believe that we should not simply assume any more recent civilization incapable of making discoveries independently of older cultures.

Furthermore, this aspect of study is notoriously difficult and subjective.
The second point is slightly more serious, David states: 'Following the establishment of the medical school at Alexandria, Egyptian and Greek medicine probably followed separate routes, with the Greek doctors providing treatment for the Greek elite while their Egyptian counterparts continued to treat the indigenous population.

In time, however, these two parallel systems undoubtedly influenced each other...' Other scholars would suggest a more rapid interaction than David suggests.
Frankfurter states that Egyptian priests were interacting with the Greek elite as early as the Ptolemaic period, as we have seen some priests had some form of medical function.

Furthermore, the Greek Magical Papyri indicate Greek and Egyptian magical/medical interaction; some of the papyri are dated to the first century BC.
My final criticism concerns the lack of working definitions within the article.

Namely the use of the terms 'rational' and 'irrational' are both unhelpful and not applicable to Egypt or Greece.
This problem is not an isolated one and various scholars utilize a variety of terms when discussing this subject.

Although I have difficulty in deciding upon accurate terms to use for this discussion, in my opinion, the term 'irrational' must be abandoned for several reasons.
Firstly, the term 'irrational' is not remotely applicable to Greek or Egyptian medical practice let alone any other culture, since what we tend to deem as 'magical' are based on complex theories about the nature of the cosmos and are, therefore, hardly irrational.

Furthermore, David is the first to use 'irrational' and intriguingly even the title of the work in which her article is published is called 'Magic and Rationality in Ancient Near Eastern and Graeco-Roman Medicine'.
Another term used by Pinch in place of 'irrational' is 'supernatural'.

This in itself is a slight improvement.
However, an observation by Collins that in Classical thought, the gods were not supernatural but within the confines of nature, makes this a dubious replacement, since Egyptian thought probably matched Classical thinking.

The real culprit word in terms of definitions is 'medicine'.
To a reader it evokes modern medical science and already causes anachronism before any talk of ancient healing practices are mentioned.

The way in which it is used is also instructive to those examining modern attitudes to the art of healing.
No mention is ever made in scholarship of a 'medical amulet' or a 'medical spell'; they are called 'healing amulets' and 'healing spells'.

The one exception to this rule concerns the 'medical papyri' but often this appears to be allowed since no one papyrus is either solely 'magical' without accompanying prescriptions or anatomical works.
Even in the one case where the term 'medical' is applied to an ancient document, the reader unacquainted with the text may not be aware of its partly 'magical' content.

The use of 'magical' in healing contexts only exacerbates the situation since the word 'magic' is overloaded with negative connotations.
All this makes it clear that English is not capable of handling the ancient distinctions in healing (when there are any).

The choices of terms are either charged with negative connotations or evoke too modern a concept.
Nevertheless, I think we can reasonably dispose of the terms 'irrational' and 'rational' altogether when discussing ancient healing.

To conclude briefly, David's article employs unsuitable terminology, some of which is not found in other authors of the field.
The distinct similarities between 'magical' cures within the Greek and Egyptian medical systems are not discussed and the distinctions between Egyptian healers are too pronounced and what is more contrary to the evidence.

The Carolingian Renaissance is simultaneously one of the most crucial and seemingly controversial periods concerning the transmission of Latin texts.
In fact every period to which is assigned the term 'Renaissance' by scholars suffers the misfortune of becoming a 'potential hotbed of controversy'.

In this essay, I shall examine a number of issues that are integral to any assessment of the period's role in the history of Latin texts and their transmission.
Let us begin with a brief survey concerning the state of classical Latin literature during the Carolingian period.

It has been estimated that only 6 per cent of Latin literature survived the period between the fall of Rome and the rise of Charlemagne.
The rest of Latin literature (a very disheartening 94 per cent) perished before the Carolingian period.

Due to the efforts of the scholars of the age, many Latin texts were recopied and saved for posterity.
However the treatment of various ancient authors varied considerably, from being copied a good many times to not being copied at all.

The following ancient authors were copied frequently during the period: Cicero, Horace, Justin, Juvenal, Lucan, Persius, Pliny the Elder, Sallust, Terence and Vitruvius.
Other authors were copied less often and were much rarer.

A selection of these authors comprises of Ammianus Marcellinus, Apicius, Martial, Petronius, Suetonius and Valerius Flaccus.
While the last group were rare, they were however available.

Moreover, some ancient authors were extant in a few manuscripts, occasionally just one, and others were not copied during the Carolingian period at all.
These lucky survivals are Apuleius, Catullus, some parts of Livy, Manilius, Cornelius Nepos, Paterculus, Propertius and Tacitus.

Why was there such a disparity concerning the copying of ancient Latin authors?
In order to determine the cause, let us analyse the choices made by Carolingian scholars when excerpting or copying ancient authors.

Cicero's works feature very greatly in the compilations of the period.
The collectaneum of Hadoard is comprised chiefly of Cicero's works and supplemented by Sallust, Servius and Macrobius, each only represented by one or two excerpts.

In addition Sedulius also includes Cicero in his collectaneum along with Frontinus, the Historia Augusta, Macrobius, Valerius Maximus and Vegetius.
The choices made by Hadoard and Sedulius demonstrate that they favoured instructive texts, be they literary (Macrobius), moral (Valerius Maximus), pratical (Vegetius) or philosophical (Cicero).

This statement can be applied to many of the texts chosen for copying by Carolingian scholars.
Ancient Latin authors were copied if they satisfied one of two criteria: a) the chosen text was of practical use and could advise the reader on various practical fields or b) the texts could be used to validate Christian philosophy and teachings.

One gets the distinct impression, when reading the list of neglected Latin historians, that history itself was not keenly studied in the Carolingian period, but this would in fact be inaccurate.
The 'pagan' Latin historians may not have been well read but early Christian historical writers were more popular.

In addition, early Christian writers saw 'sacred' and 'profane' history as one single unit and therefore integrated the two together in their works.
Moreover, they received a great quantity of information from Greek writers by means of a Latin translation.

For example Jerome's translation of Eusebius' Chronica allowed scholars to access the wealth of material, especially concerning Eastern history.
In one such Chronicle by Freculpius, belonging to the ninth century, some of the passages focus more on Greece, Persia, the Hellenistic kingdoms and Rome than 'sacred' history.

History, then, was studied in the Carolingian period, but the subject was often used towards a Christian end, as ancient authors were used to prove the validity of the Bible.
One should stress that this was not always the case and some scholars did study history for its own worth.

To summarise, ancient authors were copied if they had information of practical value contained therein or could be used to validate the notions of Christian history.
Let us now investigate another vital question; what features of the Carolingian period are discernable that would produce the attitudes of their scholars?

The Carolingian period is typically described as a 'revival of learning in conjunction with a movement to reform...
the institutions of the church'.

Many scholars would agree with the statement that the period was 'the most momentous and critical stage in the transmission of the legacy of Rome'.
Both the revival of learning and the church reform movement were connected and as Trompf explains: 'The Carolingian Renaissance has found its embodiment in none other than the person of the emperor Charlemagne himself, alike acclaimed as unifier of the West, defender of the church, restorer of stability and justice to barbarian Europe and grand patron of a cultural and artistic revival'.

The Carolingian empire is described as (at the very least) an attempt at a unified political, religious and cultural whole, bound by the personality of the emperor of the Frankish people.
Its own significance is inflated by the nature of the preceding Dark Ages, when the copying of manuscripts apparently ceases and many ancient works perished.

Another characteristic of the period was the attempt to integrate the various peoples of the empire socially and culturally.
One of the strategies involved the founding of monasteries across the empire and the spreading of Christianity.

This act, as posterity testifies, was crucial for the survival of the ancient Latin authors, the 6 per cent known to us as the 'Latin classics'.
The texts were found in monasteries across Europe right up to the Renaissance.

Thus, 'the link between the Carolingians and the church was to form one of the central pillars of the Carolingian empire'.
The church was also host for the learning revival, the aims of which were learning and copying information that was useful, educating the current clergy and also developing a 'fundamentally Christian political philosophy' by selecting and appropriating symbols and ideas from the past.

Such aims are compatible with the choices made by scholars of the period when excerpting or recopying material from ancient Latin authors.
Hence finding copies of Frontinus and Vegetius informs us that these were seen as manuals; Charles the Bald was given a personal copy of Vegetius because a member of his court thought it would be useful.

Any pieces of good Latin found in the classical authors were used as models; Sallustian phrases were integrated into a variety of contemporary literature such as histories, letters, lives of saints and theological treatises.
The Roman poets and Cicero were all copied and used slavishly by Carolingian scholars.

The patronage by the emperors is also held to be a crucial factor in Carolingian identity and a distinctive feature of the period.
To summarise, three of the most crucial aspects that identify the Carolingian period are:

The attempt to integrate the various people of the empire socially, politically and culturally, for example by the founding of monasteries.The movement to educate the current clergy and recopy ancient, practically useful knowledge, as well as create a new Christian political philosophy.
The patronage of the emperors and the use of courts as centres of learning.

These factors would therefore be relevant to the history of Latin texts.
The patronage of the emperors allowed new monasteries to be founded where classical texts would be stored for centuries to follow.

These new centres of learning allowed the clergy to be educated and royal patronage allowed scholars of the period to recopy ancient texts believed to be of use to Carolingian development and innovation.
These features appear to give the period its distinctive qualities.

However, this conventional view of the Carolingian period held by Carolingian scholars is by no means accepted by scholars of other periods of the Middle Ages and beyond.
Sullivan argues that recent scholarly work has undermined the former notion that the Carolingian period was either culturally united or distinctive.

He states that: 'If the Carolingian era constituted the discrete period during which something both unique and significant happened, then one should be able to delineate specific chronological boundaries marked by decisive events denoting clear disjunctures with what went before and what came after'.
Therefore, let us attempt to compare and contrast the periods immediately preceding and the later periods succeeding the Carolingian age, (which is commonly dated to the eighth and ninth centuries), to determine if the period is indeed as culturally distinctive as has been suggested.

Let us begin with the Dark Ages; this era has been seen as a disruptive and unstable era following the fall of Rome.
The period from 550-750 AD is said to be a time of 'unrelieved gloom' for the Latin classics, the copying of which virtually ceased.

Economic and cultural stagnation appear to be the main features of this desolate period, or were they?
After the fall of Rome the people of Western Europe were freed from the grueling effects of taxation and finances could be redirected towards more local concerns.

Furthermore, the coinage of the Merovingians, (predecessors of the Carolingians) was extremely similar, undermining the concept of a clear disjuncture, at least in terms of economic trends, between the Merovingians and the Carolingians.
Upon looking at the Dark Ages, one also discovers that one supposedly distinctive Carolingian feature was already occurring, the founding of monasteries.

Classical Latin authors were already being placed under the care of the monasteries before the empire of Charlemagne came into being.
What is more, two foundations occurred before the period of 'unrelieved gloom'; Vivarium in 540 and Montecassino in 529.

Vivarium was founded by Cassiodorus on one of his estates and he equipped it with a library.
Most importantly he put a strong emphasis on the copying of manuscripts.

Moreover, he foresaw the need for translations of Greek work on exegesis before such work began in the Carolingian period.
In addition to the work of Cassiodorus in the sixth century, the work of another in the seventh has relevance here.

The Chronicle of Fredegarius was a forerunner to the interests supposedly characteristic of the Carolingian period.
Furthermore, there were a number of other writers that performed similar work valued by the later Carolingian scholars.

These writers were themselves highly used in the Carolingian age and are four in number: St Benedict, St Gregory the Great, St Isidore of Seville and Bede.
All of this indicates that all of the distinctive features of the Carolingian period had historical precedents.

It would appear that the Carolingian period is not highly distinctive from the Dark Ages.
This is indeed for me a particularly problematic situation.

If one is to adhere to the notion that the Carolingian period was similar to the Dark Ages, the only significance of the age was its temporal position within the history of Latin texts, not so much because it was independently unique, but because it was totally dependent on the cultural decline of the Dark Ages.
Now the Dark Ages no longer seem as dark, then the state of classical Latin literature actually dictates the significance of the Carolingian period.

Furthermore, when one compares the Carolingian period with succeeding periods up to and including the Renaissance, one finds that the period has all of its formerly distinctive features in common with several eras of history.
Let us examine the succeeding periods.

Eisenstein grants us an overview of the history of Latin texts from the Carolingian period to the advent of printing.
She believes: 'To put it briefly, the two medieval renaissances were limited and transitory; the Renaissance [involving the dawn of printing] was total and permanent'.

She also explains at length how, in terms of the transmission of Latin texts, little changed after the Carolingian period until the beginning of printing.
The continual use of patronage of the local elite in each renaissance rules it out as a unique feature of the Carolingian period.

Her study also raises issues concerning the impact of the Carolingian period in terms of the survival and perpetuation of classical Latin texts.
When reading the works of Carolingian history, one regularly gets the impression that the Carolingian monks saved the remains of the classical Latin heritage.

Eisenstein removes the shroud by placing the period in context; the texts had to be saved many times over in the course of time until printing finally saved the texts from their continually precarious state.
Petrarch and his contemporaries eventually had to perform the same function as the Carolingian scholars.

Furthermore, until printing began, the movement of texts from one place to another for copying deprived one place of a vital text, enriching one field of study would impoverish another as the logistics of scribal labour were diverted.
Now let us bring all of these points together for the conclusion.

The Carolingian period appears to have many of its supposedly distinctive features in common with preceding and succeeding periods in the history of Latin textual transmission.
It was one of at least two medieval renaissances required to perpetuate the Latin classics until the advent of printing finally released them from their precarious situation.

In the Carolingian period only a selection of the Latin classics were recopied, the unfortunate texts yet to be copied had to wait for the next renevatio.
The disparity is not only apparent in the number of texts regularly copied but also in the proportion of classical texts held in the Carolingian monasteries.

While some monasteries had a substantial range of classical texts such as Corbie, others had remarkably few such as St Gall, which had only four pagan writers in four hundred manuscripts overall.
Likewise, the library of Hartmut had sixty seven books and only five of them contained works of classical authors.

In conclusion, the Carolingian period may share many cultural features with earlier and later periods of history, but its position within the history of Latin textual transmission grants it higher significance than other revivals solely because of the precarious state of the literature by which the Carolingians were confronted.
It is possible then, to suggest that had the Carolingian period been an intermediate revival, such as the renevatio of the twelfth century, and not the first such revival immediately succeeding the Dark Ages, then the Carolingian period would lose some of its historical significance.

In my opinion, the sheer importance of the Carolingian revival rests solely on the dilapidated state of the Latin classical heritage.
Moreover, had the heritage been in a better state, the Carolingian period would be one of a long line of necessary and unremarkable periods of recopying and the Renaissance, bringing with it the dawn of printing, would indeed be worthy of the epithet given to the Carolingian period by Reynolds and Wilson: '...without doubt the most momentous and crucial stage in the transmission of the legacy of Rome'.

Experimental Procedure
100mL standard solutions were prepared by serial dilution of 0.1M NaF solution to give solutions of concentration: 10mM, 1mM, 100M, 10M and 1M.

25 mL of each standard was placed in a separate polyethene beakers labelled A to E. 25 mL of saturated solutions of each MgF 2, CaF 2, SrF 2, BaF 2 and PbF 2 were placed in 5 more polyethene beakers, labelled F to J.
To each of these beakers 25mL of total ionic strength buffer (TSISAB) was added.

The 10 beakers were mixed well and left to equilibrate in a water bath at 25C.
Unfortunately, the water baths were not functioning very well, so the temperature of each of the solutions was recorded with the EMF to be taken into account in calculations as the temperature wasn't very constant.

A Fluoride ion selective electrode was cleaned with distilled water and then placed in the first solution (A).
Once the potentiometer had settled to a stable reading, the potential difference and temperature were recorded.

Results
Questions

1.
The fluoride ion selective electrode, was used in conjunction with a standard calomel electrode as an external reference electrode in this experiment.

The ion selective electrode comprises a chamber which is impermeable, except via a solid state crystalline pressed pellet of europium doped lanthanum fluoride (LaF3) which acts as a membrane.
The chamber contains an electrolyte solution of sodium fluoride (NaF) and sodium chloride (NaCl).

2.
FORMULA where E is the electrode potential, E is the standard electrode potential, R is the gas constant (8.314 JK-1mol-1), T is the temperature in Kelvin, 0.1 is the concentration of fluoride ions in the fluoride ISE, and [F-] is the concentration of fluoride ions in the solution.

3.
The nernst equation given in question 2 may also be written as: FORMULA Since E/ E is the potential difference and log0.1 = -1, this is the equation for the calibration curve (Figure 1): FORMULA Therefore I would expect the gradient of the graph to be - RT/F.

FORMULA The gradient for the calibration curve (Figure 1) was found to be 0.0532.
the difference between this value and the theoretical value calculated above, is probably due to the variation in temperature.

A mean value for T of 297.5 was used in the calculation above, however, the actual values of the temperatures of the solutions varied by approximately 0.5K about this value.
This discrepancy was unavoidable as the water baths were broken, but if the experiment were repeated, it would be essential to ensure that all the solutions were at the same temperatures.

4.
The Total Ionic Strength Adjustment Buffer (TISAB) is added to ensure the ionic strength of the solutions remains constant.

This is important because the electrode measures the activity of the fluoride ions, which is affected by both the concentration of the ion, and the ionic strength.
By ensuring that the ionic strength will be same for each of the solutions, the effect of different concentrations of ions alone on the potential difference can be measured.

5.
The plot of the concentration of fluoride ions in the saturated solutions against the ionic radius of the metal ions has a minimum, close to the point for Ca, and rises fairly sharply from this point.

The curve rises most sharply for ionic radii larger than that of Ca.
This is because the size of these ions is much greater than the relative size of the molecules of the solvent and the other ions in solution (F-).

This means that many small molecules of solvent or fluoride ions may be 'packed' around large metal molecules and solvate them.
The electrostatic repulsions between 2 metal ions of charge +2 are reduced in this way.

The solubility of ions with ionic radii smaller than that of Ca increases because the metal ions have atomic orbitals that are approximately the same size as those of the solvent molecules and F- ions so these orbitals can overlap more easily and therefore they are more easily solvated.
The solubility of the ions increases as the ionic radius decreases from this point, as more metal ions can be packed into the solution.

6.
The calcium flouride has the lowest solubility because it is of comparable size to the fluoride ions.

This means that it is more difficult to pack the ions together, as for each calcium ion there are two fluoride ions, to pack in with the solvent molecules.
Introduction

There are many factors that affect the geometries of transition metal complexes, including thes relative energies of the metal's d orbitals, the number of d electrons the metal has, the oxidation state of the metal and the size of both the ligands and the metal.
In this experiment, three d 8 first row transition metal complexes will be synthesised and their structures determined from their infrared spectra and magnetic properties.

Experimental
1. Synthesis of CoCl(PPh3)3

1.2080g of Cobalt chloride hexahydrate was dissolved in 140mL of ethanol, in a 250mL 2-necked round bottom flask, fitted with a nitrogen inlet.
4.092g of tripheylphosphine was added to the bright blue solution.

The flask was flushed with nitrogen, and heated in an oil bath to 35 C.
The solution was stirred magnetically, while a nitrogen atmosphere was maintained and the temperature remained between 30 and 40C.

A solution of 0.1881g of sodium borohydride in 10mL of ethanol was added to the round bottomed flask, causing the solution to gradually turn from dark turquoise through to brown/ black in colour and small bubbles of gas were evolved.
The solution was stirred for a further 15 minutes after the addition, then the fine brown crystals were filtered off on a glass sinter with suction.

The crystals were washed with three 30mL portions of ethanol, then two 20mL portions of water, followed by a further two 20mL portions of ethanol.
The crystals were dried in a vacuum dessicator for an hour before the yield and melting point were determined.

Figure 1 shows the IR spectrum of the product, the magnetic susceptibility of the complex is shown in table 1.
FORMULA

2. Synthesis of NiCl2(PPh3)2
45mL of ethanol was placed in a 100mL round bottom flask, 2.8178g of triphenyphosphine and 1.2076g of nickel (II) chloride hexahydrate was added to it.

Two anti-bumping granules were also placed in the flask and the solution was refluxed in an oil bath for 30 minutes.
The apparatus was then set to downward distillation and 30mL of ethanol was distilled off.

The black crystals were filtered off on a small sintered glass filter with suction and washed with 15mL of 95% ethanol, and 15mL diethylether.
Once the crystals were dry the melting point and magnetic properties were determined (table 1) and the IR spectrum of the compound was obtained (Figure 2).

FORMULA
3. Synthesis of Ni(NCS)2(PPh3)2

1.4022 g of nickel (II) nitrate hexahydrate was dissolved in 16mL of 95% ethanol in a 100mL round bottomed flask.
0.8105g of finely ground sodium thiocyanate and two anti bumping granules were added to this.

The mixture was refluxed for 20 minutes on and oil bath.
The mixture was then cooled on ice, and the sides of the vessel were scratched with a glass rod to encourage crystalisation of sodium nitrate.

The crystals of sodium nitrate were filtered off through a sintered glass crucible.
2.7962g of triphenylphosphine was dissolved in 25mL of propan-2-ol in a round bottomed flask, by heating with two anti-bumping granules with a reflux condenser attatched.

The filtrate of nickelthiocyanate, from the previous part of this experiment was heated on a hot plate, and slowly added to the triphenylphosphine through the tom of the condenser.
This caused the colourless solution to turn a vibrant red colour.

The flask was then removed from the oil bath and cooled in an ice bath, forming red crystals.
These crystals were then filtered off on a small sintered glass filter with suction, washed with 15mL of 95% ethanol, then 15mL of diethylether, and dried on the filter.

The IR spectrum of the product was obtained (figure 3), its magnetic susceptibility was determined (table 1) and its melting point was found to be 234C.
FORMULA Table 1: summarises the magnetic properties of the three complexes synthesised in this experiment.

X g is mass susceptibility of the complex, given by the formula: FORMULA where l is the length of the sample in sample tube, C is the calibration constant (1.275) R is the reading from the evans balance, R 0 is the reading for the empty sample tube and m is the mass of sample analysed.
m is the molar magnetic susceptibility which is calculated by multiplying g by the molar mass (M r) of the complex in question.

A is the molar magnetic susceptibility for the complex, which has been corrected for the diamagnetism of the components of the complex.
The corrections used are: 13x10 -6 for the metal ions, 167x10 -6 for (PPh 3), 23x10 -6 for Cl- and 34 x10 -6 for (NCS).

Discussion
There are two possible co-ordinations for four co-ordinate complexes: square planar and tetrahedral.

The crystal field splitting diagrams for these two geometries are shown below filled with 8 d electrons as in the complexes in this experiment: As these diagrams show, the square planar geometry is often more favourable for d 8 complexes, because all the electrons may be paired up in low energy orbitals, leaving the highest energy orbital unoccupied.
However, the terahedral geometry is sterically much more favourable, because the bond angle in this case is 109, compared with 90 for the square planar geometry.

Steric factors are much more important when considering first row transition metals, because the metals are much smaller, so the repulsion between the electrons in metal-ligand bonds will be increased.
This means that bulky ligands will usually force the complex to adopt a tetrahedral geometry.

The energy difference between the e and t 2 set of orbitals in the tetrahedral geometry (T) also affects the geometry of the complex, sine the crystal field stabilisation energy (CFSE) of the complex depends on this (CFSE = - 0.8 T).
The magnitude of T is affected by the oxidation state of the metal: a greater oxidation state pulls the ligands closer to the metal, increasing the splitting effects; and the field strength of the ligand: stronger field ligands such as (PPh 3) and (NCS) increase the splitting.

The magnetic moments of the complexes synthesised in this experiment (see in table 1) indicate that CoCl(PPh 3)3 and NiCl 2(PPh 3)2 contain 2 unpaired electrons, and Ni(NCS)2(PPh 3)3 is paramagnetic.
This means that CoCl(PPh 3)3 and NiCl 2(PPh 3)2 adopt a tetrahedral geometry, while Ni(NCS)2(PPh 3)3 exists in a square planar arrangement.

From this it is clear that steric factors out weigh the benefits of the low energy square planar configuration in the case of the CoCl(PPh 3)3 and NiCl 2(PPh 3)2 complexes.
Conversely the Ni(NCS)2(PPh 3)3 complex contains two relatively strong field ligands, which increase the size of T making the tetrahedral configuration even more energetically unfavourable than the square planar complex.

Geometric and linkage isomerism are possible for this complex: The co-ordination of the (NCS) ligand can be determined with the infrared spectrum of this complex (figure 3).
If the ligand co-ordinates to the metal atom via the sulphur atom, the stretching of the CS bond appears at 700cm -1 in the spectrum, and the CN bond absorbs at approximately 2100cm -1.

If the ligand co-ordinates to the metal via the nitrogen atom, however, these stretching frequencies are different; approximately 800cm -1 for the CS bond, and approximately 2050cm -1 for the CN bond.
In figure 3 the absorbances at 2084 and 692cm -1 are most likely to correspond to the CS and CN bond stretches in the ligand, which are closest to the values quoted for bond stretches where the ligand bonds through the sulphur atom.

As there are only two absorption bands for each of these bonds, I expect that the complex adbots a trans geometry.
This is because on the asymmetric stretch for a trans complex will cause a change in the dipole moment, so only one absorption will be observed.

For a cis complex, both the symmetric and asymmetric stretches of the metal-ligand bond will cause a change in the dipole moment, resulting in two absorbtion bands around this region of the spectrum.
This is consistent with steric arguments, as the cis complex would place the two large (PPh 3) ligands next to each other with an angle between them of 90.

This means that the Ni(NCS)2(PPh 3)3 complex exists as structure 4 shown above.
Introduction

Iron thiocyanate complexes are deep red in colour.
Many different species of the complex may be formed when solutions containing Fe 3+ and SCN- ions are mixed, depending on the concentrations of the different ions.

These complexes generally have the formula: FORMULA In general, the wavelength of light absorbed increases as the thiocyanate concentration increases.
The aims of this experiment are:

To determine the formula of the simplest iron thiocyanate complex.To determine the value of the reaction quotient (Q) for the formation of this ion.
FORMULA To determine the concentration of the complex formed (c or [X]).

Experimental
Initially three standard solutions were prepared:

100 cm 3 of 0.3M Fe(NO 3)3.9H 2O, including 1cm 3 of concentrated HNO 3 to suppress the hydrolysis of the Fe 3+ ion.100 cm 3 of 0.1 M KSCN.100 cm 3 of 0.01 M KSCN.
The absorbance of the complex was first measured at different concentrations of SCN-, while the concentration of Fe3+ was kept constant.

This was done by producing 6 reaction mixtures containing:
25.0 cm 3 of stock solution S (2M HNO 3 and NaNO 3)3.00 cm 3 of 0.3M Fe(NO 3)32 drops of benzyl alcohol

Between 0.50 cm 3 and 2.50 cm 3 at 0.50 cm 3 increments of the 0.01 M KSCN solution were added to five of these mixtures, and they were all made up to 100 cm 3 with distilled water.
The Jenway spectrophotometer was set up to emit light of wavelength of 450 nm, which is close to the flat maximum of the absorption spectrum for the complex ion.

The spectrophotometer was calibrated using a 1 cm cuvette containing the prepared solution containing no SCN-.
The absorbance of each of the 5 solutions was then measured using the same cuvette, which was rinsed out between the different solutions.

To measure the effect of different concentrations of Fe 3+ on the absorbance on the complex 8 solutions were made up as follows:
25.0 cm 3 of stock solution S (2M HNO 3 and NaNO 3)1.00 cm 3 of 0.01M KSCN2 drops of benzyl alcohol

The following volumes of 0.3M Fe(NO 3)3 were then added to 7 of these solutions: 0.60, 1.00, 1.50, 2.00, 3.00, 5.00, and 10.0 cm 3.
Each of these solutions were then made up to 100 cm 3 with distilled water and the absorbance of these solutions was then measured as before, after calibrating the spectrophotometer with the solution containing no Fe(NO 3)3.

Results
From the equation for the equilibrium concentration quotient (Q) for the formation of iron thiocyanate complexes (equation (1) in the introduction) the following formula may be derived: FORMULA In order to calculate Q the concentrations of the ions are defined in terms of their stoichiometric concentrations as follows: FORMULA Where a, b and c are the stoichiometric concentrations of Fe 3+, SCN- and X respectively.

Substitution of these expressions into equation (2) gives: FORMULA This expression may be simplified because this experiment was conducted at low thiocyanate concentrations, in an excess of Fe 3+ ions so a>>c while b and c are comparable.
This gives the expression: FORMULA The conditions of the procedure also ensure that only the simplest of the complex ions were formed, which means that n = 1.

This means that equation 4 may be further simplified: FORMULA or FORMULA From equation 6 it can be seen that a plot of logb against logA (see Figure 1) will produce a straight line graph, where the intercept is [log{(1/Qa)+1} - log l] where is the molar absorption coefficient and l is the length of the sample in meters.
The gradient of this graph was found to be 1 proving that n=1 and therefore the absorbing species is the simplest iron thiocyanate complex ion: FeSCN 2+.

Using the formula for the thermodynamic equilibrium constant (K) for this reaction: FORMULA where Q is the equilibrium concentration quotient and f is the activity coefficient factor.
The solution S used in this experiment helps to ensure the value of f is independent of the concentrations of the reacting ions as the [NO 3-] ions don't complex with the Fe 3+ ions and the H+ ions suppress its hydrolysis.

Assuming that this keeps f constants then Q will be constant for the swamping medium used (solution s in this case).
Taking this into consideration and substituting a, b and c for [Fe3+] [SCN-] and [FeSCN2+] respectively gives the expression: FORMULA Assuming that c is very small compared with a but not b, this can be simplified to: FORMULA From the Beer-Lambert Law for light of a single wavelength FORMULA Substituting for c in (9) gives: FORMULA Thus a plot of 1/A against 1/a where b is kept constant produces a straight line where the gradient and intercept are as follows:

Discussion
The results obtained show clear trends, although in the second part of the experiment an anomalous value was obtained as shown on figure 2.

This is most likely to be because the complex had deteriorated more than in the other samples.
The reason for this may be that a longer period of time had elapsed between the addition of the Fe(NO 3)3 solution and the absorbance being measured, or possibly because slightly less benzyl alcohol had been added to this particular solution.

The rapid deterioration of the complex due to oxidation is likely to be the greatest source of error in this experiment, although steps were taken to reduce this effect such as the addition of the benzyl alcohol and measuring the absorbance of the solutions as quickly as possible after making then up.
This degree of uncertainty is added to by the difficulties in reading values from the graphs accurately to more than 2 significant figures in some cases, this may have had an effect on the precision of the values obtained for and Q.

In order to calculate f, the activity coefficient, I would use the following formula: FORMULA Where z is the charge on the ion, so in this case +2, and I is the ionic strength.
Conclusion

It was found that the complex formed was the simplest form of the iron thiocyanate complex, with the formula: [FeSCN]2+.
The reaction quotient (Q) for the formation of this complex was found to be 110 mol -1dm 3.

From this the molar absorption coefficient () was found to be 430 molm 2 for this substance.
Introduction

The reaction between Bromate (BrO 3-) and Bromide (Br-) ions under acidic conditions and in the prescence of phenol and methyl red is a 'clock' reaction.
At a specific point in the reaction, the solution goes from pink to colourless.

This property will be used to calculate the activation energy of the reaction, by conducting the experiment at temperatures between 20 and 70C and producing an arrhenius plot of the results.
Previous experiments have indicated that the rate-determining step for this reaction is between the Br- ions and an intermediate H 2BrO 3+ the formation of this intermediate involves two pre-equilibrium reactions: FORMULA If step (3) is rate determining: FORMULA Research has indicated that this step results in the formation of Br 2 and H 2O.

The Br 2 then reacts with the phenol, when all the phenol has reacted the bromine then reacts with the methyl red, causing the solution to turn colourless.
FORMULA As the concentration of H 2BrO 3+ may be expressed in terms of the total concentration of Br(V) series as: FORMULA Substituting for[HBrO 3]: FORMULA substituting for [BrO 3-]: FORMULA Substituting for [HBrO 3]: FORMULA Substituting for [H 2BrO 3+] in Expression II: FORMULA at very low concentrations of H+: FORMULA so that [BrO 3-] 1/3[Br(V)] substituting for [Br(V)] in (Expression IV) gives : FORMULA Where k = k 1k 2k 3 This indicates that the reaction is fourth order overall, second order with respect to H+, and first order with respect to BrO 3- and Br -.

The aim of this experiment is to determine the activation barrier for the reaction between Bromate and Bromide ions and give an estimate of the uncertainty in this value.
Experimental

Four sets of boiling tubes were prepared as described below and placed in the thermostat to equilibriate at each temperature.
Tube 1: 10 cm3 of 0.01 M phenol, 10 cm3 of the bromate/bromide solution (0.0833M Br- and 0.0167 M BrO3), 4 drops of methyl red.

Tube 2: 5 cm3 0.5 M H 2SO 4 When the solutions had reached the correct temperature, the contents of tube 1 was poured into tube 2 and shaken to mix.
From when the solutions are mixed the time taken for the pink colour to disappear was recorded.

The experiment was conducted at 5C intervals between 20 and 70C.
The time taken for the pink colour to disappear was recorded.

Discussion
From integration of the rate law for first order rate law (below) and the rate constant (k) can be expressed in terms of the time taken for the reaction to reach a specific point (t) and the concntration of product at this point [A]t and the initial concentration of the product [A]0.

FORMULA As the concentration of [A]0 will be constant for all the reactions and all the reactions will be terminated when [A]t reaches a certain point, ln[A]0 - ln[A]t may be considered to be a constant at the times recorded for each of the reactions which will be denoted 'x' from here onwards.
Substituting for k in the Arrhenius equation gives: FORMULA Where A is the pre exponential factor, E a is the activation energy for the reaction in joules, R is the gas constant and T is the absolute temperature.

FORMULA This shows that a plot of lnt against 1/T will give a straight line, of gradient Ea/R.
Results

FORMULA The true value calculated from the 31 values contributed by other groups isn't very close to the value calculated from the value obtained from this experiment alone.
This is may be due to a difference in opinion as to the exact point at which the solution had gone colourless.

This is the chief source of error for this experiment, especially at higher temperatures where the reaction occurred very fast.
In this experiment the same person decided the point at which the solution had turned colourless to minimise this source of error, however other groups may have had a different perception of when the solution was colourless.

Using a colorimeter to determine when the solution was colourless would eliminate this error by selecting a specific absorbance to signify the 'end' of the reaction.
This difference accounts for the large amount of uncertainty in the value for the entire class, nearly 20% of the true value found.

Ideally, the experiment would be repeated many more times, using a colorimeter as described above to reduce the uncertainty.
Often the colour change would happen more rapidly towards the bottom of the tube than at the top, even though the tube was shaken throughout the experiment to reduce this.

Using a machine to agitate the solution would be much more effective than shaking it by hand.
The accuracy of these results could also be improved by reducing the delay between mixing the solutions and starting the stopwatch and between the colour change occurring and stopping the stopwatch.

This could be done by using continuous flow equipment to mix the solutions and measure the time taken for the colour to change.
Conclusion

The activation energy for this reaction was found to be 51.59 kJmol -1 in this individual experiment, however the activation energy was found to be 65.78 kJmol -1 for the entire group, however the results were very broadly spread due to the differing perception of the point at which the solution was colourless.
Introduction

The technique of flash photolysis was developed in 1949 by Norris and Porter in order to study the kinetics of reactions that proceed at a very rapid rate.
Initially a "flash" is emitted, which triggers the start of the reaction by breaking bonds to form radicals, removing a protective group from a reactant, producing a precursor to the rapid reaction or generating an excited electronic state, as in this experiment.

Another probe beam is used to monitor the progress of the reaction.
In this experiment, the flash produces and excited electronic "triplet" state from the sample of anthracene.

The probe beam is absorbed by ground state anthracene only, so a trace of the decay of the excited state can be obtained.
From this trace it is possible to determine the intensity of the light transmitted and thus the absorbance of the ground state anthracene at a fixed time interval from when the flash is emitted.

The aims of this experiment are:
To obtain a decay trace for anthraceneTo calculate the transmitted light intensity (I), the absorbance (A) at regular time intervals for the decay, and thus confirm that the decay is 1 st order using a plot of lnA vs t.Calculate the lifetime () for the decay of the triplet species.

Experimental
The sample of anthracene, impregnated into a Perspex cylinder was placed into the sample compartment of the flash photolysis unit.

The filter was aligned to allow only light of wavelength 424 nm to probe the sample and the computer was set up to record a trace between 0 and 5 volts, between - 10 and 90 ms, in relation to the flash.
The trigger delay was set to - 10%.

The Trigger was set to "mode auto" and a sheet of black card was used too block off light from the sample.
The back off unit was altered so that the voltage was 4.75 volts.

The black card was then removed and the iris diapragm was altered so that the voltage level was set as 0.25 volts.
The trigger was then changed to "mode single" and the threshold set to 10%.

The photoflash was then charged up to 400 volts, and then trigger button was pressed after a seconds delay.
Results.

Figure 1 shows the decay trace obtained for anthracene.
FORMULA FORMULA FORMULA

Discussion
Application of the Beer-Lambert Law allows the lifetime () to be calculated without finding the extinction co-efficient () or the concentration of anthracene in the excited state (c) because the absorption of anthracene (A) is known: FORMULA Application of the integrated rate law for first order reactions shows that a plot of lnA against t will give a straight line of gradient - k if this decay is first order, because A is effectively the change in concentration of anthracene: FORMULA Table 2 and Figure 2 show the values obtained for k and by the two different methods and their associated errors.

These values are in very good agreement with each other, which indicates that the difference between these values and the literature value of between 0.02 and 0.035 is due to the data obtained during the experiment, not the calculations.
The discrepancy is most likely to result from inaccuracies incurred when extrapolating values of I from the decay trace.

This was made particularly difficult because the initial absorbance of the sample was just over 2.5 volts, making the decay curve quite shallow, so the values for I were all fairly close together, and difficult to determine accurately using the scale.
Another reason for this disagreement maybe that the apparatus is old, and had failed to produce decay traces in previous attempts.

Repetition of the experiment would clarify whether the flash and probe beam had functioned properly on this occasion.
The method of filtering the probe beam could have been more accurate, especially as the required wavelength was quite specific.

It is possible that the filter had been moved slightly, which would have affected the results.
Also as the voltage of the flash began to decline quite rapidly once it had been charged up it was difficult to trigger the flash as it was at the correct voltage.

If the voltage of the flash had been unsteady or not at the level expected the decay trace may have been inaccurate.
More modern equipment would have eliminated both of these possible sources of error as these factors could be automated.

It is also possible that the sample used had deteriorated, as the age of the sample was unknown and this may have effected the lifetime of the triplet state.
There is also a possibility that the temperature of the lab was not exactly 298k as quoted for the literature value, but it is unlikely that the temperature was far off this, and this difference is unlikely to have a great effect on the results.

It wouldn't be possible to measure lifetimes shorter than 0.01 seconds using this apparatus because this is the shortest time interval the probe can monitor.
Conclusion

It was proven that the decay of the triplet state of anthracene is first order and the lifetime for anthracene in Perspex was found to be 0.135 +/- 0.0090 seconds; however, it would be necessary to repeat the experiment to be confident of this value due to the unreliable nature of the apparatus.
Since the middle of the twentieth century, the agriculture and horticulture industries relied heavily on a broad range of synthetic chemical insecticides, fungicides and herbicides to maintain crop yields and quality.

Many of the active ingredients of these compounds were legacies of chemical warfare developments of the two World Wars (Carson, 1962).
Public and political pressure, resulting from high profile studies into the ecological affects of the long-term use of these chemicals, as well as their effects on human health has resulted in the withdrawal of many active ingredients in the last fifteen years.

This has been as a consequence of either direct bans by legislative bodies, or by the increased costs of re-registering products (due to increased levels of testing) due to more stringent risk assessment regulations (Pesticide Safety Directorate, 2006).
The horticulture sector has experienced the largest proportional withdrawal of chemicals due to the relatively small size of the industry and thus smaller economies of scale for pesticide producers.

Secondary to this factor, some serious arthropod pests have become resistant to many hydrocarbon-based pesticides, making the problem more acute.
This has paved the way for an increase in the use of biological control in horticultural production, mainly to manage or eradicate arthropod pests with the introduction of natural predators and parasites / parasitoids.

The use of biological control is generally accepted as being environmentally benign and side-effect free.
There have, however, been some instances where the non-target effects of a control organism or biological compound has had a detrimental effect on ecosystems, even causing extinctions.

The use of biological control agents in horticulture dates back to the late nineteenth century (Perkins, 1897).
Many of these early attempts at biological control were, however, unsuccessful.

It is likely also that all such introductions were uncontrolled and that no assessment of possible ecological effects was ever made.
Increased world trade of both fresh produce and plant material, combined with the likelihood of increased temperatures may lead to increased numbers of exotic pests in UK horticulture and hence a need to find effective controls for these pests.

Regulatory bodies must therefore ensure that the inevitable rise of biological control is safe both in ecological and human welfare terms.
Indeed, current legislation covering the use of biological control is identical to that governing conventional pesticides.

More than five thousand introductions of exotic arthropod agents for control of arthropod pests during the past 120 years have rarely resulted in negative environmental effects, yet risks of environmental effects caused by releases of exotics are of growing concern (van Lenteren, 2006).
Between 1998 and 2002 the EU funded ERBIC project ('Evaluating Environmental Risks of Biological Control Introductions into Europe') reviewed past and current arthropod biological control and attempted to create guidelines to assess the potential efficacy and risks of future biological control programmes (Lynch et al, 2001).

Biological agents fall into numerous categories.
The most prevalent types in horticulture exploit insect-insect or fungus-insect relationships.

Some overlap exists, especially in open field situations between biological and cultural control methods, and between biological and chemical control.
An example of such would be the use of allelopathic compounds, either produced by companion plants or extracted remotely and applied to crops as bio herbicides or bio insecticides.

Each type of control carries a different type of risk, however the focus here will be on the ecological effects of insect and fungal controls and on the risks of phytochemical toxicity.
One term often used when describing the action of a biological control agent is 'host-specific'.

In many cases, the agents introduced to control a pest are specifically adapted to prey on or parasitize that single organism.
These types of organisms tend to be used in controlled environment situations where an established pest population guarantees a food source for the control organism.

Whitefly is one such pest, controlled by a number of parasitic wasps, and whilst field biological control studies have been carried out, the results are very limited.
The controlled environment of protected crops in northern Europe is an ideal situation for the safe use of classical biological control.

The risk of escape is minimal, climatic conditions outside the glasshouse are unfavourable for the control organism and the specificity of the control agent means it is unlikely to find any prey / hosts outside.
Open-field or unmanaged situations, however, normally employ more generalist predators in the control of both pests and weeds.

The polyphagous nature of these organisms greatly increases the risk of non-target effects and thus studies of species interaction within the ecosystem in question must be closely examined before the programme is implemented.
The majority of outdoor arthropod biological control is conservation-type control or augmentation of naturally-occurring species.

Such methods can be beneficial for both crop production in terms of pest / weed control and other animals due to an increased food supply (Tooley, 2001) The activities of the control organism once the pest or weed problem is under control must also be examined.
Much of the recent debate regarding the ecological effects of biological control stems from the lessons learnt in the control of alien Carduus thistle species in the USA by the flowerhead weevil Rhinocyllus conicus Fröl.

(Gassmann & Louda, 2001).
This introduction was made into both Canada and the USA in the late 1960's following ten years of research on the feeding habits of R. conicus in northern Europe (ibid).

During the twenty years from its introduction, R. conicus successfully expanded its host range to include native Cirsium thistles (ibid).
This was not fully expected as it had been assumed that other, less damaging insects would competitively exclude R. conicus from this trophic niche (ibid).

The opposite, in fact occurred, with a native insect associated with the native thistle being displaced by R. conicus (Louda et al 1997, Louda et al 2003).
There are numerous other examples of biological control practitioners overlooking the possible long-term effects of a biological control agent.

Experiences such as those experienced in Australia following the introduction of the cane toad highlight the paradox of classical biological control in an external situation: the control must be able to establish successfully in order to permanently control the pest, but it must also fit into the natural food web so it does not become a pest in itself.
Even if the control agent is perceived as a low-risk introduction, the ability for a species to make a host-shift either due to environmental changes or merely an eradication of its food source over a long period may still exist.

This unpredictability, combined with the potential permanency of the control agent when introduced means that some biological control agents may have similar effects to the chemicals they have replaced.
The upsurge in anti-pesticide feeling over the last thirty years mainly stems from data pertaining to the persistence of pesticides such as DDT in the environment (Carson, 1962).

In contrast, modern pesticides have been synthesised to have very short half lives, with synthetic pyrethroids showing values of less than four days in some cases (Awasthi, 1989).
Non-insect biological control tends to encompass the use mainly of fungi or bacteria.

The former is a means of delivering the latter.
The ability to apply fungal, microbial, or viral biological control agents using spraying technology developed for conventional pesticide application, but with none of the associated risks to human health, makes them attractive to biological control practitioners.

The use of inanimate controls represents less of a challenge in terms of education when agents are to be used in the developing world, the Green Muscle® project being a prime example (LUBILOSA, 1999).
The combination of this factor with the low risk of insect resistance means that many such compounds can be advantageous for many producers.

There is, however, evidence that the use of the mycoinsecticides may eventually lead to the development of resistance in some pests.
Most notably, Lepidopteran species have shown potential for resistance to Bacillus thuringiensis (Bt) toxins, meaning a re-evaluation of Bt toxin-producing crops (through genetic modification) may be required.

The non-target effects of mycoinsecticides have also been highlighted.
Where the pest is closely related to indigenous species, products such as Bt may have undesired effects on these organisms.

Again, Lepidopteran species are most at risk (Wagner et al, 1996).
Generally, though, fungi tend to be highly host specific and thus harmless to humans and other organisms in the environment, hence the large amount of interest and investment in them as biological control agents.

Viruses also have the potential for use as biological control agents but their need for a living host means that the method of production and application presents a problem in the control of plant pests.
The most notable use of a virus as a biological control agent is in the control of the European rabbit (Oryctolagus cuniculus) in the UK, where Myxoma has helped to control populations since the 1950's in the UK (BBC, 2002).

The use of this virus does, however highlight one of the most complex issues in biological control; that of indirect effects on an ecosystem.
Whilst the control of a well established pest may be desirable from a human perspective, the diversity of the ecosystem of which the pest is part may suffer much more degradation than just the removal of that one species.

Indeed, the reduced numbers of rabbits feeding on grassland indirectly resulted in the extinction of one of the UK's rare butterflies, the Large Blue, by 1979 (Oates, 2001).
This is obviously one of the most difficult aspects of pest control to predict, whether the agent is biological or chemical.

The potential for organisms introduced as control agents to effect the members of higher or lower trophic levels of an ecosystem than that to which they are introduced is the main focus of much of the criticism of biological control outside a controlled environment.
The use of phytochemicals as insecticides and herbicides is an area of research with great potential.

Pyrethrins, commercially extracted from Chrysanthemum cinerariaefolium (Pyrethrum) capitula are the world's most economically important bio insecticide (Hitmi et al, 2000).
Yet these substances are nerve poisons, with the same mode of action as DDT (Journal of Pesticide Reform, 2002).

The main difference is however that pyrethrins have a short half life and thus pose a reduced risk compared to persistent chemicals such as DDT.
Whilst this may be advantageous, it could be argued that the necessity to repeatedly apply short half-life compounds somewhat reduces their environmental benignity.

Allelochemicals are also cited as replacements for synthetic herbicides.
Whilst the negative effect on crop yields produced by intercropping with weed suppressant plants makes this aspect of allelopathy unlikely to be used commercially, the use of extracted allelochemicals and their analogues as herbicides may have a future.

As with pyrethrins, many of the compounds isolated from plants and fungi as bioherbicides have high mammalian toxicity also.
For example, AAL-toxins and fumonisins, extracted from cultures of Alternaria and Fusarium respectively have demonstrated herbicidal properties, but at the same time are suspected carcinogens (Abbas et al, 1995) The trade of fresh produce worldwide is increasing and the affluent developed world demands exotic horticultural products all year round.

With supermarkets currently attempting to present and increasingly environmentally friendly image to the consumer (Tesco, 2006) and at the same time wielding more and more political power, it is not unforeseeable that a reduced reliance upon synthetic chemicals in crop production may be forced on suppliers in the developing world.
These producers do not always have access to the relevant education or resources to use non-chemical pest control techniques and hence will not be able to supply to those outlets.

There is the added danger that such production could then fall into the hands of multinational companies who do not necessarily have the best interests of the local producers at heart.
Such a scenario may be pure speculation, but this type of infiltration by large western companies is not uncommon in other industries and is becoming more unpalatable for many in both the developed and developing world.

Conclusions
Overall, the small number of documented cases of negative ecological impacts where biological control agents have been administered suggests that it is a safe method of pest control.

The increasing acceptance by the scientific community that these recorded cases may only be the 'tip of the iceberg' means that thorough risk assessments are required for new biological agents.
The absence of data on the indirect effects of biological control does not mean that it does not occur.

Whilst legislation requiring complex assessments of the potential effects of an alien organism may seem superfluous in some cases, it would appear that without a full understanding of the complex interactions of species within an ecosystem, these procedures are a necessity.
Biological control agents are controlled under directive 91/414/EEC (The Plant Protection Products Directive) and 2000/29/EU (controlling the introduction of non-native species).

The development of pesticides from natural sources is attractive to many as there is a perception that theses substances will be harmless to humans and the environment.
Whilst this is broadly true, and in the case of host specific fungi and parasites, the risk to non-target organisms is non-existent, the toxicity of some is as serious as many synthetic chemicals.

Finally, the tendency for developed nations to force their will on the developing world for the perceived greater good must at times be tempered.
One only has to examine the negative effects of the worldwide ban of DDT, and the reversal of the ban this year (BBC, 2006), to see that sometimes responsibly-used synthetic pesticides have their place.

Abstract
Aquaponics, the integration of aquaculture and hydroponic crop production represents a more environmentally benign and energy efficient method of production than each method practiced in isolation.

This study sought to ascertain the viability of aquaponics as an alternative to conventional hydroponics when producing lettuce (Lactuca sativa) cv.
'Trinity'.

Three treatments produced lettuce in recirculating systems in four trials between August and December 2006.
Control treatments supplied the plants with a full spectrum of nutrients, whilst two aquaponic treatments supplied the plants with water from cold-water aquariums containing either solely goldfish (Carassius auratus), or from identical goldfish aquariums with additional iron (FeEDTA) at two parts per million.

The final trial examined the effects of fish stocking density.
Growth rate and leaf chlorophyll content were measured to examine the effects of each treatment.

Tissue analysis of dried leaf tissue from the second and fourth trials ascertained mean N, P and K concentrations in leaf tissue for each treatment.
Statistical analysis showed significantly lower growth rates in both aquaponic treatments compared with the control in the initial three trials.

In trial 4, no significant differences were observed in growth rates.
Leaf chlorophyll concentration was significantly different to across treatments in one trial and leaf tissue analysis showed some differences in N, P and K concentrations across the three treatments in one trial.

The study ascertained that lettuce variety 'Trinity' may grow in an aquaponic system at rates comparable to conventional hydroponics, as long as fish stocking densities are high enough.
Introduction

This trial was designed to compare growth rates and tissue composition of lettuce (Lactuca sativa) grown using a commercial mix hydroponic nutrient solution with lettuce grown in water taken from a cold-water aquarium.
Hydroponic crops obtain their total nutrient requirement from soluble inorganic salts whilst aquaculture generally discharges nutrient-rich wastewater to maintain fish health (Lucas & Southgate, 2002).

Aquaponics is the integration of recirculating aquaculture and hydroponic crop production (Rackocy, 2004) and mirrors the natural process of nitrogen uptake that would occur in an aquatic ecosystem; the fish waste acting as a nutrient-source for the plants, which, in turn remove unwanted products from the aquaculture system (fig. 1.1).
Principles of an aquaponic system

Fish health is maintained through phytoremediation of tank water Plant growth is maintained through fish-derived nutrients.
The amount of nitrogenous waste produced dependent upon protein content of feed (Lennard, 2004) Hydroponic substrate and plant roots can provide habitat for nitrifying bacteria (ibid) The design of this trial was developed from literature available on the Aquaponics.com webpage "Building a mini-aquaponic system" (Nelson, 2004).

Literature review
Food production has recently come under scrutiny from the environmental lobby and the media, whilst the depletion of sea fish stocks through over fishing means that on-shore fish farms may become more prevalent in coming years (Purvis, 2003).

Warm water species such as Tilapia and Barramundi are already beginning to be farmed in intensive, indoor fish farms in the UK (New Forest Barramundi, 2006, Times Online, 2005).
An increase in the number of Nitrate Vulnerable Zones in the last ten years (DEFRA 2005; Appendix 1) and concerns regarding the amount of nitrogenous waste entering the environment (Hooper, 2006) means these businesses may find it increasingly difficult to dispose of their waste products.

In addition, industries such as horticulture and agriculture are also being made aware of the need to use non-renewable energy sources and water more wisely.
Aquaponics goes some way to reducing the environmental impact of food production in the following ways:

Increased efficiency in waste management and water conservation (Wilson, 2006)Reduced energy use per unit of produceCompatibility with urban situation can reduce the 'carbon footprint' by cutting down on pollution caused by transportation ("food miles") (ibid) Increased crop yields compared with soil-based systems as well as conventional hydroponic systems (Savidov, 2006)
The above factors also go some way to increasing the profitability of both the aquaculture and horticultural produce.

Disadvantages in aquaponic production systems include high start-up costs and reliance upon skilled staff to create and maintain a nutrient-balanced system.
Knowledge of both crop production and fish rearing is required, and thus most current systems are personal hobbies, small, family-run businesses, or academic demonstrations of the technique.

The time required before the grower sees the benefits of an aquaponic system must also be seen as a drawback.
It is currently estimated that growth in aquaponic systems will not reach its full potential until at least six, and ideally twelve months of operation (Savidov 2006).

The intensive nature of large-scale food production in the UK probably limits potential for diversification into aquaponics to smaller growers.
One possibility, is for aquaponics to be used to produce organic fish and organic hydroponic fruits/vegetables under glass.

However, current rulings, in the UK at least, by regulatory bodies such as The Soil Association stipulate that produce must be grown in soil to be certified organic.
The need to work in harmony with soil flora and fauna means that even organically-derived nutrient solutions are not acceptable.

It could be argued however that soilless growing preserves the soil-borne organisms as the soil is left completely undisturbed.
Furthermore, recent work at the Crops Research Centre in Alberta, Canada suggests that the micro-organisms may be key to the success of an aquaponic system, Savidov (2006) noting that the 200% increase in Genovese basil yield over four years was likely due to bio-stimulants released by populations of benthic organisms.

Atkin and Nichols (2004), using manure-derived nutrient solutions in conventional hydroponics, researched the possibility of organic hydroponics in New Zealand but the results were disappointing.
The major wastes from aquaculture and the major fertilisers for crop production are ammonia in its ionic form, ammonium (NH 4+) and nitrate (NO 3-), as well as phosphates (PO 43-).

Other chemical by-products of fish production may also serve as plant micronutrients.
This has meant a wide range of crops, including cucumbers (Cucumis sativus), beans (Vicia faba) (McMurty, 1986), strawberries (Fragaria x ananassa) (Takeda et al, 1997), tomatoes (Solanum lycopersicum) and melons (Cucumis spp.) (UVI, 2001) have been successfully grown using aquaponics in previous experiments.

A large proportion successful research, however has used lettuce and leafy herbs in the hydroponic component of the system, possibly because of its lower demand for potassium (K) (Resh, 1995), a nutrient not generally found in high concentrations in fish-culture water.
Furthermore, with only one growth stage (as a commercial crop) lettuce does not generally require the change in nutrient formula of a fruiting crop such as tomatoes, as it begins to produce flowers and fruit (ibid).

Lettuce is a widely grown crop in the UK and can be produced under glass all year-round.
It is feasible that growers could consider using an aquaponic system to create a second income stream from fish farming alongside lettuce production, simultaneously reducing their fertiliser budget.

It is possible to supplement the aquaculture water with plant nutrients harmless to fish (Rakocy et al, 2004, Chaves et al, 2000) or even grow fish in complete hydroponic nutrient solution (Seawright et al, 1998).
Previous studies in aquaponics have often been carried out as aquaculture-based research projects, the hydroponic crop being seen as a by-product of the low-tech biological filtration.

The majority of these systems have farmed warm water Tilapia (Oreochromis spp.
) in the aquaculture portion of the system, as these fish reach saleable size quickly and tolerate high nitrate loading in their grow-out tanks (Dontje et al, 1999).

Successful trials have been carried out using cold water species such as trout (McMurty, 1986), but owing to the lower stocking densities tolerated by cold water species (due to relatively high oxygen demand), research has tended to favour warm water species.
Most studies have included edible fish; however, a proposed project to take place in the Netherlands will use ornamental fish (Aquaponics Journal, 2006).

The hydroponic production method has varied in previous trials.
Where Nutrient film Technique (NFT) has been twinned with fish production systems with dedicated solids removal, aeration and biological filtering, floating polystyrene rafts have been used in gullies of aquaculture effluent (Fig. 1.2) (UVI, 2001, Rakocy, 2004).

In substrate-based systems, the movement of the water through the medium provides aeration and can be used to provide mechanical solids filtering as well as providing the necessary surface to be colonised by nitrifying bacteria (Lennard & Leonard, 2004, McMurty, 1986).
Indeed the necessity of removing solid waste from aquaponic systems is a current cause of disagreement amongst hobbyists and researchers such as Rackocy, Lennard and Savidov.

Anecdotal evidence from Rakocy at The University of the Virgin Islands also suggests that the balance of macro and micronutrients can be controlled by altering the amount of solid waste removed from the fish production element, although no work has been published in support of this theory.
Jones (1997) also notes that non-ionic soil-borne substances such as amino acids, simple proteins, carbohydrates and urea can be assimilated in conventional soil-based cultivation by mass flow and contribute to plant growth.

Substrate-filled grow beds may also have practical advantages such as temperature regulation and water-holding capacity if pumps fail.
Flood-and-drain and continuous flow irrigation have been trialled, with initial work suggesting that reciprocating flow (flood and drain) provided greater aeration and more even nutrient distribution through the substrate (McMurty, 1997 cited in Lennard & Leonard, 2004).

Lennard and Leonard (2004) however disproved this theory, attaining higher yield [lettuce], higher dissolved oxygen concentrations and smaller water losses from a continuous-flow irrigation system.
Hydroponic lettuce production

As Jones (1997) comments, although there Have been many published formulas for the preparation of hydroponic nutrient solutions from as long ago as the late 19 th Century, the appropriate tailoring of the concentration of each constituent to the crop and production method is still largely down to the personal preference of the grower.
Madestein UK Ltd, a well established produced of lettuce plants, supplied the formulation used for the control treatment in this trial.

The effects of nutrient deficiency on the appearance and growth of plants is well documented and hydroponic nutrient solutions need to provide a complete spectrum of essential nutrients for crop growth and often supply these elements in excess to ensure maximum crop growth.
Whilst many of these elements can have a deleterious effect on growth at high concentrations, supplying nutrients at supra-optimal levels ensures that assimilates are not used to produce excess root as the plant is forced to 'forage' for nutrition.

Authors such as Resh (1995 and 2006) and Jones (1993) suggest the following approximate concentrations for lettuce production:
Nutrient content of aquaculture waste

Waste products from aquaculture are either solid, organic matter, or aqueous, inorganic compounds, mainly nitrogen and phosphorus based.
Whilst the majority of these compounds are not lethal to fish, it is beneficial to keep levels low as the effects on human health of eating fish grown in nutrient laden water are not entirely understood (Lucas and Southgate 2003).

Methaemoglobinemia, or 'blue baby syndrome' due to high levels if nitrate in drinking water, however is well documented (Hooper 2006).
Failure to remediate the water used in a recirculating fish production system may lead to a reduction of efficiency of fish production (Stickney 2005).

High levels of nitrate and phosphate in such systems left unchecked lead to the build up of algae, which must be removed, adding to the operating costs.
Intensive aquaculture systems, even those that recirculate water from the growth tanks, produce a high density of waste products, and must either remove a portion of the water (usually on a daily basis (New Forest Barramundi 2006)) to maintain acceptable levels of aqueous waste or remove unwanted compounds on site.

This may be followed by water re-use or by discharging the cleaner effluent into watercourses or sewage.
Effluent treatment may consist of removal of inorganic nutrients through sewage treatment e.g. constructed wetlands, or expensive denitrification steps in recirculating systems.

Secondary and tertiary water treatments before disposal may significantly reduce aquaculture profits and may mean large outlay for no return.
Whilst a retrofitted hydroponics setup is costly, this outlay can be recouped from the sale of crops unlike the capital invested in reed bed treatment for example (Lucas & Southgate 2003).

Solids removal is another costly but necessary process in recirculating fish farms.
The break-down of faecal matter in the growth tanks uses oxygen as well as obstructing water flow.

Most systems for solids removal rely upon belt removal or vortex generators that allow solids to collect at the bottom of a conical tank enabling minimal water loss in their removal (New Forest Barramundi 2006).
Fish farms in Israel, where environmental conditions and scarcity of water may be limiting factors, have incorporated greenhouse technology into aquaculture to control humidity, light and temperature (Kolkovski et al.

2003).
This, as in controlled environment horticulture, gives producers greater control over the growth rate and quality of their product.

In order to maximise available water, producers use super intensive recirculating systems, with up to 100kg of fish per cubic metre of water, meaning 10% of the water volume is fish biomass.
Incorporating hydroponics into such systems have successfully produced crop yields twice that of conventional agriculture in the same region (ibid).

Savidov (2006) has also researched the potential for aquaponics in a cool-climate, protected cropping scenario.
This work has trialled many crops, both edible and ornamental since 2002 using a system based around that at the University of the Virgin Islands (ibid).

Aquaponics - a more holistic approach
Aquaponics does not merely combine the technology of aquaculture and hydroponics.

Recent theories by aquaponics researchers on the importance of a well established microbial population on plant growth e.g. those of Savidov (2006) are echoed in the work of hydroponics researchers such as Vernieri et al (2006).
These works reinforce the ideas of aquaponics proponents who believe the method to represent a more holistic approach to crop production by harnessing natural plant, animal and microbial interaction in order to reduce external inputs.

The horticultural focus of this study made goldfish (Carassius auratus) a sensible choice for the fish component of the system as they are hardier, and thus require less specialist care, than tropical or commercial food species.
Cold water aquaria present less of a problem when attempting to maintain levels of dissolved oxygen as lower temperatures allow for higher amounts of the gas to be dissolved.

As the tank water would be flowing past respiring plant roots and there would potentially be a large amount of decomposing fish waste in the grow beds, both of which consume oxygen, a cold water set-up was considered more appropriate.
With no need to heat the aquaria water, there was also reduced risk of equipment failure.

Furthermore, as goldfish flake food is approximately 40% protein, which is, in turn 16% nitrogen (Wurts 2006) this species would excrete more nitrogen-rich waste.
Most literature aimed at hobby-aquarists stress the need to feed goldfish only moderately as they will consume much more food than they require, leading to a rapid deterioration of water quality (O'Neill 2004).

Although this poses a problem in domestic fish tanks, this trait was seen as advantageous in the aquaponic systems to be trialled at.
Objectives

This study was designed to compare the growth rate, chlorophyll content, and leaf tissue N, P and K content of lettuce in two aquaponic treatments (one with and one without supplementary iron) and a nutritionally complete hydroponic control treatment.
This comparison was achieved by comparing various growth parameters before and at harvest along with tissue analysis post-harvest.

Other data were recorded (nitrate content, pH and electro-conductivity of the nutrient solutions, and root production by the plants) in order to explain any differences in growth rates.
The following hypotheses were tested: There is no significant difference in growth rates of lettuce grown in aquarium effluent and lettuce grown in a commercial strength hydroponic nutrient solution.

Leaf chlorophyll content of lettuce grown in aquarium effluent with added FeEDTA chelate is significantly greater than that of lettuce grown in aquarium effluent alone.
There is no significant difference in leaf tissue N, P and K levels between lettuce grown in aquarium effluent and lettuce grown in a commercial strength hydroponic nutrient solution.

Materials and Method
The design of the systems used in this project is loosely based around the work published in 2004 by Lennard and Leonard: a continuous-flow lettuce production system.

These systems will however also draw from the work of Malcolm (Backyard Aquaponics 2006), whose grow beds contain expanded clay aggregate.
Expanded clay aggregate

Expanded clay aggregate gives fast drainage, allowing for high flow rates.
The size of the substrate particles and porous nature of the material may also give the water a relatively high oxygen content, advantageous for both fish and lettuce growth (Goto et al 1996; UVI 2001).

Smith (1996) also suggested this substrate had the advantage of maintaining a dry surface due to less capillarity, again because of particle size, that results in reduced algae formation.
It was anticipated that the clay would also give both pH buffering and Cation Exchange Capacity (CEC) benefits (ibid).

The honeycomb structure of the material also gave a large surface area to be colonised by nitrifying bacteria.
Experimental design

It was initially planned to randomise the treatments through the glasshouse in order to reduce the effects of environmental differences within.
However, due to the heavy reliance on glasshouse staff for feeding, the replicates of the treatments were arranged so that all tanks containing fish were grouped together.

Three different treatments, each with two replicates, were used to produce lettuce plants hydroponically.
Six plastic tanks contained the following:

Tanks 1 and 2
Approx. 70 litres water from mains supply.

Approximate pH 8.4Hydroponic lettuce nutrients: The stock solutions were added to tanks 1 and 2 in equal quantities until an E.C of approximately 1.5 mS/cm was reached.
(Feed formula detailed in Appendix A)

Tanks 3 and 4
Approx. 70 litres dechlorinated mains water, approximate pH 8.48 goldfish (Carassius auratus), each approximately 60mm in body length, to be fed daily with approximately 8g of proprietary flake food

Tanks 5 and 6
Approx. 70 litres dechlorinated mains water, 8 goldfish (Carassius auratus), each approximately 60mm in body length, fed twice daily with proprietary flake food as in tanks 3 and 4.Iron chelate, Fe-EDTA (13% Fe) diluted to 2 ppm Fe.

(concentration taken from Resh 1995; Demeyer et al 2001; UVI 2001).
FeEDTA is intended for use in hydroponic systems when the pH of the nutrient solution is around 6.5 - 7.0.

However, even at a pH of around 8.0, limited iron is still accessible (Figure 2.1), and since neither the control treatment nor the Aquaponic + Fe treatment was pH-altered, the iron availability was considered constant across these two treatments.
The preferable pH range in aquaria to ensure goldfish health and maintain a efficient action of nitrifying bacteria is around 7.5 - 7.8 (Bio-Con Labs inc.

no date; O'Neill 2004).
The decision was therefore made not to alter the pH of the water in any tanks; however, if iron deficiency symptoms, namely chlorosis of younger leaf tissue (Jones 1997), was observed in either of the two treatments containing iron, this would be reviewed.

Nutrient Supply
The water from each tank was pumped into to a 1.5m length of rain gutter filled with expanded clay aggregate.

The gutters were inclined at the end furthest from the tanks so that water returned to the tank (Figure 2.2).
The pumps in each tank continuously recirculated the water.

Each tank was covered in plastic sheeting.
This was black on the inside to prevent algal growth on the tank walls and white on the outer side to reflect sunlight and prevent the water becoming too warm.

Initial construction took place on 6th July 2006.
Water was supplied from each tank using a Blagdon MiniPond 700 pump to the substrate-filled gutters via standard 13mm irrigation tubing.

The piping was attached along one interior side of the guttering using zip-ties threaded through holes drilled in the gutter.
Eight holes approximately 3mm in diameter were punched at regular intervals of around 150-200mm along the section of pipe that ran inside the gutter.

The water flow was adjusted using the pumps' control valves to ensure minimum water was lost through splashing whilst still maintaining a rapid flow rate to ensure maximum aeration of the water.
During a run-in period, when no fish or plants were present, significant water loss had been observed due to evaporation from the surface of the tanks and from splashing as the water returned to the tanks.

In order to minimise these problems, black horticultural shade cloth was used to cover the tanks in an attempt to reduce evaporation.
The same material was also attached to the lower end of the gutters so the returning water ran down the cloth and into the tanks with minimal splashing (Figure 2.3).

This also had the secondary effect of increasing the surface area available to be colonised by nitrifying bacteria.
Action of nitrifying bacteria and tank 'cycling'

In order to efficiently convert harmful ammonia and nitrite into less toxic nitrate all aquaria rely on biological filtration.
Ammonia (NH 3) is excreted directly from the gills of fish as protein is metabolized (below), as well as being produced by the bacterial breakdown of solid wastes and excess food (Bio-Con Labs 2007) FORMULA (where R = organic nitrogen radical) The two most important genera of nitrifying bacteria, which facilitate the conversion of NH 3 and the less toxic ionic form ammonium (NH 4+) to nitrite (NO 2-) and then nitrate (NO 3-) are Nitrosomonas and Nitrobacter respectively (University of Minnesota 2006).

These bacteria are obligate chemolithotrophs, deriving their energy from the oxidation of inorganic salts and fulfilling their carbon requirements from carbon dioxide (CO 2) (Bio-Con Labs 2007).
They are also obligate aerobes as the following equations demonstrate: FORMULA This occurs in alkaline conditions, where the ammonia does not hydrolyze into its ionic form so readily.

Then the nitrite is oxidised to nitrate: FORMULA (University of Minnesota 2006; Randall No date) These bacteria need time and a source of ammonia to reach a useful population size.
Their reproduction rates are around sixty times slower than heterotrophic bacteria, taking around 15-20 hours to divide due to the low energy derived from the oxidation reactions (Bio-Con Labs 2006).

After initial construction, the four tanks that were to contain fish were 'cycled' using a bacterial starter colony solution and small amounts of fish food as a source of ammonia.
This process coated the growing medium with enough bacteria prior to the addition of the fish to minimise toxic build-up of ammonia when the fish were introduced.

The bacteria are non-motile and colonies attach to all submerged surfaces in the tank and continuously wetted substrate with a secreted slime matrix (ibid).
The fish were added to tanks 3-6 after two weeks of cycling.

Addition of fish and nutrient build-up
Eight goldfish were added to tanks 3 - 6 on 27th July and ammonia, nitrite, nitrate and hardness levels in the four fish tanks were monitored by dipping Esha Aquatest 5-in-1 test strips into the fish tank water during the following week.

Six days after adding the fish, test-strips indicated approximately 50-100 ppm NO 3-, which was deemed adequate to support lettuce growth.
Selection and addition of lettuce plants

Prior to their addition to the systems, the lettuce plants were grown from seed in rockwool propagating blocks and irrigated with water.
The variety of lettuce used in this trial was 'Trinity' grown from seed supplied by S&G Seeds.

Fifty-six seedlings in rockwool propagating cubes were selected.
These were arranged in a tray in a grid with seven columns and eight rows.

The seven plants for each treatment were selected by taking one from each column but from a random row number.
The following data for each of the remaining 14 plants was recorded:

Diameter (mean of two measurements taken at right angles through centre of plant) - measured to the nearest 0.5cm using a standard 30cm ruleHeight - measured to the nearest 0.5cm using a standard 30cm rule Number of leavesTotal leaf area (cm 2) - measured using a Delta-T leaf area meterLeaf fresh weight (g) - all non-senescent leaves removed level with top of rockwool blockRoot fresh weight (g) - roots protruding from the propagating block were removed and weighedLeaf dry weight (g) - fresh leaf material dried at 70˚C for approximately two weeksRoot dry weight (g) - fresh root material dried at 70 ˚C for approximately two weeks.
All weights were recorded using a Sartorius 1212 MP balance.

Root fresh and dry weights were taken throughout the project as the weight of those roots that protruded from the rock wool propagating block (Figure 3.4).
This process was replicated prior to each growth trial, each set of mean measurements from these plants serving as a base line when comparing the effects of each treatment at the end of the trial.

Trials 1, 2 and 3
The 42 lettuces to be grown on, in their rock wool propagation cubes, were pushed into the clay substrate at approximately 20cm intervals, allowing direct irrigation from each of the holes punched in the irrigation pipe.

Nutrient solution was added to tanks 1 and 2 until an EC value of 1.5 mS was reached (175ml from both tank A and B).
This represents a widely used concentration for most hydroponic crops.

The first batch of lettuces was harvested after 21 days (23 rd August).
At this time a second batch of pre-grown transplants were introduced.

Tanks 1 and 2 were replenished with fresh water and nutrient solution to bring the EC back to 1.5 mS and the plants were then grown on for 21 days.
A third trial was conducted using the same procedure from 2nd to 23rd October.

Prior to the third trial, the entire nutrient solution in tanks 1 & 2 was again discarded and replaced with fresh water and nutrient solution.
This procedure was also carried out prior to trial 4.

Trial 4
In order to assess the effects of fish stocking densities, a fourth trial was conducted along similar lines as the previous three, only with 16 fish in each of the aquaponic systems, double the number used in the first three trials.

In order to facilitate this without purchasing further fish, the aquaponic + Fe treatment was omitted.
This trial ran from 27th November to 18th December with the addition of glasshouse heating and supplementary overhead lighting.

Data Recording
Daily

Maximum and minimum air temperature (from glasshouse max/min thermometer).
Weekly

Height and diameter of each plant.EC of the nutrient solutions (using a Hanna instruments HI 933000 EC meter).Water parameters of each tank using, measuring:pH of the irrigation water (using a Hanna Instruments HI 8014 pH meter)Nitrate content of irrigation water (ppm NO 3--N) using Aquarium Pharmaceuticals Inc.
nitrate test kit.

This method proved more accurate than 5-in-1 test strips.
The overall appearance and presence of any pests, diseases or signs of nutrient deficiency was also noted for plants in each system.

At harvest (in addition to weekly measurements)
Plant fresh weight (g).Plant root fresh weight (g).Plant total leaf dry weight (g).

Plant root dry weight (g).
Chlorophyll content of a young leaf on each plant measured in relative using a Hansatech chlorophyll content meter.

Iron is a relatively immobile element and deficiencies will first be apparent in younger leaves.
Miscellaneous Data

Fish mortalities were recorded along with the dates of their replacement.
Nitrate testing

Nitrogen supply, either in the form of the ammonium cation (NH 4+) or the nitrate anion (NO 3-) is generally regarded as having the largest effect on plant growth of all nutrients (Jones, 1997).
The nitrate content of aquarium water is simple to measure using either reactive pads on paper strips (which proved inaccurate) or a standard reagent test kit, commonly used for domestic aquariums (this test was used for the majority of the project).

When using the reagent method, two reagents must be added to the water under test and then the sample must be set aside for five minutes whilst a colour change takes place.
On comparison with a colour chart, he intensity of the colour indicates the level of nitrate in parts per million (ppm) (figure 2.4).

Where the colour intensity appeared to fall between two concentrations on the colour card, an estimate was made; e.g. at the beginning of the each trial, the control solutions gave readings that were definitely darker than the 80 ppm reading on the card but not as dark as the next 160 ppm gradation.
In this instance, the readings were recorded as 100 ppm.

Leaf tissue analysis
Dry leaf tissue from trials 2 and 4 was crushed and sent to NRM laboratories for N, P and K analysis.

Leaves from all plants in each system were amalgamated to give two samples for each treatment in each trial.
This resulted in six sets of data for trial 2 and four sets of data for trial 4.

Data Analysis
All data were collated using Microsoft Excel.

All significance testing was carried out using the ANOVA function in Genstat version 9.0.
Results

Nitrate Levels
The plants were added to the substrate as soon as nitrate levels had reached levels acceptable for plant growth.

Test strips indicated NO 3- levels of 50-100ppm on 1st August and the plants were added the following day.
Later in the first growth period, comparisons of nitrate levels using these strips alongside another type of test using liquid reagents showed the strips to indicate much higher levels of nitrate than the reagent test.

The limited growth shown in the aquaponic systems during this period appeared to reinforce the inaccuracy of the strips and they were no longer used.
During the first trial, nitrate levels in the aquaponic systems had fallen to 10-20 ppm after seven days of lettuce growth (Figure 3.1).

Towards the end of the first trial, the amount of food applied to tanks 3 - 6 was greatly increased.
Glasshouse staff were instructed to feed two 'large pinches' to the fish daily.

This equated to approximately 8g per day and as a result, nitrate levels of 40 ppm were recorded on 31 st August.
Considerable water loss was noted in systems 3 and 5 during the first week of trial 1 as irrigation jets became blocked and water was forced over the side of the gutters.

The blockages resulted from particles of algae becoming trapped in the irrigation holes.
This algal 'bloom' is a well-documented phenomenon in new aquaria (O'Neill 2004).

The lost water in these tanks was replaced with water drawn from the mains, thus diluting any nutrients that had accumulated in solution.
During second trial, increased feeding rates helped nitrate concentrations climb to 40 ppm in systems 3, 4 and 6 by the end of the trial.

The concentration in system 5 remained 10-20 ppm lower than the other aquaponic systems during this time, following water loss and the death of one goldfish.
At the beginning of the third trial, nitrate levels in the four aquaponic systems had reached 40 ppm.

During the 21-day growth period, this concentration remained constant.
Increasing the stocking density of goldfish in the two aquaponic systems caused nitrate levels to reach approximately 80 ppm in the period between the end of trial 3 and the commencement of trial 4.

This concentration was closer to that found in the control systems than at the start of any of the previous trials.
pH

All treatments began with pH readings of 8.4, that of the mains water supply.
The pH of the aquaponic systems showed a downward trend throughout the project.

This was expected as a result of the action of nitrifying bacteria and fish respiration, however the fall in these tanks was not sufficient in relation to the control treatment to make any elements significantly more or less available (Figure 3.2).
By the end of the trial, and after an unexplained fall in pH in all systems at the start of the final trial, the water of the two remaining aquaponic systems had a pH of 7.1.

Electro-Conductivity
At the beginning of the study, electro-conductivity (EC) readings between 0.55 and 0.58 mS were recorded across the aquaponic systems.

In comparison, stock nutrient solution was added to the control systems to obtain EC values of 1.44 and 1.49 mS.
The EC of the aquaria water showed a general upward trend over the entire project, however when significant water-loss necessitated the addition of mains water, the EC of these fell, most notably in tanks 4 and 5 during the first and second trials (Figure 3.3).

One fish mortality occurred during the first trial, in tank 5, on 22nd August.
This was replaced during the first week of the second trial, and this tank showed lower EC levels (Figure 3.3) as well as lower nitrate concentrations (Figure 3.1) than the other aquaponic tanks throughout the second trial.

One further fish was found dead during the second trial on 12 th September in tank 4.
This fish was replaced in the second week of trial 3.

Growth Rates
Plant size

The diameter and height of each plant was measured weekly during each trial.
These measurements enabled the calculation of a 'volume' measurement for each lettuce and allowed a non-destructive comparison of growth rates between planting and harvest.

The decision was made to consider both height and diameter as some plants tended towards a narrower but taller morphology, and some towards a flatter, wider morphology.
Measuring the plants in this way does, however mean that differences in canopy density may lead to inaccuracies when making a direct comparison.

Figure 3.4 shows the slow increase in plant size in both aquaponic treatments compared to the control treatment during the first growth trial.
The mean volume of the control plants at the end of this trial was over 3000cm 3 compared to 210.95 and 235.21cm 3 in the aquaponic and aquaponic + Fe treatments respectively After seven days of growth in the second trial, the mean volume of plants in the aquaponic treatment supplemented with FeEDTA was marginally higher than the control (Figure 3.5) and the un-supplemented aquaponic treatment (878.38 cm 3, compared to 846.59 cm 3 and 742.06 cm 3 respectively).

After 14 days, the mean volume of plants in the control treatment was the greatest (3142.26cm 3).
By this time, the nitrate concentration in the aquaponic systems 3, 4 and 6 had fallen to 20 ppm, whilst that in system 5 had fallen to 10 ppm (Figure 3.1).

Volume measurements recorded after seven days' growth in the third trial were comparable across all three treatments, although the control plants were marginally larger.
Fourteen days into the trial, however, there was a marked difference in plant size between the control treatment and the two aquaponic treatments (Figure 3.6).

By the end of the grow-out period, the control plants' mean volume was significantly greater than both of the aquaponic treatments' plants.
As in the two previous trials, there was no significant difference in mean plant volume between the two aquaponic treatments (lsd = 258.7 at P=0.05) Throughout the final growth trial, the mean volume of plants in the aquaponic treatment was greater than that of the control treatment.

After the 21-day grow-out period, the mean volume of plants in the aquaponic treatment was 1520.8 cm 3 compared to 843.4 cm 3 in the control treatment (Figure 3.7).
It was noted though, that the plants grown in the aquaponic treatment displayed a much more open head structure than the control plants, which accounted for the marked difference in mean plant volume.

Leaf area and number increase
To compare the size of plants in each treatment more accurately, the number of new leaves and the total leaf area increase was utilised (Tables 3.1 and 3.2).

The destructive nature of this comparison however meant it could only be carried out after harvest.
In the first trial, the control treatment produced plants with a significantly greater mean leaf area increase and a significantly greater number of leaves than the two aquaponic treatments.

Plants from the aquaponic + Fe treatment showed a leaf area increase between 10% and 20% greater than the un-supplemented aquaponic treatment in trials 1-3, although this was not significant at the P=0.05 level.
After the fourth trial the mean leaf area increase of plants from the aquaponic treatment was significantly higher than that of plants from the control at the P=0.05 level.

Analysis of the number of new leaves produced during the first three trials showed significant differences between the control and each of the aquaponic treatments but not between the aquaponic treatments themselves at the P=0.05 level (Table 3.2).
After the fourth trial, however, there was no significant difference in leaf number increase between the control and the aquaponic treatment.

Leaf fresh and dry weight increase
The growth rate was calculated for each plant using both fresh and dry weights by subtracting the final weight from the mean starting weight, which had been calculated from the 14 reference plants.

The resulting gain was then divided by 21 days to give a simple growth rate in g d -1.
In the first trial, plants from the hydroponic control treatment showed a mean leaf fresh weight increase of 2.96g d -1 over the three weeks (Figure 3.8).

At over ten times the fresh weight increase per day, shown in either the aquaponic or aquaponic + Fe treatments, the difference was highly significant.
There were significant differences in leaf dry weights across the three treatments.

The mean leaf dry weight increase in the control treatment (0.12g d -1) was significantly higher than that in the aquaponic treatment and the aquaponic + Fe treatment, whose mean leaf dry weight increase per day values were identical at 0.06g d -1 (lsd = 0.016 at P=0.05).
In the second trial, the control treatment continued to outperform the two aquaponic treatments in terms of fresh weight gain per day with a growth rate close to that observed in trial 1 (Figure 3.9).

There were significant differences between the control treatment (2.88g d -1) and both the aquaponic treatment (1.07g d -1) and the Fe-supplemented aquaponic treatment (1.22g d -1) at harvest, but not between the two aquaponic treatments (lsd= 0.374 at P=0.05).
After drying, significant differences were again seen across the three treatments.

The mean leaf dry weight increase in the control treatment (0.11g d -1) was significantly higher than that in the aquaponic treatment and the aquaponic + Fe treatment (lsd = 0.016 at P=0.05).
The mean leaf dry weight increase per day was identical for the aquaponic treatments (0.06g d -1).

Trial 3 showed much the same pattern in growth rates, with the control plants showing a mean fresh weight increase per day over four times higher than each of the aquaponic treatments (Figure 3.10).
The P=0.05 lsd of 0.879 indicated no significant difference in growth rate between the two aquaponic treatments.

Mean leaf dry weight increases in trial 3 were 0.012 g d -1, 0.003 g d -1 and 0.004g d -1 for plants from the control, aquaponic and aquaponic + Fe treatments respectively.
Statistical analysis again revealed significant differences between the control and both aquaponic treatments, but not between the aquaponic treatments, at the P=0.05 level (lsd = 0.0024).

In the final trial, when fish stocking density was doubled in the aquaponic treatment, no significant differences were observed in fresh weight increase per day (Figure 3.11, P=0.05 lsd = 0.113).
After drying, the mean leaf fresh weight increase per day for both treatments was identical (0.0063 g d -1).

Root dry weight
The level of root production was measured and analysed across treatments in all four trials.

In the first trial, analysis of the mean root dry weight across the three treatments showed very significant differences (P<0.001), significant differences occurred between each aquaponic treatment and the control at the P=0.05 level, but not between the two aquaponic treatments (Table 3.3).
The control treatment was found to have approximately twice the mean root dry weight of each of the aquaponic treatments.

Upon removal from the substrate-filled gutters, it was immediately apparent at the end of trial 2 that the lettuces grown in the two aquaponic treatments had much more extensive root systems than those from the control treatment.
There was however a noticeable colour difference, the roots of the plants grown in the control were bright white and healthy, whilst those from the two aquaponic treatments were generally brown in colour and appeared less healthy.

Significant differences were observed in dry root weights between the control and each of the aquaponic treatments (Table 4.4).
At P=0.05, ANOVA testing showed no significant difference in mean root dry weight between the two aquaponic treatments.

After the third trial significant differences were only observed between the control and the aquaponic + Fe treatment at P=0.05.
Analysis of mean dry root weight after the fourth trial failed to show significant differences in between the two remaining treatments (P=0.737).

Chlorophyll content
Although the two aquaponic treatments gave slightly higher mean leaf chlorophyll content readings in the first trial, on analysis of the data, no significant differences were observed between the three treatments (P=0.728) (Figure 3.13).

Analysis of data from the second trial, however, showed significant differences in mean leaf chlorophyll content between the control and aquaponic treatments, the control and aquaponic + Fe treatments, and also between the two aquaponic treatments themselves (lsd = 0.60 at P=0.05) (Figure 3.14)..
In the third growth trial, the highest mean chlorophyll content was observed in the control treatment.

There were significant differences between the control values and each of the aquaponic treatments (P=0.05 lsd=0.534) but not between the aquaponic and aquaponic + Fe treatments (Figure 3.15).
In the final trial, the lowest chlorophyll content levels of the project were recorded, with plants from both treatments having mean values below 3.5 (Figure 3.16).

There was no significant difference between the two treatments (P=0.107).
Leaf tissue composition

Dried leaf tissue samples were analysed by NRM laboratories for nitrogen, phosphorus and potassium percentage by weight (w/w) for each treatment.
Samples representing each separate system were prepared from trials 2 and 4 as growth rates in the control and aquaponic treatment(s) were most similar in these trials.

Statistical analysis showed no significant differences in total leaf nitrogen concentration across treatments in either the second or final growth trial.
The total nitrogen content was higher in plants from the control treatment in trial 2 whereas in trial 4 the aquaponic treatment produced plants with the highest leaf nitrogen content (Figure 3.17).

Control treatment leaf tissue samples from the second growth trial contained significantly higher phosphorus concentrations than each of the aquaponic treatments (Figure 3.18) but there was no significant difference between the aquaponic treatments (lsd = 0.098 at P=0.05).
In trial 4, the leaf phosphorus content was higher in the aquaponic treatment, but not significantly.

In trial 2, mean potassium concentrations in plants from the control treatment were significantly higher than those in plants form each aquaponic treatment, but no significant differences were observed between the aquaponic treatments themselves (Figure 3.19, lsd =2.125 at P=0.05).
No significant differences in leaf potassium concentration were apparent in trial 4.

Pests, diseases and signs of deficiency
Several plants in systems 3 and 4 suffered caterpillar damage during the second week of the first trial (Figure 3.20).

Owing to the low vigour with which these plants were growing, two were unable to recover and thus were omitted from the results.
The leaves of lettuces in the two aquaponic treatments were noticeably thicker and more brittle in the first trial.

Furthermore, some of these plants also displayed spotting in the leaf surfaces.
These appeared as small necrotic patches on closer inspection and occurred both within the inter-venal areas of the leaf lamina and on the veins and midribs themselves (Figure 3.21).

During the third growth trial, a number of small flies were observed on all plants and several translucent larvae approximately 5-10mm in length were observed in the sludge that had accumulated amongst the substrate in the aquaponic systems (solid fish waste).
Visible larvae were removed and the systems monitored closely but no damage became apparent.

Discussion
The design of the systems proved robust enough to support the growth of both lettuce and fish.

Only two fish were lost over the period of the project, and those surviving approximately doubled in size over the duration of the project with no symptoms of disease.
The plants were able to remove nutrients from the aquaria water and, where enough fish were present to produce sufficient nutrients, growth rates were comparable to the control treatment.

The increasing electro-conductivity of the aquaria water, did however suggest that some dissolved nutrients were not being removed in the aquaponic systems at the rate at which they were being produced.
This may have resulted from the lack of faeces removal, and its subsequent mineralization.

The pH of the water in all aquaponic systems followed a downward trend over the project, this resulted from the action of nitrifying bacteria, the dissolution of CO 2 from fish respiration in tank water to form carbonic acid, and also from the bacterial breakdown of faeces and uneaten food to form other organic acids (Randall no date).
It is fair to assume that longer-term aquaponic projects of this nature may require the addition of buffers to mitigate this acidification.

The use of domestic goldfish in this project necessitated low densities of fish per unit volume of water.
The size of the fish in the trial was not closely monitored, however those surviving until the end of the project had approximately doubled in size.

One of these fish, which appeared to be representative of an average size at the end of the project was weighed.
A unit weight of 18.7g was recorded, meaning that the total weight of fish in each tank during trials 1-3 and trial 4 equated to 2.14kg m -3 and 4.27kg m -3 respectively.

When compared to commercial stocking densities up to 17 kg m -3 in the UVI systems and up to 100kg m -3 in super-intensive aquaculture systems (Lucas & Southgate 2003) the aquaponic crop growth rates observed in this project, especially in trials 2 and 4 must be seen as encouraging.
The brands of fish food used during the project contained approximately 40% protein.

In turn, proteins consist of around 16% nitrogen (Wurts, 2005).
In a commercial system, where the food conversion ratio of a fish species is known, it is possible to estimate how much protein will be converted to fish biomass and how much will enter the water as ammonia.

However, these calculations can be further complicated when the conversion of ammonia to nitrates is considered, with the rate of nitrification being dependent upon water temperature, pH and dissolved oxygen (DO) concentration (Stickney 2005).
The growth of plants in the aquaponic treatments was up to ten times lower than the control plants in the first three trials.

In addition, appearance of the plants in these treatments during the first trial suggested some nutrient deficiencies; the leaves of the plants were thicker and more brittle than those in the control treatment.
Their dark green appearance and the generally stunted growth was consistent with a phosphorus deficiency (Jones 1997; Salisbury and Ross 1992).

As aquaria nitrate concentrations did not rise above 40 ppm during the first three trials, it is also reasonable to suggest this low rate of growth was also significant of nitrogen deficiency.
It is worthy of note, however, that no chlorosis was observed, often apparent in plants starved of nitrogen (ibid).

Other than the above, there were no other outward signs of mineral deficiencies.
The use of mains water, which contains traces of both macro- and micro-nutrients (Thames Water 2005) (Appendix 3), twinned with the lack of solids-removal may help to explain this.

Leaf tissue analysis of plants from the aquaponic treatments in trials 2 and 4 showed concentrations of nitrogen, phosphorus and potassium in the normal range for lettuce (Jones 2003).
Whilst leaf concentrations of phosphorus and potassium are good indicators of the levels of these elements available to the plant, excess available nitrogen can lead to 'luxury uptake' (Greenwood et al 1980).

It is worth noting that in the final trial, the aquaponic treatment showed higher leaf tissue concentrations of nitrogen than the control treatment, presumably because the nitrate levels in this treatment did not fall as rapidly as those in the control, exposing the plants to more nitrogen over the 21 days.
In temperate zones where there are periods of low light during the year, the propensity for leafy salads to store excess nitrogen as NO 3- means that lettuce are routinely tested for nitrate concentration (Assured Produce 1997).

The possibility of higher nitrate concentrations in aquaponically-produced lettuce may therefore be in need of further investigation.
Root to shoot ratio in the second and third trials were higher in the aquaponics treatments than in the control, suggesting a lack of available nutrients.

Salisbury & Ross (1992) imply that this is most indicative of low nitrogen availability, which is borne out by low recorded NO 3- concentrations in the fish tanks of these treatments during these trials compared to the control tanks.
Low levels of root production in the aquaponic treatments in trial 1 may also be indicative of low phosphorus availability at that early stage in the project.

Small necrotic spots surrounded by a yellow halo were also observed on the plants from the aquaponic treatments (Figure 3.21), which was identified as an early infection of Xanthomonus campestris (Agrios 2005).
Whilst necrotic lesions can also be indicative of magnesium deficiency, the symptoms were not confined to inter-venal regions, and thus the initial diagnosis was accepted.

The control plants did not show any sign of such infection, one possible reason being the vigour with which they were growing.
The brown appearance of the roots of plants from the aquaponic treatments in the second trial would seem to suggest that there may have been at least some occurrence of anoxic pockets of water within the substrate, causing root death.

Sampling of the dissolved oxygen in the irrigation water (when the water temperature was 17 °C) showed levels of approximately 8mg l -1.
This is within the desired range for freshwater aquaria and well above the minimum levels acceptable for good growth in hydroponic crops (Goto et al 1996).

Continuous flow systems through a substrate can lead to the water taking a 'path of least resistance' through the particles with some areas flooded but not moving.
In such areas, denitrification (nitrate reduced to nitrogen gas) can even occur.

Such a phenomenon would perhaps explain why some root tissue appeared necrotic.
The root systems of these plants tended also to trap solid fish excreta and hence impede the flow of oxygenated water past the roots, possibly another reason for the root death.

Dense root mats can be problematic in conventional Nutrient Film Technique (NFT) hydroponics where they can also starve themselves of oxygen by reducing solution penetration into the root mass (Jones 1997) The fluctuations in nitrate content of the aquaria water during the first three trials would appear to suggest that the fish stocking rates and hence feed input were insufficient to produce the amount of nitrogen required by the plants.
However, during the third trial, nitrate levels remained at 40 ppm for the entire trial.

During this period (early October) day temperatures in the glasshouse were noticeably lower than in previous trials (Appendix 4).
The corresponding fall in water temperature in the aquaria may have reduced the feeding rate of the fish, meaning less protein from the food was converted to fish biomass and more uneaten food may have merely decomposed than in previous trials.

The lower temperatures also corresponded to lower rates of plant growth, and thus less demand for nitrogen, although nitrate concentrations in the control systems did fall in line with the two previous trials.
Higher growth rates (although not significantly) were observed in the aquaponic + Fe treatments in the first two trials but not in the third.

Furthermore, only in the second trial did the addition if iron to the aquaponic systems have a significant effect on leaf chlorophyll content.
The addition of iron did not therefore appear to have any consistent significant effect during the project.

Plants are able to grow well in soilless systems where mineral concentrations are as low as those in soil solutions, as long as the nutrients can be replenished at a comparable rate to their uptake (Salisbury and Ross 1992).
This continual renewal is central to the success of an aquaponic system, where levels of nitrate, for example remain much lower than those found in conventional hydroponics.

From a fish-rearing perspective, treatment of aquaculture effluents are seen as problematic, especially in open systems, as the large volume flows carry relatively dilute nutrients (Adler et al 2000).
Rackocy (no date) and Lennard (2004) have demonstrated the ability of aquaponic systems to produce leafy herbs when nitrate levels are as low as 40 ppm.

It is theoretically possible to match nutrient production to nutrient uptake (at least for nitrogen) and thus for aquaponic systems to reach a nutrient equilibrium (Lennard 2004).
Plant scientists generally acknowledge the possibility that organic compounds such as vitamins can have a positive effect on plant growth (Salisbury and Ross 1992).

The importance of chelating agents in the uptake of nutrients, especially relatively immobile ones, is also widely known, and in soils it is known that these chelating agents are often of microbial origin (ibid).
Commercially available bio-stimulants for use in soilless systems aim to enhance inorganic nutrient uptake in a similar way and indeed, Vernieri et al (2006) proved the capability of one such substance to maintain yields of rocket (Eruca sativa) when the nutrient content in a hydroponic system was reduced to 25% of normal levels.

In aquaponic systems, the understanding of the action of micro-organisms in the production of bio-stimulants is growing.
The action of micro-organism-derived chelating agents as well as humic compounds from the breakdown of solid waste may well be a key factor in the success of aquaponics.

Savidov (2006 cited in Wilson 2006) has highlighted the need for aquaponic systems to 'mature' in order to produce economic crop yields.
The speed at which the system reaches this maturity would obviously be important to any business application of the technique.

The aquaponic systems in this project began to produce comparable yields to the hydroponic control after approximately four months of operation.
The decision not to remove solid fish waste may have been important in achieving this as the organic particles can act as carriers for bacteria and other beneficial micro-fauna (Kolkovski et al 2003) Aquaculture-derived manure has been cited as a useful slow-release fertiliser (Westerman et al 1993).

Naylor et al (1999) analysed fresh trout-farm manure, finding levels of nitrogen, phosphorus, potassium, calcium and magnesium (Mg) on a dry-weight basis comparable with cattle, poultry and swine manures.
The amount of solid particles present in the system is also linked to the dissolved oxygen concentration, with the aerobic breakdown of excreta and uneaten food increasing the amount of oxygen removed from the water.

This is the main reason why systems that are intensively stocked with fish, such as that of the UVI, remove solids (Rackocy no date).
Over the project as a whole, there were much lower levels of water use in the aquaponic systems.

Whilst the control systems required a 100% water change between trials as well as supplementary water to replace that lost through evapotranspiration, the aquaponic systems only required the latter.
Each aquaponic system thus used approximately 75% less water than the control.

Conclusion
The project demonstrated the ability of aquaponic systems to produce lettuce (Lactuca sativa cv.

'Trinity') using goldfish (Carassius auratus) aquaria as the nutrient source albeit at a slower rate than the control hydroponic treatment.
The rate at which lettuce can be produced in these systems related directly to the amount of food added to the aquatic portion of the systems, which would normally be in direct relation to the stocking density of the fish used.

Although the hydroponic control treatments produced a greater crop yield in three of the four trials, it is worth considering the amount of water used and the cost in terms of financial outlay for the inorganic nutrients and in terms of the energy used to produce these chemicals.
Aquaponic systems also have the advantage of producing two saleable products and, as has been demonstrated in the United States, the possibility of generating further income from agri-tourism.

Although there are obvious benefits to producing crops using aquaponic systems, the likelihood of commercial-scale aquaponic production occurring in the UK in the near future must be seen as unlikely.
It is possible however, that large increases in energy costs and restrictions on water use and pollution discharge may make such techniques more important in years to come.

Future Work
It was apparent towards the end of this project that the expanded clay aggregate bio-filters/grow-beds were not large enough to cope with 5 months of goldfish faeces.

It became necessary during the later trials to periodically remove the plug at the end of the irrigation hose to remove some solids from the pipe.
Larger, deeper bio-filters would therefore enable the retention of more soild waste and possibly, therefore higher concentrations of micronutrients and micro-organisms.

Lennard and Leonard (2006) assessed the comparative abilities of gravel bed, floating raft and NFT hydroponic systems to remove fish-derived nutrients in an aquaponics system, concluding gravel beds worked most efficiently.
Combinations of grow-bed volume and various substrates could therefore be trialled along with the removal or retention of fish faeces.

Now suggested as being an important factor in aquaponic systems, populations of water borne micro flora and micro fauna take some time to establish.
Savidov (2006 in Wilson 2006) has suggested that for populations of these organisms to reach useful size may take up to twelve months, and that from that point forward aquaponics can outperform conventional hydroponics in terms of crop yield.

A full investigation of these theories is currently under way by Savidov (Wilson 2006), however the limited time-scale for this project must be cited as a major limiting factor in view of these preliminary hypotheses.
Possibly, future research could assess the effects of commercially-available plant boistimulants in aquaponics where 'virgin' aquaculture systems are used to provide crop nutrition.

Finally, the increasing need to reduce energy consumption also presents the possibility of using warm water fish production alongside glasshouse crops to store excess heat during periods of high temperatures to reduce the need for night-time heating.
Forest Growth Rates

For this exercise, a reflection coefficient of 0.18 was used, representing a deciduous forest.
The growth rate of the forest, derived by multiplying the amount of absorbed solar radiation by the Radiation Use Efficiency (RUE) of the vegetation, was seen to change in response to variations in incoming solar radiation, leaf area index, extinction coefficient and RUE.

The effect of changing each of these parameters in isolation, as well as their interaction, is discussed below.
Incoming Solar Radiation (Rn)

When the other parameters under examination remained constant, the growth rate of the forest was seen to have a linear relationship with the amount of incoming solar radiation.
A 1% increase in R n produced approximately a 1% increase in growth rate (Table 1).

Monteith (1973) and many others have highlighted the agronomic importance of this relationship and the cumulative amount of solar radiation is often a good indicator of yield.
It is worth noting that this phenomenon is a driving force behind the evolution of glasshouse design in protected cropping.

In such intensive systems, maximising the amount of Photosynthetically Active Radiation (PAR) reaching the crop allows growers to maximise yields.
Some (notably Demetriades-Shah et al, 1992) have also highlighted the over-simplification that this relationship makes.

The photosynthetic rate is a function of several soil, environmental and internal biological factors, light intensity being only one of them.
(ibid).

When considering the yield response to changes in R n, one must also consider the interaction between R n and leaf area index (L).
Although not shown in this exercise, an increase in R n without a corresponding increase in L will eventually lead to the leaves becoming "light saturated", This light-response curve (fig. 1) shows that after the point of light saturation, increases in R n have no effect on the rate of photosynthesis (i.e. dry matter production) and may even cause a reduction due to heat stress.

The data from this exercise would appear within the linear part of the photosynthesis-light response curve, the gradient of the line representing RUE.
Leaf Area Index (L)

This value represents the ratio between the area of leaf for a given area of ground.
The values used in this exercise were 3.0 and 6.0.

Larger values for L mean that there is a greater area of leaf to intercept the incoming solar radiation, R n, and thus for a given amount of light the level of photosynthesis and dry matter production will initially increase with L.
This relationship is not, however linear.

When all other variable parameters remained unchanged, a two-fold increase in L produced only a 20-30% increase in the forest growth rate (table 1).
This is as a direct result of the exponential relationship between L and the proportion of R n intercepted (Beer's Law).

Whilst a denser canopy is able to intercept more light, there will come a point when the lower leaves of the canopy are not receiving high enough levels of light to photosynthesise (Monteith, 1973).
These leaves are still respiring and will therefore reduce the net rate of photosynthesis.

Extinction Coefficient (k)
The extinction coefficient represents the factor by which increasing leaf area index (L) increases the proportion of Rn intercepted.

The morphology of the leaves within the canopy and the sun's elevation determine the size of k and thus the value will change throughout the year as the sun changes position through the seasons or as leaves change their orientation as they mature.
Chartier (1966) in Rosenberg, Blad and Verma (1983) gives the following equation (k is expressed as k c): FORMULA where: FORMULA The effect on growth rate of changing k depends on the leaf area index.

When L=3.0, the effect of a 20% reduction in k was a 10% reduction in growth rate (table 1).
However, when L=6.0, the same reduction in k reduced growth rates by only 4%.

This demonstrates the exponential relationship between transmission/absorbtion and L i.e. when L is larger and the curve is at a flatter angle, a given change in k will not affect the transmission-interception ratio as much (fig.2).
Thus the growth rate was less sensitive to fluctuations in k than fluctuations in other parameters.

Monteith (1973) stated that horizontally held leaves (high k values) required a leaf area index of only 3 to achieve 95% attenuation of Rn at 1m below the crop canopy whilst those with leaves held near the vertical (lower K values) required L to be nearer 10 to achieve the same interception rate.
This is entirely due to differences in the extinction coefficient of the two types of leaf morphology.

Radiation Use Efficiency (RUE)
The RUE is a simple measure of a plant's ability to convert energy from the sun into chemical energy in the bonds of organic molecules (Biomass production).

The growth rate can be estimated for a given type of vegetation or crop if the average RUE is known and also the amount of PAR available to the plant.
Since the growth rate is the product of these two values, the growth rate and RUE show a positive linear relationship.

In a crop situation, however it is worth considering that there are many other factors influencing crop growth.
RUE gives some idea of a crop's capability to produce biomass, but where other factors, especially nitrogen availability (Rosati & Dejong, 2002) are limiting, the use of such an oversimplified method of estimating growth may not be useful.

Energy balance of sheltered and exposed crops
Sheltered and open soybean crops showed differing energy balances and rates of water use over the three days (table 2).

Where λE is greater than Rn, and C is negative, the amount of energy used by the plant for evapotranspiration was greater than that received as solar radiation.
This energy comes from the air surrounding the plant as heat, as well as directly from the sun and is used to turn liquid water into water vapour (latent heat of evaporation, λ).

Calculating water use:
The amount of water used was calculated as below λE given as an instantaneous value W m -2 = J m -2 s -1 FORMULA

a) Effects of shelter on the energy balance of the crop
The sheltered crop consumed less energy for evapotranspiration in all instances.

This energy to drive evapotranspiration comes from either solar radiation or from warm air surrounding the plant.
Movement of air across the crop allows for greater energy and mass transfer, mixing air from the mass of the atmosphere with the relatively still air directly around the plant.

Sheltering the crop would reduce this movement of air across leaf surfaces.
The boundary layer immediately surrounding the leaf is a relatively undisturbed area of air, meaning that as water vapour escapes from the stomata, this layer experiences raised levels of humidity.

In an unsheltered crop, the movement of the bulk of the air outside this layer, often in the region of 5mm in thickness around a single leaf in a sheltered position (Boulard et al, 2002), facilitates the removal of water vapour through turbulent transfer, reducing the thickness of the boundary layer and reducing aerodynamic resistance (r a).
In a stand of vegetation, such as the crop of soybeans in this exercise, there is also a boundary layer of relatively undisturbed air above the crop canopy (Monteith, 1973).

Again, mass and energy transfers occur between the vegetation and the bulk of the atmosphere via this layer.
Higher windspeeds mean increased turbulent transfer, meaning there is more mixing at the top of this layer, in effect reducing its thickness.

This is also apparent in taller, more aerodynamically rough crops, where the opposition to mass transfer, r a, is lower.
Where C is a negative value, as has already been established, the flow of energy is from the air to the crop to drive evaporation.

This means the air directly surrounding the crop is being cooled and thus its ability to hold water vapour is diminished.
In sheltered situations, where heat is not being transferred from the bulk atmosphere to this cooled area of the boundary layer as readily by turbulent transfer, the higher relative humidity (RH) in the proximity of the crop canopy further reduces the rate of evapotranspiration.

Rosenberg, Blad and Verma (1983) point out also that reference to Penman and Daltons equations and there susequent permutations, enforces the direct relationship between windspeed and rates of evapotranspiration.
b) Differences in energy balances over the three days

During day 1, the net radiation values were highest During the morning and afternoon measurement periods, for both crops, sensible heat, C was negative, although that for the sheltered crop was only -28 W m -2 between 0800 and 1200 meaning low levels of energy transfer from the crop to the air occurred in this period.
During day 2 levels of net radiation were slightly lower, although on this day there was a net warming of the air in the sheltered crop (positive morning values for C and small negative afternoon values).

The open crop still showed similarly elevated levels of energy flow from the crop to the air compared to the sheltered crop, especially in the morning.
This suggests that higher levels of evapotranspiration were mainly due to increased windspeed in the open crop in the morning.

The total net radiation level on day 3 was approximately half that of day 2.
During this day, the levels of λE and C in the open and the sheltered crops were the most similar.

The morning of 17 th July was the only time that C was a positive value for the open crop.
The lowest levels of water use also occurred on this day.

The data would seem to suggest that the lowest levels of net radiation also coincided with the lowest windspeeds of the trial.
Not until the afternoon did both crops begin to use energy from the surrounding air for evapotranspiration, and during this time the rates were similar.

This, again suggests that the microclimate of the sheltered crop was not significantly different to that of the open crop.
c) Differences in energy partitioning between morning and afternoon.

Energy used for evapotranspiration in both crops was lower in the morning than in the afternoon.
At 0800, when the morning measurements began, the air and the leaves would still have been cool, and possible covered in dew.

This means that during the early part of the morning, most of the solar energy and energy transferred from the bulk of the atmosphere by turbulent transfer would heat plant tissues and the cool air surrounding the plants, not drive evapotranspiration.
Morning levels of evapotranspiration can also be affected by dew on the leaves of the plant.

The contribution of dew as a source of water for evapotranspiration can be considerable depending upon the climatic conditions and type of vegetation (Rosenberg, Blad & Verma, 1983).
Where this occurs, the cooling effect on the surrounding air could possibly mean that the warming effects of both the sun and the bulk atmosphere may not be felt so rapidly as the day progresses, especially in the sheltered crop where the layer of cool air around the plant is less quickly removed.

The propensity for the air around the plant to be warmer during the night in the open situation (ibid), due to transfer of energy from the greater atmosphere also means that plant and boundary air layer temperatures would have probably been lower in the sheltered crop when measurements commenced at 0800.
In both situations, sheltered and open, and on all three days, the amount of energy removed from the air was greater in the afternoon.

In the afternoon the air is normally at its warmest and the sun, especially in July, when the data were collected, is likely to be providing considerable net radiation.
In sheltered areas, where the amount of turbulent transfer is reduced, the removal of sensible heat is reduced (ibid).

During this time, it is likely that air and leaf temperatures will reach their highest levels.
The plants need to cool themselves through increased transpiration and even on 17 th July, when R n was at its lowest, C was negative in both the open and sheltered crops in the afternoon.

The direct heating effect of incoming solar radiation on a crop means that leaves can become hotter than the air surrounding them.
If the rate of evapotranspiration is too high and water loss too rapid, however, plants may invoke a stress response whereby they close the stomata to prevent further water loss.

The cooling effect of air movement can therefore be more effective in reducing transpiration than is the wind in decreasing the boundary layer and thus increasing evapotranspiration (Salisbury & Ross, 1992) In these situations, where the dissipation of heat is aided by turbulent transfer, sheltered crops could then possibly show lower growth rates than those in the open field.
This is obviously more likely to be problematic for C3 plants.

Introduction: EU directive 91/414
The Plant Protection Products Directive (91/414/EEC), 'The Authorisations Directive' came into force on 26 July 1993 and is implemented in the UK by the Plant Protection Products Regulations 2003 (Pesticide Safety Directorate, 2006).

The directive lists (as annex I) all active ingredients approved for use in the horticulture and agriculture sectors.
Any additions to this annex can only be made once the EU commission has approved the compound for use, generally on an agreed range of crops and situations.

Additions to the annex or renewal of time-bounded approvals are subject to involved risk assessments and re-registration procedures.
For small markets, such as horticulture, the cost to herbicide producers of such tasks often outweighs the projected profit from that product.

The result, therefore, of this legislation is that the UK horticulture sector now has access to a reduced number of herbicide active ingredients.
Implications for UK horticulture

Combined with increasing herbicide resistance in weed species, and increased labour costs, this legislation represents a challenge for horticultural weed management processes in years to come.
Indeed, Baumann et al (2000) cite the former two factors alone as strong reasons for producers to re-embrace traditional, cultural methods of weed control such as selection of crop planting date, rotation, cover crops, and polyculture cropping.

The implication of re-introducing such practices will, however, increase the ecological complexity of agro ecosystems, demanding a higher level of knowledge and management skill (ibid).
Alternative methods of weed control

In order to continue effective weed management, producers have begun, and must continue to practice integrated weed management strategies.
This will often mean a switch from a chemical-driven, reactionary approach to a more strategic, integrated weed management methodology with a long term view of control rather than eradication.

Whilst novel strategies for weed control have been reported, more traditional methods probably have more practical value and may be implemented at much lower cost (Bárberi, 2002).
Many important horticultural crops in the UK are grown undercover, where weeds do not present such a problem, thus the amount of research carried out in the field of weed control specifically for horticulture is relatively minor.

Horticulturists must therefore draw upon traditional crop management techniques as well as contemporary research in the agricultural sector to devise programmes of integrated weed management (IWM) which have relatively minor reliance on synthetic herbicides.
Soil cultivation

When the reliance upon chemical control is reduced, the application of a weed management strategy is affected most importantly by modified tillage and cropping practices (Aldrich & Kremer, 1997).
Type, frequency and timing of tillage activities have a profound effect on the size of the weed seed bank, the establishment of weed seedlings, and on the competitive ability of established plants.

Repeated tillage will effectively control weed growth but has a profound, negative effect on soil quality through increased erosion, mineralization and nutrient leaching (Håkansson, 2003).
Inversion tillage is commonly used as part of soil preparation activities and helps to reduce the weed seed bank in a number of ways.

However, the effects on weed seeds are more complex.
Those seeds requiring darkness to germinate will not have sufficient food reserves to break the surface.

Furthermore, once buried, the seeds are more susceptible to decomposition by micro organisms or loss of viability.
It is worth noting that repeated inversion tillage of the same soil will bring buried seeds back, or near, to the surface and viable seeds may germinate (WRAG, 2003).

The mechanical scarifying effect on seed coats by soil particles and plough may also be a factor in increased seed mortality or germination promotion (Håkansson, 2003).
The increased aeration provided by shallow tillage may also be beneficial in weed management, increasing the aerobic activity of micro organisms, and promoting seed-coat degradation and seed germination.

Germination promotion may be desirable, especially when integrated with some chemical control or other direct, not chemical control e.g. mulching, solarization.
The timing of crop planting, especially delaying the planting date of crops traditionally sown in early spring, has been explored as a way of increasing the competitiveness of the crop plant.

Légère (1997) however found that delayed planting of early barley and oat did not achieve the expected levels of control of Galeopsis tetrahit and that yields were reduced by a shorter growing season.
Exploiting weed phenology to reduce competition, has however been demonstrated when delaying sowings of soybean (Glycine max) to control Abutilon theoprasti (Oliver, 1979)

Mechanical weed control
Individual 'roguing' of weeds by hand may be practical only for very localised infestations in small horticultural situations; however, most growers must employ mechanised weed control methods.

The effectiveness of mechanical weed control practices such as harrowing or hoeing is largely regulated by the subsequent crop-weed competition (Håkansson, 2003).
These procedures must strike a balance between the level of weed control and the negative impact this has on the crop and the soil structure (Kurstjens et al, 2000).

Between row weeding is normally carried out using tractor-pulled hoes, harrows cage weeders or rotary cultivators (HDRA, 2004).
The timing and frequency of any weeding activities is related to the crop being grown.

Those with dense, rapidly expanding canopies may require only one early weeding operation before they shade competing weeds, whilst slow-growing crops or those with smaller leaves may require frequent operations (ibid).
The cost of such control is by no means a small influence on producers.

Labour and fuel costs mean running machinery reduce profits, and may become more expensive in the future as 'green taxes' come into force to control carbon emissions.
More complex mechanical finger weeders and brush weeders have value for intra-row weeding, uprooting weeds rather than covering them with soil, however, soil type and condition may influence their effectiveness (Bárberi, 2002).

In order to minimise the competitive effects of weeds, the crop needs to establish quickly and gain a competitive advantage for light and nutrients over any weeds.
The height of the mature plant (combined with plant spacing) will also determine the amount of shade cast on surrounding weeds.

Data available for wheat indicates that short stemmed cultivars (often selected as they are less prone to wind damage) produce less shading and therefore allow greater weed growth.
Aldrich & Kremer (1997) and Bridges & Chandler (1988) suggest that four times as many annual grass weeds occurred in short-stemmed varieties.

Low-growing cultivars also produce less crop residue, meaning there may be more weeds in the next crop in a low-till regime through reduced ground cover (ibid).
Successive monocultures mean the same cultivation practices at the same time each year.

This means that a weed favouring this regime will become prevalent.
Rotating crops and sowing, harvesting and tilling at different times each year means the cycle can be broken and the weed is unable to become so established.

Current recommendations are for growers to alternate spring and autumn sown crops, although this is mainly to reduce instances of herbicide resistance (WRAG, 2003).
The relative crop morphologies of subsequent crops in a rotation will also play a major role in weed control.

Weeds suited to the slow development and limited canopy of crops such as those in the allium family, for example, may be successful in setting seed in that year, however, if a rapidly establishing, large leaved crop follows, the weed seeds may germinate, but the weed may not be able to complete its lifecycle as it is out-competed for light and nutrients.
This is advantageous as the weed seed bank is reduced.

Organic production already relies heavily on crop rotation; however, its benefits are mainly seen as nutritional in this case.
Whilst crops tend to be seeded in agricultural systems, horticulture, with higher value crops has more of a tendency to use transplants.

These small plants are able to be planted at their final stand density and, with there root systems already developed, can establish much more rapidly than seed.
This reduces the impact of emerging weeds as the crop has a competitive advantage for both light and nutrients.

Novel technologies in mechanical weed control
New technologies have been trialled in weed management, including laser weed measurement and cutting (Heisel et al, 2002) and microwaves (Sartorato et al, 2006).

Whilst these methods have given some success, they will probably remain prohibitively expensive on a farm scale (Bárberi, 2002).
The withdrawal of methyl bromide as a soil sterilant also means that other techniques such as solarization and steam sterilization may become more prevalent, although steam sterilizing is an expensive and slow process and solarization relies on favourable weather and the ability to take areas of land out of production during the growing season.

Reducing the onset of herbicide resistance
A reliance upon one group of herbicides helps to speed up resistance in many weeds, with resistant genotypes rapidly become dominant.

This is clearly demonstrated by the swift increase in resistance to group B herbicides (ACCase inhibitors) since the early 1980's (Heap, 1997).
Bodies such as the Weed Resistance Action Group (2003) therefore advocate the rotation of herbicide groups.

Increased ability to identify patches of resistance is also key in dealing with the problem as removal of localised proliferations of a genotype will restrict its spread.
Irrigation and fertilisation techniques

It has been noted in field trials that some weeds reach a point of water stress earlier than the crop in which they are growing, thus becoming less competitive.
It may therefore be prudent in some known crop-weed interactions for growers to restrict irrigation, providing only enough for the required crop growth.

Timing the delivery of nitrogen fertilizer can also prove a key factor in weed control however the effects are dependent upon crop type and weed type.
For example, the early application of nitrogen to a sugar beet crop has been proven to increase its competitiveness with Sinapsis arvensis whereas a late application increases its competitiveness in relation to Chenopodium album (Paolini et al, 1999).

Mulches, cover crops and intercropping
These methods all control weeds by intercepting sunlight.

Furthermore, cover crops and intercropping techniques compete with weeds below the soil surface for water and nutrients.
Planting horticultural crops through plastic sheet mulch is already common practice and is effective in retaining soil moisture and increasing root zone temperatures as well as controlling weeds.

Organic mulches must be well-composted in order to prevent the introduction of weed seeds and pathogens.
Cover crops can be used between harvest and sowing of a cash crop, where they are ploughed in or killed with herbicide prior to cropping, or be grown with the cash crop (Hartwig & Ammon, 2002).

Intercropping uses the same principle but grows two or more saleable crops in a polyculture.
It is paramount that the negative effects of inter-specific competition do not render the produce unprofitable.

Baumann et al (2000) successfully reduced the occurrence of Senicio vulgaris offspring by 98% by intercropping leeks with celery but the trial returned significantly lower yields of lower quality leeks compared to a pure stand.
Biological control

Biological agents have a long history in the control of insect pests and although several examples of 'classical' type control of exotic weeds by arthropods exist, these successes have been in non-crop situations.
Examples include the control of Opuntia spp.

in Australia in the early 20 th century by the moth Cactoblastis cactorum and water hyacinth in a number of American and African waterways by weevil species.
Increased awareness of the indirect ecological effects of such projects means that modern biological weed control of weeds is largely through the use of host-specific fungi.

Some mycoherbicides have been developed, some becoming commercially available in the United States, however none are in widespread use at this time.
Loose smut fungus Sphacelotheca holci has been successful in infected johnsongrass systemically, nearly eliminating seed set (Maission & Lindow, 1986 in Aldrich & Kremer, 1997) but again, this is not used commercially.

A reduction in seed production can also be achieved by seed feeding insects.
Niesthrea louisianica has reduced viable seeds in velvetleaf between 5% and 90% (Spencer 1988 in Aldrich & Kremer, 1997).

Seed predation after seed rain is also an important factor in weed seed bank depletion.
In arable crops, it has been estimated that Carabid beetles account for up to 70% of predation losses, slugs and snails up to 30% and birds/mammals up to 10% (Tooley, 2001).

This highlights the importance of biodiversity in the crop and the potential of conservation biological control as a weed management tool.
Allelochemicals & crop residues

Phytochemicals may prove to be an important source of active ingredients for future weed control with new sites and modes of action (Duke et al, 2000).
The exact effects of allelopathy are, however, very difficult to quantify when other factors such as nutrient and light competition are present (Bárberi, 2002).

Furthermore, the unpredictability of the amounts and quality of allelochemicals produced by entire plants due to environmental conditions (ibid) probably means the application of plant extracts as bioherbicides is a more probable scenario than polycultures that include allelopaths.
Many plant-, fungus- and microbe-derived compounds have successfully been shown to inhibit weed seed germination or to promote suicidal germination and several studies have shown crop residues may release weed seed germination inhibitors (Putnam, 1988).

Conclusion - An Integrated approach
The withdrawal of a large proportion of herbicides from use in the horticultural sector means that growers must implement a more holistic approach to crop management.

Weed management is an integral part of such a strategy and whilst the over-reliance on synthetic herbicides in the last fifty years has reduced the breadth of knowledge of other possible control methods, there has been sufficient research in this field to arm growers with alternative management programmes.
No single method of weed control can replace synthetic herbicides, but a combination of cultural weed management techniques and a greater understanding of weed population biology are likely to prove invaluable when used in conjunction with those chemicals still available.

An increasing demand for organically produced food in the UK is likely to lead to an expansion of the amount of land given over to organic production both in the agriculture and horticulture sectors in coming years.
This is, however, expected to happen more slowly than during the late 1990s and early 2000s (Firth et al, 2005).

The resulting challenge for a larger number of growers will therefore be to ensure the maintenance of economic yields by using organically-certified methods to maintain soil fertility.
In order to embrace the true principles of organic crop production, conventional producers would need to move away from reliance upon external inputs altogether.

Instead they would need to learn to build and preserve soil fertility by working in harmony with natural processes such as legume nitrogen fixation in crop rotations, and animal manure recycling.
These methods of nutrient cycling prove slightly more difficult in organic horticulture systems than in organic agriculture as they rarely include livestock within the system and cannot practice such diverse rotation, as is common in organic agriculture.

In order to minimise external inputs, plant nutrients must remain in the system wherever feasible, and the cornerstone of any organic crop production is the maintenance of as closed a system as possible (Blake, 1987).
In a mixed farm scenario where livestock feed is produced alongside crops for sale, the use of animal manure as fertiliser can return nutrients to soil.

In stockless systems, where the sale of large amounts of produce will take soil nutrients out of the system, this nutrient replenishment requires external inputs.
A strategy to maintain adequate levels of essential nutrients must be developed from accurate 'nutrient budgeting' (Stockdale and Watson, 2002), where the amount of each essential element taken out of the soil must be replenished in order to ensure the long-term sustainability of an organic system.

Interestingly, Stockdale and Watson's work concluded that organic horticultural systems, by importing significant amounts of manure, showed the greatest mean nitrogen (N), phosphorus (P) and potassium (K) surpluses in their soils of all types of organic production surveyed.
Nutrient inputs

The introduction of off-farm manure can be costly in both financial and environmental terms, especially if the manure must be transported long distances.
A widely accepted fact of agriculture in the UK is that there is generally an East/West split of agricultural crop and livestock production respectively, mainly due to topographical and meteorological constraints.

Whilst this may pose a problem for organic agricultural crop production in the East in terms of manure procurement, horticulture, which is generally practiced on a smaller scale, is less geographically restricted and may thus find external manure input more feasible.
In addition to green and animal manures, and nitrogen fixation by legumes, organic growers are able, under guidelines laid down by regulatory bodies, to apply other sources of essential elements.

These can be animal-derived e.g. fish meal or hoof and horn, plant-derived e.g. seaweed and extracts mined or minerals and industrial by-products e.g. rock phosphate and slag.
However, those who remain sceptical organic production's environmental credentials are quick to point out the unsustainable nature of some of these supplements (Trewavas, 2004).

Along with compounds released by weathering of parent material, the decomposition of dead, naturally occurring soil biota, and dissolved compounds in rainwater, many of these supplements also represent important sources of essential macro and micro nutrients which may otherwise become depleted with prolonged cultivation.
The elements to which the majority of attention is paid when formulating nutrient budgets are however nitrogen, phosphorus and potassium:

Nitrogen
The level of available nitrogen in any type of system will normally be the greatest influence on crop yield (Jones, 1993) and reduced yields in organic systems are in part due to the lower availability of this element (DEFRA, 2003).

Nitrogen is, however, the most readily available of all the essential elements in plant growth.
In addition to that introduced as animal wastes, green manures act as nitrogen sources.

Unlike other major nutrients, producers are also able to harness atmospheric nitrogen from the growth of leguminous crops in their rotation (Blake, 1987).
The overall amount of nitrogen applied to an organic system may be comparable to that in conventional production, but the level of availability may be less controllable when growers rely upon microbial breakdown of manure for example.

In contrast, in conventional systems, synthetic sources of plant-available nitrogen can be applied at the stages in crop development when it is most needed.
The release by cultivation of nutrients from green manures and legumes may also prove problematic.

If a period of heavy rainfall ensues between the incorporation of a green manure, and establishment of the cash-crop, much nitrogen may be lost through leaching.
Again, in conventional production, the application timing of high-N fertilisers is much more flexible and can minimise leaching.

Efficient use of nitrogen to minimise losses is achieved through careful consideration in a rotation sequence of each crop's nitrogen requirements.
In order to capitalise on the release of nitrogen from a fertility-building step in a rotation, crops with high nitrogen demand, such as brassicas, must comprise the following step in the rotation.

As a general rule, rotations sequentially grow crops with reducing nitrogen demands (Blake, 1987).
Phosphorus

As is well documented, once applied to the soil, phosphorus can rapidly move from labile orthophosphates into insoluble compounds with metal cations (Brady, 1990) Movement from unavailable to available forms takes place by mineralization of these compounds within the soil structure.
Today's organic producers who grow on land that has been in conventional cultivation for many years, currently reap the benefits of decades of necessary over-application of phosphate fertilisers.

Due to this fact, Blake (1987) assured that phosphorus supply in organic systems would not present an issue.
Contrary to this assumption, and although phosphorus is derived from both animal and green manures, organic growers are permitted by regulatory bodies to add supplementary rock phosphate as necessary to maintain the required levels of availability.

Notably for phosphorus, but applicable to many elements forming insoluble compounds is the fact that, since it is not in the soil solution but within soil particles, phosphorus loss can occur through soil erosion (DEFRA, 2003).
Potassium

Potassium (K) is required in relatively large amounts and is relatively soluble in the soil water (Blake, 1987).
Furthermore, exchangeable K+ can be adsorbed onto negatively-charged particles of clay and organic matter.

Indeed, Alfaro et al (2004) showed that light, sandy soils, with a low clay content, are most prone to potassium leaching, reinforcing the importance of organic matter content when using organic production methods on these types of soil.
Of the three most commonly-studied plant macro-nutrients, maintaining adequate levels of potassium represents the greatest challenge for organic growers and research has questioned the level of potassium self-sufficiency possible in organic production (Öborn et al, 2005).

The addition of potassium-rich mineral salts is generally required to supplement that released by weathering on nearly all soil types, with the exception only of young, clay rich soils (ibid).
Alternative sources of nutrients

Although reliance upon external inputs is not encouraged in organic production, there may be certain situations where environmental benefits may be realised by adding nutrient rich products to the soil from external sources.
One example would be the use of composted municipal green waste, although the variable quality and variable organic status of this material could make it unsuitable for organically-certified growers.

Seaweed, either incorporated into the soil, or applied foliar sprays also represents a useful source of plant nutrients (Malaguti et al, 2002).
The incorporation of crop residues returns nutrients to the soil that were removed in the production of the crop.

Whilst this strategy appears an obvious way of maintaining soil fertility, the high carbon content of some residues often means that its microbial break-down robs the soil of precious nitrogen.
Micro organisms draw the nitrogen from the surrounding soil in order to maintain the carbon to nitrogen they require to consume the material.

Secondly, returning residues increases the chance of pest or pathogen establishment.
In order to break pest and disease cycles, the composting of crop residues before returning them to the soil would therefore seem a more favourable method of reclaiming these nutrients.

The importance of organic matter
The addition of organic matter is an important factor in maintaining soil fertility in low-external-input systems.

All forms of organic matter added to the soil stimulate the growth of micro-organism populations where sufficient air is present.
Whilst the addition of adequate essential elements to the soil may be easily achieved, the challenge for some growers is the retention of those nutrients in an accessible form.

The ability of soil organic matter (humus) content, to hold on to available ionic nutrients is essential to the success of organic production on low-clay-content soils.
The high cation exchange capacity if soil organic matter prevents mobile minerals such as potassium being leached from light soils with downward water movement.

Johnston (1986) reported twice the level of soil organic matter in soils which had a long history of treatment with manure compared to soils that had received solely synthetic fertilisation for a similar amount of time.
Johnston also cites the relationship between soil organic matter and crop yield, reporting a direct positive correlation.

The fraction of increased yield attributable to the nutrient supplying qualities of organic matter in comparison to the increased water holding capacity of soils with a high organic matter content is not, however discussed.
The soil as a living organism

An original concept of bio-dynamics, from which today's organic movement was born, stressed the need for growers to work in complete harmony with nature.
The underlying concept that the soil must be treated as a living organism is one that remains the central theme behind the ideas of organisations such as the Soil Association.

The continuous breakdown of applied organic matter by both micro-organisms and saprophytic fungi, as they access carbon and other elements for their own growth, and its physical distribution through the soil profile by earthworms and other fauna is key to the steady supply of plant nutrients to organically-grown crops.
There is also evidence that healthy populations of soil microbes can improve the uptake of those elements that form relatively insoluble compounds by the exudation of chelating agents (Salisbury and Ross, 1992).

The use of such 'bio-stimulants' in a refined form, essentially products which allow plants to access soil nutrients more efficiently, along with mycorrhizal fungi in fruit growing for example (Hoepfner et al, 1983) could also represent a way of allowing organic crops to access nutrients when they are available in lower concentrations in the soil solution.
Tillage regimes and crop choice

The concentration each plant nutrient within the soil profile is variable.
A proportion of tillage operations performed in organic systems, therefore need to be deep enough to bring much needed nutrients from below crop rooting depth into the topsoil.

Deep cultivation also aerates the lower levels of the soil, increasing the speed of oxidation reactions and of microbial breakdown of organic matter and the parent material.
Such operations may, however, disrupt the habitats of larger subterranean fauna; the increased rate of mineralization could also lead to nutrients becoming available more quickly than they can be taken up and hence being leached from the soil.

Refinement of the crop rotation to include plants with varying rooting depths and morphologies can also allows each sequential crop to access nutrients at different levels of the soil structure.
Blake (1987) refers to this a drawing of nutrients from lower soil strata as 'vertical shift'.

Conclusions
The popularity of organically-produced food is highly likely to continue, due to public perceptions of its wholesomeness and of its reduced environmental impact.

To remain successful, growers moving from conventional to organically-certified production must adopt strategic, long term approaches to soil fertility.
By developing skills in fertility building and maintenance through a greater understanding of the chemical and biological processes occurring in their soil, organic producers can attain the maximum yield and product quality with minimal external fertiliser inputs.

INTRODUCTION
Many physical and chemical factors e.g. light, temperature and nutrient, can overcome seed dormancy.

When a single factor is applied, the effect on germination can be small or none, but the combined effects of two or more factors could be considerate.
In this experiment, seeds of an important annual weed, Chenopodium album L.

in the goose foot family (Chenopodiaceae), which grows in waste and cultivated ground throughout the British Isles (Stace, 1991), were used.
Murdoch et al.

(1989) showed the effect of alternating temperatures on C. album seeds germination in the present of nitrate nutrient and light.
In this experiment, tests were set up to measure the effects of these three factors, alternating temperatures, light and nitrate nutrient, which are the possible main factors in determining the germination of weed seeds in the soil environment.

Appropriate controls, constant temperature, darkness and water, as well as all possible combination of treatments were used to measure the separate effect of one factor, and any possible interactions of two or more factors.
The result is a 2 3 factorial experiment with eight different treatments.

MATERIALS AND METHODS
Table 1 summarises the set-up of the experiment.

Each sample was a batch of 50 C. album seeds subjected to one of the eight treatments and each treatment had 3 replicated samples, which gave a grand total of 24 samples in the whole experiment.
Each sample of 50 seeds was tested in Petri dish on top of 2 circles of Whatman Grade 181 Seed Testing Paper.

The paper was moistened either by 4.5 ml of purified water or 4.5 ml of 0.01 M potassium nitrate solution.
The Petri dishes with seeds were either kept in clear polythene bags and exposed to light during test, or wrapped in aluminium foil for dark treatment.

Two incubators were used, one set at 20 ˚C constant and the other at alternating between 10 ˚C for 12 hours and 20 ˚C for 12 hours in a day.
After 14 days, the number of germination for each treatment was counted.

ANOVA (analysis of variance) statistical analysis was carried out on germination data by using a statistical program, MINITAB Release 14 and the model of analysis was set to General Linear Model.
As ANOVA analysis is base on the following two assumptions: the observations of any treatments are normally distributed; the variance of this distribution is the same for all other different treatments (i.e. 1 = 2= 3...

= n).
However, percentage data may not satisfy these two assumptions if there is a wide range of values (lower that 20% or higher that 80%).

To correct this problem, angular transformation (angular transform equation and raw experiment data are in Appendix 1) was carried out before ANOVA analysis.
All the data present here after are angular data except otherwise stated differently.

RESULTS
Seeds under 20 ˚C constant temperature, in water and darkness had the lowest germination, whereas seeds under alternating 10 ˚C / 20 ˚C temperature, light and potassium nitrate solution had the highest germination with more than 5 times increases in term of mean percentage than the former treatment (Table 2).

By simple comparisons of the mean germination angles between dark and light, between water and nitrate and between constant and alternating temperatures, they seem to suggest all these factors contribute to the increase in germination.
However, the results from ANOVA analysis (Table 3) paint a different picture.

The only two factors with significant effects on germination are medium and temperature (both have p-values < 5%), but not illumination or light (p-value = 26.7%), even thought there were some increases in the mean angles (Table 2 and Table 4(a)).
The t-test results (Table 5) also support this assertion (detail of t-test calculation is in Appendix 3).

p-values of interactions (Table 3) also indicate that there were no significant interactions between any factors.
This is confirmed by the interaction plots in Figure 1 (the lines are almost parallel to each other in all the interaction plots) and the results of t-test on two factors interaction (Table 6).

As there were no significant interactions, the increase in germination in combination of different treatments was due mainly to the additive effects of different factors.
CONCLUSIONS

A 2 3 factorial experiment on the effect of three different factors, alternating temperatures, light and nutrient on C. album seeds overcoming dormancy had been carried out.
Results of ANOVA analysis on seeds germination data show that alternating of temperatures and nitrate availability are the significant factors.

Although there were some increases in seeds germination under light, but these increases were not significant.
Moreover, the results of the experiment also suggest that there were no significant interactions between light, alternating temperatures and nitrate.

The increase in seeds germination when two or more treatments were combined was due to the additive effects.
INTRODUCTION

The rate and uniformity of germination of non-dormant seeds can be enhanced by "seed priming" before being sown.
This pre-sowing treatment is carried out by imbibing seeds in sufficient water so that many metabolic processes could occur, but insufficient water for full germination.

By putting seeds in this high moisture content environment for a suitable period, all the viable seeds will be in the verge of germination.
When these seeds are finally sown, both the rate and uniformity of germination could be improved.

In this experiment, three batches of onion, Allium cepa L. seeds were primed in 3 different osmotic potentials and their germination performances were compared with that of a batch of un-primed seeds control.
Evaluation was made about the effect of priming in term of changes in viability, rate of germination and uniformity of germination.

MATERIALS AND METHODS
Four batches of 50 onion seeds were used.

Priming treatments were carried out for the first 3 batches of seeds and the forth batch was the untreated control.
The level of moisture content in the seeds during priming was controlled by placing seeds in contact with solutions of know osmotic potential.

Polyethylene glycol (PEG) was used as osmoticum because it is an inert organic substance of high molecular weight (6000-7000) it would not be absorbed by the seeds or has any toxic effect on them.
Three concentrations were used: 299, 341 and 379 g PEG per kg H 2O which are respectively estimated to produce osmotic potentials of - 1.14 MPa, - 1.46 MPa and - 1.79 MPa ( - 1.79 MPa is permanent wilting point for onion) at 20 ˚C (Michel, 1983).

Each batch of 50 seeds for priming was treated in a Petri dish on top of 2 circles of Whatman Grade 181 Seed Testing Paper.
The paper was moistened by 5 ml of different osmotic potentials PEG solutions.

As the seeds were placed on top of the seed testing paper, adequate aeration was ensured.
The Petri dishes was then placed in a polythene bag and incubated at 20 ˚C for one week.

After one week of priming treatment, the three batches of seeds were rinsed in purified water and dry superficially on absorbent paper before being sprinkled on to another Petri dishes with moistened seeds testing paper for germination tests.
The seeds testing paper in these Petri dishes was moistened by 5 ml of purified water.

The forth batch of un-primed seeds was also set up for germination test.
The four Petri dishes were placed in a polythene bag and incubated at 20 ˚C for germination.

Germination counts were carried out at least twice a day for the first 4 days of the germination test and once a day after.
Seedlings with 2 mm or longer radicle were counted as germinated or viable and removed from the Petri at each count.

The effect of priming was evaluated by comparing the values of mean germination period, mean rate of germination and variance of germination periods between different priming treatments and the control seeds.
The germination count raw data and details of calculation of mean germination period, mean rate of germination and variance are in the Appendix.

RESULTS
Most primed seeds from different osmotic potentials germinated at least 1 day earlier than the un-primed seeds control (Figure 1).

However, in this experiment, about 4% of the - 1.14 MPa primed seeds had already germinated during the priming treatment.
There was only a slight decrease, at most 8%, in viability when compare primed seeds with un-primed seeds (Figure 2) and this seems to suggest priming probably did not damage most of the onion seeds.

There were considerable increases in the mean rate of germination in primed seeds and the results in Figure 3 show there seem to be some positive proportional relationship between priming osmotic potential and mean rate of germination.
If mean rate of germination is the indication of vigour in the seeds, the results of this experiment seems to suggest priming did invigorate onion seeds.

Also, the variance of germination periods (Figure 4) suggests priming would make germination more uniform, although there was no improvement from - 1.46 MPa to - 1.14 MPa in this experiment.
DISCUSSION

As there was only one batch of seeds (one sample) for each treatment and control in this experiment, no statistical analysis can be carried out and therefore there was no statistical evidence to support any assertion from this experiment.
More replicated samples were needed for this experiment if the beneficial effects in priming were to be proved.

The slight decrease in viability in the primed seeds was mainly due to fungi attack during priming treatments.
If the priming processes were to carry out in sterilised environments, there might be an even smaller decrease or no decrease in viability when compare with the un-primed seeds.

As seedlings with 2 mm or longer radicle were counted as viable, and germination tests on them were not continued, there was no way of telling whether they were normal seedling or abnormal seedlings.
Although priming did not seem to have detrimental effect on onion seeds up to the stage of redicle emergent, this experiment failed to test whether priming had any damaging effects on later stages of development.

In this experiment tests were only carried out in small amount of seeds, PEG might not be the most suitable osmoticum to use when large number of seeds were to be primed in an agri-industrial scale, as oxygen might not be able to diffuse easily through thick layer of PEG solution.
An alternative osmoticum or priming method is needed for large scale of priming treatment.

As around 4% of seeds had already germinated during priming treatment in - 1.14 MPa osmotic potential and there were no obvious improvement in uniformity in germination comparing with - 1.46 MPa (Figure 4), the optimum priming osmotic potential may be some where between - 1.14 and - 1.46 MPa.
When seeds are wet after priming, they are difficult to handle for sowing in the field, and for practical propose, seeds might need to be dried after priming treatment before being sown.

In this experiment the effects of priming followed by drying were not tested.
Another experiment combines the effect of priming and drying might be needed.

INTRODUCTION
Seed quality includes both vigour and viability of seeds.

During seeds storage, seed quality deteriorates and the loss in both viability and vigour demonstrates this deterioration in seed quality.
In this experiment the effect of two main factors, temperature and seeds moisture content during storage, on the loss in vigour and viability in pea (Pisum sativum L.) seeds was examined.

This was carried out by storing the pea seeds at two temperatures and two moisture contents and testing seed viability and vigour after various storage periods.
Viability was measured by germinating the seeds and counting the number of normal and abnormal seedlings (ISTA, 1979).

Vigour was estimated as the electrical conductivity of water in which seeds had been soaked for 24 hours (ISTA, 1995).
MATERIALS AND METHODS

The pea seeds at 13.5 and 15.7% moisture contents were divided into various lots and then pre-packed into laminated aluminium foil packets to maintain the seed moisture content during storage.
These packets were then stored for various periods at different temperatures (Table 1).

After storage treatment, packets were stored at 3 ˚C to prevent further deterioration before germination and vigour tests.
Germination Test

3 replicate samples of 40 seeds for each of the 11 storage treatments were used.
Each sample of 40 seeds were placed between moistened rolled non-stained and non-bleached paper towels and the rolled towels were placed in polythene bags before put in 20 ˚C for germination.

After one week germinated at 20 ˚C, because of the time constraint, samples were remoistened and store in cold room for another week before being counted for the number of normal seedlings, abnormal seedlings and dead seeds.
Electrical Conductivity Vigour Test

One sample of 50 seeds for each of the 11 storage treatments was used.
50 seeds sample was weighted and put in a beaker with 225 ml deionised water for 24 hours at 20 ˚C.

Two beakers of deionised water without seed also kept at 20 ˚C for 24 hours and these were used as control for the electrical conductivity measurement.
Seeds were removed from the water before electrical conductivity measurement.

Electrical conductivity per gram of seed was calculated using the following equation.
FORMULA ec is electrical conductivity.

RESULTS
Loss of Viability

Raw germination data are in Appendix 1.
The mean percentages of normal seedlings from 3 replicated samples were used for the survival curves in Figure 1.

The 15.7 % moisture content survival curve deteriorated faster than the 13.5 %, even though both samples were stored at same temperature, 50 ˚C.
Both survival curves are negative normal cumulative distribution functions and this shows pea seeds have orthodox seed storage characteristics.

The negative normal cumulative distribution survival curves can be modelled by straight lines when percentage is transformed into probit percentage (Graph 1) and this linear relationship can be described by Equation 1, where v are probit percentage viability, p is storage period,  is the standard deviation of the frequency distribution of seed deaths in time and Ki is a constant for the seed lot.
Notice that both survival curves of different moisture contents intercepted the Y - axis at the same point, this is because the seeds were from the same seed lot and had the same initial viability, Ki.

FORMULA However, according to Ellis & Roberts (1981), the viability v could be calculated by Equation 2.
Where t is temperature in ˚C, m is moisture content in percentage of fresh weight and Ki, KE, CW, CH and CQ are constants.

FORMULA And for a limited range of temperatures, a simpler Equation 3 can be used (Ellis, 1988).
FORMULA By comparing Equation 1 with Equation 3,  can be expressed in term of t and m as in Equation 4.

FORMULA
Sensitivity of Longevity to Changes in Moisture Content

From Graph 1 viable seeds survival curves (broken lines), the standard deviation of the survival period,  for 15.7 and 13.5 % moisture contents were 2.9 and 6.2 days.
As Equation 4 shows, there is a linear relationship between the logarithmic value of  and the logarithmic value of moisture content, m (Table 2 and Figure 2).

The slope of the line in Figure 2 gives the value of CW, which is 5.0 for viable seeds.
This in comparison with data obtained from normal seedlings survival curve (solid lines) in Graph 1,  were 2.8 and 5.4 days for 15.7 and 13.5 % moisture contents and this lead to CW = 4.3 and the two CW values are similar.

Sensitivity of Longevity to Changes in Temperature
When storage temperature was reduced from 50 ˚C to 40 ˚C, the standard deviation of the survival period,  increased (increase in longevity).

Only one storage treatment was 40 ˚C in this experiment, and even after 8 days of storage, the germination was still 100% (Appendix 1), so viable seeds survival curves could not be used to estimate temperature constant, C2 in Equation 4.
Table 3 data was obtained by using the normal seedlings survival curves (solid lines) in Graph 1.

Equation 4 models the relationship between log 10 and temperature t as linear (see Discussion section), estimation of C2 could be made by using two points calculation of the slope, which gave C2 = 0.0646.
Value of Constant K E

With approximate values of CW and C2, constant KE in Equation 4 could be estimated.
As normal seedlings survival curves were used to calculate the value of temperature constant, C2, 4.3 is chosen for CW (estimated from normal seedlings survival curves).

By substituting  = 2.8 days, m = 15.7 % and t = 50 ˚C into Equation 4, KE is estimated to be 8.82.
Loss of Vigour

The increase in electrical conductivity in seed steeped water was due to seeds exudation and this is a good indicator of the integrity of the seed coats.
If the seed coats of most seeds were intact, the electrical conductivity of the water would be low and the seeds would show high vigour.

Conversely, if the seed coats of most seeds were damaged, the electrical conductivity of the water would be high and the seeds would show low vigour.
Figure 3 shows electrical conductivity increase over storage period (raw electrical conductivity data in Appendix 2).

Storage temperature and seeds moisture content had an effect on the rate of conductivity increase.
With high storage temperature and high seeds moisture content gave higher rate of increase in electrical conductivity in time.

Vigour tests, such as electrical conductivity test, often correlate well with other aspects of seed quality.
Graph 2 shows there is a negative correlation between seed steeped water electrical conductivity and probit percentage viability and the correlation coefficient is - 0.8875.

Data details and calculation of the correlation coefficient are in Appendix 3.
DISCUSSION

Although all the constants, KE, CW, C2, in Equation 4 were tentatively estimated and in theory, Equation 4 could be used to predict pea seeds longevity in any temperatures and moisture contents, however, caution must be taken.
CW estimation was base on only 2 moisture contents, 15.7 and 13.5 % and a two points interpolation can only give an approximate estimation.

Also, no data were available outside these moisture contents in this experiment and extrapolation of CW could easily lead to erroneous results.
In actual fact, Ellis et al.

(1989) demonstrated that as moisture content lower, there would come to a limit to the logarithm relationship between seed moisture and longevity.
For pea seeds, Ellis et al.

(1989) suggested that this critical moisture content is around 6.2 %, and lowering of moisture content below this critical point will not improve longevity.
Conversely, as moisture content increases, there will be a point where free water become available inside the seeds and Equation 4 longevity model no longer hold.

The temperature term in Equation 4 is a simplification from the original term in Equation 2, which shows that relationship between logarithmic of longevity and temperature is not linear.
Equation 4 is only an approximation on Equation 2 over a small range in temperatures.

Similar to moisture content, any extrapolation of longevity from 50 ˚C or 40 ˚C to sub-zero temperature using Equation 4 is very likely to give erroneous results.
Moreover, C2 in Equation 4 was tentatively estimated by using just one single result at 40 ˚C, and this value is subject to substantial error.

Graph 2 shows there is a negative correlation between seed steeped water electrical conductivity (vigour) and probit percentage viability.
However, this relationship would not be obvious if linear percentage scale was used to plot Graph 2 and this is even more if viability is very high.

Small changes in viability are subjected to sampling error and difficult to detect.
Even when detected, the very small drop in viability would seem rather insignificant when compare with the increases in electrical conductivity or decrease in vigour, and this would make vigour and viability appear to be independent.

This also shows the important of using either electrical conductive vigour test or seed storage aging germination test when trying to estimate the quality of a seed lot.
If only a single germination test was used to estimate the seed quality without aging, an apparent non-significant drop in germination would have a significant effect on the seeds longevity.

Graph 1 shows the viability and normal seedlings curves with the same storage temperature and seeds moisture contents are almost parallel and this implies every individual seed would go through similar period of time between at the point of losing the ability to produce normal seedlings and death, no matter when this process take place (this also implies the 2 curves would give similar CW).
In the case of storage temperature at 50 ˚C with 15.5 % moisture content, this period is about 2.8 days and 5.4 days for 13.5 % moisture content.

This shows the length of this period is affected by the seeds storage environment, harsher environment would give shorter period and gentler environment would give longer period.
However, the height difference of the two curves would always be the same, which is the difference of seed lot constant, Ki between viable seeds and normal seedlings survival curves.

As the high differences between the two curves in probit percentage scale are the same through out storage period, this implies number of seeds, which produce abnormal seedlings, will be highest when viability is near 50%.
Also, the storage conditions would not affect the percentage of abnormal seedling at a particular viability when the same seed lot go through deterioration in storage (i.e. two lots of seed originally taken from the same seed lot and stored in different temperatures and moisture contents, as long as the final viability is similar, they would have similar percentage of abnormal seedlings in germination tests).

There should be more replicated samples in the germination test.
As 3 replicates of 40 seeds were a relatively small sample size when one try to observe small changes in viability often less that 1%.

As probit percentage of viability data from the shorter storage periods (data near the Y - axis) shown, they were subjected to a lot of noise.
The germination test would also be more effective, if there were more samples taken around the periods when viability was near 50%, as this would improve the accuracy for the estimation of standard deviation of the frequency distribution of seed deaths, .

Before any tests were carried out on the seeds, it might be beneficial to humidify the seeds first, as this might avoid the possibility of imbibition damage during rehydration.
Germination test were carried out in relatively dark environment and the seedlings showed sign of etiolation and this made checking of normal germination more difficult, as no normal chlorophyll pigment developed in the seedlings.

Despite of all the shortcomings in this experiment, the estimated value of CW, 4.3 to 5.0 is similar when compares with other published value, 5.39 suggested by Ellis et al.
(1989).

This probably shows the robustness of the underlying philosophy behind the testing method and theory about seeds longevity.
Question 1 Forest Growth Rates

A forest has an extinction coefficient for solar radiation ( k ) of 0.5.
Calculate the fraction of incoming solar radiation intercepted by the forest when the leaf area index ( L ) is 3.0 and when it is 6.0.

FORMULA
Assuming that the radiation use efficiency (for intercepted solar radiation) of the forest is 2.0 g MJ-1, calculate the daily growth rates of the forest (g m-2 day-1) for both of the above values of leaf area index when the incoming solar radiation is either 15, 20 and

25 MJ m-2 day-1.
FORMULA

Repeat the above calculations with k reduced by 20% and with RUE also reduced by 20%.
You should calculate results for each combination of the parameters to give eight daily growth rates at each level of incoming solar radiation.

By substituting all the different values of L, k, RUE and Rs into Equation 1, we have the following table of daily growth rates, rG:
Discuss the relative sensitivity of the changes in growth rate to the changes in k and RUE, and how this depends on L and incoming solar radiation.

Let FORMULA be the relative sensitivity of the changes in growth rate, rG to the changes in k. FORMULA can be considered as the ratio of the changes in rG to the changes in k, when changes in k is small: FORMULA By substituting Equation 1 for rG: FORMULA When FORMULA, in order to find the limit, we apply l'Hospital's Rule to Equation 2: l'Hospital's Rule: FORMULA  FORMULA This implies when L is small (very few leaf cover in the forest, i.e. at the beginning of growing season in a deciduous forest), the effect of changes in k on growth rate is comparable to the changes in k, but it becomes less significant when the forest become thicker and more leaves over lapping each other.
The relationship between FORMULA and L are not linear (Figure 1).

Also, incoming solar radiation, RS, has no effect on FORMULA FORMULA As Equation 2 and Figure 1 model is for very small changes in k, 20% reduction in k is a rather large change.
We can model this approximately by using the mean value of FORMULA when k is 0.5 and 0.4.

From Equation 2, when L = 3.0, FORMULA Average of FORMULA and change in growth rate is: FORMULA From Table 1 data, when FORMULA change in growth rate = FORMULA Likewise, when L = 6.0, from Equation 2 the average of FORMULA is 0.1983 and this gives change in growth rate = - 0.0397, which is similar to the calculated result of - 0.0431 from Table 1 data.
And these show predication from Equation 2 agrees with Table 1 data.

Similarly, let FORMULA be the relative sensitivity of the changes in growth rate to the changes in RUE: FORMULA By substituting Equation 1 for rG: FORMULA  FORMULA This model shows growth rate, rG will follow the same changes in RUE.
If RUE is decrease by 20%, growth rate will decrease by the same amount and this can be verify by data in Table 1.

For example, from Table 1 data, when L = 3.0, k = 0.4, RUE = 2.0 and RUE = 1.6 and RS = 25: change in growth rate = FORMULA However, in an actual forest, RUE varies with changes in incoming solar radiation, RS.
RUE is bigger if RS is small.

FORMULA only holds, if RS remain the same.
Question 2 Energy Balance of a Sheltered and Exposed Crops

a) what effects shelter has on the energy balance of the crop
Shelter reduces wind speed and this lead to increase of aerodynamic resistance, ra and decreases both the rate of evapotranspiration, E and heat energy transfer from air to crop surface.

As less energy is taken up by evapotranspiration in sheltered crops, this some time could reverse the direction of heat energy transfer between air and crop surface.
This reversal of energy transfer direction effect can be seen on data from 15 July am, energy was taken from warm dry air for evapotranspiration in exposed crops whereas in sheltered crops, a faction of solar net radiation was transferred from crop surface to the air and very often this lead to higher air temperature in sheltered crops.

Data on 14 July pm for sheltered crops were wrong, as they cannot be balanced in the energy balance equation: FORMULA (G can be ignored because of it was always less than 10 Wm -2.) FORMULA As data from the opened crops can be balanced by the energy balance equation, Rn = 502 Wm -2 was very likely to be correct.
As the value of Rn was fairly high, 14 July pm could be a sunny period.

From the data of other time and date, it seems that on sunny day, is usually around 30% less for sheltered crops than exposed crops and this seems to suggest  = 628 Wm -2 was probably correct.
If value of  was 628 Wm -2, C should be around 126 Wm -2.

b) any differences in energy balances between different days
From the values of Rn, there seems to be sunny days (14 and 15 July) and overcast day (17 July).

Values of E were higher in sunny days than in overcast day, also shelter had a bigger effect of reducing E in sunny day than in overcast day.
In sunny day the air tended to be warmer and dryer, and substantial amount of energy from the air was transferred to the crops and taken up by E.

In overcast day, much smaller amount of energy was transferred from air to crops and in the morning, energy was actually transferred from crops to air.
c) whether the partitioning of energy differs between morning and afternoon

On the whole, E was higher in the afternoon than in the morning, this was due to higher air temperature in the afternoon and more sensible heat was transferred from air to crops (values of C were more negative in the afternoon) and led to higher E.
For each day calculate the water use of the crops totalled over the eight hour day.

Express your answers in mm of water.
Evapotranspiration energy of 1 mm of water: volume of water in 1 mm depth over 1m 2 area = (0.1 x 100 2) cm 3 = 1000 cm 3 As density of water = 1 g/cm 3, this gives: weight of water in 1 mm depth over 1 m 2 area = 1000 g Latent heat of water,  = 2450 J/g, this gives: energy to evaporate 1 mm depth of water over 1 m 2 area = (1000 x 2450) J = 2.45 MJ As E was mean values over period of 4 hours, value of conversion factor, c, which converses E in Wm -2 to cE mm of water is: FORMULA By multiplying c with values of E gives the following results:

INTRODUCTION
Both Macrochloa tenacissima and Lygeum spartum are widespread perennial tussock grass in semi-arid southern Spain.

Although they appear in the same geographical region, the two species generally grow in different sites.
L. spartum is thought to be more salt tolerance than M. tenacissima (Freitag 1971) and they appear to grow in different ecological niches and soil salinity is the putative factor that determines which species is more dominance in a particular site.

This project is to investigate whether soil salinity is a determining factor for both M. tenacissima and L. spartum population size and area of coverage at a specific location, and which species, M. tenacissima or L. spartum are better adapted to a high level of salt in soil.
FIELD SITE AND METHODS

A site was chosen in a valley near La Joya, just over 1 km south of Las Negras (see the attached map).
The site is relatively flat that makes access and survey easier and it has a mix population of M. tenacissima and L. spartum.

The soil is calcareous alkaline and the altitude of the site is some where between 50 to 100 m above sea level and about 100 m from the coast.
Survey was carried out on twenty (5 rows by 4 columns) 5x5 square-meter quadrats.

The orientation of the quadrats is shown in Figure 1 and micro-relief of the area was estimated by eyes (Figure 2).
For each quadrat, the number of individual M. tenacissima and L. spartum were counted, height of some of the plants were measured and percentage of area coverage of both species as well as bare soil area were estimated.

Two soil samples were taken in two opposite corners for each quadrat.
As some M. tenacissima and L. spartum plants were not in flower during the survey, this made the job of identification more difficult and guideline in Table 1 were drawn up to aid identification.

However, there were a few plants that were too small to identify and they were ignored in the survey.
In order to obtain the salinity data of the soil samples, we need to have information about soil moisture content of the samples.

To measure soil moisture content, samples of around 50 g of weighted moist soil were put in the oven at 60 ˚C over night (approximately 12 hours) and the dry soil samples were weighted again.
The different in weight between the moist soil sample and dry soil sample is the water content in the soil.

For the soil salt content, 10.0 g of moist soil samples were mixed with 50 ml of de-ionized water to dissolve the salt from the soil sample into the water, a Primo conductivity meter was used to measure the soil steeped water conductivity.
Detail of procedure of salinity calculation is in Appendix 2.

RESUTLS
Quadrat Salinity

The variation of salinity between quadrats appears to be small, however, in quadrat C4, D2 and D3, the two soil samples within the same quadrat seem to have substantially different salt levels and this lead to rather big standard errors of salinity in these quadrats (Graph 1 and Table A2.2 in Appendix 2).
Correlation Between M. tenacissima and L. spartum Population and Soil Salinity There seems to be some correlation between soil salinity and the number of M. tenacissima and L. spartum within a quadrat (Graph 2).

The general trend from Graph 2 seems to suggest that L. spartum is the dominance species in high salinity site whereas M. tenacissima favours less salty area.
However, the result is slightly different if we plot percentage of plant area coverage against soil salinity (Graph 3).

L. spartum roughly remains the same (not going up) percentage of coverage from low to high salinity sites, but M. tenacissima coverage area decrease as salt level increases and this decrease in coverage area correlate to the decrease number of M. tenacissima.
The effect of high salinity can also be seen on other vegetation, as the percentage of bare soil increase with the increase level of salt in soil (Graph 4).

Two-sample t-test
Each quadrat is categorized as either high salt level if the mean salinity level is above 0.25 g / kg of dry soil, or low salt level if it is below 0.2 g / kg of dry soil or undeterminable if the salt level is between 0.2 to 0.25 g / kg of dry soil (Table 2).

Data from the quadrats with undeterminable salt level are ignored in the subsequence two-sample t-tests.
Two-sample t-tests were carried out using statistical program Minitab to compare within a species, either M. tenacissima or L. spartum, if there are any significant different between the means of plant number in high and low salinity quadrats.

The same t-tests also apply for percentage area coverage of plant in high and low salt levels of quadrats.
Two-sample t-test results show only M. tenacissima has sigificant different in the mean value of plant number and percentage of area coverage between high salinity site and low salinity site (Table 3).

This seems to suggest salt level only affect M. tenacissima but not L. spartum and this seems to be in contradiction with the result in Graph 2 (more L. spartum in high salinity site).
DISCUSSION

The salt measure method in this experiment is only approximate as other ions would be present in the soil, (e.g. potassium, nitrate, phosphate and sulfate ions) and they could affect the overall conductivity in the soil steeped water and give a higher reading in the conductivity meter than it would be if sodium and chloride were the only ions in the water.
The calculated salinity values could be over estimated.

Also, as there seems to be little variation in salinity between quadrats (Graph 1), but substantial variation in salt level between soil samples within some quadrats (e.g. C4, D2 and D3), it would be better if more soil samples were taken from each quadrats as getting accurate salinity data is very important for this project for all the results and interpretation of them are based on the salinity values, however, due to constraint in resources (time and equipment), this was not possible.
It also might be better if more than one site was surveyed, as this might lead to bigger variation in soil salinity and more drastic changes in plants population and coverage.

When there were no flowers or florescence on the plants, it was very difficult to distinguish between M. tenacissima and L. spartum and misidentification could very easily be made.
Plants that were too small to identify were ignored.

All these could introduce error in the survey data.
Also, percentage of coverage was estimated by eyes and this could be very subjective.

In order to be more consistent, coverage estimation was done by the same person through out the project.
Even counting number of plants could run into problem, sometime it was difficult to tell how many plants were there in a large clump, where did an individual plant start and end?

There appears to be no correlation between plant number and percentage of area coverage for L. spartum (see Graph 2 and 3, plant number goes up in high salinity but percentage of coverage goes down).
This could be due to the size of L. spartum become smaller in high salt level site.

Which data, plant number or percentage of area coverage, to use as an indication of population changes can be difficult to decide.
As in this case, they can give a very different picture.

t-test results show there appears to be no significant different in number of plant or area of coverage between high salinity site and low salinity site for L. spartum, despite there seem to be an increase in number of plant in high salt level area (Graph 2).
This apparent contradiction could be due to high variation in the number of L. spartum plants between quadrats and this gives a bigger spread in the standard error and lead to higher P-values in the t-test.

However, caution must be taken for all the t-test results, as the categorized of quadrats into high or low salt levels is based on salinity data of each quadrat and we saw that these salinity values are subjected to a lot of possible error (see Graph 1).
Moisture content in the soil could be another factor that determine which species is more dominant in a particular area.

Number of L. spartum seems to increase with moisture content in soil (Graph 5), however, this increase in population does not seem to reflect in the percentage of area coverage (Graph 6).
To conclude, M. tenacissima seem to be less salt tolerance and their population (in term of plant number or area coverage) seem to be affected more by salt level in soil than L. spartum. Salt could be the determining factor for M. tenacissima distribution in a site.

However, salt level seems to have little effect on L. spartum and other soil factors may be the determining factor for their distribution.
Abstract

Impatiens noli-tangere produces both chasmogamous and cleistogamous flowers and the genetic mechanism that controls the dimorphism in floral development is not known.
As genomic DNA methylation is one way to encode epigenetic information that control development.

There may be some connection between floral dimorphism in I. noli-tangere and DNA methylation.
Methylation-sensitive restriction enzyme digestion coupled with inter-simple sequence repeat (ISSR) PCR as well as a novel method that coupled enzymes digestion with real-time PCR were used trying to detect any methylation polymorphism between immature pods from chasmogamous and cleistogamous flowers.

However, as I. noli-tangere is not a modelled plant, very few sequences are available for primers design for real-time PCR.
This as well other constrain on resources, no gene or locus was located that showed to have methylation-sensitive polymorphism between chasmogamous and cleistogamous samples.

Key words
chasmogamous, cleistogamous, DNA, epigenetic, Impatiens noli-tangere, ISSR, methylation, methylation-sensitive, real-time PCR.

1 Introduction
1.1 Impatiens noli-tangere flower dimorphism, and epigenetics

Impatiens noli-tangere L. (Balsaminaceae) is a native annual herbaceous plant that grows mostly in shady damp places in woodlands, and it is probably only native to the Lake District and Wales in the UK (Hatcher, 2003).
The plant produces both chasmogamous (CH) flowers (Figure 1.1) that attract pollinators, and closed cleistogamous (CL) flowers (Figure 1.2) that are self-fertilized, and it is known that in open and sunny sites more CH flowers will be produced whereas in shady conditions, it has mostly CL flowers (Hatcher 2003).

The genetic mechanism that controls the dimorphism in floral development of I. noli-tangere is not known.
In the past decade epigenetics has rapidly advanced and it addresses some of the genetic phenomenon that cannot be explained by Mendelian genetics as well as elucidates the mechanism of gene regulation in plant development.

Grant-Downton and Dickinson (2005 and 2006) have written a substantial review about epigenetics in plant biology and they asserted that DNA (cytosine base) methylation is one of the three ways to encode epigenetic information, beside chromatin (histone proteins and their post-translational modifications) and RNA.
DNA methylation in plants, as in mammals, has two functions, the first is in defence against invading DNA and transposable elements (Miura et al.

2001) and the second is in gene regulation as DNA methylation can inhibit binding of regulatory proteins, and methylation of promoter and coding sequence of genes can repress transcription (Finnegan et al.
1998).

In plants, methylated cytosine occurs both in symmetrical sequences, e.g. CpG (5'CG3') and CpNpG (5'CNG3'), and in asymmetrical sequences, e.g. CpApTp (5'CAT3') and CpTpT (5'CTT3') (Staiger et al.
1989).

The relative importance of symmetric and asymmetric methylation in regulating gene expression is unknown, but methylation at symmetric sequences can be transmitted through cycles of DNA replication (Bird 1978b).
There are many evidences to suggest there is a correlation relationship between gene silencing and DNA methylation (Jeddeloh et al.

1998, Ye and Signer 1996, Paszkowski and Whitham 2001, Jacobsen and Meyerowitz 1997, Staiger et al.
1989, Grant-Downton and Dickinson 2005 and Finnegan and McElroy 1994).

Richards (1997) suggested that up to 20-30% of the cytosines are methylated in the nuclear genome of many flowering plants.
However, the methylation patterns, as well as the methylation levels are not static as the methylation level of DNA from young seedlings are approximately 20% lower than in mature leaves of both tomato and Arabidopsis and changes in DNA methylation can be correlated with changes in gene expression in a tissue-specific or developmentally regulated manner (Finnegan et al.

1998).
Arabidopsis data indicate that DNA methylation level increases throughout development from the seedling stage to reproduction (Ruiz-Garcia et al.

2005) and loss of DNA methylation affects plant development (Finnegan et al.
1996) as methylation play an important role in plant development (Finnegan et al.

2000).
Thus, it seems logical to make the assumption that there may be some connection between DNA methylation and floral dimorphism development in I. noli-tangere.

There are two objectives in this project.
The first is to develop a novel methodology to detect methylation polymorphism in genomic DNA.

The second is to test the hypothesis that there is correlation between genomic DNA methylation and floral dimorphism development in I. noli-tangere, but before we move on, it would be useful to look at some general principles in methylation detection first.
1.2 Detection of methylation-sensitive polymorphism

One of the most used methods to detect methylation-sensitive polymorphism in genomic DNA is to use isoschizomers with different methylation-sensitivities (Bird 1978a and Cubas et al.
1999).

Isoschizomers are restriction enzymes with an identical restriction site.
If the digestion activity of a restriction enzyme can be blocked or impeded by methylation at the restriction site, this restriction enzyme is said to be methylation-sensitive.

Whilst one restriction enzyme is methylation-sensitive, an isoschizomer of that enzyme is not and these differences in methylation sensitivities can be used to detect methylation-sensitive polymorphisms in DNA templates, which have identical sequence but different methylation patterns.
The detection process normally involves restriction enzyme digestion of the sample DNA followed by some kind of PCR amplification of loci that contain the restriction site to determine the digestion result (Rein et al.

1998 and Singer-Sam et al.
1990) and this is summarised in a schematic diagram (Figure 1.3).

However, not all methylation-sensitive polymorphism detection methods are based on using isoschizomers, as we will see in the next section.
1.3 A survey of existing methods for detecting methylation polymorphism

The following list of methods contains the more common ones being used and is by no mean an exhaustive list.
Chromatographic estimation of global DNA methylation rates (Chakrabarty et al.

2003):
Enzymatic hydrolysis is first carried out on genomic DNA by using nuclease and phosphatase and the resulting nucleosides are subjected to high-performance liquid chromatography (HPLC) separation.

The amount of 5-methyl-deoxycytidine (5mdC) in the effluent can be detected and this indicates the extent of methylation in the entire genomic DNA.
Chakrabarty et al.

(2003) used this method on Siberian ginseng (Eleuterococcus senticosus) tissue cultures and tested the different levels of methylation in two different cell lines.
However, this method cannot provide any information about the location in the genome of methylated nucleotides and it may only work if there are substantial differences in the methylation level between two genomic samples.

Methylation-sensitive restriction fragment length polymorphism (RFLP) (Jaligot et al.
2002):

This method combines methylation-sensitive restriction enzyme digestion on genomic DNA and Southern blot analyses.
Genomic DNA is first digested by methylation-sensitive restriction enzyme and the resulting fragments are separated by electrophoresis on an agarose gel.

The fragments are then transferred on to a membrane and hybridised with a radioactive labelled probe.
Photographic films are used to detect the radiation signal from the hybridised probe.

Jaligot et al.
(2002) used this method to study oil palm.

It is a complicated and laborious process and also there is a need for a cDNA library from prior sequence information to create high number of required detection probes, but the amount of information obtained is relatively low.
Moreover, it needs large amount of plant material to extract around 10 g of genomic DNA and this method would not be suitable for small samples such as an immature pod or a small flower bud.

Methylation-dependent fragment seperation (MDFS) by capillary electrophoresis from bisulfite-converted DNA (Boyd et al.
2006):

Bisulfite treatment of genomic DNA will selectively deaminate cytosine (C) and converts cytosine to uracil (U) without significant conversion of 5-methylcytosine (5mC).
Therefore, PCR amplification of bisulfite-treated genomic DNA will amplify 5mC as C and U will amplify as thymine (T).

After bisulfite treatment, a pair of labelled primers is used in PCR to amplify a region of interest in the genomic DNA.
The resulting PCR products are put to capillary electrophoresis to analyze.

As T and C have different migration times, the PCR products from methylated and unmethylated genomic DNA will separate and show up as different peak positions because of the composition difference due to different numbers of C and T in the products.
However, this method may have problems to detect methylation polymorphism in a very short locus with only a methylation polymorphism site that consists of one or two 5mCs, as the differences in migration time between methylated and unmethylated PCR products may not be large enough to be detected.

This method also needs to have specific primers for any particular locus.
Methylation-sensitive amplification polymorphism (MSAP) :

This is probably one of the most widely used methods for detecting methylation polymorphism.
It is very similar to AFLP and one of the restriction enzymes used in double digestion of the genomic DNA is methylation-sensitive.

The basic outline of the procedure is double digestion followed by ligation of the DNA fragment with adapters and PCR pre-selective amplification.
This is then followed by further PCR selective amplification and the amplified products from the last PCR are either analysed on agarose gel or put to capillary electrophoresis to analyze.

Chakrabarty et al.
(2003) and Ruiz-Garcia (2005) used this method on Siberian ginseng and Arabidopsis respectively.

As restriction enzymes recognition sites are normally only four or six base pair long, these sites are fairly common throughout the whole genome and in effect, MSAP screens the entire genome of methylation polymorphism in the recognition site.
Another benefit of using MSAP is that there is no requirement for any specific primers or prior sequence of the study organism.

Nevertheless, this method can only investigate a small proportion of the cytosine in the genome restricted to recognition sites of the enzymes used.
Also, it is a very sensitive method and is subject to noise and variability of the experimental conditions.

Moreover, the location of the methylation polymorphism in the genome cannot be located with this method.
Coupled restriction enzyme digestion and random amplification (CRED-RA) of genomic DNA (Cai et al.

1996):
This is a simple novel method to detect DNA methylation using random amplification coupled with restriction enzyme digestion and is very similar to the ISSR method in Section 2.5.

Prior to amplification the DNA templates were digested with restriction enzymes with different sensitivities to cytosine methylation and after PCR amplification their amplified product were further digested with the same enzymes.
The digested and pre-digested PCR product was then analyzed on agarose gel.

The primers used in this method are 10 base pair long oligonucleotide random amplification polymorphic DNA (RAPD) markers primers.
Cai et al.

(1996) used this method on Citrus.
No prior sequences are required for any specific primers in this method, but it could face similar problem as in the ISSR method (Section 3.1 and 4.1) There will be discussion about some of the above methods in the Discussion section, but before we move on to the Materials and Methods section, just to restate the two objectives in this project: To develop a novel methodology to detect methylation polymorphism in genomic DNA; and to test the hypothesis that there is correlation between genomic DNA methylation and floral dimorphism development in I. noli-tangere.

2 Materials and Methods
2.1 Materials and methods introduction

Section 2.2 gives details of the growing and harvesting of plant material followed by Section 2.3 about DNA extraction of plant samples.
In Section 1.2, a brief outline of procedure was given for the detection of methylation-sensitive polymorphism by using isoschizomers and Section 2.4 gives details about isoschizomers, HpaII and MspI digestion.

To detect the digestion results, we need some kind of PCR amplification of loci, which contain the restriction site of the digestion.
Section 2.5 detailed the attempt of using ISSR-PCR to amplify some of the loci.

However, the results were less than satisfactary, see Section 3.1, and in Section 4.1, possible reasons are given for the poor results as well as problems of using ISSR-PCR to detect methylation-sensitive polymorphism from restriction enzyme digested genomic DNA samples.
A suggestion was made to use a quantifying method to evaluate the extent of enzyme digestion on the genomic DNA, but normal PCRs are ill suited for this.

However, real-time PCR (see Section 2.8) can be use to quantify the initial amount of template, but this technique can only be carried out for the amplification of a single PCR product, or a specific locus.
We also need to find or design specific primers for I. noli-tangere.

These primers not only would give a single PCR product, but the PCR product had to have the restriction site 5'CCGG3' (HpaII and MspI restriction site) in it as well as a robust PCR amplification reaction.
As there are virtually no nuclear genes sequenced for I. noli-tangere, in order to find suitable primers, 54 pairs of Kesanakurti's universal primers were tested, see Section 2.6 and Section 3.2.

Only three pairs of universal primers PCR products have the restriction site, but they only produce very weak PCR products and cannot be used in real-time PCR.
Section 2.7 outlined the process of designing new primers and Section 3.3 gave the results of testing of the new primers.

The last section, Section 2.8 gives the method used for real-time PCR.
2.2 Growing and harvesting of plant material

Impatiens noli-tangere seeds collected from Coniston Water, Cumbria in early September 2005 were sown in John Innes II Potting Compost at the University of Reading in September 2005.
The plants were grown under approximately 50% shade in 9cm pots and re-potted to 15cm pots on 12 June 2006.

Plant material, leaves, flower buds and immature pods were collected from 26 plants between 3 July and 26 August 2006.
A sample could be one of the follwings, a leaf, a flower bud or an immature pod (Table 2.1).

In order to distinguish pods from either chasmogamous (CH) or cleistogamous (CL) flowers, the plants were inspected every few days during their flowering period and a piece of cotton was tied around the pedicel of any CH flower when it was open.
Collected plant material was put inside a 1.5 ml micro-tube and frozen with liquid nitrogen immediately to prevent any deterioration of the plant samples.

The samples were kept at - 80˚ C freezer until DNA extraction was carried out.
2.3 DNA extraction

A simplified sodium dodecyl sulfate (S.D.S.) method (This method was used at Reading University 1998 MSc plant biotechnology course for extraction of DNA from tomato leaves, but it was never published.) was used for DNA extraction.
Plant material inside the micro-tube was frozen in liquid nitrogen, and was ground to a fine powder by using a small plastic grinder while still inside the micro-tube.

230 l of preheated extraction buffer was then added to the micro-tube, and mixed well by a bench top vortex machine and incubated in a 65˚ C water bath for 15 minutes.
Afterward, 70 l of 5M potassium acetate was added and the sample incubated on ice for 20 minutes.

The debris and protein were precipitated by spinning at 13000 rpm in a bench top centrifuge for 10 minutes and the supernatant was transferred to a clean micro-tube.
Further debris and protein were removed by spinning at 13000 rpm for 4 more minutes and the supernatant was transferred to another clean micro-tube.

Then 175 l of ice-cold isopropanol was added and the sample was incubated at - 20˚ C for 30 minutes to precipitate out the DNA.
The pellet was precipitated by spinning at 13000 rpm for 5 minutes and rinsed twice in 100 l 70% ethanol.

The residue of any ethanol was evaporated in a heated spin-vac (Labconco centrivap concentrator).
The pellet was then re-suspended by 10 or 20 l of 1:10 TE buffer (50 mM Tris-HCl pH 8.0 and 10 mM EDTA, di-sodium salt solution).

The resulting DNA sample stock solution was quantified by using a spectrophotometer.
Eighty pods samples were extracted with one half from CL flowers and the other half from CH flowers (Table 2.2).

2.4 Digestion with restriction enzymes HpaII and MspI
The most common methylated base is 5-methylcytosine and very often it is in a dinucleotide, 5'CG3', which also known as a CpG island (Grant-Downton and Dickinson 2005).

Isoschizomers, HpaII and MspI, cleave the restriction site 5'CCGG3' with different methylation sensitivities (Table 2.3) (Korch and Hagblom 1986; McClelland et al., 1994), and they can be used to detect methylation-sensitive polymorphism of CpG islands in DNA samples.
The protocol for HpaII and MspI digestion is given in Table 2.4.

The digestion time was 2 hours in a 37˚ C oven and then put under 65˚ C for 10 minutes to stop the reaction of the enzymes.
2.5 Explorative ISSR-PCR

As Impatiens noli-tangere is not a modelled plant or an important crop, only very few genes have been sequenced.
On the National Center for Biotechnology Information (NCBI 2007a) GenBank web site, there are only 9 sequenced genes of I. noli-tangere and 8 of them are chloroplast genes, and there are virtually no useful gene sequences to design any specific primers for this experiment.

An explorative attempt was made to use primers, which target inter-simple sequence repeats (ISSR), to amplify any loci that may contain the 5'CCGG3' restriction site.
The UBC (University of British Columbia) Primer Set #9 (Microsatellite): 807, 809, 812, 818, 827, 885, 887, 888, 890 (Table 2.5) were used with the protocol in Table 2.6.

PCRs were performed on an Applied Biosystems thermocycler using the following programme: 2 minutes at 94˚ C at the beginning; (30 seconds at 94˚ C, 30 seconds at 55˚ C, 1 minute at 72˚ C) for 35 times followed by a 7 minutes extension period at 72˚ C. Spare DNA samples extracted from leaves were used for this explorative experiment.
Electrophoreses of 1% gel (1g of agarose with 100 ml of TAE buffer and 2 l of 10 mg/ml ethidium bromide), which run at 120v for 30 minutes were conducted as well as micro-capillary fragment analysis were carried out to the products of ISSR-PCRs.

2.6 Screening of Kesanakurti's universal primers
Kesanakurti (2006) designed about ninety pairs of universal primers for different genes and they can be used for PCR on a number of different families of plants.

54 pairs of these universal primers were tested with the DNA extracted from a spare leaf sample of I. noli-tangere.
The protocol of the PCRs universal primers screening test run is listed in Table 2.7 and they were performed on either Applied Biosystems or M J Research thermocyclers using one of the following programmes: Programme 1: 1 minute at 94˚ C at the beginning; (30 seconds at 94˚ C, 30 seconds at T m, 2 minutes at 72˚ C) for 40 times followed by a 7 minutes extension period at 72˚ C. Programme 2: 1 minute at 94˚ C at the beginning; (30 seconds at 94˚ C, 40 seconds at T m, 1 minute at 72˚ C) for 40 times followed by a 7 minutes extension period at 72˚ C.

The annealing temperature, T m was between 40˚ to 57˚ C for different primers (details in Table 3.1).
Electrophoreses of 1% gel (1g of agarose with 100 ml of TAE buffer and 2 l of 10 mg/ml ethidium bromide), which run at 90v for 50 minutes were conducted to check the results of PCRs.

As some of the PCR products were very weak and in order to improve the visibility of the faint bands on the gel of these products, a post-stain of the gel was carried out.
The post-stain procedure was as follows: after electrophoresis, the gel was put in a plastic container with 500 ml of post-stain solution (500 ml of 1xTAE and 100 l of 10 mg/ml ethidium bromide) on an electric rocker for 20 minutes.

The post-stain solution was then poured back into a bottle for future use.
About 400 to 500 ml of RO (single distilled) water was put in the plastic container and put back on the rocker for 15 minutes to rinse off excessive ethidium bromide.

The samples which showed some bands were subjected to further testing for the presence of restriction site 5'CCGG3' in the PCR products.
Isoschizomers, HpaII and MspI, were used with protocol in Table 2.8.

As the digestion reactions were only for the testing of the existence of the restriction site, complete digestion was to be ensured.
The samples were incubated in a 37˚ C oven for at least 10 hours before put under 65˚ C for 10 minutes to stop the reaction of the enzymes.

2.7 New primers design
There were two approaches in the search for primers, which could be used in real-time PCRs.

One approach was to improve on the original primers of loci 39, 58 and UREASE.
The second approach was to use a closely related species of I. noli-tangere, I. balsamina, which had a few of its gene sequenced and could be used to design primers which might work on I. noli-tangere.

In general, if primers contain more complement by base pair to the template, the PCR reaction will be more likely to be successful.
As the universal primers were trying to encompass as many species as possible, they might not be well tuned to I. noli-tangere.

However, if the numbers of species being catered for were reduced, and especially, those remaining species are related to I. noli-tangere, the chance of a successful PCR might improve.
The original universal primers for loci 39, 58 and UREASE have the following nucleotide sequences: FORMULA A web base gene sequence search tool, Basic Local Alignment Search Tool (BLAST) from National Center for Biotechnology Information (NCBI 2007b), was used to carry out searches in the GenBank (NCBI 2007c) for genes that would match the sequence of the above primers.

The gene sequences (Table 2.9) were only selected if they were 'related' to Impatiens.
The taxa relatedness in here was referring to the families or order level and was based on the work from The Angiosperm Phylogeny Group (2003).

The selected sequences were then aligned to form a 'stack' by using another web-based tool, Clustal, from European Bioinformatics Institute (EBI 2006).
Once the sequences were aligned, a consensus sequence was constructed by using degenerative bases to include all the sequences (see page 21 to 23).

Well-preserved regions (regions with the least occurrence of degenerative bases) of the consensus sequence were marked for the design of new primers by a web based primer design program, Primer3 (Whitehead Institute for Biological Research 2006).
Primer3 takes the consensus sequence, with parameters which specify approximate regions for forward and reverse primers and primer length, and it searches for the best possible primers with regards to melting temperature, avoiding any secondary structure of primer and possible primer-dimers formation.

The following are stacks for primer 39, primer 58 and UREASE.
The '*' character are present if all the sequence have the same base, '>>>>>' shows the position of the forward primer and '<<<<<' show the position of the reverse primer.

Stack for Primer 39 FORMULA Stack for primer 58 FORMULA Stack for UREASE FORMULA The resulting new primers had the following nucleotide sequences: FORMULA There are a few sequenced nuclear genes of I. balsamina in NCBI GenBank.
Two of these genes, AGAMOUS (Locus Identifier: AJ888759) and LFY (Locus Identifier: AJ888755) were chosen for the design of new primers.

Intron regions were selected for PCR amplification as this would be the mostly likely area subjected to gene regulation activity by methylation and each resulting locus would only contain just a single 5'CCGG3' restriction site and this was to ease the subsequence data analysis.
Primer3 was used to design the primers.

The resulting primers for AGAMOUS and LFY genes have the following nucleotide sequences: FORMULA A gradient thermocycler was used to test all the new primers by varying the annealing temperature, T m. None of the new primers produced successful PCR initially.
However, when the original UREASE Forward primer combined with the new UREASE Reverse 2 primer, they produced a good PCR result (see Section 3.3).

The protocol for this particular reaction is in Table 2.7 and the thermocycler was programmed to programme 1 with 49˚ C annealing temperature.
2.8 Real-time PCR

In real-time PCR, amplified product is detected and quantified using a fluorescent reporter and the concentration of PCR product is measured during each PCR cycle.
A real-time PCR reaction uses similar re-agents as a conventional PCR, but in addition contains a fluorescent report.

In this experiment, we used a fluorescent DNA-binding dye, SYBR green.
SYBR green binds to the minor groove of double-stranded DNA and emits light (wavelength around 520 nm) when excited by light of wavelength near 490 nm (McPherson and Møller 2006).

SYBR green does not bind to single-stranded DNA and only emits weak fluorescent in solution, so the level of fluorescent signal is direct proportion to the amount of amplification product in the reaction.
As the level of fluorescence signal is measured during every PCR cycle, it is possible to identify the exact PCR cycle (the threshold cycle) at which the product concentration increases exponentially (Figure 2.1 (a) and Figure 4.2).

The threshold cycle, C T correlates to the initial concentration of the template, and higher the template concentration at the beginning of the reaction, the smaller C T will be.
The relationship between the logarithmic of template concentration and C T is linear and a standard curve (Figure 2.1 (b)) can be used to quantify a sample with unknown concentration of template from its C T value.

To draw the standard curve, a set of standards of known template concentration is used and the corresponding C T values can be used to calibrate the standard curve.
The original UREASE forward and newly design UREASE reverse2 primers were used in the experiment.

The protocol for the reaction is given in Table 2.10 and ran on a Rotor-Gene 6000 machine from Corbett Life Science on a 72-well rotor with the following thermal cycle programme: 10 minutes at 94˚ C at the beginning; (10 seconds at 94˚ C, 15 seconds at T m, 30 seconds at 72˚ C) for 60 times.
Each 16 samples, 8 samples of CL pods and 8 samples of CH pods (Table 2.11), was split into 3 samples.

First sample was subjected to HpaII digestion; second sample to MspI digestion (protocol in Table 2.4) and the third sample was the control under the same conditions as the other two but without any enzymes.
All the samples were incubated for 2 hours at 37˚ C and put under 65˚ C for 10 minutes to stop the digestion process.

Eight standards with a series of 2-fold dilution in concentrations of DNA template (Table 2.12) were made and three replicate for each concentration were used to calibrate the standard curve of the real-time PCR.
3 Results

3.1 Explorative ISSR-PCR
With the nine ISSR primers (Table 2.5) tested on three DNA samples of leaves from three different plants, A12, A19 and A27, only three primers, 809, 812 and 887 gave some weak and fuzzy banding patterns on agarose gels (Figure 3.1) on A19 and A27 samples.

This was also shown by the small peak height in the following fragment analyses (Figure 3.2).
The fluorescence unit in Figure 3.2 is some arbitrary unit as the graphs were generated by GeneMapper 4.0 software from Applied Biosystems, and there were no information about the fluorescence unit.

Figure 3.2 (g) is a graph that combines one of the sample and the size standard (4 fmol of GeneScan 500 ROX from Applied Biosystems) curve and it shows the relative small peaks of the sample.
3.2 Screening of Kesanakurti's universal primers

In all 54 pairs of Kesanakurti's universal primers tested, only seven produced single strong bands, another seven produced single weak bands and another seven produced random banding patterns (Table 3.1).
Out of the 14 single band producing primers, only three had the restriction sites and all these three pairs of primers only produced weak bands which were not robust enough for the use in real-time PCR.

3.3 New primer PCR test results
The combination of original UREASE Forward and new UREASE Reverse is the only primer pair, which gives good PCR results.

The followings are tests results of the primers tested on A27 leaf DNA sample.
Figure 3.3 shows that with the original primers, it gives a rather weak PCR product and this shows up as a faint band on the gel when compare with other primer, MAGO which gives a much stronger PCR product.

The optimal annealing temperature, T m, for UREASE Forward and Reverse2 is between 47.7˚ to 50.3˚ C (Figure 3.4).
49˚ C was chosen as T m for all the subsequence experiments.

The PCR product from the primers also tested for the restriction site (Figure 3.5)
3.4 Real-time PCR

Table 3.2 gives the rotor position and sample name of each sample and standard for the real-time PCR.
The real-time PCR raw data curve (Figure 3.6) and the standard curve (Figure 3.7) were generated by Rotor-Gene 6000 Series Software V1.7.

The raw data curve uses a normalized fluorescence unit and the maximum value is 100.
For the standard curve, Rotor-Gene 6000 Series Software optimizes the correlation coefficient, R 2 by adjusting some parameters, e.g. the level of fluorescence threshold for C T, the slope and intercept of the standard curve.

The software also extrapolates the standard curve to accommodate any sample with C T value outside the set of calibration standard.
The raw data curve (Figure 3.6) shows very poor results, as most of the curves were crowded together, and had similar C T. Also note that most of the control (green colour curves) had bigger C T than the digested samples (red and blue curves) and this should not have happen.

As digested samples should have lower concentration of intact template than the control and this suggests some problem with the results.
Problems also show in the standard curve (Figure 3.7), as it has a very low correlation coefficient (R 2 = 0.04716), also, the slope should never be positive (see Figure 2.1 (b)).

All the samples also have very similar C T values (Table 3.3).
For the following statistical analyses, we used values of sample pairwise differences (Table 3.4) (i.e. the differences in C T values between control and any enzyme digestion of the same sample) and this is to avoid any error due to variation in DNA concentration between samples.

A statistical analysis program, MINITAB® (Release 14), was used to calculate one-sample (Table 3.5) and two-sample (Table 3.6) t-tests.
There seems to be some significant differences in C T between HpaII to control, and HpaII to MspI for the CL samples (Table 3.5).

And more interestingly, there appears to have some significant differences between CL and CH samples, when the pairwise comparison of C T is made between HpaII and MspI digested samples (Table 3.6).
However, caution must be taken here, as the results of this real-time PCR are aberrant (see Section 4.3).

4 Discussion
4.1 Explorative ISSR-PCR

Inter Simple Sequence Repeats-PCRs (ISSR-PCRs) use primers that are complementary to Simple Sequence Repeats (SSRs also known as microsatellites), and that contain a 1-3 base 'anchor' at either 3' or 5' end (Zietkiewicz et al.
1994).

Microsatellites are short tandem repeats dispersed through out the genome (Graur and Li 2000) and this makes them very useful genetic markers for exploring potential loci for any non-modelled plants.
As there are numerous microsatellite sites in the genomes of most eukaryote organisms, one would expect the results of most ISSR-PCR to be littered with bands on an agarose gel.

However, this was not to be the case in our results (Figure 3.1), this was also confirmed by the small peak height in the subsequent fragment analyses (Figure 3.2).
Part of the reason for the poor results could be the age (a few years old) of the primers being used, as well as possible impurity in the samples that might cause inhibition in the PCR (Smith and Maxwell 2007).

As different samples might have different amount of impurity and this could contribute to the different results between samples as can be seen in Figure 3.1.
Also, the frame shifting effect of primers during annealing can be seen in Figure 3.2 (a) and Figure 3.2(b) and these multiple spikes were caused by the primer binding in a succession of two bases different positions in the microsatellite regions on the genomic DNA.

Another main reason for this method being abandoned by me is the difficulty in accurately determining the digestion results.
It is difficult to have a 100% efficient restriction enzyme digestion in a 2-hour incubation period; on the other hand, the blocking methylated sites are not completely immune from the digestion action of the restriction enzymes.

The length of digestion incubation period is always a compromise between the complete digestion of the non-methylation sites on one hand and the preservation of the integrity of methylated sites on the other hand.
Hence after digestion, the sure way to determine the methylation status of the restriction sites is by comparing the relative amount of intact template between digested and pre-digested samples.

However, any normal PCRs, including ISSR-PCR are ill-suited for quantifying the initial amount of template in a reaction.
According to McPherson and Møller (2006), the kinetics of PCR has 3 distinct phases (Figure 4.1).

E is the early phase of relatively slow building up of PCR product; M is the middle phase during which product concentration increases exponentially; L is the late phase where product accumulation rate reaches a plateau.
Most PCRs finish during the late phase L, and the initial concentration of template cannot be determined by the final amount of product in the reaction, because any differences in the initial amount of template would be very quickly compensated by exponential accumulation of the product during middle phase, M.

For example, if the PCR is 100% efficient during the M phase, every thermal cycle will have an increase of product by a factor of 2.
If the initial concentration of template were reduced by a factor of 2, this would only take a single cycle to compensate this reduction in the template concentration.

4.2 The relentless quest for primers and restriction enzymes
The major obstacle in using the method of restriction enzyme digestion followed by real-time PCR, to search for methylation markers in a non-modelled plant is the need for suitable primers.

Primers that give robust PCR and the amplified loci have to have a 5'CCGG3' restriction site.
As the results in Section 3.2 shows, after screening of 54 pairs of Kesanakurti's universal primers, only 3 pairs give loci that have 5'CCGG3' site, even though they only give very weak PCR amplification.

Some of the universal primers, e.g. MAGO, give much stronger products (Table 3.1 and Figure 3.3), but they do not have the required 5'CCGG3' restriction site.
However, as 5'CCGG3' is just one of the restriction sites that include the CpG island, in theory, we could use any restriction enzymes as long as the restriction site include the CpG island, e.g. 5'ACGT3'.

Attempts were made to find alternative restriction enzymes.
There seems to be no shortage of possible enzymes on the Restriction Enzyme Database (REBASE 2007), but actually only some of them are commercially available and even fewer with information about their methylation sensitivity, and at the end there are no other alternative for 5'CCGG3'.

The design of AGAMOUS1, AGAMOUS2, LFY1, LFY2 and LFY3 primers were based on the sequenced nuclear genes of I. balsamina in NCBI GenBank.
Intron regions were selected for the primers, as this would be the mostly likely area subjected to gene regulation activity by methylation.

However, none of the above primers worked and this could be due to the intron regions not being well preserved across different species.
If the primers were based on the coding regions (exon) flanking the intron region in between, the primers might be more successful.

However, this could cause other problems, the sequence of the amplifying locus might be too long (say over a few 1000 bp) and PCR could be difficult to carry out.
Also, the locus may contain more than one restriction site and this would complicate the subsequent analysis.

At the end, after the redesigning of six pairs of universal primers and a number of experiments by combining new and original primers, only one pair of primers seen to work and used for the real-time PCR experiment.
This only gives us one locus from one gene to test for a possible methylation maker, when one thinks about there are well over 25,000 genes in the model plant Arabidopsis thaliana; one single locus is far from sufficient.

4.3 Real-time PCR
The coupling of restriction enzymes digestion and real-time PCR as a way to detect methylation polymorphism is a novel method.

This method not only can locate a gene or a locus with methylation polymorphism, it is also a very sensitive method as real-time PCR is very sensitive (Valasek and Repa 2005).
Moreover, it can provide quantitative information about the level of methylation from a very small tissue sample.

However, the raw data curve in Figure 3.6, the standard curve in Figure 3.7 and samples C T values in Table 3.3 all show the results of real-time PCR are very poor.
There are two possible reasons for these results.

Firstly, real-time PCR is a very sensitive process and the purity of reagents and samples used in the reaction are very important (Terry et al.
2002 and Bar et al.

2003).
The DNA extraction process, S.D.S.

method used in this experiment might be adequate for most normal PCRs, but it might not be the appropriate method for real-time PCRs.
If the DNeasy kit supplied by QIAGEN was used for the DNA extraction, the extracted samples would have been purer and they might work better with real-time PCRs (Smith and Maxwell 2007 and Smith et al.

2005).
As the decision of using real-time PCR was made after DNA extraction had been carried out, DNeasy kit was not used.

Secondly, even though the combination of original Forward and new Reverse 2 primers of UREASE seemed to perform well in normal PCRs (Figure 3.4 and Figure 3.5), the primers might not match the sample template perfectly and as real-time PCR is a more sensitive process, any mismatch between primers and template might cause problems in real-time PCRs.
To have perfectly matched new primers, the weak PCR product from the original UREASE Forward and Reverse primers could be accumulated from replicate reactions and have it sequenced and then used the resulting sequence to design the new primers.

However, due to time and resource constrains, no PCR products were sequenced.
Any analysis of the real-time PCR results should be carried out on data of the initial concentration of the template.

As the standard curve (Figure 3.7) is abnormal, we cannot use it to calculate any meaningful data of initial template concentration.
Nevertheless, attempts were made to use the C T values directly for the statistical analysis and sample pair wise different between control and enzymes digested template C T values were used.

In the one-sample t-test (Table 3.5), there are significant different in the average of different C T values between HpaII digested and control (non-digested) CL samples, and also between HpaII and MspI digested CL samples.
Interestingly, in the two-sample t-test results, there seems to be significant different between CL and CH samples, when we compare the different C T values between HpaII and MspI digestions.

However, caution must be taken here, as the sample size is very small, eight samples for both CL and CH.
More importantly, the results of this real-time PCR are very unreliable as well as aberrant.

This apparent different between CL and CH samples results could well be caused by other factors or artefacts rather than the methylation pattern different between CL and CH samples.
As real-time PCR is relying on the principle that all the samples have the same PCR efficiencies (Kontanis and Reed 2006), any impurities that inhibit the DNA polymerase in the samples would cause the results to be skew.

To avoid this problem, the samples should be further purified by using any commercially available DNA purification kits or other purification methods.
The primers also have to be fully match with the sample template, but this can only be achieved with template sequence information.

The real-time PCR standard should be in 10-fold dilution instead of 2-fold dilution series, as this would cover a wider range of values for the standard curve.
4.4 The way forward and possible future work

It might be useful to use chromatographic estimation of global DNA methylation rates procedure (Chakrabarty et al.
2003) to test if there are any different in the level of methylation between CH and CL samples first.

If the result was positive, this would support the hypothesis of the second objective of this project fairly early on and this would give impetus for any further work.
However, a negative result might not prove the hypothesis was wrong, it could be just that the difference in methylation between CH and CL samples is small and cannot be detected by using the above method.

As finding the right primers for candidate loci is a major obstacle in this method of detecting methylation markers for a non-modelled plant, a possible solution is to use ISSRs to screen candidate loci.
In order to have repeatable results, the condition of the reaction (i.e. all the reagent concentration, annealing temperature and thermal cycle profile) has to be strictly control as well as using purer DNA templates from a more reliable extraction method.

The screening procedure is as follows: Check the ISSR PCR products on agarose gel and extract separated PCR products from any bands on the agarose gel.
Use the extracted PCR products as templates to rerun PCR and check for the desired restriction site.

If a locus contains the restriction site, have that particular PCR product sequenced and use the resulting sequence to design primers.
Also, it might be worth to try out CRED-RA method (Cai et al.

1996) as this method is very similar to ISSRs, but with a very different type of primers.
As the results of the real-time PCR were less than satisfactory, it would be useful if further experiments were carried out to establish the causes of the problems.

The real-time PCR can be rerun with the product from a normal PCR as template instead of using genomic DNA, as product from the normal PCR would match perfectly with the primers and this would be useful to find out if the problem was caused by the mismatch of the primers and the genomic template.
It would also be useful to evaluate how effective is this method to detect methylation polymorphism.

Products from any PCRs are without any methylated bases and methylation can be carried out in vitro by using DNA methyltransferase.
By mixing methylated and non-methylated PCR products, we can have different concentration ratios of methylated and non-methylated templates, and these mixtures can be used as normal sample for restriction enzymes digestion followed by real-time PCR.

As we know the starting concentration of the methylated template, the effectiveness of this method can be evaluated.
4.5 Possible merit of this method

Methylation-sensitive amplification polymorphisms (MSAP) (Chakrabarty et al.
2003 and Ruiz-Garcia 2005) are commonly used genetic markers to analyse DNA methylation.

In comparison with our novel method, MSAP markers cover the entire genome and no specific primers are required and it is very often used for any non-modelled plants or plants with very few sequenced genes.
However, methylation-sensitive MSAP also has some drawbacks.

It is like any other MSAP experiment in that it can be technically difficult and the results are subjected to noise.
Sometimes, there are wide variations in results from different institutions or laboratories, even if they followed the same protocols.

Moreover, although methylation-sensitive MSAP will detect methylation polymorphisms in the genome, it certainly will not locate the gene nor will it be able to verify methylation polymorphisms in a specific gene or locus.
This is where our novel method can be useful.

As long as there are reliable primers for that specific locus, our method should able to determine whether there are polymorphisms.
Another major different between our novel methods and most other methods mentioned in this report is that our method can provide quantitative information about the extent of methylation in a specific gene from the sampled tissue.

A fundamental question one could ask is if methylation is quantitative or qualitative state?
Perhaps, methylation-sensitive MSAP and our novel method can compliment each other.

E. Warwick Slinn describes dramatic monologue as 'the flagship genre of Victorian poetry', which provided 'a medium for multiple forms of cultural critique'.
In Victorian England, the capacity of this form to highlight social and political problems was facilitated by both male and female poets, including Augusta Webster, Christina Rossetti and Robert Browning among others.

These writers used their poetry to display concern for the dominance of men over women in the workforce and the home, as well as to discuss the injustice of class divisions.
The dramatic monologue is a particularly effective method of cultural critique because it firmly detaches the poet from the narrator of a poem.

This allows the poet to speak authentically through the voice of the oppressed or oppressor who is personally affected or affects these cultural problems.
Bonnie J. Robinson reminds us that Victorian poets were often 'liberal thinkers and activists as well as writers', who wrote with an aim to promote cultural awareness and remove injustices from society.

For example, Elizabeth Barrett Browning's The Runaway Slave at Pilgrim's Point was published in the Boston anti-slavery annual in 1848, contributing to the American and British abolition debate in offence of slavery (Slinn, "Dramatic Monologue", p. 90).
While the reader is compelled to sympathise with the illegitimate child voiced in Rossetti's dramatic monologue, The Iniquity of the Fathers Upon the Children, Browning invites us to judge the narrator of his poem, Porphyria's Lover, by offering a psychological insight into the mind of a man who is guilty of domestic violence.

Although dramatic monologue provides poets with an opportunity to elude their gender, there is certainly a pattern which attaches female speakers with female poets and male with male.
Browning's criticisms of his male narrator in My Last Duchess exemplify that writers do not necessarily voice an example of their gender in order to defend these examples or remove themselves from sympathies with the other sex.

However, women writers were keen to extend a single female voice to speak for a plurality of oppressed women who have also been victims of patriarchal society.
For example, Dorothy Mermin describes how Webster's A Castaway 'reached across the iron boundaries of respectability, money, and class to declare sisterhood with sexually disgraced women' in an attempt to relieve tensions of her gender in 19 th century Britain.

In this dramatic monologue, Webster defends disreputable women from society's stereotypical presentation of prostitutes by highlighting flaws within society that lead ordinary women to such a degrading occupation.
Christine Sutphin observes that 'Webster showed up not only the economic and moral oppression of women, but the poverty of the transformative ideology'.

Webster's narrator, Eulalie, displays society's disillusion with the marriage ideal, which confines women to the mundane, domestic sphere, monogamy and dependence on men for economic and moral welfare.
Eulalie claims that she does not at all envy married women, who she prefers to address as 'Dianas under lock and key', suggesting her resentment for the entrapment of women in the home.

Sarah Ellis did not see this confinement as totally negative, but uncommonly viewed it as critical 'in fostering female self-respect' and re-establishing 'a balance of power between husband and wife'.
While Eulalie cannot perceive this balance, she can identify with women who thrive in the domestic realm since her diary informs her that she was once a 'budding colourless young rose of home' (8) with 'no wishes, no cares' and 'no hopes' (21).

While she goes on to admit that her domestic days were 'better days' (194), she relates this experience to being a child, rather than a woman; to someone who is growing up, not someone who has grown up only to achieve this simple life.
It becomes clear that Eulalie does not reject domesticity, but desires to firmly detach herself from its associations.

She questions the justice in perceiving that a woman is only as good as she is good to her husband (392), pities women who wear 'no kisses but their husband's upon lips' (123) and the girl who 'not in the woman's true place, the wife's place' (509-11) is not able to pilot her own life.
For Eulalie, it is not natural to become a wife or a mother since she hates men and believes that her occupation has not designed her as the role model she needs to be in order to bring up a child.

Sutphin sees her rejection of motherhood as the poem's most controversial statement: That motherhood is inadequate as a solution to any of the problems raised in the poem is perhaps Webster's most powerful departure from even the more enlightened discourse of her contemporaries.
(Sutphin, "Human Tigresses", p. 526) In a society where motherhood was synonymous with being a woman, Eulalie completely divorces herself from society's expectations of womanhood, implying that these designs need to undergo drastic change before they can satisfy this social group.

Webster displays her ambivalence towards the morality of prostitution through the use of techniques which are specifically applicable to the dramatic monologue.
In reading this form of poetry, the reader should be aware that it 'creates speakers who are less concerned with truth than they are with "trying to impress it on the outside world"'.

Therefore, the reader must acknowledge that the speaker has designed this poem with an intention to defend disreputable women and, therefore, is advised to be reluctant to rely upon Eulalie's honesty.
Webster directs the reader to dishonest words when the speaker claims that she is modest, which is immediately contradicted with a confession that she flouts herself.

The speaker realises her inconsistency and hesitantly stutters, 'but yet, but yet' (58), indicating her eagerness to defend herself and continue to engage her audience.
Webster interjects irony into her poem when Eulalie decides that, since her personal thoughts will not meet judgement, there is no need to worry about being a hypocrite (59-62).

This logic could lead her to liberate her voice from untruth since there's no one to judge her or, contrastingly, from truth as there's no one to disprove her lies.
This ambiguity is skilfully placed in the text to remain in our minds while the speaker argues that her trade is relatively moral to that of a lawyer or a journalist.

This technique allows Webster to distance herself from these culturally damning perceptions as they can be excused as opinions which, like the speaker, she does not truly believe.
This idea becomes more probable when considered in conjunction with Sutphin's suggestion that, while women writers were keen to defend disreputable women, 'they were not immune to the idea that prostitutes were irrevocably "diseased"' (Sutphin, "Human Tigresses", p. 521).

Webster may be keen to identify society's restrictions upon women as the root of this culture and suggest that the prostitute was once the ordinary woman framed by social pressures, but she is certainly reluctant to display prostitution as a decent way of living.
In many ways, highlighting its degrading weight on women would assist her argument in support of social change.

Webster does not hesitate to criticise men throughout this poem by reminding the reader that their immorality allows the speaker to continue her immoral trade by creating demand for prostitution.
Eulalie does not feel that her occupation is particularly harmful because the only people she affects are wives and she does not take anything off these 'poor fools' that is worth keeping (100).

She strengthens her argument by establishing herself as an experienced figure to judge men as she can 'perceive there are more men than one', unlike blinded, monogamous women (103).
Eulalie challenges society's perception of men by removing any illusion that her clientele are 'simple' and the victims of her predatory advances (157).

To the contrary, they 'have their favourite sinnings planned / to do them civilly and sensibly' (158-9).
The speaker insists that she attracts supposedly civil, sensible and educated men who society would regard as honourable.

Webster locates weaknesses in a society which cannot correctly separate the moral from the immoral, and places the prostitute and her client in opposing categories based upon gender.
Kathryn Gleadle deposits prostitutes in a band of women who 'appear to have perceived their labour as a necessary, if often grim, strategy which would enable them to earn much-needed money'.

In expressing a poorly-educated, single woman, with only a neglectful brother to boast as family connections, Webster attaches Eulalie's circumstances to financial insecurity.
While the speaker is concerned for the goodness of her soul, her immediate concern is the survival of her body and she is therefore required to prioritise money over morals.

She explains that, while her brother's university education was funded, her rights to education were neglected but she 'saw never aught to murmur at' (496) in this system.
She believes that ''Twas no one's fault' (503) that her parents chose to invest money into the education of their son as they were sensible to perceive that this would lead him to a respectable occupation, unlike a woman.

Eulalie regards Britain's attempts to elevate women to the level of men through education as pretence of 'teaching them / what no one ever cares that they should know' (377-80).
She argues that offering women education is simply an outward display of efforts, with no serious intent on valuing their ability in the workforce.

The narrator elevates herself above the common prostitute because she has a home 'all velvet and marqueterie and pastilles' (71) rather than exhibiting herself 'at infamous corners' (49).
She becomes accustomed to these luxuries, admitting that the refuge did not suit her because it reduced her lifestyle from 'pampering to half-famishing' (244).

At this point in the poem, the reader realises that, though Eulalie does not enjoy the dirtiness and loneliness of the prostitute lifestyle, she cannot give up the luxuries that it has afforded her.
It becomes clear that, while prostitution is not necessary to preserve her life, it is necessary to afford her lifestyle and, with her poor education and skills, an alternative occupation that maintains her wealth does not exist for her.

Christina Rossetti's The Iniquity of the Fathers Upon the Children similarly defends a disgraced class of people by voicing Margaret, an illegitimate child who is interminably stained with the personal and social shame of her ignoble birth.
She blames her father who 'wrought' upon the 'foolish youth' (525) of her mother.

There is certainly some suggestion that the mother was a victim of physical and sexual abuse, suggested by the dynamic force of the father's actions and the emphasis on the mother's youth.
In conjunction with these implications, she perceives her mother as a victim, like herself, who suffers a greater burden than that passed down to her: 'A lifelong lie for truth' (529).

As well as defending the shames brought on women by the immoral conduct of men, Rossetti describes the ill treatment Margaret endures by society in order to provoke pity from the Victorian audience and encourage them to reshape their opinions of these innocent children.
Without the skills or authority to voice their own unhappiness, Rossetti enters the mind of a defenceless child in protection of her interests.

When Margaret is removed from her biological mother, Rossetti introduces the total antithesis of the young girl in the form of a generous, 'jewelled' and 'grand' lady (240), who does not allow her superior class and respectability to interfere in nurturing Margaret like a daughter.
In the child's idolisation of her Lady, Rossetti displays admiration for a woman who is able to overlook external class divisions in search for the child's internal morality.

While Rossetti designs a role model for society in this woman, she contrasts her modesty with that of her visitors, who, for all their nobility, cannot conceal their contempt of Margaret.
The child narrates that 'the women speak and stare / And mean to be so civil' (334-5) and highlights the excessive pride of those 'who sit as struck with blindness' as if she wasn't there (341).

By rejecting and sneering at her existence, Margaret tolerates their torment until she is 'almost ready' (494) to wish death upon herself.
While she begins to feel like she belongs to her Lady, indicated by her eagerness to define herself as 'almost child' to this figure, this progress is undone by her contemptuous guests, which signals how her happiness is completely dependent on society's acceptance of her.

While Browning often facilitates the freedom of the dramatic monologue's unrestricted structure to develop a conversational tone, Rossetti chooses to regulate her verse using iambic trimeter.
The nursery-rhyme echoes of the iambic rhythm successfully connect the verse with children, assisting Rossetti in authentically immersing herself in a young mind.

Unlike Webster's Eulalie, this voice is not vulnerable to judgement from the reader as its youth suggests innocence that is yet to be corrupted by skills in deceit.
Robert Browning's My Last Duchess voices a historical figure, the Duke of Ferrara, who was suspected of poisoning his seventeen-year old bride, Lucrezia de Medici in 1561.

The poet traces back to an event in history to display a contemporary problem in Victorian Britain.
Melissa Valiska Gregory reports that 'historical work on the subject of sexual violence within the Victorian home suggests that it was a relatively common feature of domestic life'.

While this is highly problematic in a society which sought to create a sanctuary for women in the domestic setting, Browning portrays 'the dynamics of the home as deeply painful for both men and women...in the struggle for sexual dominance between husbands and wives' (Gregory, "Robert Browning", p. 494).
An example of female dominance can be seen in Browning's Andrea del Sarto which voices the unhappiness of a man who justifiably suspects his wife of infidelity.

However, these roles are completely reversed in My Last Duchess, where the jealous and untrusting Duke cannot control his suspicions, leading him to cruelly assert control over his wife through murder.
Spoken by the Duke, Browning shapes possible psychological motives behind such an attack and the reader becomes more educated about the speaker than his Duchess, in discordance with the poem's title.

The reader is encouraged to judge the morality of the Duke as this form of poetry ironically allows the reader to understand 'more than the speaker understands' (Slinn, "Dramatic Monologue", p. 82).
Browning speaks through the voice of the Duke to reveal his flawed character to the reader.

The reader builds an awareness of the power of the speaker's imagination to shape immorality in his wife, while, blinded by his jealousy and pride, the narrator remains completely ignorant of these details.
The opening line immediately indicates The Duke of Ferrara as a possessive and unaffectionate collector of wives.

The use of the demonstrative pronoun, 'That's', perfectly demonstrates his impersonal attachment to his 'last' Duchess in a sequence of unfortunate wives.
He prefers to speak indirectly about his spouse through her painting, rather than the lady herself, and avoids the familiarity of naming her personally throughout this monologue, suggesting his scornful rejection of her.

In describing his wife's treatment of him and other men, he fails to convince the reader that, regardless of punishment by death, her behaviour deserved to be punished at all.
He suggests that she practiced flirtatious behaviour with other men as 'her looks went everywhere' (24), but he better implies his damaged pride that she didn't spend her days looking up to him.

He expresses that she thanked men for gifts 'somehow - I know not how' (32), but unintentionally displays his irrationality in using only his imagination as evidence of his wife's offences.
He snobbishly cannot tolerate that she reduced his 'gift of a nine-hundred-years-old name' to 'anybody's gift' (33), revealing his arrogance and excessive pride in his noble birth.

R.W. Buchanan described Browning's eagerness to judge men as comparable with 'a messenger from heaven, sent to teach the highest of all lessons to rashly-judging men'.
The Duke certainly doesn't escape Browning's criticisms as the reader is invited to judge him while he enjoys unfairly judging his late wife.

Following revelations of weaknesses in his character, he goes on to explain that his wife was completely ignorant of his dissatisfaction with her behaviour as she received no warning from him.
He places what he should have said in speech marks throughout the poem, yet these words remain a mere unperformed rehearsal of his intentions, as he was too cowardly to verbally address marital problems with his wife.

Browning implies that he never spoke of them through fear that he would fail in managing her and exercises his new freedoms now she 'stands' (4) there in a stationary form as a manageable painting.
In a description of their final engagement together, the Duke confesses that he only gave commands immediately before 'all smiles stopped together' (46).

While his frustration with her treatment of other men discarded his authority in their relationship, she is reduced to the role of the passive by his imperatives and violent termination of her life.
Browning heavily criticises the merciless nature of a man who rashly punished his wife, not only without evidence of her convictions, but without delivering a warning of his dissatisfaction.

In Webster's A Castaway, we imagine that Eulalie writes or speaks her monologue in isolation without a present auditor, causing her to develop an argument in anticipation of objections from her absent, imagined judge.
However, Browning imagines an engagement between the Duke and the father of his next bride where the Duke presents his last Duchess in the form of a threat to her successor.

Browning indicates the presence of the auditor by using the formal address, 'Sir' and the use of the second person pronoun.
While the auditor in dramatic monologue is 'constituted more by the speaker's perceptions than by any separate reality' (Slinn, "Dramatic Monologue", p. 81), there is only a single indication that the father managed to insert any speech into the conversation.

This can be found in the Duke's response 'Nay, we'll go / Together down, sir' (53-4) near the end of their acquaintance, implying that the Duke has been questioned.
Gregory perceives the parallels between the 'rhetorically violent' speaker who forces his speech upon the listener and the dominance of domestic violence (Gregory, "Robert Browning", p. 495).

The Duke displays modesty only once throughout his speech, when he ironically claims to lack skill in this very area.
The reader is aware of the strength of his rhetoric, placing him closer to his flawed character with pretended modesty.

While the use of the dramatic monologue to criticise society is common to both male and female writers, their genders often decide that of the speaker they decide to voice.
Therefore, female writers commonly voice the oppressor, and male, the oppressed.

The task of women writers is particularly remarkable because they place prostitutes and illegitimate children, not only as the subject, but the narrators of the poem, which was a brave practice in this hostile society.
For example, Dante Gabriel Rossetti uses the prostitute, Jenny as the subject of his poem of the same name, but does not attribute her with a voice or active role in the poem.

Men and women are united in their concern for oppressed gender, but women writers discuss issues indirectly in defence of themselves and their interests.
The dramatic monologue requires the writer to adopt different techniques and styles to authenticate themselves as the speaker of the poem and they are keen to design dramatic monologue as a political statement by encompassing a plurality of voices in a single voice.

While Browning often fails to solve moral problems in his dramatic monologue, women writers provide society with answers in ambitious hope to inspire social change.
Hugh Blair voices an attack on the practices of 18th century poets in his Lectures on Rhetoric and Belles Lettres for the circulation of artificial and lofty pieces among the intellectual classes which detached poetry from the concerns of the everyman.

Blair exposes the harmful effect on works that are motivated by money and reputation rather than using the imagination with an aim to educate, inspire, move, delight and communicate with the reader.
Wordsworth and Coleridge's Lyrical Ballads indicated a revolution in literary history in reaction to the poetry created in the Augustan period, favouring tastefulness, elegance and civility.

Wordsworth and Coleridge experiment with a new way of using language in this volume, eradicating features of 18 th century poets by composing organic, liberated and passionate verses inspired by real people and concerning real life experience.
We Are Seven offers an excellent example of the power of simple and direct language to relate personal emotion and experience that is successful in accessing a wider readership.

Wordsworth emancipates the common man into art by elevating the natural to the supernatural, entirely worthy of his glorification.
Wordsworth favours the ideas of childhood by allowing the thoughts of the naive, inexperienced and innocent little maid to triumph over the narrator's need to affect her young mind with the reality and cynicism of an adult's.

While the narrator strives to educate the young girl, her view of death forces him to revaluate his own comprehension of this concept and, ironically, her insistence eventually succeeds in educating him.
Wordsworth challenges the reader to consider if a child's incomprehension of death is any different from that of an adult.

The process of death is only as factual as far as a person's heart stops beating and their physical body begins to decay.
Everybody is limited to this scientific knowledge and therefore unable to ascertain what happens to a person when they die, making this an issue of great religious concern.

While the adult attempts to correct the child, Wordsworth highlights that 'we are seven' is a statement which cannot be corrected as, in the absence of an absolute truth, the child has the freedom to believe whatever she chooses.
Wordsworth's imitation of a child is entirely convincing throughout this work by his application of simple diction that avoids any decoration including modifiers, figurative devices, and abstract nouns.

In addition, the child's lexis is often limited to monosyllables: And he lies by her side (60) This use of basic language is appropriate to voice a child of limited education and experience.
The simplicity of her speech also helps her speak plainly and directly without any ambiguities, projecting an honest, transparent, young mind.

Wordsworth continues to voice the young girl by using a euphemism to describe the death of her sister, Jane.
And then she went away (52) It is typical of a parent to relate death to a child using a euphemism in order to protect them from the pain and sorrow caused by death.

This phrase avoids any implication of permanence as the use of 'away' is very casual, like a person leaving a room or going on a holiday.
The child certainly does not see death as an indication of the end of a relationship or a person which is a refreshing perception.

The child speaks of death without any fear, sorrow or mystery and treats the loss of her brother in a similarly hopeful way: My brother John was forced to go (59) The child continues to avoid referring to the loss of her siblings as 'death' and, on this occasion, she implies this process is obligatory and her brother's involvement is purely passive.
Rather than demonstrating incomprehension here, the child expresses a very mature attitude by accepting that it was simply her brother's turn to leave.

The infant perceives death as a kind image by describing the moans of her ill sister when 'God released her of her pain' (51).
She implies that God intervened in her sister's life when the quality of her life diminished by relieving her of her sickness in death.

The child displays very Christian ideas by using God as the figure who decides who should live or die.
Christianity embraces a very positive perception of death in the belief that while it marks an end to life on earth, it creates the beginning of a new life with God in heaven.

It is not at all surprising that Christianity has shaped the hope of this child.
The little maid does not seem to feel at all distanced from her siblings in death.

She appears to enjoy regular visits to the graveyard where she sings to them, eats with them and plays around their graves.
Death has not prevented her from keeping them company and they are still a comforting presence to her.

The child is unfazed by being the only active figure in their ongoing relationship: My stockings there I often knit My kerchief there I hem, And there upon the ground I sit, I sit and sing to them.
(41-44) Wordsworth has manipulated the natural syntax of these lines by end-focusing the activity of the young girl in order to emphasise her as independently fulfilling the active role while the deceased are merely passive figures.

This stanza opens with three simple clauses which supplement the undecorated diction of the child, lacking the complexity of adverbs and adjectives.
This list of verbs describes day-to-day occupations of the young girl, suggesting that she doesn't distinguish the graveyard from other locations of her day.

Since her family has moved from her home to the graveyard, she has simply extended her surroundings and feels equally comfortable in a setting that can be bleak and upsetting for others.
While she provides company for her siblings, she understands they shall never suffer solitude as their bodies have been buried next to each other: Twelve steps or more from mother's door, And they are side by side.

(39-40) It does not concern the child that her deceased relatives cannot interact with each other or that they are separated by coffins.
Her concerns are straightforward; if their bodies are physically close together they are accompanied in death.

It becomes increasingly difficult not to pity the young child who has experienced so much of death although her age should limit her experience.
Wordsworth uses the seasons to convey the passing of time between the girl's loss of one sibling and then the next.

In one stanza, she enjoys the summer playing around her sister's grave with her brother and in the next she loses that same brother: And when the ground was white with snow, And I could run and slide, My brother John was forced to go, And he lies by her side.
(57-60) The seasons are always a cause for excitement to children as the weather allows them to enjoy different outdoor activities but there is a sense that she misses out on these ordinary childhood experiences.

The conditional verb, 'could' suggests there was an opportunity for her to enjoy the new season but this was eradicated by the death of her brother, helping the audience to sympathise with the young girl.
Wordsworth uses an oxymoron by including this piece in his collection of Lyrical Ballads as the lyric poem is conventionally non-narrative, unlike the style of We Are Seven.

Wordsworth freshly interprets the old ballad tradition by retaining the simplicity of the ballad but incorporating a greater depth and complexity in the lyric form that is commonly used to convey an emotion or state of mind.
Wordsworth ironically accompanies the grave subject matter of this poem with a light-hearted tone embroidered in the ballad form.

The poet adopts many conventions of this form including a four-line stanza with a simple abab rhyme scheme and heavy use of dialogue.
Wordsworth uses iambs in tetrameter followed by trimeter and repeats this metre pattern to complete the stanza: Their graves are green, they may be seen, The little maid replied, (37-38) At the end of the iambic trimeter, a natural pause is formed before continuing to the second tetrameter that assists the formation of a sing-song, childlike rhythm of a nursery rhyme.

Wordsworth adds to the buoyant, cheerful mood of this poem with internal rhyme on the words, 'green' and 'seen' in this particular stanza.
The flow of the ballad is interrupted in the final verse where Wordsworth extends the stanza into five lines and modifies the rhyme scheme to abccb.

This verse marks a change of speaker to the exasperated adult following a long period of dialogue from the young girl: But they are dead: those two are dead!
Their spirits are in heaven!

(65-66) The adult introduces and bluntly repeats 'dead', exposing a great contrast between these two voices following the child's spirited euphemisms that avoid the grim reality of this description.
Wordsworth's use of punctuation emphasises the adult's frustration that the child insists on referring to life and death as a unified existence.

Wordsworth suggests a level of sophistication and complexity in the mind of the adult that does not pervade the mind of the child by using enjambment for the first time in this poem: Twas throwing words away, for still The little maid would have her will And said, 'Nay, we are seven!' (67-69) By allowing the child the last word, Wordsworth encourages the reader to allow the child's way of thinking to triumph over the adult's.
Wordsworth challenges the reader's perceptions of death in this poem by offering two opposing reactions to this process.

The poet asks if the child's cheerful perception of death is simply attached to the incomprehension of her age or if it is an inspirational and heart-warming attitude that even adults could adopt.
Wordsworth questions if an adult is better prepared for the emotional distress of this experience or if years just develop a more bleak and sorrowful reaction to loss that can never return to a child's beautifully simplistic methods of dealing with adversity.

'The first thing to remember about Donne,' writes critic, John Carey, 'is that he was a Catholic; the second that he betrayed his faith." John Donne was born into a staunch Catholic family in Elizabethan England.
At this time, followers of this religion were sensible to display an outward obedience to the Protestant consensus in order to avoid persecution.

At the accession of James I to the English throne, there was little adjustment to the oppressive, religious framework laid out by Queen Elizabeth in the sixteenth century.
While the pacifistic king offered greater freedoms for Catholics, it was clear that Catholicism in England was becoming an old, dying tradition which suffered a severe decline in numbers with the passing of each decade.

Donne proved to be no exception to the masses who sought to protect their interests by rejecting their socially damaging roots.
Donne's conversion to the Church of England allowed him greater access to employment and connections, which were formerly restricted by his faith, such as his position as Member of Parliament.

It would be unreasonable to suggest that Donne abandoned personal, theological beliefs in his decision to leave the Catholic Church, as his passionate concern with his relationship to God is hardly concealed in his divine poetry.
However, the Holy Sonnets seldom display a man who is entirely convinced by the doctrines he has chosen to follow.

It is possible that the pressures of the Jacobean age framed his decision in converting to Protestantism.
While a member of the Church of England has the freedom to adopt the beliefs of Laudianism, Arminianism or Calvinism among others, N. Tyacke considers Calvinism as 'the theological cement of the Jacobean church'.

There are a number of controversial concepts inherent to this tradition, formulating a far from moderate religion, which is difficult for man to accept.
P.G. Lake highlights that it is not necessary to maintain that all the English Calvinists 'were indeed convinced Calvinists,' and it appears that Donne is indeed unconvinced.

The Holy Sonnets display the poet's struggle to accept this unforgiving religion, which will not allow man to escape from the heavy burden of original sin and offers little hope in return for reconciliation.
Donne uses the sonnet form to express his frustration, incomprehension, desperation and fears that he associates with the boundaries of Calvinism.

In 1796, John Byrne printed The Articles of Belief, professed by the followers of Calvin, Luther, and Arminius with an aim to dissolve the disagreement among Protestants concerning the five beliefs of their three major doctrines.
Firstly, Byrne writes that the Synod of Dort agrees to the concept of 'divine predestination' where God elects a small number of mankind to be saved, without consideration of their 'faith, or obedience whatsoever,' and commits the remaining masses to 'eternal damnation, without any regard to their infidelity or impenitency'.

Understandably, Donne struggles with this concept because it leaves him helpless, without a guarantee, or even a favourable chance that his consistent devotion will direct him to salvation.
John Calvin admits that this is 'a cumbersome question' which many think 'nothing to be less reasonable,' but Calvin insists on condemning 'the curiousness of man' for questioning the fairness of God.

Discontent with this explanation, Donne seeks comprehension by directing a series of arguments towards God in his Holy Sonnets.
Sonnet IX uses a conversational style to address God directly, and, not dissimilar to others in this sequence, the conversation is one-sided by providing only silence in response from God.

Lawrence Beaston suggests that God's apparent lack of response 'need not be an image of despair' or 'a surrender of doubts about his concern for humankind,' but rather, his absence can be perceived as a method of highlighting the great differences dividing mankind from God's divinity.
While Donne's outspoken nature towards God can be criticised as confident to the level of arrogance at times, he is firmly put in his place as this sonnet develops.

He opens the poem by challenging God with the following argument: If poisonous mineral, and if that tree, Whose fruit threw death on else immortall us If lecherous goats, if serpents envious Cannot be damn'd; Alas; why should I bee?
(Sonnet IX, 1-4) In Calvinist terms, Donne has answered his own question in the framework of this interrogative by returning to the fall of man.

Donne refers to Satan as 'the serpents envious' and blames the tree of knowledge for bearing fateful fruit.
Yet the poet has consciously emitted the role of Adam and Eve in this biblical story, who engineered their freewill to succumb to temptation.

Donne perceives freewill as a curse on mankind, which animals and inanimate trees are lucky to have avoided.
He argues that simply being a member of the fallen race staggers his spiritual progress as it 'makes sinnes, else equal, in mee more heinous' (6).

While these arguments conduct the opening octave of this sonnet, Donne forms the volta at his moment of realisation that he is unworthy to challenge the mystical ways of God: 'But who am I, that dare dispute with thee?' (9).
Sentiments of Calvin are echoed here in his belief that it is not fitting 'that man should freely search those things which God hath willed to be hidden,' (Calvin, The Institution of Christian Religion, p. 750) as wisdom would be adopted that God does not wish to extend to man.

Within the brevity of the sonnet form, Donne has expressed the identical crave for knowledge that was responsible for the fall of humankind.
He displays man's vulnerability to temptation, which constantly leads him off the path of righteousness.

Byrne published that 'by Adam's fall, his posterity lost their freewill,' so their acts of good or evil are 'predestined by the eternal and effectual secret decree of God' (Byrne, The Articles of Belief, p. 4).
This idea implies that man is unable to please its leader because the human race cannot escape from the weight of original sin brought into the world at its creation.

In the first of the Holy Sonnets, Donne expresses his fear that the depths of hell cannot be avoided by mere mortals:...my feeble flesh doth waste By sinne in it, which towards hell doth weigh; (Sonnet I, 7-8) Donne can accept that his fallen race is vulnerable to 'our old subtle foe,' (11) causing sin to pervade every part of one's being, but he struggles to comprehend why God chose to release death into the world as punishment for this fall.
He opens his sequence of divine meditations by challenging the methods of his leader with an interrogative directed towards God: Thou hast made me, And shall thy worke decay?

(1) Donne's inability to, in any way, control his path after death creates the fear exposed throughout this sonnet.
Without freewill, he cannot do any evil or good that God will consider in assessing if he is deserved of an afterlife.

The opening octave of this Petrarchan sonnet is weighted with terms that emphasise the pace and unexpected nature of death.
Donne describes it chasing him in 'haste', how it meets him 'fast' and he dares not to move his 'dimme eyes any way' (2-5) in fear that he will be struck down at any moment.

However, Donne offers a starkly alternative view of death in Sonnet X of this sequence, where the poet directs insults towards a personified version of this terminating force.
He warns Death against believing the fearful compliments many attach to him that deem him 'mighty and dreadfull' (2).

The poet lists forces capable of undermining Death, as evidence that this process does not hold supreme control.
He is described as a 'slave to Fate, chance, kings and desperate men' (9) and Donne argues that this force simply places a person in a rest or sleep, which 'poppie, or charmes' (11) can also activate.

More importantly, Death's blow is only temporary, unlike the eternity of the soul.
This sentiment is repeated in his sermon preached at the funeral of Sir William Cockayne Knight where Donne comforted his congregation by preaching death's impermanence.

He conveyed that, although the body and soul are physically separated in death, 'they are not divorced; they shall returne to one another againe, in an inseperable re-union in the resurrection'.
This comfortingly peaceful presentation of the afterlife persuades the congregation that God sacrificed himself for the everyman.

This perception rejects Calvinist principles of the elect and provides a hopeful perception, which lies closer to the doctrines of Catholicism or Arminianism in the belief that 'Jesus Christ suffered death for all and everyman'.
(Byrne, The Articles of Belief, p. 8) The penultimate statement of the Calvinist's Articles of Belief concerns God's persistence in converting those who oppose him.

While God would be expected to elect only the faithful for salvation, Calvinists believe that God saw equal merit in converting hateful and sinful hearts.
Donne describes his heart as 'iron' in the closing line of Sonnet I, conveying that God is indeed able to draw the cold, hard, opposing heart like 'adamant'(14).

While Donne appears to be a passionately devoted follower to the reader, he is unconvinced of his goodness.
In Sonnet XIV, Donne invites God to inflict physical violence upon him, persuading him that his kind treatment is not enough to repair his sinful ways.

He syndetically lists monosyllabic, dynamic verbs including, 'breake, blowe, burn' (4) to convey the pain he feels he deserves.
Donne repeats this attitude in Sonnet XI, which he addresses to the Jews who were responsible for the crucifixion of Christ.

He condemns them for crucifying the only one 'who could do no iniquitie', while foolishly passing him by who has 'sinn'd, and sinn'd' and inevitably will do again (3-4).
Donne entreats the Jews to show him equal ferocity to that bravely endured by Christ, but the poet suggests that, to 'buffet, and scoffe, scourge, and crucifie' (2) him would not be a gross injustice but a fair penalty for a man who doesn't fail to crucify Christ daily in thanks for his sacrifice to mankind.

In a sermon preached at St.Pauls in 1626, Donne exposes his reason for desiring the feel the wrath of God: Lord let me see that thou art angry with me; I know I have given thee just cause of anger; and if thou smother that anger, and declare it not by corrections here, thou reservest thine anger to undeterminable times, and to unsupportable proportions.
(Donne, The Sermons: VII, p.183) Donne expresses his fear that God will reveal no sign of his anger to his followers until the Last Judgement when he will inflict his wrath upon them in the form of damnation.

Donne expresses these sentiments since his ordination, implying his belief that committing his life to God will not allow him freedom from destruction.
In this expression, Donne develops support of man's total depravity attached to Calvin tradition.

However, there is certainly a hope that pervades Donne's poetry through moments of desperation, despair and frustration.
Donne begs to receive God's violence because he seeks to be repaired, renewed and bent back onto the path that will lead him to eternal life.

The verb, bend, appears frequently in the poetry of Donne, establishing his belief that he can be shaped in the image of God, providing his leader grants him mercy.
This image is conveyed in Good Friday, 1613.

Riding Westward where Donne is 'carryed towards the West' when his 'soules forme bends towards the East' (10).
A.B Chambers provides this image with an explanation, aided by Donne's own words from his second volume of sermons: One must reduce the "flat Map to roundnesse," for then "East and West touch one another and are all one"; One must ride to the Last Judgement in the West to receive an oriental resurrection'.

This image suggests that the only path to God involves receiving punishment, as man must absolve his sins before he can be invited to eternal life.
Donne's dependency on God, in that he is the only one able to bend the soul of man, draws parallels with Calvinism in the belief that a man who raised up the dead can certainly convert people to his ways.

In the closing sestet of Sonnet XIV, Donne describes his relationship with God in terms of a love affair.
He asks God to understand that he loves him yet is engaged to Satan, who, as described in Sonnet II, is 'loth to lose' him (14).

It pains him to disappoint and anger God with sin, so he entreats him to either break his attachment with him through 'divorce,' or 'imprison' him so he is deprived of the opportunity to sin again.
He admits he cannot possibly 'breake that knot' (11-12) that binds him to God because he is so entirely mesmerised by him.

The only way for Donne to become 'chaste' is to be 'ravished' by God (14), creating a controversial paradox.
As mankind constantly chases sin, the only way to terminate this endless path is to be overthrown by divinity.

Donne asks to be corrected through affliction and demonstrates the purpose of this by dictating a passage from the book of Hebrews: That whom the Lord loveth he chasteneth, and he scourgeth every son whom he receiveth.
If ye be without correction, whereof all are partakers, then are ye bastards and not sons.

(Donne, The Sermons: VII, p.188) Donne continues by concisely interpreting this as an indication that 'as his love lasts, he corrects us, and as long as he corrects us, he loves us'.
Donne's dependency on God to defend him in search of a holy path displays the poet's faith in his leader as well as the Calvinist's tendency to play the role of the passive figure in relationship with God.

Donne's expressions of helplessness throughout the Holy Sonnets direct him closer to the doctrines of Calvinism, where man cannot control the desires of God.
Donne's crave for affliction to absolve his sins fits comfortably within this tradition, as does his perception that he is utterly diseased by sin, because Calvinists cannot forgive the total depravity of man.

Indications of hope can be found in his deviation from this religion, indicating Calvinism's punishing nature compared to the softer doctrines of Arminianism and Catholicism.
While he despairs at his subjection, it is possible that he is drawn to this religion because it offers less kindness to man which he believes to be just treatment of the fallen race.

Susan Wiseman calculated that the latest possible date for the composition of The Famous Tragedie was only four months after Charles' execution as George Thomason, a collector of contemporary publications, received this text on 26 th May.
The speed at which this work was composed and printed displays the desperation of fervent Royalists to defend themselves and their murdered King.

Dale B. J Randall perceives that the Interregnum was a time when many 'set down their own reactions as spontaneously and passionately as a man might curse', which often produced tastelessly conspicuous literature.
A Puritan church leader, Richard Baxter, expressed his concern that the freedom of the press allowed 'every ignorant, empty braine (which usually hath the highest esteems of it selfe)' to communicate with the masses through poor quality literature.

During the civil war, the press provided a tool to connect the upper and lower classes by offering, whether true or false, political information.
This freedom contributed to the production of low standard material as it encouraged the common, less educated man to have an opinion and express it through writing.

However, Lois Potter suggests that the pressure of a heavily informed, divided nation even affected the creative output of capable writers as they sometimes rejected 'their theories about style and vocabulary when, abandoning them, they could score a point'.
Both Wiseman and Randall agree that, while The Famous Tragedie is neither a dull nor weak play, it is heavily flawed as a piece of persuasive literature.

Its strong, aggressive attack on the Reformation and abandonment of any 'degree of religio-political analysis' limits its readership only to 'those already hostile to parliament' (Wiseman, Drama and Politics, p. 68).
Although the success of this propaganda, in persuading its seventeenth-century readership, cannot be measured, this text was hardly written in vain as it is widely believed that royalist writings strongly assisted the restoration of the monarchy in 1660.

Although Parliament attempted to censor politically-sensitive material in the 1640s, beginning with the abolition of Star Chamber in 1641, the severity of penalties was not enough to silence Royalists until the Printing Act of 20 th September 1649, which promised heavy fines or imprisonment in response to treasonable behaviour (Potter, Secret Rites and Secret Writing, pp. 18-19).
Therefore, The Famous Tragedie was printed at a time when masses of politically-biased literature was produced.

This competitive market developed an attraction to attaching a writer's name to a work as 'a work written by a saleable author was considerably more saleable and authoritative than an anonymous one' (Potter, Secret Rites and Secret Writing, p. 23).
Considering this advantage, it is possible that the writer chose anonymity in adherence to the courtly tradition not to publish material, rather than to protect himself from Parliament's wrath.

The writer's decision to apply the vernacular to the large majority of his work displays courage as the accessibility of this language to the public increased the threat to the Reformists and, as a result, to the writer.
However, a single bilingual sentence has been inserted in the introduction, which begins, 'Basely Butchered by those who are' and concludes in Latin.

Although the play's entirety is the work of the writer's imagination, the dramatic choice of propaganda allows him, elsewhere, to indirectly place damaging criticism in the voices of others.
The use of Latin cleverly disguises his personal attack on Parliament, while the bilingual sentence is enough to indicate to readers, who cannot comprehend Latin, that this section contains cutting insults.

Potter explains that a true indication of the risk of creating royalist propaganda heavily depended on the anonymity of the publisher, as it was more difficult for them to be concealed (Potter, Secret Rites and Secret Writing, p. 23.
The publisher of The Famous Tragedie has not attached his name to this work, suggesting that he perceived danger in performing illegal practices and chose to protect his income and reputation with anonymity.

Nevertheless, Royalists were keen to write and publish, secretly if necessary, to continue the work of their King who controlled the first royalist literary activity.
They were also eager to demonstrate disregard for Cromwell's authority through rebellion during the Interregnum.

The writer is determined to emphasise that, while the Parliamentarians act under the pretence of improving the nation, they perceive progression in destruction.
This becomes apparent before the play has even begun.

In the prologue addressed to Charles II, the writer uses italics to imply that the term adopted by Parliamentarians, to describe their practices, is ironic: Oh Reformation' dire, that kils our King (III) The author asks his readership how the termination of a valuable life can possibly be a progressive step towards a better nation.
He suggests that the term, 'reformation', is an unconvincing disguise for the evil motives and practices behind parliament, and warns the public of their sly rhetorical complexity.

From September 1642, public theatres were closed on the grounds that the times were unfit for public sports.
By creating this dramatic piece of political satire during this period, the writer, like so many others, displays his contempt for parliamentary decision and seeks a return to the conservative past.

While these constraints cause him to write for a readership, rather than an audience, he comfortably uses theatrical devices throughout his play, such as the inset masque, promenading and soliloquy.
These devices suggest an intention to perform the text which, in itself, undermines Parliament's authority, without the additional aggravation of inserting high-profile Reformists as leading characters in his play.

The writer voices contempt for the 'beastly Ignorance' of the Reformists who initialised a project to 'raze, our Theaters to the ground' (VI).
The use of antithesis in this criticism emphasises the devolutionary effect of the supposedly evolutionary Reform on the nation.

Potter observes that Parliament imposed transformation on old English customs, which led the Stuarts to become identified with 'the same golden past that had previously been associated with Elizabeth I' (Potter, Secret Rites and Secret Writing, p. 28).
The prologue to the gentry, included in this work, outlines the author's hostility to the transformation on social hierarchy that had previously been provided by the monarchy.

He enumerates literary icons, such as Johnson and Devenant, in order to promote the cultural aesthetic of Charles I's court, and declares the Reformists 'villaines' (VI) to have thieved such traditions from society.
Potter asserts that this argument often distanced the public from support of the Royalist party as Parliament's "levelling" design was more attractive to the ordinary man.

However, the writer's presentation of power-hungry Cromwell suggests that the abolition of social hierarchy is an unattainable ideal.
He implies that, while Cromwell may publicly reject the hierarchical structure of the monarchy, he privately hopes that the regicide will earn him the crown.

Cromwell's hypocrisy is particularly implied in his soliloquy where, in privacy from his army, the reader has the opportunity to witness his true thoughts.
Following compliments to Peters' rhetorical skill in previous scenes, he does not hesitate to mock his 'deere Buffoone' (20) in his own company.

Fairfax is also a victim of his leader's contempt.
Cromwell's General and Commander in Chief is rejected as a 'silly Foole' (20) in private, and immediately transformed into 'our great Generall the Lord Fairfax' (21) when Cromwell is immersed in company.

The writer implies that Cromwell is a dishonest and manipulative leader who, in elevating himself above his men, hypocritically parallels the structure of the monarchy and places himself in the highest possible position.
The inseparable connection between religion and politics in the 1640s is emphasised as early as the Patron's introductory note to this work: He that can read thy Play, and yet forbear For his late Murthered Lord, to shed a tear, Hath an heart fram'd of Adamant and may Pass for an Atheist the Reformed way (V) The Patron implies that a man's political stance during the civil war was an indicator of the sincerity of his faith in God.

Essentially, the war emerged from conflicts in religious thinking.
The Royalists favoured the Church of England, as governed by the King, who heavily encouraged Archbishop Laud's influence.

In stark opposition, the Reformists connected Laud's worship with popery and sought to destroy anything which attached Protestantism to the Roman Catholic Church, including The Book of Common Prayer, church festivals and images of saints, angels and persons of the Trinity.
John Morrill came to the conclusion that a compromise between Parliamentarians and Royalists could have been attainable by returning to 'a sort of Anglican Congregationalism' where episcopacy was abandoned.

However, the dismissal of this peaceful settlement, by a small number of committed puritans, caused the war to happen.
Although the Reformists won the war, they had to fight a second battle to win the peace.

Their decision to kill the King hardly demonstrated conformity to Christian ethics of forgiveness or repentance, creating a necessity to dramatically transform their image from murderous rebels into virtuous saints.
Throughout The Famous Tragedie, the writer aims to thwart these efforts by depicting the leaders of the Parliamentarian army as licentious, deceitful villains.

The writer shapes Reformists as destructive devils whose plans are 'hatch'd in Hell' (II).
This comparison implies that to be a Reformist is to support the greatest enemies of God and assist them in infecting Christianity.

In addition, the writer implies that Cromwell and Peters are sophisticated Machiavellians who lack the moral capacity that provides the foundations of any religion.
Peters is seen extending his unethical behaviour to Mrs Lambert by successfully persuading her to be unfaithful to her husband, while Cromwell's application of his intellectual strength to deceive his public is a constant reminder that he cannot be trusted.

While these attacks boast some plausibility, Wiseman is unconvinced that this technique is entirely successful.
She considers that the writer's extension of satire towards 'physical attributes and sexual transgressions' simply registers 'the increasing hopelessness of the royal position' (Wiseman, Drama and Politics, p. 68).

The writer certainly enters unstable territory when he directs an insult towards Cromwell's nose, using a comparison to Mount Etna, in the unlikely voice of Peters.
He also suggests Cromwell would dare to taint the sanctity of his own, or another's, marriage.

Unsurprisingly, this corrupt image of Parliament is heavily contrasted with flawless representatives of the royalist party, such as Charles Lucas and George Lisle, who demonstrate tremendous faith, comradeship and valour throughout this drama.
They are clearly outlined as virtuous victims of the siege of Colchester as, despite surrendering, they are ruthlessly killed.

Wiseman notices that the author only scars his polarisation of Royalists and Parliamentarians through his presentation of Fairfax (Wiseman, Drama and Politics, p 68).
The General is witnessed suggesting a verbal negotiation with the Royalists in protection of Colchester, which displays him as a balanced, pacifistic leader.

The author implies that his name can be used as an aptronym, as he is both fair in name and nature.
His attractive attitude towards warfare is emphasised by the inclusion of Rainsborough, who alternatively demonstrates strong aggression and blood thirst in his treatment of the opposition: I'le see thee hewne to pieces, and thy curst Body throwne unto the Dogs.

(10) The author's presentation of Fairfax is echoed by Milton's Sonnet XV, On the Lord General Fairfax at the Siege of Colchester, which celebrates the Reformist victory.
However, Milton indicates that he has faith in this General to serve his country in 'nobler' means than war.

While he can perceive 'valour' (13) in Parliamentarian warfare, this is often disguised by bloodshed.
He appeals to Fairfax to rapidly restore 'truth and right' (11) to the nation in order to limit further destruction.

This disagreement amongst the men of Parliament exemplifies a wider problem.
Throughout this play, the writer indicates that the Reformists are a great distance from being a united force.

Peters demonstrates obedience to Cromwell throughout this play until he questions why he behaves so wickedly, while the soldiers commanded to murder Lucas and Lisle cannot forgive themselves for their sins and seek repentance by murdering Rainsborough.
Potter underlines this problem by observing that 'Cromwell may have said that he knew what he did not want, but not what he did' (Potter, Secret Rites and Secret Writing, p. xiii) which questions how an army can be expected to work together towards a single goal when they are ignorant of their target.

The writer uses this tragedy to look towards a hopeful future by relating sorrowful events of the past.
In the second Act, before the King meets his death, Sir Charles Lucas draws parallels between him and Christ: Nations have suffer'd cause their Kings were ill, But Britains Charles, His People's sinnes did kill.

(13) The grief of losing their King can, in part, be consoled by shaping a martyr out of Charles I.
As early as May 1640, Archbishop Laud stated that 'no man in England (was) more ready to be a martyr for our religion than his majesty' and Potter notes that some writers ambiguously abbreviated both Christ and Charles with 'Ch' (Potter, Secret Rites and Secret Writing, p. 186).

The writer's emphasis of Charles' Christ-like quality presents the opposition in an incredibly harsh light by comparing them to the Jews who bestially and unjustifiably crucified a virtuous man.
This technique also endorses the depiction of the Reformation as a battle against God and Christianity.

One of the writer's strongest arguments against Parliament concerns royal posterity, as Cromwell's interference in the natural order of national leadership is presented as thieving the Prince of Wales of his kingdom.
The writer indicates that, while the nation's ideas of justice are conflicted, the higher powers are observing Parliament's barbarous interference and preparing to correct the balance of power by punishing them: But Joves all potent thunder shall divide Their plots, and sinke them, in their height of pride.

(V) This threat is endorsed by verses dedicated to Charles II, which encourage him to overthrow the Reformation and avenge his father's death by accessing his rightful position on the throne.
His persuasive technique is carefully crafted, in these verses, by flattering the Prince and stressing the negative effect of the 'holy miscreants, and Religious Fiends' (II) on his nation.

The writer implies that, although the Royalists' current efforts have not yet been rewarded, their names will be immortalised when their present becomes history: Let us provide us fame when we are dead, that the next Age, when they shall read the Story of this unnaturall, uncivill Warre, and amongst a crowd of Warriours find our Names filed with those that durst passe through all horrors by death and vengeance for their King and Soveraigne.
(14) This is heavily ironic as the writer aims to glorify Royalist heroes throughout this play in appreciation of their efforts to preserve King and monarchy.

He looks forward to a time when great men, including Charles I, Charles Lucas and George Lisle, will be rewarded for the sacrifices they made to assist the Restoration of the monarchy.
As a persuasive text, The Famous Tragedie's cleverly crafted technical foundations display potential.

The writer chose to express Parliament's destructive influence on the nation, present the Reform's transformation on Christianity as an infection, and display hostility to their interference in, what could have otherwise been, a peaceful decade.
However, this potential is lost amongst sensationalised rumours that are stretched outside credible boundaries.

While it may have been necessary to produce shocking material in competition for readership, the writer dramatically reduces the impact of this text by inventing unconvincing accusations in an already fictional work.
Therefore, The Famous Tragedie is heavily flawed, as its lack of restraint creates an unreasoned argument that says more about the desperation of the Royalist position than the injustice of Parliament.

Ibsen's Hedda Gabler and Lorca's Yerma display two female protagonists who are seen to exercise masculine behaviour throughout their tragedies.
Hedda's inactivity and dependence on marriage for financial support is typical of a woman in late 19 th century, Norwegian society.

However, as a woman, Hedda is expected to find her raison d'être in being a wife and a mother, but these designs do not at all content her.
By rejecting the traditional practices of her gender, she becomes more masculine, which is displayed in her treatment of George Tesman and the power she attains over Eilert Løvborg.

Unlike Hedda, Yerma desperately longs to use her ability to reproduce and, in conjunction with society, thinks it's degrading and wasteful to remain barren.
As much as she yearns for motherhood, her husband is not at all compelled to have children, discarding Yerma's basic freedoms as a woman.

In some ways, Yerma will never have the responsibilities of a man, the strength, or dominance in Spanish society.
However, by neglecting the unique, reproductive capabilities of her womanhood, she is no longer defined by her gender and her lifestyle becomes closer to that of a man.

Camilla Collett was one of the first writers to highlight the feminist problem in Norway.
She vented her frustrations with the inert existence of the middle-class woman in her diary: My life passes uselessly without importance, suffocated by the eternal question whether this is really the kind of existence to which I am destined.

It awakens me suddenly at night, I arise with it in the morning, and when I put on my nightcap in the evening and contemplate what I have done the whole day since I took it off, I am saddened and ask myself why I dress at all.
There is little doubt that Hedda is similarly dissatisfied with her inactive, married life.

She constantly complains of boredom as her presence at home is merely decorative, while her husband is heavily involved in his scholarly interests.
B.J. Hovde observes the unjustness that 'Love and marriage was a woman's career', yet they were not free to marry for love (Hovde, The Scandinavian Countries in A Sourcebook on Naturalist Theatre, p. 73).

Along with women of her time, Hedda did not exercise many freedoms in choosing a suitable husband.
In conversation with Brack, she confesses that she married Tesman because he was determined he would be able to support her, none of her admirers made a similar offer and, above all, her 'time was up'.

Hedda's decision was accelerated by her increasing age as she depended on her beauty to attract men and, with little else to support her, she was required to consider the financial stability of her partner.
These factors led Hedda to an ill-matched marriage with a man who, regardless of love and affection, she treats with an air of indifference.

Unfulfilled by her husband and with no talent for motherhood, Hedda is compelled to create her own amusement to relieve herself from boredom.
She considers that Tesman could inject some excitement into her life if she could persuade him into an occupation that interests her, such as politics, as she is not at liberty to pursue this career herself.

In the opening scene of this play, she offends Miss Tesman by identifying her new hat as that belonging to the maid and later confesses to Brack that she consciously upset her for her own entertainment.
While this mischievous behaviour is not exceptionally harmful, her objective to control the life of another can hardly be described in the same terms.

Ibsen is keen to excuse her conduct as a consequence of her gender in patriarchal society.
He argues that 'women have no influence on external matters of government.

Therefore they want to have an influence on souls'.
Ibsen successfully highlights this as a desire among women by extending similar behaviour to Mrs Elvsted.

The introduction of her character encourages the audience to compare her with Hedda, as they are both married women of a similar age.
It is certain that Elvsted is proud to have influenced Løvborg in the way Ibsen describes, but to influence a person is quite different from a desire to govern lives.

Elvsted explains how she directed Løvborg away from his drinking habits: It was as if I'd some kind of power over him.
He gave up his old ways.

I didn't ask him.
I didn't dare.

But he could see I disapproved.
So he...gave up.

(27) While Hedda actively seeks control of Løvborg with the intention to destroy him, Elvsted's power is not something she sought, planned or understands.
Hedda scornfully perceives that she 'saved' Løvborg, but her positive influence is completely undone by Hedda, who consciously moves him to destruction.

Although Ibsen delegates a similar aspiration to these women, it is considerably more dangerous in the hands of the malicious, treacherous protagonist who applies greater pressure to Løvborg and achieves extreme results.
In comparing these women, it becomes clear that Elvsted does not experience inertia like Hedda in this play.

She escapes her unhappy marriage because she cannot bear to simply be a convenience to an old man who 'cares for no one but himself' (25).
Despite the criticisms of society and monetary concerns, she pursues a better life with Løvborg, who respects her and, through education, has elevated her to a level of equality with him.

She seeks her 'comrade in arms' because she enjoys his company and he provides her with a focus by allowing her to assist him in writing his revolutionary works.
J. Chesley Taylor observes that 'Hedda is a character who is torn between two selves: the romantic self, which desires to be liberated from all social restrictions, and the conformist self, which wants society's approval and thus dares not violate its taboos'.

This causes Elvsted and Hedda to react opposingly to identical situations.
Hedda is similarly unhappy in her married life with Tesman but, while she admires Elvsted for taking a brave leap, she would never repeat her actions because she lacks courage to challenge society's conventions.

She used to enjoy Løvborg's company, like Elvsted, but she left him out of cowardice when their friendship began to develop.
Therefore, while Ibsen realises that the contained lives of women can lead them to a useless, mundane existence, he displays hope in Elvsted who is an active figure because, unlike Hedda, she does not fear scandal for the sake of her happiness and a motivational object in life.

In a letter to Moritz Prozor in 1891, Ibsen explains the androgynous nature of his leading role: The title of the play is Hedda Gabler.
My intention in giving it this name was to indicate that Hedda, as a personality, is to be regarded rather as her father's daughter than as her husband's wife.

Ibsen certainly succeeds in his intentions as it soon becomes clear that, although Hedda agreed to marry Tesman, she in no way belongs to him.
Throughout the play, both Brack and Løvborg reject Hedda as Tesman's wife by addressing her by her maiden name.

Theoharis C. Theoharis believes that 'Tesman makes a quantum leap in choosing his spouse', suggesting that society will challenge the idea of them as a couple and Hedda's class and elegance will always elevate her above her husband.
The imbalance in their relationship means that Tesman will forever be indebted to her.

He struggles financially to provide everything she wants in the house because he would not want to disappoint his wife and never asserts any masculine authority over her in fear that she would rebel.
She constantly pushes the boundaries of his tolerance by behaving rudely to his family and refusing to spend time with him.

However, most things escape his notice because his naivety cannot contend with her advanced skills in manipulation and deceit.
An example of this can be seen in the fourth act of this play when Hedda pretends that she burnt Løvborg's manuscipts in concern for his interests.

While he should see through her lies, he is too delighted and overwhelmed with his wife's pretended display of affection to question her honesty.
Hedda's use of pistols displays a predatory aggression which can associate her with the 'femme fatale'.

She is seen threatening only men with weapons, which Brian Parker interprets as an enjoyment in reducing men 'to their biological level'.
She delights in the authority that her pistols attach to her, but provokes entirely different reactions from her victims, Tesman and Brack.

While Brack controls her actions with calm, authoritative imperatives such as, 'Don't point that thing at me' and 'Don't play the fool' (37), Tesman cannot trust her not to operate the pistols.
He panics, expresses sheer terror and begs her to put them away, highlighting her manly control over him.

By showing both reactions, Ibsen implies that Hedda's relationship with Tesman allows her to become more masculine because, as perceived by Theoharis, his 'gender is neuter' (Theoharis, "Hedda Gabler and "The Dead", p. 797).
In Hedda's company, he becomes effeminised by submitting to her will and establishing no control over her.

Meanwhile, she is at liberty to exercise the authority that is traditionally employed by the man within patriarchal society, where Ibsen locates his play.
Although Løvborg is less pitiable, Hedda also succeeds in directing his actions.

Theoharis explains why the audience should be particularly impressed by Hedda's triumph over Løvborg: She will have controlled a man's destiny without paying the usual price women owe for such power, sexual surrender.
(Theoharis, "Hedda Gabler and "The Dead", p. 796) Hedda is a sophisticated Machiavellian, whose intelligence outwits that of two learned scholars.

Her demonic capabilities are thoroughly compelling and exemplify a woman's capacity for ingenuity.
At the end of this play, Hedda's situation is reversed as her life is placed in the lecherous hands of Brack.

However, Brack stumbles upon this knowledge and promises that it will be contained, unlike Hedda's skilled, pre-meditated attack on Løvborg.
Hedda cannot bear to be controlled by another, although many women of her time were disciplined by men.

She ends the action of the play by returning the control to her fatal hands and, in doing so, she restores her masculine independence.
Yerma's tragedy is set in Rural Spain where women's lives were similarly governed by patriarchy and society's code of honour.

Through the dialogue of women, the audience becomes aware of the harsh restrictions placed upon them by men, such as arranged marriages.
In the opening scene, Yerma describes how some girls 'trembled and cried before they got into bed with their husbands', suggesting that unwilling, ill-prepared girls were coerced into marriage at too young an age.

In his essay entitled, "Honor, Blood, and Poetry in Yerma", Gustavo Correa outlines the Spanish woman's physical boundaries as 'the walls of her house' and her occupation as simply 'matrimony'.
This is echoed by a young woman who complains of her arranged marriage and the expectations attached to this role: I'm nineteen years old and I don't like to cook or clean.

Well, now I have to spend the whole day doing things I don't like!
And what for?

Why is it necessary for my husband to be my husband?
(78-9) The young woman resents the forced coupling of men and women and doesn't understand the necessity of such a system.

She is keen to express the unfairness of a society which locates women in the suffocating parameters of the house, while the men are allowed the freedom of the field.
Lorca firmly establishes Yerma in the home by confining her to this space for the full length of the opening scene, while her husband and friends come and go.

More than a physical restriction, her stationary position can be viewed as a symbol for her immobility in life, which is rapidly becoming stale without the development of children.
Unlike Hedda, Yerma is keen to embrace the woman's life of matrimony and motherhood.

However, matrimony alone is not enough for her, as she cannot be certain she loves the man her father chose.
She admits that, 'from the first day' she was engaged to Juan, she began 'to think about having children' (76) which, she believes, is not too much to expect from him.

Not dissimilar to Hedda, Yerma requires an object to which she can devote her time and attention.
While she doesn't reject the household occupations attached to her gender, there is a frustrating void in her routine that craves a child: You're right!

The women in their houses.
If their houses are not tombs!

If chairs get broken, and linen sheets wear out from being used!
But not here!

Every night, when I get into bed, the bed seems to be newer, shinier, as if it had just been brought from the city.
(90) The cleaner the house, the more Yerma is reminded that this is her only responsibility and focus for her care and attention.

She argues that her house is dying from a tense atmosphere between husband and wife and an emptiness which craves some lively, new energy.
Throughout the play, the audience is encouraged to feel greater sympathy with Yerma as the protagonist's anticipation and preparedness for a child is emphasised.

She has a naturally feminine, maternal quality, as evident when she voices concern for Juan's health and nags him to improve his diet.
She offers first-time mother, Maria, advice on pregnancy but is unable to provide information from first hand experience as she has not had the opportunity to practice her theories.

Yerma longs for a child, despite her full awareness that it can be a demanding and exhausting struggle: Having a child is not a bouquet of roses!
We have to suffer for them to grow up.

It must drain half our blood.
But that's good, healthy, beautiful!

(72) This was argued in response to a woman's complaints of motherhood, highlighting Yerma as fully-prepared and eager to experience a suffering that provokes moans from other women.
By presenting the audience with the desperation of a barren woman and a host of examples of others who complain of their maternity, Lorca encourages great pity for Yerma, as she appears more deserving and willing than those blessed with children.

Yerma is located within a society where accordance with nature was celebrated and those who neglect the natural designs of their gender were vulnerable to ridicule and torment.
Correa outlines the theme of maternity as essentially Spanish because 'the structure of Spanish society, with all the powerful force of tradition, has imposed upon its women a duty, a mission; before all else she is to be a mother' (Correa, "Honor, Blood and Poetry in Yerma", p. 98).

To a modern audience, it would not be unreasonable to assume that the weight of society's pressure on Yerma would lead her to another man who can provide her with what belongs to her.
This play opens with an illustration of Yerma's dream where a shepherd leads a child by the hand.

As the only characterised shepherd of this play, Lorca suggests Victor could be the subject of Yerma's desires and does not abandon this idea as the play progresses.
Although they are only ever seen innocently speaking to each other, their relationship triggers suspicions throughout the village, which could be enough to ruin her reputation.

Juan is concerned for his family's honour, prompting him to regularly remind his wife that stepping outside of her home and simply speaking to others is a dangerous occupation for a woman.
In particular, Yerma's barren state increases her vulnerability to gossip because she is expected to create a scandal by seeking another man.

The washerwomen cruelly place Yerma in a category of unfortunate women who 'put on face powder and rouge' and 'wear a sprig of oleander in pursuit of a man who is not their husband!' (83-4).
These expectations cause Yerma to be watched under a particularly strict eye in anticipation of a scandal.

However, the women are not aware of the importance Yerma places on preserving her honour, as emphasised by Correa: Society has assigned her fixed and immutable relations with men.
Illicit love is out of the question.

(Correa, "Honor, Blood and Poetry in Yerma", p. 99).
The protagonist displays her admirable morality by refusing the pagan old woman's offer of her son to provide her with a child.

Yerma is greatly offended that the woman should presume she could be led to such a development by her desperation.
As this option is closed to her, she is completely dependent on her stubborn and unfeeling husband, establishing her firmly in the submissive position of the woman.

In an argument with Juan, Yerma exclaims 'How I wish I were a woman!' (82) which commences the new fluidity of her gender.
Without a child, she no longer shares the unique quality that unites and defines women and she is therefore no longer restricted to the occupations of her gender, to submission to Juan or dependence on him.

Lorca engages Maria and Yerma twice in this play, with a gap of three years between the two meetings.
In their first conversation they are perceived as very intimate friends, sharing secrets and advice with each other.

However, it is clear that Yerma has become estranged from Maria in their latter engagement as her child has created a distance between them.
Yerma has replaced her hope with bitterness after a painful five years of emptiness and cannot help but feel deprived when she sees her friend with a baby.

Yerma no longer feels she belongs to the female sex, allowing her to extend her behaviour into masculine territory.
As Yerma's frustration progresses, her attitude to Juan undergoes great transformation.

Yerma evolves from a submissive, disciplinable woman into an argumentative and opinionated power as she begins to prioritise her own interests.
At the beginning of the play, she is seen sombrely agreeing to Juan's request to stay indoors but her days of yielding come to an end when she begins to dominate in arguments with Juan by confusing him with sophisticated metaphors and firmly expressing her criticisms.

Yerma explains the reasons behind her transformation in the third act: At least let my voice be free, now that I'm falling into the darkest part of the pit!
Let my body send out just one beautiful thing, and let it fill the air.

(104) With little else to lose, Yerma refuses to resign herself to a barren life without a fight.
She sees her rebellious words as a beautiful defence of the son she will never bear.

Her powerful words become action in the final scene where she violently triumphs over her husband in terminating his life.
Yerma's new freedoms allow her to spend her time in a different way: I often go down to feed the oxen - which I never used to do, because women don't - and when I walk through the dark shed, my footsteps sound to me like those of a man.

(94) In performing a man's task, Yerma further detaches herself from her sex by crossing the boundaries that divide women from men.
Yerma seeks a new independence in departing from her gender.

'If only I could have them all by myself' (101) suggests her desire to separate herself from men and displays her resentment for their necessary involvement in pregnancy.
Yerma never wants the life of a man, yet she is placed nearer to this gender by her husband in disallowing her children.

She resents her ability to exercise new freedoms in this role because she never requested independence, power or to extend her occupations outside of the home.
However, Hedda resents her inactivity and dependence on men that is attached with being a woman.

She rejects motherhood and matrimony in favour of mastering skills that aid her control over men.
While Hedda aspires to acquire the authority of a man, Yerma wants to use her abilities as a woman, yet neither of these figures are presented as traditionally belonging to a single gender, making them tragic, androgynous figures.

In the exploration of punishment in Greek tragedy, the targeted characters are ones of power; kings and the divine with the authority to administrate punishment to the guilty.
Guilt commonly follows crime or sin and the level of justice in the given punishment must be assessed.

For example, while a punishment is directed at a single person, it often affects the lives of many, which increases the severity of this penalty.
The level of justice will assess the ability of people in power to make judgement and decisions.

Euripides' Bacchae and Sophocles' Antigone display the capability of both man and the divine to error but the consistency lies in the authority of the gods.
While man can be judged, the gods are the highest level of authority and are therefore able to design an image of justice that cannot be challenged by man.

These are both fierce and bloody plays that do not attempt to disguise the conflicts between men, in the case of Antigone, while the dissonance lies between god and men in Euripides' tragedy.
Euripides introduces the gods as brutal and vengeful figures in the opening of this play through Dionysus' explanation of his birth: Dionysus; he who Semele of yore, 'Mid the dread midwifery of lightening fire, Bore, Cadmus' daughter.

Dionysus is keen to publicise the truth of his parentage to the doubting city of Thebes in order to establish himself as a new Olympian god who is worthy of their worship and praise.
He believes that he is the son of the most powerful of all gods, Jove, who had an affair with Semele, Cadmus' mortal daughter.

Dionysus implies a precarious birth which he later explains as the product of 'Herè's immortal vengeance' (page 1) against his mother.
Many sources agree that Dionysus was the product of two mothers as, provoked by Herè, Jove revealed himself to pregnant Semele who was struck down by the sight of the divine figure that mortals cannot endure.

Euripides suggests that while Herè's jealousy enraged the action, Jove's fury was the guilty, catalytic power that 'Struck dead the bold usurper of his bed' (2), forcing him to bear Dionysus in his thigh to save him.
By blaming Jove for Semele's death, Euripides attaches a brutality, ruthlessness and disregard for human life to this god that will be echoed by his son during the play.

This account warns of the inharmonious relations often found between humans and the divine and emphasises the superior power of the gods over mortals through divine intervention.
Bacchus continues to prepare the audience for the tragic drama they should expect to unfold by emphasising his intentions in visiting Thebes:...soon I will terribly show That I am born a god (2) Bacchus' intentions are wholly destructive, directing a play which, 'rather than a cautionary tale, is a vision of total despair'.

He doesn't only wish to convince the city of his power but intends to inflict a gruesome punishment on the people for their sinful behaviour and instigate fear in them like their fears of any divine authority.
The target of Dionysus' vengeance is collective as he believes that each and every citizen is guilty of persecuting him, rejecting his name in holy prayer and denying him as a god.

Dionysus' first stage in his intricate plan of vengeance targets the women of Thebes who, possessed by their malicious leader, have been forced from their homes and reunited with the raw landscape where they perform Bacchic rituals involving dancing in bare feet and sacrificing animals.
Their harmony with the bare mountains and the beasts erodes the barriers between humans and the natural world, portraying the women as wild, savage and uncivilised.

Their transformation into maenads pollutes the rational, innocuous order of the Theban lifestyle.
Diller describes the effect of the injection of Bacchanals into Greece as 'the colourful intermingling of wild ecstasy with calm tranquillity, of every day life with unaccustomed events' which 'constitutes both its attraction and danger'.

The attraction of the Bacchic culture lies in its devotion to liberation but this detachment from the rigidity of the civil order infuses a dangerous chaos within the city.
As the embodiment of customary political authority, Pentheus immediately perceives Dionysus as a threat to his city and spurns the 'womanly man' who has infected Thebes with 'a new disease' (13) that he is determined to control.

As the leading representative of Thebes, it is inevitable that Pentheus will receive the greatest impact of Dionysus' punishment.
However, he further attracts the angry god's vengeance by ignoring the advice of Tiresias and Cadmus who think it wise to demonstrate an outward recognition of the god even if 'he were no god' (12).

Although these elderly citizens follow their own advice, this does not allow them exemption from Dionysus' cruel retribution.
In particular, Cadmus is heavily affected as he is exiled from the race he established and the murder of his grandson terminates the future of his family.

While these facts highlight the merciless nature of Bacchus who is quick to cut down anyone who doesn't immediately praise him, it is possible that this god refuses to save these individuals as he can see through their pretence.
Cadmus admits personal motives behind his Bacchic celebration.

He is concerned for his treatment by the gods in death that he fears is soon approaching and is delighted that his daughter is believed to have given birth to an immortal: It were a splendid falsehood If Semele be thought t'have borne a god; 'Twere honour unto us and to our race.
(12) Considering Bacchus' treatment of Cadmus and Tiresias, there is no guarantee that, had Pentheus reacted differently to his intrusion, he would have been saved from the wrath of the unforgiving god.

However, by actively opposing this force, Pentheus commences a heavily imbalanced war with the divine that originates from his desire to defend himself and his city from something he fails to comprehend: He has decided, out of human self-defence and on the basis of everyday experience, to fight the unusual force which is confusing and erupting, overwhelming and tearing people from their accustomed environment.
(Diller, "Euripides' Final Phase", p.364) Furthermore, he is outraged that someone should have the effrontery to undermine his authority over the city that he governs.

Pentheus allows his anger to inspire a foolish hubris in him which guides his sense and judgment to a fight with the inevitable.
Diller perceives that the winner of this fight will be dependent on who has the greatest measure of sophia which denotes 'the grasping of a situation or task and the ability to master it'.

While Pentheus arrogantly believes he has a clear understanding of how to overcome the threat of Dionysus, Bacchus is realistically confident he will effortlessly triumph over his enemy (Diller, "Euripides' Final Phase, p. 364).
Cadmus demonstrates the dreadful consequences of challenging an immortal through an account of the tragedy of Actaeon who claimed to be more skilful in hunting than Artemis.

He reports an equally brutal punishment to that Pentheus painfully experiences at the end of this play; he was 'rent in wrath to pieces' (13).
Pentheus' intentions are purely theomachein (hopeless) but he is utterly blinded by his pride.

In pursuit of conflict with a god, Pentheus is described as 'crazed' and 'at the height of madness' (13) by Tiresias who is frequently portrayed as the voice of wisdom and rationality.
Dionysus and his foreign cultures have been condemned with similar descriptions throughout this play by both the chorus and Pentheus himself.

Their irrational behaviour is just one of the numerous similarities these characters share: It is not difficult to make a case that, in those central confrontations between the two characters, Pentheus is having to deal with a part of himself, a part he does not recognise as his (or doesn't want to).
(Johnston, "An Introductory Note", p. 3) Johnston implies that Pentheus' conflict with his cousin forms from a rejection of Bacchus as a member of his family as well as a divine figure.

Following his capture of the women of Thebes, the object of Dionysus' punishment becomes narrowly focused on the individual, Pentheus.
Firstly, Euripides creates dramatic irony by fooling the king into believing he is only a follower of Dionysus, rather than the god himself.

He then further ridicules him by encouraging him to wear women's clothes after creating an earthquake that shatters Thebes to ruins.
The peak of Dionysus' cruelty is in his design of Pentheus' death where the immortal delivers a double blow; not only is Pentheus killed by his citizens possessed by his enemy but his very own mother.

Pentheus' last words before his death form a desperate plea to his mother to return to her senses and recognise him as her son: I am thy child, thine own, my mother!
Pentheus, whom in Echion's house you bare.

Have mercy on me, mother!
For his sins, Whatever be his sins, kill not thy son.

(43) Euripides conforms to a popular convention of Greek theatre by verbalising Pentheus' death through the narration of the messenger, rather than designing this tragedy to be visualised on the stage.
While the complexity of this particularly bloody scene would have been the primary motive in the playwright's methods, simply an aural account of the action allows the audience to use their imagination which often feels more realistic and dramatic than simply a presentation of reality.

Pentheus shows a failure to recognise the extent to which he is responsible for his own fall, expressing no guilt or regret.
Therefore, it is difficult to perceive Pentheus as the tragic hero of this tale as he does not have the opportunity to reflect upon his actions.

However, Schechner describes the actions of Agave, who, it can be argued, adopts the role of tragic hero in her brief but essential presence in the play: A boy is killed, by his own mother.
Not only murdered, but mangled, cannibalised.

The ecstatic mother innocently dances with the severed head of her man-son, not recognising him.
It is not unreasonable to suggest that Agave is delivered the most severe punishment by Dionysus in this play as she performs unforgivable sins outside of her control.

She then continues to inspire the audience's pathos by unconsciously expressing her delirium following her lethal actions.
Most importantly, she is returned to consciousness to suffer the process of anagnorisis when she discovers the harsh reality of her hunt.

I am no more the Maenad dancing blithe, I am but the feeble, fond, and desolate mother.
I know, I see - ah, knowledge best unknown!

While Pentheus suffers bodily torture, Agave experiences a mental agony that will be intrinsic to her being for the duration of her life.
Diller suggests that the audience can particularly sympathise with Agave because, unlike her son, 'we learn nothing of her resistance' (Diller, "Euripides' Final Phase", p.360).

Her punishment is not terminated there either as Dionysus then thinks it appropriate to exile her and her family from Thebes.
This is where the severity of this god is put into question as the audience is provoked to consider what kind of justice is administered here.

Dionysus provides the people with a single warning in the play, towards Pentheus by advising him not to attempt to defeat this god.
This is the only trace of mercy offered by this god who perceives his victory as a god in the devastation of a city and its people.

Euripides displays a version of divine justice that breeches on insanity, greatly accentuated by the pleasure this god takes in wreaking his rage.
In Sophocles' Antigone, Creon plays the role of an equally proud, power-driven king who allows these ambitious discrepancies to misguide his character towards a catastrophic error of judgement.

In this drama, the brutality of the gods seen in the Bacchae is transferred to the humans who are shown to be just as capable of bitter vengeance.
However, Creon demonstrates tremendous guilt and regret for his merciless actions, highlighting the essential difference between gods and mortals; mortals are human, who sin and evaluate their actions with a conscience that does not allow them defence against lamentation.

When both heirs to the Theban throne are killed in battle, Creon feels inclined to establish his new authority as king with a merciless command to deny Polynices his burial following his traitorous actions to his fatherland.
Creon invades the territory of the gods by rejecting an unwritten law that a dead body must be buried in order to be passed into the underworld.

The king ambitiously attempts to overrule the authority of the gods who, he believes, are sure to support him in this decision as they are gods of the city who would not 'celebrate traitors'.
However, Antigone correctly believes that the gods would fully encourage her actions and thinks Creon deluded to suppose that 'a mere mortal, could override the gods, / the great unwritten, unshakeable traditions' (82).

Kitto suggests that Antigone 'is working with the gods, and the gods are working in her - exactly as Aphrodite, later, works in Haemon against Creon'.
While there are no immortal characters in this play, divine justice is alternatively demonstrated in the actions of characters, including Antigone and Haemon but particularly in the warnings of Tiresias.

The seer complains that the 'the public altars and sacred hearths are fouled' (111) with the carrion torn from the unburied corpse.
He warns the proud king that the gods cannot possibly support this disrespect to their places of worship.

More importantly, Creon has 'robbed the gods' (115) of a child of the earth and this injustice will only be resolved by surrendering a child of his own to them.
While Creon is the leading power of Thebes, his decisions are also judged by the gods whose greater power allows them to justly punish the king.

Creon exhibits a hunger for cruelty in this play that increases in severity the more his pride and authority is challenged.
In the king's opening speech he introduces his pitiless character by graphically describing his intentions for Polynices' corpse; that it become 'carrion for the birds and dogs to tear'.

Creon understands that his law completely disrespects Polynices and his family but his concerns lie with the state.
Knox reminds the contemporary audience of the common Greek reaction to Creon's punishment: He represents a viewpoint few Greeks would have challenged: that in times of crisis, the supreme loyalty of the citizen is to the state and its duly constituted authorities.

While the explicit description is unnecessary, Creon's actions are not thoroughly irrational at this point.
He demonstrates an unwavering dedication to the state in his promise of death to anyone who acts against the interests of Thebes by disobeying his law.

However, when it is revealed that his law has been violated, he is outraged that somebody should have the nerve to undermine his authority and his pride guides him to irrationality.
He is determined that someone should suffer the punishment of death, regardless of whether they are guilty.

He threatens his sentries with death for failing to seek out the culprit and Antigone's misfortune becomes their fortune when she is caught.
When the king is challenged by Antigone, he cannot bear that a woman should mock his authority and promises her 'the most barbaric death' (83).

Ismene indicates that killing Antigone would severely punish his son, Haemon, who is lovingly devoted to her, but Creon does not at all intend to reconsider his decision.
However, when Haemon appeals to Creon, it becomes clear that rejecting his son's wishes is more than a rejection of the interests of his family over the state.

This interaction between father and son is very revealing of the king's motives in the deliverance of his punishment.
Haemon intelligently begins a plea to his father by feeding his ego with praise, declaring that he is subordinate to his father in everyway and he will never hesitate to obey his word.

Unsurprisingly, Creon thoroughly delights in these compliments until Haemon indicates his support for Antigone.
Rather than asking Creon for mercy based on his personal interests, Haemon relates the grief of the city who wholly sympathise with Antigone as they believe she has acted gloriously.

At this point, Creon proves the instability of his devotion to the state by expressing 'The city is the king's - that's the law!' (97).
The king is determined not to expose any weakness by altering his decision on the words of others, which he considers to be secondary to his own.

Whether his actions are supported by the city is of no interest to this leader who solely perseveres to establish his authority over the city.
Creon seems to believe that the greater the punishment, the greater the fear, inspiring him to kill Antigone in front of his son's eyes.

While Creon's intentions are utterly inhumane, he is certainly not the only character in this play who is guilty of irrationality.
Antigone rashly decides to disobey Creon's command in the understanding that she will be punished with death.

Kitto outlines Antigone's reasons for her rebellion as 'loyalty to her family, love of her brother, religious duty' and 'sheer physical and emotional revulsion against the horror' (Kitto, Sophocles, p.53).
Antigone is single-minded, wilful and passionate, disallowing her to consider the sense in her sister's reasoned argument.

Instead, she shames Ismene by condemning her for acting selfishly and betraying the interests of her family for the law of Creon.
She provokes her into guilt by no longer referring to Polynices as 'our brother' but 'my brother' (63) and condemning her dishonour of the gods.

Although Creon's punishment extends to a loss of marriage and children for Antigone, she shows little regret throughout the play, but rather an acceptance of the consequences of supporting her brother.
When Creon finally regrets his decision and sees reason, it is too late for Antigone who, 'independent to the last, has chosen her own way to die'.

(Knox, "Introduction", p. 53) Unlike Dionysus, Creon both receives and delivers punishment in this play as he is subordinated by the omniscient gods.
His decision to abandon Polynices' body to the elements punished Antigone's family.

and indirectly destroyed his own.
He deservedly receives a twofold punishment in the loss of his son and wife in the final scenes of Antigone.

Knox discusses the suitability of this punishment: His savage dismissal of the claims of that blood relationship Antigone stood for has been punished with exquisite appropriateness, in the destruction of his own family, the curses of his son and wife.
(Knox, "Introduction", p.53) Creon, like Thebes in the Bacchae, has painfully learnt a humbling lesson.

This lesson encompasses the importance of using wisdom, of finding a balance of reverence to the gods and controlling pride.
This lesson could similarly be applied to Pentheus whose lack of wisdom and pride leads him to the inevitable task of challenging the gods.

While man is blamed for the tragic consequences of Antigone, Euripides has been considered as 'a critic of the Olympian gods' (Diller, "Euripides' Final Phase, p.356), suggesting that this play condemns the actions of irrational Dionysus.
Both plays adopt the idea that, without rationality, power can be a dangerous thing to possess, both among humans and the divine.
